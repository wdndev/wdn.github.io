<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>37.2° Blog | 37.2° Blog</title><meta name="author" content="Dongnian"><meta name="copyright" content="Dongnian"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="3.流水线并行 - wolai 笔记在数据并行训练中，一个明显的特点是每个 GPU 持有整个模型权重的副本，这就带来了冗余问题，虽然，FSDP 可以缓解冗余的问题，但是对于超大规模模型来说，仅使用数据并行进行分布式训练没办法使模型的参数规模进一步提升。因此，另一种并行技术是模型并行，即模型被分割并分布在一个设备阵列上，每一个设备只保存模型的一部分参数。模型并行分为张量并行和流水线并行，张量并行为">
<meta property="og:type" content="website">
<meta property="og:title" content="37.2° Blog">
<meta property="og:url" content="https://wdndev.github.io/note/llm/llm_concept/04.%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/3.%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C/3.%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="3.流水线并行 - wolai 笔记在数据并行训练中，一个明显的特点是每个 GPU 持有整个模型权重的副本，这就带来了冗余问题，虽然，FSDP 可以缓解冗余的问题，但是对于超大规模模型来说，仅使用数据并行进行分布式训练没办法使模型的参数规模进一步提升。因此，另一种并行技术是模型并行，即模型被分割并分布在一个设备阵列上，每一个设备只保存模型的一部分参数。模型并行分为张量并行和流水线并行，张量并行为">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2024-12-08T03:56:27.934Z">
<meta property="article:modified_time" content="2024-12-08T03:56:27.934Z">
<meta property="article:author" content="Dongnian">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/note/llm/llm_concept/04.%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/3.%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C/3.%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Dongnian","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '37.2° Blog',
  isPost: false,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-08 11:56:27'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-solid fa-magnifying-glass"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/note"><i class="fa-fw fa-regular fa-bookmark"></i><span> Note</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-solid fa-magnifying-glass"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/note"><i class="fa-fw fa-regular fa-bookmark"></i><span> Note</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="page"><h1 class="page-title"></h1><div id="article-container"><!DOCTYPE html>
<html lang="zh-Hans-CN"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=Edge"/><link rel="stylesheet" type="text/css" href="../../css/modern-norm.min.css"/><link rel="stylesheet" type="text/css" href="../../css/prism.min.css"/><link rel="stylesheet" type="text/css" href="../../css/katex.min.css"/><link rel="stylesheet" type="text/css" href="../../css/wolai.css"/><title>3.流水线并行 - wolai 笔记</title><link rel="shortcut icon" href="data:image/svg+xml,%3Csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 800 800&apos;%3E%3Cdefs%3E%3Cstyle%3E.cls-1%7Bfill:%23fff;%7D%3C/style%3E%3C/defs%3E%3Cg%3E%3Cpath class=&apos;cls-1&apos; d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Z&apos;/%3E%3Cpath d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Zm4.72,88.9H185.2L172.42,89c-32.78.62-43.68,3.24-54.71,9.14a45.84,45.84,0,0,0-19.54,19.54c-6.61,12.36-9.11,24.55-9.27,67.49V614.8L89,627.58c.62,32.78,3.24,43.68,9.14,54.71a45.84,45.84,0,0,0,19.54,19.54c12.36,6.61,24.55,9.11,67.49,9.27H610.08c46.79,0,59.41-2.44,72.21-9.28a45.84,45.84,0,0,0,19.54-19.54c6.61-12.36,9.11-24.55,9.27-67.49V189.92c0-46.79-2.44-59.41-9.28-72.21a45.84,45.84,0,0,0-19.54-19.54C669.93,91.56,657.74,89.06,614.8,88.9ZM233.33,493.33A73.34,73.34,0,1,1,160,566.67,73.35,73.35,0,0,1,233.33,493.33Z&apos;/%3E%3C/g%3E%3C/svg%3E"></link></head><body><header><div class="image"></div><div class="title"><div class="banner"><div class="icon"></div></div><div data-title="3.流水线并行" class="main-title"></div></div></header><article><div id="eZFQLqYTQzaQSKikbX4r57" class="wolai-block wolai-text"><div><span class="inline-wrap">在数据并行训练中，一个明显的特点是每个 GPU 持有整个模型权重的副本，这就带来了冗余问题，虽然，FSDP 可以缓解冗余的问题，但是对于超大规模模型来说，仅使用数据并行进行分布式训练没办法使模型的参数规模进一步提升。因此，另一种并行技术是</span><span class="yellow inline-wrap"><b>模型并行</b></span><span class="inline-wrap">，即</span><span class="red inline-wrap"><b>模型被分割并分布在一个设备阵列上，每一个设备只保存模型的一部分参数</b></span><span class="inline-wrap">。</span></div></div><div id="rJDeNsyXRd2WwhRWzc1wRS" class="wolai-block wolai-text"><div><span class="inline-wrap">模型并行分为张量并行和流水线并行，张量并行为层内并行，对模型 Transformer 层内进行分割、流水线为层间并行，对模型不同的 Transformer 层间进行分割。</span></div></div><div id="4s16GzLNPH1CaTtrKR7NWc" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image.png" style="width: 100%"/></figure></div><h3 id="7UUiP4wUZFAjQRBMGJJp7P" class="wolai-block"><span class="inline-wrap">1.简介</span></h3><div id="visoBaXbpHpNiUJr4FquJw" class="wolai-block wolai-text"><div><span class="inline-wrap">所谓流水线并行，就是由于模型太大，无法将整个模型放置到单张<span class="jill"></span>GPU<span class="jill"></span>卡中；因此，将</span><span class="red inline-wrap"><b>模型的不同层放置到不同的计算设备</b></span><span class="inline-wrap">，降低单个计算设备的显存消耗，从而实现超大规模模型训练。
如下图所示，模型共包含四个模型层（如：Transformer<span class="jill"></span>层），被切分为三个部分，分别放置到三个不同的计算设备。即第 1 层放置到设备 0，第 2 层和第三 3 层放置到设备 1，第 4 层放置到设备 2。</span></div></div><div id="j1kBh1Sn7MzWaC36irjk2T" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_1.png" style="width: 680px"/></figure></div><div id="hCnRsSnk859o1GS8XxUgke" class="wolai-block wolai-text"><div><span class="inline-wrap">相邻设备间通过通信链路传输数据。具体地讲，前向计算过程中，输入数据首先在设备 0 上通过第 1 层的计算得到中间结果，并将中间结果传输到设备 1，然后在设备 1 上计算得到第 2 层和第 3 层的输出，并将模型第 3 层的输出结果传输到设备 2，在设备 2 上经由最后一层的计算得到前向计算结果。反向传播过程类似。最后，各个设备上的网络层会使用反向传播过程计算得到的梯度更新参数。由于各个设备间传输的仅是相邻设备间的输出张量，而不是梯度信息，因此通信量较小。</span></div></div><h3 id="2uuYSvFT56fkeE8ZqVBWKn" class="wolai-block"><span class="inline-wrap">2.朴素流水线并行</span></h3><div id="wAnh3LDGFH5Gf6DYC4WL1z" class="wolai-block wolai-text"><div><span class="inline-wrap">朴素流水线并行是实现流水线并行训练的最直接的方法。我们将模型按照层间切分成多个部分（Stage），并将每个部分（Stage）分配给一个 GPU。然后，我们对小批量数据进行常规的训练，在模型切分成多个部分的边界处进行通信。</span></div></div><div id="bvVBHfVFS5NoSCdDqGfTvc" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_2.png" style="width: 100%"/></figure></div><div id="n7ykuVxUah71UoF1o6CRY8" class="wolai-block wolai-text"><div><span class="inline-wrap">下面以 4 层顺序模型为例：</span></div></div><code-block id="hPQ6hDVfTvCZ76Ckson3mv" class="wolai-block"><div class="wolai-pre"><div data-lang="Bash" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token assign-left variable">output</span><span class="token operator">=</span>L4<span class="token punctuation">(</span>L3<span class="token punctuation">(</span>L2<span class="token punctuation">(</span>L1<span class="token punctuation">(</span>input<span class="token punctuation">))</span><span class="token punctuation">))</span>
</pre></div></code-block><div id="fVif4DV6HHRHEyDfHyCAcb" class="wolai-block wolai-text"><div><span class="inline-wrap">将计算分配给两个 GPU，如下所示：</span></div></div><ul class="wolai-block"><li id="tSzjGFvNwTd1KvsdDo8d6o"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">GPU1 computes: </span><span class="inline-wrap"><code>intermediate=L2(L1(input))</code></span></li><li id="iBVYPsWDFWdUS4ckNDqkTw"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">GPU2 computes: </span><span class="inline-wrap"><code>output=L4(L3(intermediate))</code></span></li></ul><div id="oXMGgU3377tddHrXQaFKEw" class="wolai-block wolai-text"><div><span class="inline-wrap">为了完成前向传播，我们在 GPU1 上计算中间值并将结果张量传输到 GPU2。 然后， GPU2 计算模型的输出并开始进行反向传播。 对于反向传播，我们从 GPU2 到 GPU1 的中间发送梯度。 然后， GPU1 根据发送的梯度完成反向传播。 这样，流水线并行训练会产生与单节点训练相同的输出和梯度。 朴素流水线并行训练相当于顺序训练，这使得调试变得更加容易。</span></div></div><div id="7HSpnxsXPgddNeQKsp6uvG" class="wolai-block wolai-text"><div><span class="inline-wrap">下面说明了朴素流水线并行执行流程。 GPU1 执行前向传播并缓存激活（红色）。 然后，它使用 MPI 将 L2 的输出发送到 GPU2。 GPU2 完成前向传播，并使用目标值计算损失，完成之后开始反向传播。 一旦 GPU2 完成，梯度的输出被发送到 GPU1，从而完成反向传播。</span></div></div><div id="wGejBzgjTxJCrN6n7EJNA3" class="wolai-block wolai-text"><div><span class="inline-wrap">请注意，这里仅使用了点到点通信（MPI.Send 和 MPI.Recv），并且不需要任何集体通信原语（因此，不需要 MPI.AllReduce）。</span></div></div><div id="fYZZQh7iNc3kh2eXDq27cf" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_3.png" style="width: 100%"/></figure></div><div id="o4C6oZP2GQUECqW5pgUcf6" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>朴素流水线并行存在的问题</b></span><span class="inline-wrap">：</span></div></div><div id="6fUorryB1xi6qKetGcy7q9" class="wolai-block wolai-text"><div><span class="inline-wrap">那么该方法为什么被称为朴素流水线并行呢，它又有什么缺陷呢？</span></div></div><div id="eK1bfRnJxCnDiESxDW3iVa" class="wolai-block wolai-text"><div><span class="inline-wrap">主要是因为该方案在任意给定时刻，</span><span class="red inline-wrap">除了一个 GPU 之外的其他所有 GPU 都是空闲的</span><span class="inline-wrap">。因此，如果使用 4 个 GPU，则几乎等同于将单个 GPU 的内存量增加四倍，而其他资源 (如计算) 相当于没用上。所以，朴素流水线存在很多的<span class="jill"></span>Bubble。朴素流水线的 Bubble <span class="jill"></span>的时间为 </span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>K</mi><mo>−</mo><mn>1</mn></mrow><mi>K</mi></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(\frac{K-1}{K})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2173em;vertical-align:-0.345em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span><span class="inline-wrap">，</span><span class="inline-wrap"><b>当<span class="jill"></span>K<span class="jill"></span>越大，即<span class="jill"></span>GPU<span class="jill"></span>的数量越多时，空置的比例接近<span class="jill"></span>1，即<span class="jill"></span>GPU<span class="jill"></span>的资源都被浪费掉了</b></span><span class="inline-wrap">，</span><span class="red inline-wrap">因此，朴素的流水线并行将会导致</span><span class="red inline-wrap"><b>GPU<span class="jill"></span>使用率过低</b></span><span class="red inline-wrap">。</span></div></div><div id="eQf2Hd7TxqUtXPZrmVhVTz" class="wolai-block wolai-text"><div><span class="inline-wrap">另外，还需要加上在</span><span class="red inline-wrap"><b>设备之间复制数据的通信开销</b></span><span class="inline-wrap">；所以， 4 张使用朴素流水线并行的 6GB 卡将能够容纳 1 张 24GB 卡相同大小的模型，而后者训练得更快；因为，它没有数据传输开销。</span></div></div><div id="qamJ712dW12J87ATFJBTYH" class="wolai-block wolai-text"><div><span class="inline-wrap">还有</span><span class="inline-wrap"><b>通信和计算没有交错</b></span><span class="inline-wrap">的问题：当我们通过网络发送中间输出 (FWD) 和梯度 (BWD) 时，没有 GPU 执行任何操作。</span></div></div><div id="xq2Tp98WPTaJELMSZwMGgP" class="wolai-block wolai-text"><div><span class="inline-wrap">除此之外，还存在</span><span class="inline-wrap"><b>高内存需求</b></span><span class="inline-wrap">的问题：先执行前向传播的<span class="jill"></span>GPU（如：GPU1）将保留整个小批量缓存的所有激活，直到最后。如果批量大小很大，可能会产生内存问题。</span></div></div><h3 id="f3prbEawCNnz4eQC9uX6RS" class="wolai-block"><span class="inline-wrap">3.微批次流水线并行</span></h3><div id="t2Cz9ergyeKFr2scc815dj" class="wolai-block wolai-text"><div><span class="red inline-wrap">微批次（MicroBatch）流水线并行与朴素流水线几乎相同，但它通过将传入的小批次（minibatch）分块为微批次（microbatch）</span><span class="inline-wrap">，并人为创建流水线来解决 GPU 空闲问题，从而允许不同的 GPU 同时参与计算过程，可以显著提升流水线并行设备利用率，减小设备空闲状态的时间。目前业界常见的流水线并行方法 GPipe 和 PipeDream 都采用微批次流水线并行方案。</span></div></div><div id="a39xB6215Fk8xky8UB59Xh" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_4.png" style="width: 100%"/></figure></div><h3 id="dFfywagYBjKodu25iHYYz5" class="wolai-block"><span class="inline-wrap">4.GPipe</span></h3><div id="jSCJbX83DzuLCYr9yAGRPJ" class="wolai-block wolai-text"><div><span class="inline-wrap">GPipe（Easy Scaling with Micro-Batch Pipeline Parallelism），由谷歌提出的一种流水线并行方案。最早，谷歌在<span class="jill"></span>Lingvo<span class="jill"></span>框架下开源了<span class="jill"></span>GPipe，基于 TensorFlow 库进行实现的。后来，Kakao Brain<span class="jill"></span>的工程师用 PyTorch 来实现了 GPipe，并开源出来，也就是 torchgpipe。之后，Facebook<span class="jill"></span>的<span class="jill"></span>FairScale<span class="jill"></span>库将<span class="jill"></span>torchgpipe<span class="jill"></span>集成到项目中。再后来，Facebook<span class="jill"></span>又将<span class="jill"></span>FairScale<span class="jill"></span>库中关于<span class="jill"></span>torchgpipe<span class="jill"></span>的部分代码集成到了<span class="jill"></span>PyTorch 1.8.0 之后的版本中。torchgpipe 的这部分代码被合并到 </span><span class="inline-wrap"><code>torch/distributed/pipeline/sync</code></span><span class="inline-wrap"> 目录下。</span></div></div><div id="9LvLXPfQuc8R72qCCN54kW" class="wolai-block wolai-text"><div><span class="inline-wrap">以下代码是基于<span class="jill"></span>PyTorch<span class="jill"></span>使用包含两个 FC 层的模型跨 GPU0 和 GPU1 进行流水线并行的示例：</span></div></div><code-block id="oNY6Yvc7BfM8LjZfWiceHH" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token comment"># Need to initialize RPC framework first.</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'MASTER_ADDR'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'localhost'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'MASTER_PORT'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'29500'</span>
torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>rpc<span class="token punctuation">.</span>init_rpc<span class="token punctuation">(</span><span class="token string">'worker'</span><span class="token punctuation">,</span> rank<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> world_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 构建模型</span>
fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>fc1<span class="token punctuation">,</span> fc2<span class="token punctuation">)</span>

<span class="token keyword">from</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>pipeline<span class="token punctuation">.</span>sync <span class="token keyword">import</span> Pipe

<span class="token comment"># chunks表示micro-batches的大小，默认值为1</span>
model <span class="token operator">=</span> Pipe<span class="token punctuation">(</span>model<span class="token punctuation">,</span> chunks<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>
<span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
output_rref <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
</pre></div></code-block><div id="ngVVCbkHMjEFVJ7zitv7Y2" class="wolai-block wolai-text"><div><span class="inline-wrap">Gpipe 流水线并行主要用来解决这两个问题：</span></div></div><div id="tu95RRDMYLGWGUmVpJpw2c" class="wolai-block wolai-text"><div><span class="inline-wrap">第一，</span><span class="red inline-wrap"><b>提高模型训练的并行度</b></span><span class="inline-wrap">。Gpipe 在朴素流水线并行的基础上，</span><span class="green inline-wrap"><b>利用数据并行的思想，将 mini-batch 细分为多个更小的 micro-batch，送入<span class="jill"></span>GPU<span class="jill"></span>进行训练</b></span><span class="inline-wrap">，来提高并行程度。</span></div></div><div id="otAPWr12ksyxCqVHsJMTEP" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_5.png" style="width: 100%"/></figure></div><div id="36qrnJ4veLQxzmttyZiLSz" class="wolai-block wolai-text"><div><span class="inline-wrap">上图即为朴素流水线并行与 GPipe 微批次流水线并行对比，通过 GPipe 可以有效降低流水线并行<span class="jill"></span>bubble 空间的比例。其中，F<span class="jill"></span>的第一个下标表示 GPU 编号，F<span class="jill"></span>的第二个下标表示 micro-batch 编号。假设我们将 mini-batch 划分为 M 个，则 GPipe 流水线并行下， GPipe 流水线 Bubble 时间为： </span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>K</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>K</mi><mo>+</mo><mi>M</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(\frac&#123;K−1&#125;&#123;K+M-1&#125;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span><span class="inline-wrap">。其中，K<span class="jill"></span>为设备，M<span class="jill"></span>为将<span class="jill"></span>mini-batch<span class="jill"></span>切成多少个<span class="jill"></span>micro-batch。当<span class="jill"></span>M&gt;&gt;K<span class="jill"></span>的时候，这个时间可以忽略不计。</span></div></div><div id="tb1Gi1BGwdShbhJi13HUXd" class="wolai-block wolai-text"><div><span class="inline-wrap">但这样做也有一个坏处，那就是把 batch 拆小了之后，</span><span class="red inline-wrap">对于那些需要统计量的层（如：Batch Normalization），就会导致计算变得麻烦，需要重新实现</span><span class="inline-wrap">。在<span class="jill"></span>Gpipe<span class="jill"></span>中的方法是，在训练时计算和运用的是<span class="jill"></span>micro-batch<span class="jill"></span>里的均值和方差，同时持续追踪全部<span class="jill"></span>mini-batch<span class="jill"></span>的移动平均和方差，以便在测试阶段进行使用。这样 Layer Normalization 则不受影响。</span></div></div><div id="4b2e61L8coBVVCz48RAuWv" class="wolai-block wolai-text"><div><span class="inline-wrap">第二，</span><span class="red inline-wrap"><b>通过重计算（Re-materialization）降低显存消耗</b></span><span class="inline-wrap">。在模型训练过程中的前向传播时，会记录每一个算子的计算结果，用于反向传播时的梯度计算。</span></div></div><div id="tVaaSJP2kCbns5ABtD2Hw1" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_6.png" style="width: 100%"/></figure></div><div id="c2ejY5jsibxR317HnM7fcn" class="wolai-block wolai-text"><div><span class="inline-wrap">而 Re-materialization 可以不用保存中间层输出的激活值，在计算梯度的时候会重新计算出来这些激活值从而可以计算梯度。在 GPipe 中，应用了这个技术后，如果一个设备上有多层，那么就可以只保存多层中的最后一层的输出值。这样就降低了每个设备上内存占用峰值，同样的模型尺寸需要的显存就少了。</span></div></div><div id="aQW3LYQMhZQe3VqrRKDWL1" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>Re-materialization<span class="jill"></span>并非是不需要中间结果，而是有办法在求导过程中实时的计算出之前被舍弃掉的中间结果</b></span><span class="inline-wrap">。</span></div></div><div id="hN5SR1wsu3dm4PYiMTtNSe" class="wolai-block wolai-text"><div><span class="inline-wrap">简而言之，GPipe 通过纵向对模型进行切分解决了单个设备无法训练大模型的问题；同时，又通过微批量流水线增加了多设备上的并行程度，除此之外，还使用<span class="jill"></span>re-materialization<span class="jill"></span>降低了单设备上的显存峰值。</span></div></div><div id="iKEauGFgwmcvzaY2sfh5xj" class="wolai-block wolai-text"><div><span class="inline-wrap">上面讲述了 GPipe 流水线并行方案，接下来讲述一下 PipeDream 。讲述 PipeDream<span class="jill"></span>之前，我们先来看看流水线并行策略。</span></div></div><h3 id="wdgbLTCvXjqms577sPgciE" class="wolai-block"><span class="inline-wrap">5.流水线并行策略</span></h3><div id="ejTQC21vRtk45GkWbFVd6S" class="wolai-block wolai-text"><div><span class="inline-wrap">流水线并行根据执行的策略，可以分为 </span><span class="red inline-wrap">F-then-B</span><span class="inline-wrap"> 和 </span><span class="red inline-wrap">1F1B </span><span class="inline-wrap">两种模式。之前讲述的朴素流水线并行以及<span class="jill"></span>GPipe<span class="jill"></span>都是<span class="jill"></span>F-then-B<span class="jill"></span>模型，而后续讲述的 PipeDream 则是 1F1B 模式。</span></div></div><h4 id="nUzYvqFMXabBCdGt8NFQY1" class="wolai-block"><span class="inline-wrap">5.1 F-then-B<span class="jill"></span>策略</span></h4><div id="ie29wXLhLdwA78FfW4bHc9" class="wolai-block wolai-text"><div><span class="inline-wrap">F-then-B 模式，</span><span class="red inline-wrap"><b>先进行前向计算，再进行反向计算</b></span><span class="inline-wrap">。</span></div></div><div id="3qDkSW1qKpVJRzYHT6Ryyx" class="wolai-block wolai-text"><div><span class="inline-wrap">F-then-B 模式由于缓存了多个 micro-batch 的中间变量和梯度，显存的实际利用率并不高。</span></div></div><div id="gNJHpBm69vZuPG2TcKHYt3" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_7.png" style="width: 100%"/></figure></div><h4 id="pgauzv6TvE8K2ufVfshLmB" class="wolai-block"><span class="inline-wrap">5.2 1F1B<span class="jill"></span>策略</span></h4><div id="6FGRvFuhXoVqoeasqiXoaS" class="wolai-block wolai-text"><div><span class="inline-wrap">1F1B（One Forward pass followed by One Backward pass）模式，</span><span class="red inline-wrap"><b>一种前向计算和反向计算交叉进行的方式</b></span><span class="inline-wrap">。在 1F1B 模式下，前向计算和反向计算交叉进行，可以及时释放不必要的中间变量。</span></div></div><div id="7ZK1ZhSUzDFNNtHcHdmtEf" class="wolai-block wolai-text"><div><span class="inline-wrap">1F1B 示例如下图所示，以 stage4 的 F42（</span><span class="inline-wrap"><b>stage4 的第 2 个 micro-batch 的前向计算</b></span><span class="inline-wrap">）为例，F42 在计算前，F41 的反向 B41（stage4 的第 1 个 micro-batch 的反向计算）已经计算结束，即可释放 F41 的中间变量，从而 F42 可以</span><span class="inline-wrap"><b>复用</b></span><span class="inline-wrap"> F41 中间变量的显存。</span></div></div><div id="emxJZEgKHMj4PrFBykJp9x" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_8.png" style="width: 100%"/></figure></div><div id="68kDCHdwd6fMmkXqr2qy6R" class="wolai-block wolai-text"><div><span class="inline-wrap">研究表明，1F1B 方式相比于 F-then-B 方式，峰值显存可以节省 37.5%，对比朴素流水线并行峰值显存明显下降，设备资源利用率显著提升。</span></div></div><h3 id="pNjaaYrGa4CecvhubjHPCf" class="wolai-block"><span class="inline-wrap">6.PipeDream（非交错式<span class="jill"></span>1F1B）-DeepSpeed</span></h3><div id="9CbnarnUMFQ6giXadwmmrD" class="wolai-block wolai-text"><div><span class="inline-wrap">Gpipe 的流水线有以下几个问题：</span></div></div><ul class="wolai-block"><li id="c5jNK1sY8oUwbZF8BYcNHv"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">将 mini-batch 切分成 m 份 micro-batch 后，</span><span class="red inline-wrap">将带来更频繁的流水线刷新（Pipeline flush），这降低了硬件效率，导致空闲时间的增加</span><span class="inline-wrap">。</span></li></ul><div id="qQUYKJVgBB3QoxAVywaujh" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_9.png" style="width: 100%"/></figure></div><ul class="wolai-block"><li id="eR4f4pwv8DnRYFHDXr6deY"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">将 mini-batch 切分成 m 份 micro-batch 后， 需要缓存 m 份 activation，这将导致内存增加。原因是每个 micro-batch 前向计算的中间结果<span class="jill"></span>activation 都要被其后向计算所使用，所以需要在内存中缓存。即使使用了重计算技术，前向计算的 activation 也需要等到对应的后向计算完成之后才能释放。</span></li></ul><div id="v5hPrXUm6sWmV4Lob7XGWK" class="wolai-block wolai-text"><div><span class="inline-wrap">而微软 DeepSpeed 提出的 PipeDream ，针对这些问题的改进方法就是 1F1B 策略。这种改进策略可以解决缓存 activation 的份数问题，使得 activation 的缓存数量只跟 stage 数相关，从而进一步节省显存，训练更大的模型。其解决思路就是努力减少每个 activation 的保存时间，即这就需要每个微批次数据尽可能早的完成后向计算，从而让每个 activation 尽可能早释放。</span></div></div><div id="ef2XH4rPHN6iJcF56wGhTf" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_10.png" style="width: 100%"/></figure></div><div id="8GDwBBj99q8KQEmXUruJTi" class="wolai-block wolai-text"><div><span class="inline-wrap">注意：</span><span class="red inline-wrap"><b>微批次在 GPipe 中叫 micro-batch，而在 PipeDream 叫 mini-batch</b></span><span class="inline-wrap">。为了避免干扰，本文统一使用 micro-batch。</span></div></div><div id="gZ1hBuU42EitAKe9djuJV4" class="wolai-block wolai-text"><div><span class="inline-wrap">PipeDream 具体方案如下：</span></div></div><ul class="wolai-block"><li id="wkPwBnQ44eaurQrXicGMoF"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">一个阶段（stage）在做完一次 micro-batch 的前向传播之后，就立即进行 micro-batch 的后向传播，然后释放资源，那么就可以让其他 stage 尽可能早的开始计算，这就是 1F1B 策略。有点类似于把整体同步变成了众多小数据块上的异步，而且众多小数据块都是大家独立更新。</span></li><li id="575TxcMHhoG4UTvUdxTE7n"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">在 1F1B 的稳定状态（steady state,）下，会在每台机器上严格交替的进行前向计算/后向计算，这样使得每个<span class="jill"></span>GPU<span class="jill"></span>上都会有一个 micro-batch 数据正在处理，从而保证资源的高利用率（整个流水线比较均衡，没有流水线刷新（Pipeline Flush），这样就能确保以固定周期执行每个阶段上的参数更新。</span></li><li id="csBM6JEWJqqjaTPEUJY2qn"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">面对流水线带来的异步性，</span><span class="red inline-wrap"><b>1F1B 使用不同版本的权重来确保训练的有效性</b></span><span class="inline-wrap">。</span></li></ul><div id="kxwAxHXciXTa6q2Qq3aQWB" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_11.png" style="width: 100%"/></figure></div><ul class="wolai-block"><li id="8RMDfeHtcMF3YhvnRwbVmz"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">此外，PipeDream 还扩展了 1F1B，对于使用数据并行的 stage，采用轮询（round-robin）的调度模式将任务分配在同一个 stage 的各个设备上，保证了一个小批次的数据的前向传播计算和后向传播计算发生在同一台机器上，这就是 1F1B-RR（one-forward-noe-backward-round-robin）。</span></li></ul><div id="8jrofcMCNKye43mYjMd1Si" class="wolai-block wolai-text"><div><span class="inline-wrap">相比 GPipe，表面上看 PipeDream 在<span class="jill"></span>Bubble<span class="jill"></span>率上并没有优化，PipeDrea 流水线 Bubble 时间仍然为：</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>K</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>K</mi><mo>+</mo><mi>M</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> O(\frac&#123;K−1&#125;&#123;K+M-1&#125;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span><span class="inline-wrap">。但节省了显存之后，在设备显存一定的情况下，就可以通过增大 M 的值（增大<span class="jill"></span>micro-batch<span class="jill"></span>的个数）来降低<span class="jill"></span>Bubble<span class="jill"></span>率了。</span></div></div><h3 id="fuUWhHY7rhFqCUJfUN5ixs" class="wolai-block"><span class="inline-wrap">7.PipeDream-2BW</span></h3><div id="pPUskD1XyCRgR6s7LJrs3u" class="wolai-block wolai-text"><div><span class="inline-wrap">在之前的流水线方案<span class="jill"></span>GPipe<span class="jill"></span>和<span class="jill"></span>PipeDream<span class="jill"></span>存在如下问题：</span></div></div><ul class="wolai-block"><li id="3Rma4g1ngcpKNwUAiQR7RP"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>GPipe 维护模型权重的单一版本，输入的小批次被分成更小的微批次</b></span><span class="inline-wrap">。权重梯度是累积的，不会立即应用，流水线会定期刷新，以确保不需要维护多个权重版本。 GPipe 提供类似于数据并行的权重更新语义，但是定期的流水线刷新可能会很昂贵，从而限制了吞吐量。减轻这种开销的一种方法是在流水线内执行额外的累积，但这并不总是实用的。</span></li><li id="bh5f4KxutoGRw537XkjhjK"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">PipeDream 使用权重存储方案来确保相同输入的前向和后向传播中使用相同的权重版本。 在最坏的情况下，隐藏的权重版本总数为 d，其中， d 是流水线深度，这对于大模型来说太高了。 而且使用 PipeDream 默认的权重更新语义，每个阶段（state）的权重更新都有不同的延迟项；同时，流水线内不会执行累积。</span></li></ul><div id="3uebZYmv6VF7SsU9TCa5wA" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_12.png" style="width: 100%"/></figure></div><div id="p8q4SMn1CdCqhKQfXtzVb9" class="wolai-block wolai-text"><div><span class="inline-wrap">基于此，作者提出了<span class="jill"></span>PipeDream-2BW。PipeDream-2BW 在流水线之中只维护了</span><span class="red inline-wrap"><b>两个版本的模型权重，2BW 是双缓冲权重</b></span><span class="inline-wrap">（double-buffered weights）。</span></div></div><div id="ey4ev4y7BuKmjN83woe4An" class="wolai-block wolai-text"><div><span class="inline-wrap">PipeDream-2BW 会为每 m 个微批次生成一个新的权重版本（m&gt;=d），其中，d<span class="jill"></span>为流水线深度，但是因为有些剩余后向传递仍然依赖于旧版本模型，所以新的模型版本无法立即取代旧版本，因此，新生成的权重版本需要缓冲以供将来使用。 然而，需要维护的权重版本总数最多为<span class="jill"></span>2，因为用于生成新权重版本的权重版本可以立即被丢弃（通过该阶段的后续的输入不再使用旧的权重版本），同时，由于只保存了两个版本，这极大的降低了内存的占用。</span></div></div><div id="sVTuvC64afkGYSwJFFeWxN" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_13.png" style="width: 100%"/></figure></div><h3 id="vFV7gS3HeWcdV9evBzwgrT" class="wolai-block"><span class="inline-wrap">8.PipeDream-Flush（1F1B）</span></h3><div id="9MNuwtSgVXFjR6PhnYVRmV" class="wolai-block wolai-text"><div><span class="inline-wrap">在 PipeDream 2BW 论文（Memory-Efficient Pipeline-Parallel DNN Training）中，还提到了一种变体 PipeDream-Flush， 使用 Flush 更新权重。它的内存占用量低于 PipeDream 2BW，但代价是吞吐量较低。该调度重用了微软的 PipeDream 中的 1F1B 调度策略；但是，同<span class="jill"></span>GPipe<span class="jill"></span>一样，</span><span class="red inline-wrap"><b>只维护单个权重版本并引入定期流水线刷新</b></span><span class="red inline-wrap">（pipeline flush）</span><span class="inline-wrap">，以确保权重更新期间的权重版本保持一致，通过这种方式以执行性能为代价降低了峰值内存。下图显示了具有 2 个流水线阶段的 PipeDream-Flush 和 GPipe 的时间线。</span></div></div><div id="iZJd9cDomeo3ZDBNMVXsyd" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_14.png" style="width: 100%"/></figure></div><div id="b1dFUn6Fc37uU8r6rcbFep" class="wolai-block wolai-text"><div><span class="inline-wrap">下图展示了<span class="jill"></span>GPipe、PipeDream-Flush、PipeDream 2BW 流水线并行方法的吞吐量对比。</span></div></div><div id="6brhQrn3oGVawX56vpbiCd" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_15.png" style="width: 100%"/></figure></div><div id="c4wVVHyh5vHv93LYVctthU" class="wolai-block wolai-text"><div><span class="inline-wrap">下图展示了<span class="jill"></span>GPipe、PipeDream-Flush、PipeDream 2BW 流水线并行方法的内存对比。</span></div></div><div id="uzebW5bczuueYRF3U7P3xc" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_16.png" style="width: 100%"/></figure></div><h3 id="vorcqxAELf1TsxLpjBgCXe" class="wolai-block"><span class="inline-wrap">9.1F1B 调度（schedule）模式</span></h3><div id="jxoC5GiZuttSeiKAC43uzw" class="wolai-block wolai-text"><div><span class="inline-wrap">上面讲述了 PipeDream，在使用 1F1B 策略时，存在两种调度模式：非交错调度和交错式调度。具体如下图所示，上面的部分显示了默认的非交错式调度（non-interleaved schedule），底部显示的是交错式调度（interleaved schedule）。</span></div></div><div id="54cZLg3di2GFQSXziseSnU" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_17.png" style="width: 100%"/></figure></div><h4 id="8KTMtLNN51FVzeovfMTMw4" class="wolai-block"><span class="inline-wrap">9.1 非交错式调度</span></h4><div id="wLKzrFnMNcs48GRWmpHmdA" class="wolai-block wolai-text"><div><span class="inline-wrap">非交错式调度可分为三个阶段。第一阶段是热身阶段，处理器进行不同数量的前向计算。在接下来的阶段，处理器进行一次前向计算，然后是一次后向计算。最后一个阶段处理器完成后向计算。</span></div></div><div id="7uQK4ZUfVNYPu1Bibytj1w" class="wolai-block wolai-text"><div><span class="inline-wrap">上面的讲到微软的 PipeDream 就是使用非交错式 1F1B 调度。虽然，这种调度模式比 GPipe 更节省内存。然而，它需要和 GPipe 一样的时间来完成一轮计算。</span></div></div><h4 id="wmCmMwDLDoL4bdvLPSAoC2" class="wolai-block"><span class="inline-wrap">9.2 交错式调度</span></h4><div id="6xbd4X58dWWymnQDXdHgR5" class="wolai-block wolai-text"><div><span class="inline-wrap">在交错式调度中，每个设备可以对多个层的子集（称为模型块）进行计算，而不是一个连续层的集合。</span></div></div><div id="i5XDomi3ouP6sSg47npUsR" class="wolai-block wolai-text"><div><span class="inline-wrap">具体来看，在之前非交错式调度中，设备<span class="jill"></span>1<span class="jill"></span>拥有层<span class="jill"></span>1-4，设备<span class="jill"></span>2<span class="jill"></span>拥有层<span class="jill"></span>5-8，以此类推；但在交错式调度中，设备<span class="jill"></span>1<span class="jill"></span>有层<span class="jill"></span>1,2,9,10，设备<span class="jill"></span>2<span class="jill"></span>有层<span class="jill"></span>3,4,11,12，以此类推。在交错式调度模式下，流水线上的每个设备都被分配到多个流水线阶段（虚拟阶段，virtual stages），每个流水线阶段的计算量较少。</span></div></div><div id="qgrxh91bWctPen5amEF8QB" class="wolai-block wolai-text"><div><span class="inline-wrap">这种模式既节省内存又节省时间。但这个调度模式要求 micro-batch 的数量是流水线阶段（Stage）的整数倍。</span></div></div><div id="nWXEfT11QhiiBgUAiDPU1T" class="wolai-block wolai-text"><div><span class="inline-wrap">英伟达 Megatron-LM 的流水线并行相关的论文（Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM）中采用了非交错式 1F1B 调度。</span></div></div><h3 id="shChDvz6MyF8yeCyddwf7U" class="wolai-block"><span class="inline-wrap">10.PipeDream（交错式<span class="jill"></span>1F1B）-Megatron-LM</span></h3><div id="h6ngL2PgwsZg5yar1Ju6F6" class="wolai-block wolai-text"><div><span class="inline-wrap">Megatron-LM 基于 PipeDream-Flush 提出了一个小的<span class="jill"></span>Trick：交错式 1F1B 调度，而交错式 1F1B 调度也是 Megatron-LM 论文（Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM），virtual pipeline）中最主要的一个创新点。</span></div></div><div id="h6poQAigQxY7ZZ3cXHHXdT" class="wolai-block wolai-text"><div><span class="inline-wrap">传统的流水线并行通常会在一个设备（Device）上放置几个连续的模型层（如：Transformer<span class="jill"></span>层）。但 Megatron 这篇论文采用虚拟流水线（virtual pipeline），进行交错式<span class="jill"></span>1F1B<span class="jill"></span>并行。在设备数量不变的情况下，分出更多的流水线阶段（pipeline stage），以更多的通信量，换取流水线<span class="jill"></span>Bubble<span class="jill"></span>比率降低。</span></div></div><div id="bCc5cK1FggbbzS7osTegnB" class="wolai-block wolai-text"><div><span class="inline-wrap">例如，之前如果每个设备有 4 层（即设备 1 有 1 – 4 层，设备 2 有 5 – 8 层，依此类推），现在我们可以让每个设备对两个模型块执行计算（每个模型块有 2 层） ，即设备 1 有第 1、2、9、10 层； 设备 2 有第 3、4、11、12 层，依此类推。 通过这种方案，流水线中的每个设备都被分配多个流水线阶段（与以前相比，每个流水线阶段的计算量更少）。</span></div></div><div id="hLq9GMCAKbgq6XQRkyLRYu" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_18.png" style="width: 100%"/></figure></div><div id="j67YWcAF4RYew3esjPAxCq" class="wolai-block wolai-text"><div><span class="inline-wrap">此外，该方案要求一个小批次中的微批次数量是管道并行大小（流水线中的设备数量）的整数倍。 例如，对于 4 个设备，一个小批次中的微批次数量必须是 4 的倍数。</span></div></div><div id="wb2ujRJiPZBRogi2Fnq3B6" class="wolai-block wolai-text"><div><span class="inline-wrap">那虚拟流水线（virtual pipeline）是怎么做到的呢？</span></div></div><div id="sGvPwL8WFWGfN7uzDHBoRB" class="wolai-block wolai-text"><div><span class="inline-wrap">对照上面示例图举例说明，若网络共<span class="jill"></span>16<span class="jill"></span>层（编号 0-15），4 个 Device，前述谷歌的 GPipe 和微软的 PipeDream 是分成 4 个 stage， 按编号 0-3 层放 Device1，4-7<span class="jill"></span>层放 Device2 ，以此类推。</span></div></div><div id="8tb6aQcgwsg8frU3hd4Amf" class="wolai-block wolai-text"><div><span class="inline-wrap">英伟达的 virtual pipeline 则是按照文中提出的 virtual_pipeline_stage 概念减小切分粒度，以 virtaul_pipeline_stage=2 为例，将 0-1 层放 Device1, 2-3 层放在 Device2，...，6-7 层放到 Device4，8-9 层继续放在 Device1，10-11 层放在 Device2，...，14-15 层放在 Device4。</span></div></div><div id="2VTTPeTnWvTFDT4wi3QtQf" class="wolai-block wolai-text"><div><span class="inline-wrap">按照这种方式，Device<span class="jill"></span>之间的点对点通信次数(量)直接翻了<span class="jill"></span>virtual_pipeline_stage 倍，但空泡比率降低了，若定义每个 Device 上有<span class="jill"></span> v <span class="jill"></span>个 virtual stages，或者论文中也叫做 model chunks，在这个例子中<span class="jill"></span> v=2，这样一来，空泡比率为：</span></div></div><div id="shqoWvsthaxT2asrY35Ftu" class="wolai-block wolai-text"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>B</mi><mi>u</mi><mi>b</mi><mi>b</mi><mi>l</mi><mi>e</mi><mtext> </mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mtext> </mtext><mo stretchy="false">(</mo><mi>p</mi><mi>i</mi><mi>p</mi><mi>e</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mtext> </mtext><mi>b</mi><mi>u</mi><mi>b</mi><mi>b</mi><mi>l</mi><mi>e</mi><mtext> </mtext><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><msubsup><mi>t</mi><mrow><mi>p</mi><mi>b</mi></mrow><mtext>int. </mtext></msubsup><msub><mi>t</mi><mrow><mi>i</mi><mi>d</mi></mrow></msub></mfrac><mo>=</mo><mfrac><mn>1</mn><msub><mi>v</mi><mn>0</mn></msub></mfrac><mo>⋅</mo><mfrac><mrow><mi>p</mi><mo>−</mo><mn>1</mn></mrow><msub><mi>m</mi><mtext>柆 </mtext></msub></mfrac></mrow><annotation encoding="application/x-tex">Bubble~ time~ fraction ~(pipeline~ bubble ~size) =\frac&#123;t_&#123;p b&#125;^&#123;\text &#123;int. &#125;&#125;&#125;&#123;t_&#123;i d&#125;&#125;=\frac&#123;1&#125;&#123;v_&#123;0&#125;&#125; \cdot \frac&#123;p-1&#125;&#123;m_&#123;\text &#123;柆 &#125;&#125;&#125;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">u</span><span class="mord mathnormal">bb</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mspace nobreak"> </span><span class="mord mathnormal">t</span><span class="mord mathnormal">im</span><span class="mord mathnormal">e</span><span class="mspace nobreak"> </span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mspace nobreak"> </span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">i</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">in</span><span class="mord mathnormal">e</span><span class="mspace nobreak"> </span><span class="mord mathnormal">b</span><span class="mord mathnormal">u</span><span class="mord mathnormal">bb</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mspace nobreak"> </span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.5107em;vertical-align:-0.836em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6747em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.8442em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8305em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">b</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">int. </span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4192em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.1574em;vertical-align:-0.836em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.1574em;vertical-align:-0.836em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord cjk_fallback mtight">柆</span><span class="mord mtight"> </span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><div id="aXAPg9HBPGyHUosaYeksXp" class="wolai-block wolai-text"><div><span class="inline-wrap">从上面公式可以看出空泡比率和<span class="jill"></span> v <span class="jill"></span>成反比，降低了 v 倍。当然，流水线气泡比率的降低并不是没有成本的：这个交错式调度需要额外的通信。 从数量上来说，通讯量也增加了 v 倍。 当然我们可以通过在多 GPU 服务器（例如： DGX A100 节点）中可以通过高速的网络带宽来减少这种额外通信的影响。英伟达论文中也探讨了使用 8 个 InfiniBand 网卡来减少这种额外通信的影响。</span></div></div><h3 id="wqmjt2Gxss48WfWhEKGVWB" class="wolai-block"><span class="inline-wrap">11.分布式训练框架流水线并行方案</span></h3><div id="nPKdKBfo8ZDsHunVWiaZBK" class="wolai-block wolai-text"><div><span class="inline-wrap">上面讲述了目前主流的一些流水线并行（PP）方案，总的来说，PP<span class="jill"></span>可以细分为同步流水线并行(Sync-PP)和异步流水线并行(Async-PP)。</span></div></div><ul class="wolai-block"><li id="7vynXAFvMjgv4ZtaNii5zJ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">Sync-PP<span class="jill"></span>的代表有<span class="jill"></span>GPipe，PipeDream-flush<span class="jill"></span>等；</span></li><li id="quupmEBUSvjkjFb6tdX8L9"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">Async-PP<span class="jill"></span>的代表有<span class="jill"></span>PipeDream，PipeDream-2BW<span class="jill"></span>等。</span></li></ul><div id="dWXttfC37pWaMuZ5E5gy9X" class="wolai-block wolai-text"><div><span class="inline-wrap">同步方法与数据并行具有相同的权值更新语意，但是需要引入流水线<span class="jill"></span>bubble（空闲等待时间），会降低训练吞吐。而异步方法彻底消除的训练<span class="jill"></span>timeline<span class="jill"></span>中的<span class="jill"></span>bubble，但是需要引入不同的权值版本来解决权值过期的问题。</span></div></div><div id="wsM7CLQ4nRsrLDe4MMRv3M" class="wolai-block wolai-text"><div><span class="inline-wrap">下面我们来看看几个知名的分布式训练框架中采用的流水线并行方案：</span></div></div><ul class="wolai-block"><li id="guq6MJnM2DkhvnN7brrJKn"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">在 PyTorch 中，采用的是<span class="jill"></span>GPipe<span class="jill"></span>方案。使用的是<span class="jill"></span>F-then-B<span class="jill"></span>调度策略。</span></li><li id="aH2Ueigm7D2xbs1jKj3H7F"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">在 DeepSpeed 中，采用的是<span class="jill"></span>PipeDream-Flush，使用的是非交错式<span class="jill"></span>1F1B<span class="jill"></span>调度策略。使用这个调度方案，是为了促进最大规模的模型进行训练，在模型训练过程中中，存储多个权重缓冲可能会令人望而却步，我们的首要目标希望是一个“精确”的方法，而不需要收敛权衡。当然，DeepSpeed 引擎组件抽象出了流水线调度，你也可以自行实现其他的流水线调度方案。</span></li><li id="dxpw3XDkTiU6XHwRVKTWe6"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">在 Megatron-LM 中，基于<span class="jill"></span>PipeDream-Flush<span class="jill"></span>进行了改进，提供了一种交错式<span class="jill"></span>1F1B<span class="jill"></span>方案。</span></li><li id="wdKWThfebcb5qzWVLhSavL"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">在 Colossal-AI 中，基于<span class="jill"></span>Megatron-LM<span class="jill"></span>的交错式<span class="jill"></span>1F1B<span class="jill"></span>方案，提供了非交错(</span><span class="inline-wrap"><code>PipelineSchedule</code></span><span class="inline-wrap">) 和交错(</span><span class="inline-wrap"><code>InterleavedPipelineSchedule</code></span><span class="inline-wrap">) 调度策略。</span></li></ul><h3 id="4gjCV6gVqVd9tSa9tvdiJi" class="wolai-block"><span class="inline-wrap">12.总结</span></h3><div id="qTBi5QZNugz1h3oZ36FjNN" class="wolai-block wolai-text"><div><span class="inline-wrap">本文首先讲述了朴素流水线并行，但是朴素的流水线并行在一个流水线并行组内，每一时刻只有一个<span class="jill"></span>GPU<span class="jill"></span>运行，这样将会导致<span class="jill"></span>GPU<span class="jill"></span>使用率极低。因此，谷歌提出了 Gpipe。</span></div></div><div id="aeRcxGeViUUgx1fs7RYXgy" class="wolai-block wolai-text"><div><span class="inline-wrap">Gpipe 利用数据并行的思想，将 mini-batch 细分为多个更小的 micro-batch，送入<span class="jill"></span>GPU<span class="jill"></span>进行训练，来提高并行程度。将 mini-batch 拆分为 M<span class="jill"></span>个 micro-batch 后，导致更频繁的流水线刷新，降低硬件效率，同时，拆分为 M 个微批次之后，每个微批次反向传播过程中都会只用之前的激活值，因此，将导致内存占用更大。基于此，GPipe<span class="jill"></span>中使用重计算进行解决，前提是重计算出来的结果和之前得一样，并且前向的时间不能太长，否则流水线会被拉长太多。</span></div></div><div id="dd4jywXVc5jKVc5nUYMnp9" class="wolai-block wolai-text"><div><span class="inline-wrap">后面提到了 F-then-B 和 1F1B 这两种流水线并行策略，F-then-B 可能会导致内存占用很高。而微软提出的 PipeDream 通过合理安排前向和反向过程的顺序（1F1B<span class="jill"></span>策略）来解决内存过高的问题。</span></div></div><div id="fMWjTZKb7TeqsNcZC2KyYT" class="wolai-block wolai-text"><div><span class="inline-wrap">相对于 GPipe，虽然 PipeDream 降低了内存的使用，但是其空泡（Bubble）率并没有降低。Megatron-LM<span class="jill"></span>的流水线并行方案中提出了交错式<span class="jill"></span>1F1B<span class="jill"></span>调度策略。进一步降低空泡（Bubble）率。但是，带来了额外的通信成本。其论文中提到了使用 IB 网络来缓解额外的通信影响。</span></div></div><div id="vyXrXhHR6jY7SnQ8cGrvPn" class="wolai-block wolai-text"><div><span class="inline-wrap">说句题外话，在本文讲述的几种流水线并行方案中，除了 GPipe 之外，PipeDream<span class="jill"></span>及其变体的相关论文都有 Deepak Narayanan 的参与，真高产。</span></div></div><div id="6hpW65BNkNBm2iGY7BmwjS" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div><div id="9jUZTbBZunM6bYnmZHfhqS" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div><div id="8EsLwZgfpUucE4u5eUsJhr" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div><div id="k1HDYUy6ZdDv8B47xw5byz" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div></article><footer></footer></body></html></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/wdn_icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Dongnian</div><div class="author-info__description">A salty fish swimming in the sea of deep learning!</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/wdndev"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wdndev" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:dongnian.wang@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #24292e;"></i></a><a class="social-icon" href="https://blog.csdn.net/wdnshadow" target="_blank" title="CSDN"><i class="fas fa-rss" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to My Personal Blog! <br /> If Not, Please Visit <a target="_blank" rel="noopener" href="https://wdndev.gitee.io/"> <font color=#00BFFF>Gitee Mirror</font></a>.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#7UUiP4wUZFAjQRBMGJJp7P"><span class="toc-text">1.简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2uuYSvFT56fkeE8ZqVBWKn"><span class="toc-text">2.朴素流水线并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#f3prbEawCNnz4eQC9uX6RS"><span class="toc-text">3.微批次流水线并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dFfywagYBjKodu25iHYYz5"><span class="toc-text">4.GPipe</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#wdgbLTCvXjqms577sPgciE"><span class="toc-text">5.流水线并行策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#nUzYvqFMXabBCdGt8NFQY1"><span class="toc-text">5.1 F-then-B策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pgauzv6TvE8K2ufVfshLmB"><span class="toc-text">5.2 1F1B策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pNjaaYrGa4CecvhubjHPCf"><span class="toc-text">6.PipeDream（非交错式1F1B）-DeepSpeed</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fuUWhHY7rhFqCUJfUN5ixs"><span class="toc-text">7.PipeDream-2BW</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vFV7gS3HeWcdV9evBzwgrT"><span class="toc-text">8.PipeDream-Flush（1F1B）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vorcqxAELf1TsxLpjBgCXe"><span class="toc-text">9.1F1B 调度（schedule）模式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#8KTMtLNN51FVzeovfMTMw4"><span class="toc-text">9.1 非交错式调度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#wmCmMwDLDoL4bdvLPSAoC2"><span class="toc-text">9.2 交错式调度</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#shChDvz6MyF8yeCyddwf7U"><span class="toc-text">10.PipeDream（交错式1F1B）-Megatron-LM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#wqmjt2Gxss48WfWhEKGVWB"><span class="toc-text">11.分布式训练框架流水线并行方案</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4gjCV6gVqVd9tSa9tvdiJi"><span class="toc-text">12.总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_article/9.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BALLM/" title="检索增强LLM">检索增强LLM</a><time datetime="2024-01-12T16:00:00.000Z" title="Created 2024-01-13 00:00:00">2024-01-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/6.%E6%96%87%E6%9C%AC%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/" title="LLMs公开课 - 6.文本理解和生成大模型">LLMs公开课 - 6.文本理解和生成大模型</a><time datetime="2024-01-09T16:00:00.000Z" title="Created 2024-01-10 00:00:00">2024-01-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/5.%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/" title="LLMs公开课 - 5.高效训练&amp;模型压缩">LLMs公开课 - 5.高效训练&amp;模型压缩</a><time datetime="2024-01-06T16:00:00.000Z" title="Created 2024-01-07 00:00:00">2024-01-07</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DSA/"><span class="card-category-list-name">DSA</span><span class="card-category-list-count">24</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLMs/"><span class="card-category-list-name">LLMs</span><span class="card-category-list-count">16</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/PL/"><span class="card-category-list-name">PL</span><span class="card-category-list-count">7</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/PL/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">6</span></a></li></ul></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/DSA/" style="font-size: 1.42em; color: rgb(91, 145, 17)">DSA</a><a href="/tags/RL/" style="font-size: 1.28em; color: rgb(176, 79, 19)">RL</a><a href="/tags/Transformer/" style="font-size: 1.45em; color: rgb(127, 170, 70)">Transformer</a><a href="/tags/LLMs/" style="font-size: 1.32em; color: rgb(112, 148, 55)">LLMs</a><a href="/tags/PaperReading/" style="font-size: 1.38em; color: rgb(110, 146, 60)">PaperReading</a><a href="/tags/DeepLearning/" style="font-size: 1.25em; color: rgb(90, 48, 1)">DeepLearning</a><a href="/tags/CV/" style="font-size: 1.15em; color: rgb(82, 200, 174)">CV</a><a href="/tags/GPT/" style="font-size: 1.18em; color: rgb(7, 16, 91)">GPT</a><a href="/tags/PL/" style="font-size: 1.22em; color: rgb(17, 30, 26)">PL</a><a href="/tags/leetcode/" style="font-size: 1.35em; color: rgb(106, 126, 145)">leetcode</a><a href="/tags/algo/" style="font-size: 1.15em; color: rgb(181, 144, 151)">algo</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">January 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">December 2023</span><span class="card-archive-list-count">14</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">November 2023</span><span class="card-archive-list-count">26</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">October 2023</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/09/"><span class="card-archive-list-date">September 2023</span><span class="card-archive-list-count">4</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">87</div></div><div class="webinfo-item"><div class="item-name">Run time :</div><div class="item-count" id="runtimeshow" data-publishDate="2023-05-31T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Total Count :</div><div class="item-count">411.2k</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2024-12-08T03:57:10.055Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By Dongnian</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'oe7vzWxH80qwJJjWslYTCViT-gzGzoHsz',
      appKey: 'k89nSbK0BTbmzmpQottRHvNI',
      avatar: 'monsterid',
      serverURLs: 'https://oe7vzwxh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>