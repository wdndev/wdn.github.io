<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>37.2° Blog | 37.2° Blog</title><meta name="author" content="Dongnian"><meta name="copyright" content="Dongnian"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="0.llm推理框架简单总结 - wolai 笔记下面首先来总结一下这些框架的特点，如下表所示：LLM推理有很多框架，各有其特点，下面分别介绍一下表中七个框架的关键点：vLLM：适用于大批量Prompt输入，并对推理速度要求高的场景；Text generation inference：依赖HuggingFace模型，并且不需要为核心模型增加多个adapter的场景；CTranslate2：可在CP">
<meta property="og:type" content="website">
<meta property="og:title" content="37.2° Blog">
<meta property="og:url" content="https://wdndev.github.io/note/llm/llm_concept/06.%E6%8E%A8%E7%90%86/0.llm%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/0.llm%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="0.llm推理框架简单总结 - wolai 笔记下面首先来总结一下这些框架的特点，如下表所示：LLM推理有很多框架，各有其特点，下面分别介绍一下表中七个框架的关键点：vLLM：适用于大批量Prompt输入，并对推理速度要求高的场景；Text generation inference：依赖HuggingFace模型，并且不需要为核心模型增加多个adapter的场景；CTranslate2：可在CP">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2024-12-08T03:56:28.094Z">
<meta property="article:modified_time" content="2024-12-08T03:56:28.094Z">
<meta property="article:author" content="Dongnian">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/note/llm/llm_concept/06.%E6%8E%A8%E7%90%86/0.llm%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/0.llm%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Dongnian","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '37.2° Blog',
  isPost: false,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-08 11:56:28'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-solid fa-magnifying-glass"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/note"><i class="fa-fw fa-regular fa-bookmark"></i><span> Note</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-solid fa-magnifying-glass"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/note"><i class="fa-fw fa-regular fa-bookmark"></i><span> Note</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="page"><h1 class="page-title"></h1><div id="article-container"><!DOCTYPE html>
<html lang="zh-Hans-CN"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=Edge"/><link rel="stylesheet" type="text/css" href="../../css/modern-norm.min.css"/><link rel="stylesheet" type="text/css" href="../../css/prism.min.css"/><link rel="stylesheet" type="text/css" href="../../css/katex.min.css"/><link rel="stylesheet" type="text/css" href="../../css/wolai.css"/><title>0.llm推理框架简单总结 - wolai 笔记</title><link rel="shortcut icon" href="data:image/svg+xml,%3Csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 800 800&apos;%3E%3Cdefs%3E%3Cstyle%3E.cls-1%7Bfill:%23fff;%7D%3C/style%3E%3C/defs%3E%3Cg%3E%3Cpath class=&apos;cls-1&apos; d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Z&apos;/%3E%3Cpath d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Zm4.72,88.9H185.2L172.42,89c-32.78.62-43.68,3.24-54.71,9.14a45.84,45.84,0,0,0-19.54,19.54c-6.61,12.36-9.11,24.55-9.27,67.49V614.8L89,627.58c.62,32.78,3.24,43.68,9.14,54.71a45.84,45.84,0,0,0,19.54,19.54c12.36,6.61,24.55,9.11,67.49,9.27H610.08c46.79,0,59.41-2.44,72.21-9.28a45.84,45.84,0,0,0,19.54-19.54c6.61-12.36,9.11-24.55,9.27-67.49V189.92c0-46.79-2.44-59.41-9.28-72.21a45.84,45.84,0,0,0-19.54-19.54C669.93,91.56,657.74,89.06,614.8,88.9ZM233.33,493.33A73.34,73.34,0,1,1,160,566.67,73.35,73.35,0,0,1,233.33,493.33Z&apos;/%3E%3C/g%3E%3C/svg%3E"></link></head><body><header><div class="image"></div><div class="title"><div class="banner"><div class="icon"></div></div><div data-title="0.llm推理框架简单总结" class="main-title"></div></div></header><article><div id="pnbFmpvSiNMVG7UzNAWEf9" class="wolai-block wolai-text"><div><span class="inline-wrap">下面首先来总结一下这些框架的特点，如下表所示：</span></div></div><div id="ehFanmtXzygAEN9oFnQGQh" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image.png" style="width: 100%"/></figure></div><div id="gxiBQXRg5NvJMeUwFBPTpJ" class="wolai-block wolai-text"><div><span class="inline-wrap">LLM<span class="jill"></span>推理有很多框架，各有其特点，下面分别介绍一下表中七个框架的关键点：</span></div></div><ol class="wolai-block"><li id="vq4E3tzFmgdCvi8MgDZUtA"><div class="marker"></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm"><span><b>vLLM</b></span></a></span><span class="inline-wrap">：适用于大批量<span class="jill"></span>Prompt<span class="jill"></span>输入，并对推理速度要求高的场景；</span></li><li id="2ggXABz3zDVPUiqZdTfdNh"><div class="marker"></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://github.com/huggingface/text-generation-inference"><span><b>Text generation inference</b></span></a></span><span class="inline-wrap">：依赖<span class="jill"></span>HuggingFace<span class="jill"></span>模型，并且不需要为核心模型增加多个<span class="jill"></span>adapter<span class="jill"></span>的场景；</span></li><li id="6gXpgych4Bix61nzsWeJnA"><div class="marker"></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://github.com/OpenNMT/CTranslate2"><span><b>CTranslate2</b></span></a></span><span class="inline-wrap">：可在<span class="jill"></span>CPU<span class="jill"></span>上进行推理；</span></li><li id="wed9irVNeJ6CaEnd4fMvQL"><div class="marker"></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://github.com/bentoml/OpenLLM"><span><b>OpenLLM</b></span></a></span><span class="inline-wrap">：为核心模型添加<span class="jill"></span>adapter<span class="jill"></span>并使用<span class="jill"></span>HuggingFace Agents，尤其是不完全依赖<span class="jill"></span>PyTorch；  </span></li><li id="58uJj3rRs1xhZRLDP6DtcK"><div class="marker"></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/serve/index.html"><span><b>Ray Serve</b></span></a></span><span class="inline-wrap">：稳定的<span class="jill"></span>Pipeline<span class="jill"></span>和灵活的部署，它最适合更成熟的项目；</span></li><li id="er8HKJr2kVSMWpfRvJD9zK"><div class="marker"></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://github.com/mlc-ai/mlc-llm"><span><b>MLC LLM</b></span></a></span><span class="inline-wrap">：可在客户端（边缘计算）（例如，在<span class="jill"></span>Android<span class="jill"></span>或<span class="jill"></span>iPhone<span class="jill"></span>平台上）本地部署<span class="jill"></span>LLM；</span></li><li id="aqC7kewbQ4o7Qyp1QuTQB"><div class="marker"></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed-MII"><span><b>DeepSpeed-MII</b></span></a></span><span class="inline-wrap">：使用<span class="jill"></span>DeepSpeed<span class="jill"></span>库来部署<span class="jill"></span>LLM；</span></li></ol><div id="fZJ54AR35ieRDJuLSTfpSp" class="wolai-block wolai-text"><div><span class="inline-wrap">下面在内存容量为<span class="jill"></span>40GB<span class="jill"></span>的<span class="jill"></span>A100 GPU<span class="jill"></span>上，并且使用<span class="jill"></span>LLaMA-1 13b<span class="jill"></span>模型（因为列表中的所有库都支持它）进行七个部署框架的对比。</span></div></div><h3 id="vnp7gSbLQv5DSGMoEWrbcT" class="wolai-block"><span class="inline-wrap"><b>1.vLLM</b></span></h3><div id="bGd8BSwSZoD7NRkK1tMP2r" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_1.png" style="width: 100%"/></figure></div><div id="mnvcuGefSwQzvfMhktLJFm" class="wolai-block wolai-text"><div><span class="inline-wrap">vLLM<span class="jill"></span>的吞吐量比<span class="jill"></span>HuggingFace Transformers（HF）高<span class="jill"></span>14x-24<span class="jill"></span>倍，比<span class="jill"></span>HuggingFace Text Generation Inference（TGI）高<span class="jill"></span>2.2x-2.5<span class="jill"></span>倍。</span></div></div><h4 id="abeU3UyyoxNkywk9oeFkNs" class="wolai-block"><span class="inline-wrap">1.1 使用</span></h4><div id="9cpvvM1nWaxqrEBDiVnMWC" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>离线批量推理</b></span></div></div><code-block id="qxHte61s6uBce8h6yGgTZA" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token comment"># pip install vllm</span>
<span class="token keyword">from</span> vllm <span class="token keyword">import</span> LLM<span class="token punctuation">,</span> SamplingParams

prompts <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"Funniest joke ever:"</span><span class="token punctuation">,</span>
    <span class="token string">"The capital of France is"</span><span class="token punctuation">,</span>
    <span class="token string">"The future of AI is"</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>
sampling_params <span class="token operator">=</span> SamplingParams<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">,</span> top_p<span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">)</span>
llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">"huggyllama/llama-13b"</span><span class="token punctuation">)</span>
outputs <span class="token operator">=</span> llm<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>prompts<span class="token punctuation">,</span> sampling_params<span class="token punctuation">)</span>

<span class="token keyword">for</span> output <span class="token keyword">in</span> outputs<span class="token punctuation">:</span>
    prompt <span class="token operator">=</span> output<span class="token punctuation">.</span>prompt
    generated_text <span class="token operator">=</span> output<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Prompt: </span><span class="token interpolation"><span class="token punctuation">{</span>prompt<span class="token conversion-option punctuation">!r</span><span class="token punctuation">}</span></span><span class="token string">, Generated text: </span><span class="token interpolation"><span class="token punctuation">{</span>generated_text<span class="token conversion-option punctuation">!r</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span></pre></div></code-block><div id="kCRrqDdgFzAq8zu7eup9o6" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>API Server</b></span></div></div><code-block id="fwDZ6XHLkFe1tR4NEZLZ3c" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token comment"># Start the server:</span>
python <span class="token operator">-</span>m vllm<span class="token punctuation">.</span>entrypoints<span class="token punctuation">.</span>api_server <span class="token operator">-</span><span class="token operator">-</span>env MODEL_NAME<span class="token operator">=</span>huggyllama<span class="token operator">/</span>llama<span class="token operator">-</span>13b

<span class="token comment"># Query the model in shell:</span>
curl http<span class="token punctuation">:</span><span class="token operator">//</span>localhost<span class="token punctuation">:</span><span class="token number">8000</span><span class="token operator">/</span>generate \
    <span class="token operator">-</span>d '<span class="token punctuation">{</span>
        <span class="token string">"prompt"</span><span class="token punctuation">:</span> <span class="token string">"Funniest joke ever:"</span><span class="token punctuation">,</span>
        <span class="token string">"n"</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
        <span class="token string">"temperature"</span><span class="token punctuation">:</span> <span class="token number">0.95</span><span class="token punctuation">,</span>
        <span class="token string">"max_tokens"</span><span class="token punctuation">:</span> <span class="token number">200</span>
    <span class="token punctuation">}</span>'</pre></div></code-block><h4 id="jQ2txPAB2SFE4Cqb4nXXzX" class="wolai-block"><span class="inline-wrap">1.2 </span><span class="inline-wrap"><b>功能</b></span></h4><ul class="wolai-block"><li id="63dHRhu54RRrVdHUsjHMXo"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://www.anyscale.com/blog/continuous-batching-llm-inference"><span><b>Continuous batching</b></span></a></span><span class="inline-wrap">：有<span class="jill"></span>iteration-level<span class="jill"></span>的调度机制，每次迭代<span class="jill"></span>batch<span class="jill"></span>大小都有所变化，因此<span class="jill"></span>vLLM<span class="jill"></span>在大量查询下仍可以很好的工作。</span></li><li id="8HJnit3xhZzqJ4rrYfmuA4"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://vllm.ai/"><span class="red"><b>PagedAttention</b></span></a></span><span class="inline-wrap">：</span><span class="yellow inline-wrap">受操作系统中虚拟内存和分页的经典思想启发的注意力算法，这就是模型加速的秘诀</span><span class="inline-wrap">。</span></li></ul><h4 id="my82MwCt6viksKgNugoHjW" class="wolai-block"><span class="inline-wrap">1.3 </span><span class="inline-wrap"><b>优点</b></span></h4><ul class="wolai-block"><li id="5DKG9bFmMyr9uV54ecttFG"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>文本生成的速度</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 实验多次，发现<span class="jill"></span>vLLM<span class="jill"></span>的推理速度是最快的；</span></li><li id="tKJLFwdGfARp8JAoMKX6W9"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>高吞吐量服务</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 支持各种解码算法，比如<span class="jill"></span>parallel sampling, beam search<span class="jill"></span>等；</span></li><li id="d4PmtzaU3Xd4un1yHFwdCW"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>与<span class="jill"></span>OpenAI API<span class="jill"></span>兼容</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 如果使用<span class="jill"></span>OpenAI API，只需要替换端点的<span class="jill"></span>URL<span class="jill"></span>即可；</span></li></ul><h4 id="wZsoxo7MLCrUwhdsbtMnYP" class="wolai-block"><span class="inline-wrap">1.4 </span><span class="inline-wrap"><b>缺点</b></span></h4><ul class="wolai-block"><li id="rwgLRFqn19W8iZVFeA1et7"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>添加自定义模型</b></span><span class="inline-wrap">：虽然可以合并自己的模型，但如果模型没有使用与<span class="jill"></span>vLLM<span class="jill"></span>中现有模型类似的架构，则过程会变得更加复杂。例如，增加<span class="jill"></span>Falcon<span class="jill"></span>的支持，这似乎很有挑战性；</span></li><li id="kvgmRJC8k2UKGYLNgTuhJt"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>缺乏对适配器（LoRA、QLoRA<span class="jill"></span>等）的支持</b></span><span class="inline-wrap">：当针对特定任务进行微调时，开源<span class="jill"></span>LLM<span class="jill"></span>具有重要价值。然而，在当前的实现中，没有单独使用模型和适配器权重的选项，这限制了有效利用此类模型的灵活性。</span></li><li id="aTjXtD5X6QnTop9sJkCmmr"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>缺少权重量化</b></span><span class="inline-wrap">：有时，LLM<span class="jill"></span>可能不需要使用<span class="jill"></span>GPU<span class="jill"></span>内存，这对于减少<span class="jill"></span>GPU<span class="jill"></span>内存消耗至关重要。</span></li></ul><div id="qjnASRP13nqzFfwtbq8urB" class="wolai-block wolai-text"><div><span class="inline-wrap">这是<span class="jill"></span>LLM<span class="jill"></span>推理最快的库。得益于其内部优化，它显著优于竞争对手。尽管如此，它在支持有限范围的模型方面确实存在弱点。</span></div></div><div id="t9PuxpKuSdwjhwHpyQqRFb" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>使用<span class="jill"></span>vLLM<span class="jill"></span>的开发路线可以参考：</b></span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https%3A//github.com/vllm-project/vllm/issues/244"><span><b>https://github.com/vllm-project/vllm/issues/244</b></span></a></span></div></div><h3 id="dpuAjfqhN13H7oJh9JLHVW" class="wolai-block"><span class="inline-wrap"><b>2.Text generation inference</b></span></h3><div id="qoV6JbPVP1hAdYuZdHLQTP" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="https://pic3.zhimg.com/v2-5238573ef15a96e9fcafc28193a56d9a_b.jpg" style="width: 720px"/></figure></div><div id="gcSe2LK9n731w4vukbpr7B" class="wolai-block wolai-text"><div><span class="inline-wrap">Text generation inference<span class="jill"></span>是用于文本生成推断的<span class="jill"></span>Rust、Python<span class="jill"></span>和<span class="jill"></span>gRPC<span class="jill"></span>服务器，在<span class="jill"></span>HuggingFace<span class="jill"></span>中已有<span class="jill"></span>LLM 推理<span class="jill"></span>API<span class="jill"></span>使用。</span></div></div><h4 id="szdeth4T2wCrq6n3bnx1xa" class="wolai-block"><span class="inline-wrap">2.1<span class="jill"></span>使用</span></h4><div id="83hHd1vmmCCo8VZHAQoRTG" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>使用<span class="jill"></span>docker<span class="jill"></span>运行<span class="jill"></span>web server</b></span></div></div><code-block id="jLCVK6bfxw1BT7pnzUyp4N" class="wolai-block"><div class="wolai-pre"><div data-lang="Bash" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token function">mkdir</span> data
<span class="token function">docker</span> run <span class="token parameter variable">--gpus</span> all --shm-size 1g <span class="token parameter variable">-p</span> <span class="token number">8080</span>:80 <span class="token punctuation">\</span>
<span class="token parameter variable">-v</span> data:/data ghcr.io/huggingface/text-generation-inference:0.9 <span class="token punctuation">\</span>
  --model-id huggyllama/llama-13b <span class="token punctuation">\</span>
  --num-shard <span class="token number">1</span></pre></div></code-block><div id="gY4fvekDU1yWhJtGqrXwJr" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>查询实例</b></span></div></div><code-block id="iGTvaETBNDakeb2KTUYko7" class="wolai-block"><div class="wolai-pre"><div data-lang="Bash" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token comment"># pip install text-generation</span>
from text_generation <span class="token function">import</span> Client

client <span class="token operator">=</span> Client<span class="token punctuation">(</span><span class="token string">"http://127.0.0.1:8080"</span><span class="token punctuation">)</span>
prompt <span class="token operator">=</span> <span class="token string">"Funniest joke ever:"</span>
print<span class="token punctuation">(</span>client.generate<span class="token punctuation">(</span>prompt, <span class="token assign-left variable">max_new_tokens</span><span class="token operator">=</span><span class="token number">17</span> <span class="token assign-left variable">temperature</span><span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">)</span>.generated_text<span class="token punctuation">)</span></pre></div></code-block><h4 id="3bPrBjwJFvmEfiw9j9w2zq" class="wolai-block"><span class="inline-wrap">2.2</span><span class="inline-wrap"><b>功能</b></span></h4><ul class="wolai-block"><li id="wCVLtDiNZJetAQzdCzs7zS"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>内置服务评估</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 可以监控服务器负载并深入了解其性能；</span></li><li id="ucyAtnwv2kp3PDSH56RnUy"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>使用<span class="jill"></span>flash attention（和<span class="jill"></span>v2）和<span class="jill"></span>Paged attention<span class="jill"></span>优化<span class="jill"></span>transformer<span class="jill"></span>推理代码</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 并非所有模型都内置了对这些优化的支持，该技术可以对未使用该技术的模型可以进行优化；</span></li></ul><h4 id="pJMY5L4cXmA6nEHYDSi9en" class="wolai-block"><span class="inline-wrap">2.3 </span><span class="inline-wrap"><b>优点</b></span></h4><ul class="wolai-block"><li id="6TzdeQ2aaqyJXjXySvEYxe"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>所有的依赖项都安装在<span class="jill"></span>Docker<span class="jill"></span>中</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 会得到一个现成的环境；</span></li><li id="3BEkxwrgHAHAxNvuxzovs2"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>支持<span class="jill"></span>HuggingFace<span class="jill"></span>模型</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 轻松运行自己的模型或使用任何<span class="jill"></span>HuggingFace<span class="jill"></span>模型中心；</span></li><li id="3ZgRuZd7GhThF4W5cVEnZN"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>对模型推理的控制</b></span><span class="inline-wrap">：该框架提供了一系列管理模型推理的选项，包括精度调整、量化、张量并行性、重复惩罚等；</span></li></ul><h4 id="8KAZxiSAxC1QgLATcLzhg2" class="wolai-block"><span class="inline-wrap">2.4</span><span class="inline-wrap"><b>缺点</b></span></h4><ul class="wolai-block"><li id="aAEnA43vVRNr3Uoc8DQ3rZ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>缺乏对适配器的支持</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 需要注意的是，尽管可以使用适配器部署<span class="jill"></span>LLM（可以参考</span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DHI3cYN0c9ZU"><span>https://www.youtube.com/watch?v=HI3cYN0c9ZU</span></a></span><span class="inline-wrap">），但目前还没有官方支持或文档；</span></li><li id="qpwQHokaLB5r99uc1uH9H4"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>从源代码（Rust+CUDA<span class="jill"></span>内核）编译</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 对于不熟悉<span class="jill"></span>Rust<span class="jill"></span>的人，将客户化代码纳入库中变得很有挑战性；</span></li><li id="sZGJqTRCPyE5FF67spCkdA"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>文档不完整</b></span><span class="inline-wrap">：所有信息都可以在项目的自述文件中找到。尽管它涵盖了基础知识，但必须在问题或源代码中搜索更多细节；</span></li></ul><div id="bmq1cqq1NZvSBzfbPniY1s" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>使用<span class="jill"></span>Text generation inference<span class="jill"></span>的开发路线可以参考：</b></span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https%3A//github.com/huggingface/text-generation-inference/issues/232"><span><b>https://github.com/huggingface/text-generation-inference/issues/232</b></span></a></span></div></div><h3 id="ez867ubL7raQYU7KdeGFWK" class="wolai-block"><span class="inline-wrap"><b>3.CTranslate2</b></span></h3><div id="a3LZ7XdYcrCKWRNHFbqy5w" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_2.png" style="width: 100%"/></figure></div><div id="eiPtHf9ga84ruQWHDf3czA" class="wolai-block wolai-text"><div><span class="inline-wrap">CTranslate2<span class="jill"></span>是一个<span class="jill"></span>C++<span class="jill"></span>和<span class="jill"></span>Python<span class="jill"></span>库，用于使用<span class="jill"></span>Transformer<span class="jill"></span>模型进行高效推理。</span></div></div><h3 id="m16b7qx9PYpSW3H3Z1gtuk" class="wolai-block"><span class="inline-wrap">3.1 使用</span></h3><div id="xz6KrotPgkFwPyvAWEtfER" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>转换模型</b></span></div></div><code-block id="j9XidKuxDJeEBQB6Yw1z7i" class="wolai-block"><div class="wolai-pre"><div data-lang="Bash" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all">pip <span class="token function">install</span> <span class="token parameter variable">-qqq</span> transformers ctranslate2

<span class="token comment"># The model should be first converted into the CTranslate2 model format:</span>
ct2-transformers-converter <span class="token parameter variable">--model</span> huggyllama/llama-13b <span class="token parameter variable">--output_dir</span> llama-13b-ct2 <span class="token parameter variable">--force</span></pre></div></code-block><div id="4CgvpAQZpRnfxAEPqvFyVM" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>查询实例</b></span></div></div><code-block id="ewtLPFpeNnXVW4CgykD2Wh" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token keyword">import</span> ctranslate2
<span class="token keyword">import</span> transformers

generator <span class="token operator">=</span> ctranslate2<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token string">"llama-13b-ct2"</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">,</span> compute_type<span class="token operator">=</span><span class="token string">"float16"</span><span class="token punctuation">)</span>
tokenizer <span class="token operator">=</span> transformers<span class="token punctuation">.</span>AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"huggyllama/llama-13b"</span><span class="token punctuation">)</span>

prompt <span class="token operator">=</span> <span class="token string">"Funniest joke ever:"</span>
tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_ids_to_tokens<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">)</span>
results <span class="token operator">=</span> generator<span class="token punctuation">.</span>generate_batch<span class="token punctuation">(</span>
    <span class="token punctuation">[</span>tokens<span class="token punctuation">]</span><span class="token punctuation">,</span> 
    sampling_topk<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> 
    max_length<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span> 
<span class="token punctuation">)</span>
tokens <span class="token operator">=</span> results<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>sequences_ids<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
output <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">)</span></pre></div></code-block><h4 id="d1hQYRYCJs3WNMmyjvAomy" class="wolai-block"><span class="inline-wrap">3.2</span><span class="inline-wrap"><b>功能</b></span></h4><ul class="wolai-block"><li id="s1xk2aZFmhbdNdKUvCGqA8"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>在<span class="jill"></span>CPU<span class="jill"></span>和<span class="jill"></span>GPU<span class="jill"></span>上快速高效地执行</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 得益于内置的一系列优化：层融合、填充去除、批量重新排序、原位操作、缓存机制等。推理<span class="jill"></span>LLM<span class="jill"></span>更快，所需内存更少；</span></li><li id="or61eW45kJVm6VKEjRk1iz"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>动态内存使用率</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 由于<span class="jill"></span>CPU<span class="jill"></span>和<span class="jill"></span>GPU<span class="jill"></span>上都有缓存分配器，内存使用率根据请求大小动态变化，同时仍能满足性能要求；</span></li><li id="nnJ6AN8vyeZtk1sRWj8yS2"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>支持多种<span class="jill"></span>CPU<span class="jill"></span>体系结构</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 该项目支持<span class="jill"></span>x86–64<span class="jill"></span>和<span class="jill"></span>AArch64/ARM64<span class="jill"></span>处理器，并集成了针对这些平台优化的多个后端：英特尔<span class="jill"></span>MKL、oneDNN、OpenBLAS、Ruy<span class="jill"></span>和<span class="jill"></span>Apple Accelerate；</span></li></ul><h4 id="3sPKCcroqAwkHnz1bhFxdW" class="wolai-block"><span class="inline-wrap">3.3 </span><span class="inline-wrap"><b>优点</b></span></h4><ul class="wolai-block"><li id="d1FnNeGBPaMaCZycQqVAt"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>并行和异步执行</b></span><span class="inline-wrap">：可以使用多个<span class="jill"></span>GPU<span class="jill"></span>或<span class="jill"></span>CPU<span class="jill"></span>核心并行和异步处理多个批处理；</span></li><li id="mMzx81zwYYwmjbepCEf3CH"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Prompt<span class="jill"></span>缓存</b></span><span class="inline-wrap">：在静态提示下运行一次模型，缓存模型状态，并在将来使用相同的静态提示进行调用时重用；</span></li><li id="5DVj6BBRECrqFuYeA8N8AH"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>磁盘上的轻量级</b></span><span class="inline-wrap">：量化可以使模型在磁盘上缩小<span class="jill"></span>4<span class="jill"></span>倍，而精度损失最小；</span></li></ul><h4 id="jojKSyj5n7RjiCHY7CfWo3" class="wolai-block"><span class="inline-wrap">3.4 </span><span class="inline-wrap"><b>缺点</b></span></h4><ul class="wolai-block"><li id="iHu9Wusdqu2RexJK9kTYev"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>没有内置的<span class="jill"></span>REST<span class="jill"></span>服务器</b></span><span class="inline-wrap">：尽管仍然可以运行<span class="jill"></span>REST<span class="jill"></span>服务器，但没有具有日志记录和监控功能的现成服务</span></li><li id="8eRAEwpb2nwZWHduR6abYY"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>缺乏对适配器（LoRA、QLoRA<span class="jill"></span>等）的支持</b></span></li></ul><h3 id="tZq5VEzHeqsr9x3rBQ5Mav" class="wolai-block"><span class="inline-wrap"><b>4.DeepSpeed-MII</b></span></h3><div id="a5PergqcK3hkWA7yPGj4ZC" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_3.png" style="width: 100%"/></figure></div><div id="7nDZ1aoLuPZmLQ7WMAg8Pb" class="wolai-block wolai-text"><div><span class="inline-wrap">在<span class="jill"></span>DeepSpeed<span class="jill"></span>支持下，DeepSpeed-MII<span class="jill"></span>可以进行低延迟和高通量推理。</span></div></div><h4 id="8wZU9DZvgJrb2vUY6GjFnG" class="wolai-block"><span class="inline-wrap">4.1 使用</span></h4><div id="jTrBQxUQhfvjoXw86CQid1" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>运行<span class="jill"></span>web<span class="jill"></span>服务</b></span></div></div><code-block id="wV1A4Q4bXkik1KAChXNezH" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token comment"># DON'T INSTALL USING pip install deepspeed-mii</span>
<span class="token comment"># git clone https://github.com/microsoft/DeepSpeed-MII.git</span>
<span class="token comment"># git reset --hard 60a85dc3da5bac3bcefa8824175f8646a0f12203</span>
<span class="token comment"># cd DeepSpeed-MII &amp;&amp; pip install .</span>
<span class="token comment"># pip3 install -U deepspeed</span>

<span class="token comment"># ... and make sure that you have same CUDA versions:</span>
<span class="token comment"># python -c "import torch;print(torch.version.cuda)" == nvcc --version</span>
<span class="token keyword">import</span> mii

mii_configs <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">"dtype"</span><span class="token punctuation">:</span> <span class="token string">"fp16"</span><span class="token punctuation">,</span>
    <span class="token string">'max_tokens'</span><span class="token punctuation">:</span> <span class="token number">200</span><span class="token punctuation">,</span>
    <span class="token string">'tensor_parallel'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token string">"enable_load_balancing"</span><span class="token punctuation">:</span> <span class="token boolean">False</span>
<span class="token punctuation">}</span>
mii<span class="token punctuation">.</span>deploy<span class="token punctuation">(</span>task<span class="token operator">=</span><span class="token string">"text-generation"</span><span class="token punctuation">,</span>
           model<span class="token operator">=</span><span class="token string">"huggyllama/llama-13b"</span><span class="token punctuation">,</span>
           deployment_name<span class="token operator">=</span><span class="token string">"llama_13b_deployment"</span><span class="token punctuation">,</span>
           mii_config<span class="token operator">=</span>mii_configs<span class="token punctuation">)</span></pre></div></code-block><div id="6raLJDGNsTV52zYeFfeV3y" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>查询实例</b></span></div></div><code-block id="feyGccNtVZXi81PqtjGac8" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token keyword">import</span> mii

generator <span class="token operator">=</span> mii<span class="token punctuation">.</span>mii_query_handle<span class="token punctuation">(</span><span class="token string">"llama_13b_deployment"</span><span class="token punctuation">)</span>
result <span class="token operator">=</span> generator<span class="token punctuation">.</span>query<span class="token punctuation">(</span>  
  <span class="token punctuation">{</span><span class="token string">"query"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"Funniest joke ever:"</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">,</span> 
  do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
  max_new_tokens<span class="token operator">=</span><span class="token number">200</span>
<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span></pre></div></code-block><h4 id="tSsisrUtYPRWhxmggUrdAJ" class="wolai-block"><span class="inline-wrap">4.2 </span><span class="inline-wrap"><b>功能</b></span></h4><ul class="wolai-block"><li id="jRYYrjMJekzHhTp1UwXYR2"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>多个副本上的负载平衡</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 这是一个非常有用的工具，可用于处理大量用户。负载均衡器在各种副本之间高效地分配传入请求，从而缩短了应用程序的响应时间。</span></li><li id="iikUBCq5yMLRhejLY1ffL2"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>非持久部署</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 目标环境的部署不是永久的，需要经常更新的，这在资源效率、安全性、一致性和易管理性至关重要的情况下，这是非常重要的。</span></li></ul><h4 id="6MG1sFP3AUBHFGd5sKxrUw" class="wolai-block"><span class="inline-wrap">4.3</span><span class="inline-wrap"><b>优点</b></span></h4><ul class="wolai-block"><li id="xteruT27F3ztbr5gYZRevh"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>支持不同的模型库</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 支持多个开源模型库，如<span class="jill"></span>Hugging Face、FairSeq、EluetherAI<span class="jill"></span>等；</span></li><li id="djWqWsWXpSKNGiLU884872"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>量化延迟和降低成本</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 可以显著降低非常昂贵的语言模型的推理成本；</span></li><li id="uXhf3Wi48zKH4TeeAwhXec"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Native<span class="jill"></span>和<span class="jill"></span>Azure<span class="jill"></span>集成</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 微软开发的<span class="jill"></span>MII<span class="jill"></span>框架提供了与云系统的出色集成；</span></li></ul><h4 id="8ujNGSX4CD9BJBEkP1TjyL" class="wolai-block"><span class="inline-wrap">4.4</span><span class="inline-wrap"><b>缺点</b></span></h4><ul class="wolai-block"><li id="mC68aUSYMoQDZrqhCfwE4j"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>支持模型的数量有限</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 不支持<span class="jill"></span>Falcon、LLaMA2<span class="jill"></span>和其他语言模型；</span></li><li id="nzmZtz3nPCurVENRMqAaZQ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>缺乏对适配器（LoRA、QLoRA<span class="jill"></span>等）的支持</b></span><span class="inline-wrap"><b>；</b></span></li></ul><h3 id="xnTe5WqVr69bNKc5naC3KG" class="wolai-block"><span class="inline-wrap"><b>5.OpenLLM</b></span></h3><div id="ow8vEtxhKSJuzC7KUwmKDA" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_4.png" style="width: 100%"/></figure></div><div id="irXmTarajfKvo1FRYWdwY3" class="wolai-block wolai-text"><div><span class="inline-wrap">OpenLLM<span class="jill"></span>是一个用于在生产中操作大型语言模型（LLM）的开放平台。</span></div></div><h4 id="qFzWC3q1Z5ynduDFmVVKAj" class="wolai-block"><span class="inline-wrap">5.1 使用</span></h4><div id="6F5PXCF4n3fVcw1CbtDobj" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>运行<span class="jill"></span>web<span class="jill"></span>服务</b></span></div></div><code-block id="v7uFviwoM1tdAfSQB3cXoD" class="wolai-block"><div class="wolai-pre"><div data-lang="Bash" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all">pip <span class="token function">install</span> openllm scipy
openllm start llama --model-id huggyllama/llama-13b <span class="token punctuation">\</span>
  --max-new-tokens <span class="token number">200</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--temperature</span> <span class="token number">0.95</span> <span class="token punctuation">\</span>
  --api-workers <span class="token number">1</span> <span class="token punctuation">\</span>
  --workers-per-resource <span class="token number">1</span></pre></div></code-block><div id="tuUFyKU2aa23k7seBssH8u" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>查询实例</b></span></div></div><code-block id="g6HvkkZEonjpZUUUhDBw2E" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token keyword">import</span> openllm

client <span class="token operator">=</span> openllm<span class="token punctuation">.</span>client<span class="token punctuation">.</span>HTTPClient<span class="token punctuation">(</span><span class="token string">'http://localhost:3000'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>client<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">"Funniest joke ever:"</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></div></code-block><h4 id="nomufJiL2xAWRyegTyE5y6" class="wolai-block"><span class="inline-wrap">5.2 </span><span class="inline-wrap"><b>功能</b></span></h4><ul class="wolai-block"><li id="dyRKmaJWPZWiHHMZKDfSnT"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>适配器支持</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 可以将要部署的<span class="jill"></span>LLM<span class="jill"></span>连接多个适配器，这样可以只使用一个模型来执行几个特定的任务；</span></li><li id="eksB6Xh1nUriJwmQFv9QVg"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>支持不同的运行框架</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 比如<span class="jill"></span>Pytorch（pt）、Tensorflow（tf）或<span class="jill"></span>Flax（亚麻）；</span></li><li id="fH2qBaxhnbR55VBCkNtuxj"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main_classes/agent"><span class="red"><b>HuggingFace Agents</b></span></a></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 连接<span class="jill"></span>HuggingFace<span class="jill"></span>上不同的模型，并使用<span class="jill"></span>LLM<span class="jill"></span>和自然语言进行管理；</span></li></ul><h4 id="5baLxwJU2V5W3Jt1pY5j6Q" class="wolai-block"><span class="inline-wrap">5.3 </span><span class="inline-wrap"><b>优点</b></span></h4><ul class="wolai-block"><li id="osPP1b2L9WTW6wMsr4pZN2"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>良好的社区支持</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 不断开发和添加新功能；</span></li><li id="kHBaf6gP7BS8MVL8v6JuFX"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>集成新模型</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 可以添加用户自定义模型；</span></li><li id="hF9tF4R52BJwkWrrNnRxVS"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>量化</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> OpenLLM<span class="jill"></span>支持使用<span class="jill"></span>bitsandbytes[12]和<span class="jill"></span>GPTQ[13]进行量化；</span></li><li id="asyX8GynnnSktvgQtNMEKf"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>LangChain<span class="jill"></span>集成</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 可以使用<span class="jill"></span>LangChian<span class="jill"></span>与远程<span class="jill"></span>OpenLLM<span class="jill"></span>服务器进行交互；</span></li></ul><h4 id="n6QhKvZbKBBS5m46dyBsrW" class="wolai-block"><span class="inline-wrap">5.4 </span><span class="inline-wrap"><b>缺点</b></span></h4><ul class="wolai-block"><li id="qaqmMWnABxCjJ3dYBDowHM"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>缺乏批处理支持</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 对于大量查询，这很可能会成为应用程序性能的瓶颈；</span></li><li id="pniebEwT2pTniBhHbwFjPf"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>缺乏内置的分布式推理</b></span><span class="inline-wrap">：如果你想在多个<span class="jill"></span>GPU<span class="jill"></span>设备上运行大型模型，你需要额外安装<span class="jill"></span>OpenLLM<span class="jill"></span>的服务组件<span class="jill"></span>Yatai；</span></li></ul><h3 id="jgLacT8htd8gJ3ZXxdNTVX" class="wolai-block"><span class="inline-wrap"><b>6.Ray Serve</b></span></h3><div id="uF4Dk9xCc63fwYzyzEs4cb" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_5.png" style="width: 100%"/></figure></div><div id="74KV4YpgZbKTEE6RwBhw6D" class="wolai-block wolai-text"><div><span class="inline-wrap">Ray Serve<span class="jill"></span>是一个可扩展的模型服务库，用于构建在线推理<span class="jill"></span>API。Serve<span class="jill"></span>与框架无关，因此可以使用一个工具包来为深度学习模型的所有内容提供服务。</span></div></div><div id="c5LGvQ191c87q89cofuA4c" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_6.png" style="width: 100%"/></figure></div><h4 id="284WVsbttYUsLfPf99LAzm" class="wolai-block"><span class="inline-wrap">6.1 使用</span></h4><div id="m3B1k7tdN9caekdAPcwH3i" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>运行<span class="jill"></span>web<span class="jill"></span>服务</b></span></div></div><code-block id="tAUfiayXHDxkqXq6fiVNjo" class="wolai-block"><div class="wolai-pre"><div data-lang="Bash" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token comment"># pip install ray[serve] accelerate>=0.16.0 transformers>=4.26.0 torch starlette pandas</span>
<span class="token comment"># ray_serve.py</span>
<span class="token function">import</span> pandas as pd

<span class="token function">import</span> ray
from ray <span class="token function">import</span> serve
from starlette.requests <span class="token function">import</span> Request

@serve.deployment<span class="token punctuation">(</span>ray_actor_options<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"num_gpus"</span><span class="token builtin class-name">:</span> <span class="token number">1</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
class PredictDeployment:
    def __init__<span class="token punctuation">(</span>self, model_id: str<span class="token punctuation">)</span>:
        from transformers <span class="token function">import</span> AutoModelForCausalLM, AutoTokenizer
        <span class="token function">import</span> torch

        self.model <span class="token operator">=</span> AutoModelForCausalLM.from_pretrained<span class="token punctuation">(</span>
            model_id,
            <span class="token assign-left variable">torch_dtype</span><span class="token operator">=</span>torch.float16,
            <span class="token assign-left variable">device_map</span><span class="token operator">=</span><span class="token string">"auto"</span>,
        <span class="token punctuation">)</span>
        self.tokenizer <span class="token operator">=</span> AutoTokenizer.from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>

    def generate<span class="token punctuation">(</span>self, text: str<span class="token punctuation">)</span> -<span class="token operator">></span> pd.DataFrame:
        input_ids <span class="token operator">=</span> self.tokenizer<span class="token punctuation">(</span>text, <span class="token assign-left variable">return_tensors</span><span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>.input_ids.to<span class="token punctuation">(</span>
            self.model.device
        <span class="token punctuation">)</span>
        gen_tokens <span class="token operator">=</span> self.model.generate<span class="token punctuation">(</span>
            input_ids,
            <span class="token assign-left variable">temperature</span><span class="token operator">=</span><span class="token number">0.9</span>,
            <span class="token assign-left variable">max_length</span><span class="token operator">=</span><span class="token number">200</span>,
        <span class="token punctuation">)</span>
        <span class="token builtin class-name">return</span> pd.DataFrame<span class="token punctuation">(</span>
            self.tokenizer.batch_decode<span class="token punctuation">(</span>gen_tokens<span class="token punctuation">)</span>, <span class="token assign-left variable">columns</span><span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"responses"</span><span class="token punctuation">]</span>
        <span class="token punctuation">)</span>

    async def __call__<span class="token punctuation">(</span>self, http_request: Request<span class="token punctuation">)</span> -<span class="token operator">></span> str:
        json_request: str <span class="token operator">=</span> await http_request.json<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token builtin class-name">return</span> self.generate<span class="token punctuation">(</span>prompt<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

deployment <span class="token operator">=</span> PredictDeployment.bind<span class="token punctuation">(</span>model_id<span class="token operator">=</span><span class="token string">"huggyllama/llama-13b"</span><span class="token punctuation">)</span>

<span class="token comment"># then run from CLI command:</span>
<span class="token comment"># serve run ray_serve:deployment</span></pre></div></code-block><div id="rFP6Dz6kHjrmJ1QwvQoPyi" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>查询实例</b></span></div></div><code-block id="3pKUhzWaG7zgTzYLb9oYvf" class="wolai-block"><div class="wolai-pre"><div data-lang="Bash" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token function">import</span> requests

sample_input <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"text"</span><span class="token builtin class-name">:</span> <span class="token string">"Funniest joke ever:"</span><span class="token punctuation">}</span>
output <span class="token operator">=</span> requests.post<span class="token punctuation">(</span><span class="token string">"http://localhost:8000/"</span>, <span class="token assign-left variable">json</span><span class="token operator">=</span><span class="token punctuation">[</span>sample_input<span class="token punctuation">]</span><span class="token punctuation">)</span>.json<span class="token punctuation">(</span><span class="token punctuation">)</span>
print<span class="token punctuation">(</span>output<span class="token punctuation">)</span></pre></div></code-block><h4 id="tZ9GsSvAu2NgwW6pARhw8Z" class="wolai-block"><span class="inline-wrap">6.2 </span><span class="inline-wrap"><b>功能</b></span></h4><ul class="wolai-block"><li id="oZFXYcXts8m6wcvR1jGN58"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>监控仪表板和<span class="jill"></span>Prometheus<span class="jill"></span>度量</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 可以使用<span class="jill"></span>Ray<span class="jill"></span>仪表板来获得<span class="jill"></span>Ray<span class="jill"></span>集群和<span class="jill"></span>Ray Serve<span class="jill"></span>应用程序状态；</span></li><li id="j5KdnNee5yjijVgFifL3mB"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>跨多个副本自动缩放</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> Ray<span class="jill"></span>通过观察队列大小并做出添加或删除副本的缩放决策来调整流量峰值；</span></li><li id="tgqnwrS8aZX5aRVTwAKphw"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>动态请求批处理</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 当模型使用成本很高，为最大限度地利用硬件，可以采用该策略；</span></li></ul><h4 id="jF992UeYybCRJuZCn6x3JX" class="wolai-block"><span class="inline-wrap">6.3 </span><span class="inline-wrap"><b>优点</b></span></h4><ul class="wolai-block"><li id="2m3Qm9JTdhQzGmD8bnaeg4"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>文档支持</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 开发人员几乎为每个用例撰写了许多示例；</span></li><li id="iVsFridw428WjpvhX1n5B7"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>支持生产环境部署</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 这是本列表中所有框架中最成熟的；</span></li><li id="ccVYVLrioNmZa1z2SQFEEG"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>本地<span class="jill"></span>LangChain<span class="jill"></span>集成</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 您可以使用<span class="jill"></span>LangChian<span class="jill"></span>与远程<span class="jill"></span>Ray Server<span class="jill"></span>进行交互；</span></li></ul><h4 id="wtRui7uQFfLVfk7qJpZTqy" class="wolai-block"><span class="inline-wrap">6.4 </span><span class="inline-wrap"><b>缺点</b></span></h4><ul class="wolai-block"><li id="woeCQFbhixwGYPfGgVrUdr"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>缺乏内置的模型优化</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> Ray Serve<span class="jill"></span>不专注于<span class="jill"></span>LLM，它是一个用于部署任何<span class="jill"></span>ML<span class="jill"></span>模型的更广泛的框架，必须自己进行优化；</span></li><li id="pTKA5X4GnztLknAh8MEfqQ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>入门门槛高</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 该库功能多，提高了初学者进入的门槛；</span></li></ul><div id="jVm48YLxW48hpzb87a2nrK" class="wolai-block wolai-text"><div><span class="inline-wrap">如果需要最适合生产的解决方案，而不仅仅是深度学习，Ray Serve<span class="jill"></span>是一个不错的选择。它最适合于可用性、可扩展性和可观察性非常重要的企业。此外，还可以使用其庞大的生态系统进行数据处理、模型训练、微调和服务。最后，从<span class="jill"></span>OpenAI<span class="jill"></span>到<span class="jill"></span>Shopify<span class="jill"></span>和<span class="jill"></span>Instacart<span class="jill"></span>等公司都在使用它。</span></div></div><h3 id="oYAeAvvdNhDRdUfExrpvWH" class="wolai-block"><span class="inline-wrap"><b>7.MLC LLM</b></span></h3><div id="ageXUu8JBKX6izXi7GSG22" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_7.png" style="width: 100%"/></figure></div><div id="3SsfWBgvGiwbuWAj3bCQVk" class="wolai-block wolai-text"><div><span class="inline-wrap">LLM<span class="jill"></span>的机器学习编译（MLC LLM）是一种通用的部署解决方案，它使<span class="jill"></span>LLM<span class="jill"></span>能够利用本机硬件加速在消费者设备上高效运行。</span></div></div><div id="ad2Y8vF4DfhUuQLAY7noVD" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_8.png" style="width: 100%"/></figure></div><h4 id="8BwLvfciiwfCHupRGBgE7w" class="wolai-block"><span class="inline-wrap">7.1 使用</span></h4><div id="dRXRthUksAEzfPSr8bbg7d" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>运行<span class="jill"></span>web<span class="jill"></span>服务</b></span></div></div><code-block id="5J1qwjB5p4zst8ETSXcgKm" class="wolai-block"><div class="wolai-pre"><div data-lang="Bash" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token comment"># 1. Make sure that you have python >= 3.9</span>
<span class="token comment"># 2. You have to run it using conda:</span>
conda create <span class="token parameter variable">-n</span> mlc-chat-venv <span class="token parameter variable">-c</span> mlc-ai <span class="token parameter variable">-c</span> conda-forge mlc-chat-nightly
conda activate mlc-chat-venv

<span class="token comment"># 3. Then install package:</span>
pip <span class="token function">install</span> <span class="token parameter variable">--pre</span> --force-reinstall mlc-ai-nightly-cu118 <span class="token punctuation">\</span>
  mlc-chat-nightly-cu118 <span class="token punctuation">\</span>
  <span class="token parameter variable">-f</span> https://mlc.ai/wheels

<span class="token comment"># 4. Download the model weights from HuggingFace and binary libraries:</span>
<span class="token function">git</span> lfs <span class="token function">install</span> <span class="token operator">&amp;&amp;</span> <span class="token function">mkdir</span> <span class="token parameter variable">-p</span> dist/prebuilt <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\</span>
  <span class="token function">git</span> clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\</span>
  <span class="token builtin class-name">cd</span> dist/prebuilt <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\</span>  
  <span class="token function">git</span> clone https://huggingface.co/huggyllama/llama-13b dist/ <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\</span>
  <span class="token builtin class-name">cd</span> <span class="token punctuation">..</span>/<span class="token punctuation">..</span>
  
  
<span class="token comment"># 5. Run server:</span>
python <span class="token parameter variable">-m</span> mlc_chat.rest --device-name cuda --artifact-path dist</pre></div></code-block><div id="e8AR81niHyNidDY7PovRHB" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>查询实例</b></span></div></div><code-block id="H2PGS1LWtgEW49RcjCV7X" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token keyword">import</span> requests

payload <span class="token operator">=</span> <span class="token punctuation">{</span>
   <span class="token string">"model"</span><span class="token punctuation">:</span> <span class="token string">"lama-30b"</span><span class="token punctuation">,</span>
   <span class="token string">"messages"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"Funniest joke ever:"</span><span class="token punctuation">}</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
   <span class="token string">"stream"</span><span class="token punctuation">:</span> <span class="token boolean">False</span>
<span class="token punctuation">}</span>
r <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span><span class="token string">"http://127.0.0.1:8000/v1/chat/completions"</span><span class="token punctuation">,</span> json<span class="token operator">=</span>payload<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>r<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'choices'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'message'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'content'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></div></code-block><h4 id="kRVhzo4W2sJgetupMmWM9c" class="wolai-block"><span class="inline-wrap">7.2 </span><span class="inline-wrap"><b>功能</b></span></h4><ul class="wolai-block"><li id="ajWg5HbmPy3cEu4xKfC3pp"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>平台本机运行时</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 可以部署在用户设备的本机环境上，这些设备可能没有现成的<span class="jill"></span>Python<span class="jill"></span>或其他必要的依赖项。应用程序开发人员只需要将<span class="jill"></span>MLC<span class="jill"></span>编译的<span class="jill"></span>LLM<span class="jill"></span>集成到他们的项目中即可；</span></li><li id="2yp47NWoNqLVrVeLVMr66T"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>内存优化</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 可以使用不同的技术编译、压缩和优化模型，从而可以部署在不同的设备上；</span></li></ul><h4 id="ajSG7sFHWpVSfQVnsDqpfe" class="wolai-block"><span class="inline-wrap">7.3</span><span class="inline-wrap"><b>优点</b></span></h4><ul class="wolai-block"><li id="3W4R13Tx3kaUQRQMQXjn1b"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>所有设置均可在<span class="jill"></span>JSON<span class="jill"></span>配置中完成</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 在单个配置文件中定义每个编译模型的运行时配置；</span></li><li id="qJ2hZyxHXqk5PyFuTJXHQt"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>预置应用程序</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 可以为不同的平台编译模型，比如<span class="jill"></span>C++<span class="jill"></span>用于命令行，JavaScript<span class="jill"></span>用于<span class="jill"></span>web，Swift<span class="jill"></span>用于<span class="jill"></span>iOS，Java/Kotlin<span class="jill"></span>用于<span class="jill"></span>Android；</span></li></ul><h4 id="8P4UCjqp2ib9o9Ez5GVRBK" class="wolai-block"><span class="inline-wrap">7.4 </span><span class="inline-wrap"><b>缺点</b></span></h4><ul class="wolai-block"><li id="gSGiH6n7GkKHS4mxWpLzFG"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>使用<span class="jill"></span>LLM<span class="jill"></span>模型的功能有限</b></span><span class="inline-wrap">：不支持适配器，无法更改精度等，该库主要用于编译不同设备的模型；</span></li><li id="2Ri5gjxA2k3Ecoz9QXL1BG"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.09720"><span class="yellow"><b>只支持分组量化</b></span></a></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 这种方法表现良好，但是在社区中更受欢迎的其他量化方法（bitsandbytes<span class="jill"></span>和<span class="jill"></span>GPTQ）不支持；</span></li><li id="nQepHRbhqZyMNJ9VW1HSq9"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>复杂的安装</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 安装需要花几个小时，不太适合初学者开发人员；</span></li></ul><div id="5ajJnxuz8xRU1qCW3nyqCw" class="wolai-block wolai-text"><div><span class="inline-wrap">如果需要在<span class="jill"></span>iOS<span class="jill"></span>或<span class="jill"></span>Android<span class="jill"></span>设备上部署应用程序，这个库正是你所需要的。它将允许您快速地以本机方式编译模型并将其部署到设备上。但是，如果需要一个高负载的服务器，不建议选择这个框架。</span></div></div><div id="eHLSL71WuXAFh3zzxvWcwH" class="wolai-block wolai-text"><div><span class="inline-wrap">参考资料：</span></div></div><ul class="wolai-block"><li id="nonkk6Ef4H4gdYEdkXzsZo"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407"><span>Frameworks for Serving LLMs.</span></a></span></li><li id="wifobWsuXCCciWaPNeWKtd"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/653352979"><span>LLM<span class="jill"></span>七种推理服务框架总结</span></a></span></li><li id="teJ5xXqAA4TxpbAGfBugCT"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/625415776"><span>目前业界大模型推理框架很多，各有什么优缺点，应该如何选择</span></a></span></li></ul></article><footer></footer></body></html></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/wdn_icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Dongnian</div><div class="author-info__description">A salty fish swimming in the sea of deep learning!</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/wdndev"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wdndev" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:dongnian.wang@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #24292e;"></i></a><a class="social-icon" href="https://blog.csdn.net/wdnshadow" target="_blank" title="CSDN"><i class="fas fa-rss" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to My Personal Blog! <br /> If Not, Please Visit <a target="_blank" rel="noopener" href="https://wdndev.gitee.io/"> <font color=#00BFFF>Gitee Mirror</font></a>.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#vnp7gSbLQv5DSGMoEWrbcT"><span class="toc-text">1.vLLM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#abeU3UyyoxNkywk9oeFkNs"><span class="toc-text">1.1 使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#jQ2txPAB2SFE4Cqb4nXXzX"><span class="toc-text">1.2 功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#my82MwCt6viksKgNugoHjW"><span class="toc-text">1.3 优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#wZsoxo7MLCrUwhdsbtMnYP"><span class="toc-text">1.4 缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dpuAjfqhN13H7oJh9JLHVW"><span class="toc-text">2.Text generation inference</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#szdeth4T2wCrq6n3bnx1xa"><span class="toc-text">2.1使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3bPrBjwJFvmEfiw9j9w2zq"><span class="toc-text">2.2功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pJMY5L4cXmA6nEHYDSi9en"><span class="toc-text">2.3 优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8KAZxiSAxC1QgLATcLzhg2"><span class="toc-text">2.4缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ez867ubL7raQYU7KdeGFWK"><span class="toc-text">3.CTranslate2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#m16b7qx9PYpSW3H3Z1gtuk"><span class="toc-text">3.1 使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#d1hQYRYCJs3WNMmyjvAomy"><span class="toc-text">3.2功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3sPKCcroqAwkHnz1bhFxdW"><span class="toc-text">3.3 优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#jojKSyj5n7RjiCHY7CfWo3"><span class="toc-text">3.4 缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tZq5VEzHeqsr9x3rBQ5Mav"><span class="toc-text">4.DeepSpeed-MII</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#8wZU9DZvgJrb2vUY6GjFnG"><span class="toc-text">4.1 使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tSsisrUtYPRWhxmggUrdAJ"><span class="toc-text">4.2 功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6MG1sFP3AUBHFGd5sKxrUw"><span class="toc-text">4.3优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8ujNGSX4CD9BJBEkP1TjyL"><span class="toc-text">4.4缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#xnTe5WqVr69bNKc5naC3KG"><span class="toc-text">5.OpenLLM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#qFzWC3q1Z5ynduDFmVVKAj"><span class="toc-text">5.1 使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nomufJiL2xAWRyegTyE5y6"><span class="toc-text">5.2 功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5baLxwJU2V5W3Jt1pY5j6Q"><span class="toc-text">5.3 优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#n6QhKvZbKBBS5m46dyBsrW"><span class="toc-text">5.4 缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#jgLacT8htd8gJ3ZXxdNTVX"><span class="toc-text">6.Ray Serve</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#284WVsbttYUsLfPf99LAzm"><span class="toc-text">6.1 使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tZ9GsSvAu2NgwW6pARhw8Z"><span class="toc-text">6.2 功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#jF992UeYybCRJuZCn6x3JX"><span class="toc-text">6.3 优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#wtRui7uQFfLVfk7qJpZTqy"><span class="toc-text">6.4 缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#oYAeAvvdNhDRdUfExrpvWH"><span class="toc-text">7.MLC LLM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#8BwLvfciiwfCHupRGBgE7w"><span class="toc-text">7.1 使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#kRVhzo4W2sJgetupMmWM9c"><span class="toc-text">7.2 功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ajSG7sFHWpVSfQVnsDqpfe"><span class="toc-text">7.3优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8P4UCjqp2ib9o9Ez5GVRBK"><span class="toc-text">7.4 缺点</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_article/9.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BALLM/" title="检索增强LLM">检索增强LLM</a><time datetime="2024-01-12T16:00:00.000Z" title="Created 2024-01-13 00:00:00">2024-01-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/6.%E6%96%87%E6%9C%AC%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/" title="LLMs公开课 - 6.文本理解和生成大模型">LLMs公开课 - 6.文本理解和生成大模型</a><time datetime="2024-01-09T16:00:00.000Z" title="Created 2024-01-10 00:00:00">2024-01-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/5.%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/" title="LLMs公开课 - 5.高效训练&amp;模型压缩">LLMs公开课 - 5.高效训练&amp;模型压缩</a><time datetime="2024-01-06T16:00:00.000Z" title="Created 2024-01-07 00:00:00">2024-01-07</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DSA/"><span class="card-category-list-name">DSA</span><span class="card-category-list-count">24</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLMs/"><span class="card-category-list-name">LLMs</span><span class="card-category-list-count">16</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/PL/"><span class="card-category-list-name">PL</span><span class="card-category-list-count">7</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/PL/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">6</span></a></li></ul></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/DSA/" style="font-size: 1.42em; color: rgb(91, 145, 17)">DSA</a><a href="/tags/RL/" style="font-size: 1.28em; color: rgb(176, 79, 19)">RL</a><a href="/tags/Transformer/" style="font-size: 1.45em; color: rgb(127, 170, 70)">Transformer</a><a href="/tags/LLMs/" style="font-size: 1.32em; color: rgb(112, 148, 55)">LLMs</a><a href="/tags/PaperReading/" style="font-size: 1.38em; color: rgb(110, 146, 60)">PaperReading</a><a href="/tags/DeepLearning/" style="font-size: 1.25em; color: rgb(90, 48, 1)">DeepLearning</a><a href="/tags/CV/" style="font-size: 1.15em; color: rgb(82, 200, 174)">CV</a><a href="/tags/GPT/" style="font-size: 1.18em; color: rgb(7, 16, 91)">GPT</a><a href="/tags/PL/" style="font-size: 1.22em; color: rgb(17, 30, 26)">PL</a><a href="/tags/leetcode/" style="font-size: 1.35em; color: rgb(106, 126, 145)">leetcode</a><a href="/tags/algo/" style="font-size: 1.15em; color: rgb(181, 144, 151)">algo</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">January 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">December 2023</span><span class="card-archive-list-count">14</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">November 2023</span><span class="card-archive-list-count">26</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">October 2023</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/09/"><span class="card-archive-list-date">September 2023</span><span class="card-archive-list-count">4</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">87</div></div><div class="webinfo-item"><div class="item-name">Run time :</div><div class="item-count" id="runtimeshow" data-publishDate="2023-05-31T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Total Count :</div><div class="item-count">411.2k</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2024-12-08T03:57:10.055Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By Dongnian</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'oe7vzWxH80qwJJjWslYTCViT-gzGzoHsz',
      appKey: 'k89nSbK0BTbmzmpQottRHvNI',
      avatar: 'monsterid',
      serverURLs: 'https://oe7vzwxh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>