<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>检索增强LLM</title>
      <link href="/llms/llms_article/9.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BALLM/"/>
      <url>/llms/llms_article/9.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BALLM/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章来源：<a href="https://mp.weixin.qq.com/s?__biz=MzA5NTQ2MDEyOA==\&amp;mid=2247484380\&amp;idx=1\&amp;sn=7b0b5dc3f76dd7a634ebb77df8697a24\&amp;chksm=90be4d93a7c9c485593b6a299d607bfbcc30f05ec691b85f1fb6cf81c51ffe863dbc34759be6\&amp;mpshare=1\&amp;scene=1\&amp;srcid=1204gaSWi0sA7clI6UZEYYL5\&amp;sharer_shareinfo=f728c72f50e0aee521fb1319eb3b82b0\&amp;sharer_shareinfo_first=f728c72f50e0aee521fb1319eb3b82b0#rd" title="万字长文: 检索增强 LLM (qq.com)">万字长文: 检索增强 LLM (qq.com)</a></p></blockquote><p>ChatGPT 的出现，让我们看到了大语言模型 ( Large Language Model, LLM ) 在语言和代码理解、人类指令遵循、基本推理等多方面的能力，但幻觉问题 <a href="https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/" title="Hallucinations"><strong>Hallucinations</strong></a> 仍然是当前大语言模型面临的一个重要挑战。简单来说，<strong>幻觉问题是指 LLM 生成不正确、荒谬或者与事实不符的结果</strong>。此外，<strong>数据新鲜度 ( Data Freshness ) </strong>也是 LLM 在生成结果时出现的另外一个问题，即 LLM 对于一些时效性比较强的问题可能给不出或者给出过时的答案。而通过检索外部相关信息的方式来增强 LLM 的生成结果是当前解决以上问题的一种流行方案，这里把这种方案称为 <strong>检索增强 LLM</strong> ( Retrieval Augmented LLM )，有时候也被称为 检索增强生成 ( Retrieval Augmented Generation, RAG )。&#x20;</p><p>这篇长文将对检索增强 LLM 的方案进行一个相对全面的介绍。主要内容包括：</p><ul><li>检索增强 LLM 的概念介绍、重要性及其解决的问题</li><li>检索增强 LLM 的关键模块及其实现方法</li><li>检索增强 LLM 的一些案例分析和应用</li></ul><h1 id="1-RAG基本概念"><a href="#1-RAG基本概念" class="headerlink" title="1.RAG基本概念"></a>1.RAG基本概念</h1><h2 id="1-1-什么是检索增强-LLM"><a href="#1-1-什么是检索增强-LLM" class="headerlink" title="1.1 什么是检索增强 LLM"></a>1.1 什么是检索增强 LLM</h2><p><strong>检索增强 LLM ( Retrieval Augmented LLM )</strong>，简单来说，<strong>就是给 LLM 提供外部数据库，对于用户问题 ( Query )，通过一些信息检索 ( Information Retrieval, IR ) 的技术，先从外部数据库中检索出和用户问题相关的信息，然后让 LLM 结合这些相关信息来生成结果</strong>。下图是一个检索增强 LLM 的简单示意图。</p><p><img src="image/lr3r0h6wjf_jKUSOHnlR7.png" alt=""></p><p>OpenAI 研究科学家 Andrej Karpathy 前段时间在微软 Build 2023 大会上做过一场关于 GPT 模型现状的分享 <a href="https://www.youtube.com/watch?v=bZQun8Y4L2A\&amp;ab_channel=MicrosoftDeveloper" title="State of GPT">State of GPT</a>，这场演讲前半部分分享了 ChatGPT 这类模型是如何一步一步训练的，后半部分主要分享了 LLM 模型的一些应用方向，其中就对检索增强 LLM 这个应用方向做了简单介绍。下面这张图就是 Andrej 分享中关于这个方向的介绍。</p><p><img src="image/itxqktryzo_DYwXlmZ3eU.jpeg" alt=""></p><p>传统的信息检索工具，比如 Google/Bing 这样的搜索引擎，只有检索能力 ( <strong>Retrieval-only</strong> )，现在 LLM 通过预训练过程，将海量数据和知识嵌入到其巨大的模型参数中，具有记忆能力 ( <strong>Memory-only</strong> )。从这个角度看，检索增强 LLM 处于中间，将 LLM 和传统的信息检索相结合，通过一些信息检索技术将相关信息加载到 LLM 的工作内存 ( <strong>Working Memory</strong> ) 中，即 LLM 的上下文窗口 ( <strong>Context Window</strong> )，亦即 LLM 单次生成时能接受的最大文本输入。</p><p>不仅 Andrej 的分享中提到基于检索来增强 LLM 这一应用方式，从一些著名投资机构针对 AI 初创企业技术栈的调研和总结中，也可以看到基于检索来增强 LLM 技术的广泛应用。比如今年6月份红杉资本发布了一篇关于大语言模型技术栈的文章 <a href="https://www.sequoiacap.com/article/llm-stack-perspective/" title="The New Language Model Stack"><strong>The New Language Model Stack</strong></a>，其中就给出了一份对其投资的33家 AI 初创企业进行的问卷调查结果，下图的调查结果显示有 88% 左右的创业者表示在自己的产品中有使用到基于检索增强 LLM 技术。</p><p><img src="image/lnxah_g3hd_xfRJIDvjhU.png" alt=""></p><p>无独有偶，美国著名风险投资机构 A16Z 在今年6月份也发表了一篇介绍当前 LLM 应用架构的总结文章 <a href="https://a16z.com/emerging-architectures-for-llm-applications/" title="Emerging Architectures for LLM Applications"><strong>Emerging Architectures for LLM Applications</strong></a>，下图就是文章中总结的当前 LLM 应用的典型架构，其中最上面 <strong>Contextual Data</strong> 引入 LLM 的方式就是一种通过检索来增强 LLM 的思路。</p><p><img src="image/v0f4orzl_h_i1A38f1KVr.png" alt=""></p><h2 id="1-2-检索增强-LLM-解决的问题"><a href="#1-2-检索增强-LLM-解决的问题" class="headerlink" title="1.2 检索增强 LLM 解决的问题"></a>1.2 检索增强 LLM 解决的问题</h2><p>为什么要结合传统的信息检索系统来增强 LLM ？换句话说，基于检索增强的 LLM 主要解决的问题是什么？这部分内容参考自普林斯顿大学陈丹琦小组之前在 ACL 2023 大会上关于基于检索的语言模型的分享 ACL 2023 Tutorial: Retrieval-based Language Models and Applications</p><h3 id="（1）长尾知识"><a href="#（1）长尾知识" class="headerlink" title="（1）长尾知识"></a>（1）长尾知识</h3><p>虽然当前 LLM 的训练数据量已经非常庞大，动辄几百 GB 级别的数据量，万亿级别的标记数量 ( Token )，比如 GPT-3 的预训练数据使用了3000 亿量级的标记，LLaMA 使用了 1.4 万亿量级的标记。训练数据的来源也十分丰富，比如维基百科、书籍、论坛、代码等，LLM 的模型参数量也十分巨大，从几十亿、百亿到千亿量级，但让 LLM 在有限的参数中记住所有知识或者信息是不现实的，训练数据的涵盖范围也是有限的，总会有一些长尾知识在训练数据中不能覆盖到。</p><p><strong>对于一些相对通用和大众的知识，LLM 通常能生成比较准确的结果，而对于一些长尾知识</strong>，LLM 生成的回复通常并不可靠。ICML 会议上的这篇论文 <a href="https://arxiv.org/abs/2211.08411" title="Large Language Models Struggle to Learn Long-Tail Knowledge">Large Language Models Struggle to Learn Long-Tail Knowledge</a>，就研究了 LLM 对基于事实的问答的准确性和预训练数据中相关领域文档数量的关系，发现有很强的相关性，即<strong>预训练数据中相关文档数量越多，LLM 对事实性问答的回复准确性就越高</strong>。从这个研究中可以得出一个简单的结论 ——<strong> LLM 对长尾知识的学习能力比较弱</strong>。下面这张图就是论文中绘制的相关性曲线。</p><p><img src="image/vwb__1luhn_yItjsNxrSZ.png" alt=""></p><p>为了提升 LLM 对长尾知识的学习能力，容易想到的是<strong>在训练数据加入更多的相关长尾知识，或者增大模型的参数量</strong>，虽然这两种方法确实都有一定的效果，上面提到的论文中也有实验数据支撑，但这<strong>两种方法是不经济的</strong>，即需要一个很大的训练数据量级和模型参数才能大幅度提升 LLM 对长尾知识的回复准确性。而通<strong>过检索的方法把相关信息在 LLM 推断时作为上下文 ( Context ) 给出</strong>，既能达到一个比较好的回复准确性，也是一种<strong>比较经济的方式</strong>。下面这张图就是提供相关信息的情况下，不同大小模型的回复准确性，对比上一张图，可以看到对于同一参数量级的模型，在提供少量相关文档参与预训练的情况下，让模型在推断阶段利用相关信息，其回复准确性有了大幅提升。</p><p><img src="image/0_at8gi833_NCahb9PQK7.png" alt=""></p><h3 id="（2）私有数据"><a href="#（2）私有数据" class="headerlink" title="（2）私有数据"></a>（2）私有数据</h3><p>ChatGPT 这类通用的 LLM 预训练阶段利用的大部分都是公开的数据，<strong>不包含私有数据，因此对于一些私有领域知识是欠缺的</strong>。比如问 ChatGPT 某个企业内部相关的知识，ChatGPT 大概率是不知道或者胡编乱造。虽然可以在预训练阶段加入私有数据或者利用私有数据进行微调，但训练和迭代成本很高。此外，有研究和实践表明，<strong>通过一些特定的攻击手法，可以让 LLM 泄漏训练数据，如果训练数据中包含一些私有信息，就很可能会发生隐私信息泄露</strong>。比如这篇论文 <a href="https://arxiv.org/abs/2012.07805" title="Extracting Training Data from Large Language Models">Extracting Training Data from Large Language Models</a> 的研究者们就通过构造的 Query 从 <strong>GPT-2</strong> 模型中提取出了个人公开的姓名、邮箱、电话号码和地址信息等，即使这些信息可能只在训练数据中出现一次。文章还发现，较大规模的模型比较小规模的更容易受到攻击。</p><p><img src="image/09jzx3c10e_-l4xmNesLo.png" alt=""></p><p><strong>如果把私有数据作为一个外部数据库，让 LLM 在回答基于私有数据的问题时，直接从外部数据库中检索出相关信息，再结合检索出的相关信息进行回答</strong>。这样就不用通过预训练或者微调的方法让 LLM 在参数中记住私有知识，既节省了训练或者微调成本，也一定程度上避免了私有数据的泄露风险。</p><h3 id="（3）数据新鲜度"><a href="#（3）数据新鲜度" class="headerlink" title="（3）数据新鲜度"></a>（3）数据新鲜度</h3><p>由于 LLM 中学习的知识来自于训练数据，虽然大部分知识的更新周期不会很快，但依然会有一些知识或者信息更新得很频繁。<strong>LLM 通过从预训练数据中学到的这部分信息就很容易过时</strong>。比如 GPT-4 模型使用的是截止到 2021-09 的预训练数据，因此涉及这个日期之后的事件或者信息，它会拒绝回答或者给出的回复是过时或者不准确的。下面这个示例是问 GPT-4 当前推特的 CEO 是谁，GPT-4 给出的回复还是 Jack Dorsey，并且自己会提醒说回复可能已经过时了。</p><p><img src="image/kl782vhbz9_VKUVE7-LA7.png" alt=""></p><p>如果<strong>把频繁更新的知识作为外部数据库，供 LLM 在必要的时候进行检索，就可以实现在不重新训练 LLM 的情况下对 LLM 的知识进行更新和拓展，从而解决 LLM 数据新鲜度的问题</strong>。</p><h3 id="（4）来源验证和可解释性"><a href="#（4）来源验证和可解释性" class="headerlink" title="（4）来源验证和可解释性"></a>（4）来源验证和可解释性</h3><p>通常情况下，LLM 生成的输出不会给出其来源，比较难解释为什么会这么生成。而<strong>通过给 LLM 提供外部数据源，让其基于检索出的相关信息进行生成，就在生成的结果和信息来源之间建立了关联，因此生成的结果就可以追溯参考来源，可解释性和可控性就大大增强</strong>。即可以知道 LLM 是基于什么相关信息来生成的回复。Bing Chat 就是利用检索来增强 LLM 输出的典型产品，下图展示的就是 Bing Chat 的产品截图，可以看到其生成的回复中会给出相关信息的链接。</p><p><img src="image/m010m6_j_w_wOGzQZgDDh.png" alt=""></p><p>利用检索来增强 LLM 的输出，其中很重要的一步是通过一些检索相关的技术从外部数据中找出相关信息片段，然后把相关信息片段作为上下文供 LLM 在生成回复时参考。有人可能会说，随着 LLM 的上下文窗口 ( <strong>Context Window</strong> ) 越来越长，检索相关信息的步骤是不是就没有必要了，直接在上下文中提供尽可能多的信息。比如 GPT-4 模型当前接收的最大上下文长度是 32K， Claude 模型最大允许 <a href="https://www.anthropic.com/index/100k-context-windows" title="100K">100K</a> 的上下文长度。</p><p>虽然 LLM 的上下文窗口越来越大，但检索相关信息的步骤仍然是重要且必要的。一方面当前 <strong>LLM 的网络架构决定了其上下文窗口的长度是会有上限的</strong>，不会无限增长。另外看似很大的上下文窗口，能容纳的信息其实比较有限，比如 32K 的长度可能仅仅相当于一篇大学毕业论文的长度。另一方面，有研究表明，<strong>提供少量更相关的信息，相比于提供大量不加过滤的信息，LLM 回复的准确性会更高</strong>。比如斯坦福大学的这篇论文 <a href="https://arxiv.org/pdf/2307.03172.pdf" title="Lost in the Middle">Lost in the Middle</a> 就给出了下面的实验结果，可以看到 LLM 回复的准确性随着上下文窗口中提供的文档数量增多而下降。</p><p><img src="image/oo62b87hhs_XMk2he42vo.png" alt=""></p><p><strong>利用检索技术从大量外部数据中找出与输入问题最相关的信息片段，在为 LLM 生成回复提供参考的同时，也一定程度上过滤掉一些非相关信息的干扰，便于提高生成回复的准确性</strong>。此外，上下文窗口越大，推理成本越高。所以相关信息检索步骤的引入也能降低不必要的推理成本。</p><h1 id="2-关键模块"><a href="#2-关键模块" class="headerlink" title="2.关键模块"></a>2.关键模块</h1><p>为了构建检索增强 LLM 系统，需要实现的关键模块和解决的问题包括:</p><ul><li><strong>数据和索引模块</strong>：如何处理外部数据和构建索引</li><li><strong>查询和检索模块</strong>：如何准确高效地检索出相关信息</li><li><strong>响应生成模块</strong>：如何利用检索出的相关信息来增强 LLM 的输出</li></ul><h2 id="2-1-数据和索引模块"><a href="#2-1-数据和索引模块" class="headerlink" title="2.1 数据和索引模块"></a>2.1 数据和索引模块</h2><h3 id="（1）数据获取"><a href="#（1）数据获取" class="headerlink" title="（1）数据获取"></a>（1）数据获取</h3><p>数据获取模块的作用一般是<strong>将多种来源、多种类型和格式的外部数据转换成一个统一的文档对象</strong> ( Document Object )，便于后续流程的处理和使用。文档对象除了包含原始的文本内容，一般还会携带文档的<strong>元信息 ( Metadata )</strong>，<strong>可以用于后期的检索和过滤</strong>。元信息包括但不限于：</p><ul><li>时间信息，比如文档创建和修改时间</li><li>标题、关键词、实体(人物、地点等)、文本类别等信息</li><li>文本总结和摘要</li></ul><p><strong>有些元信息可以直接获取，有些则可以借助 NLP 技术</strong>，比如关键词抽取、实体识别、文本分类、文本摘要等。既可以采用传统的 NLP 模型和框架，也可以基于 LLM 实现。</p><p><img src="image/paomc13g0j_Z3yCu2BQdq.png" alt=""></p><p>外部数据的来源可能是多种多样的，比如可能来自</p><ul><li>Google 套件里各种 Doc 文档、Sheet 表格、Slides 演示、Calendar 日程、Drive 文件等</li><li>Slack、Discord 等聊天社区的数据</li><li>Github、Gitlab 上托管的代码文件</li><li>Confluence 上各种文档</li><li>Web 网页的数据</li><li>API 返回的数据</li><li>本地文件</li></ul><p>外部数据的类型和文件格式也可能是多样化的，比如</p><ul><li>从数据类型来看，包括纯文本、表格、演示文档、代码等</li><li>从文件存储格式来看，包括 txt、csv、pdf、markdown、json 等格式</li></ul><p>外部数据可能是多语种的，比如中文、英文、德文、日文等。除此之外，还可能是多模态的，除了上面讨论的文本模态，还包括图片、音频、视频等多种模态。不过这篇文章中讨论的外部数据将限定在文本模态。</p><p>在构建数据获取模块时，不同来源、类型、格式、语种的数据可能都需要采用不同的读取方式。</p><h3 id="（2）文本分块"><a href="#（2）文本分块" class="headerlink" title="（2）文本分块"></a>（2）文本分块</h3><p>文本分块是<strong>将长文本切分成小片段的过程</strong>，比如将一篇长文章切分成一个个相对短的段落。那么为什么要进行文本分块？一方面<strong>当前 LLM 的上下文长度是有限制的</strong>，直接把一篇长文全部作为相关信息放到 LLM 的上下文窗口中，可能会超过长度限制。另一方面，对于长文本来说，即使其和查询的问题相关，但<strong>一般不会通篇都是完全相关的</strong>，而分块能一定程度上剔除不相关的内容，<strong>为后续的回复生成过滤一些不必要的噪声</strong>。</p><p><strong>文本分块的好坏将很大程度上影响后续回复生成的效果，切分得不好，内容之间的关联性会被切断。因此设计一个好的分块策略十分重要</strong>。分块策略包括具体的切分方法 ( 比如是按句子切分还是段落切分 )，块的大小设为多少合适，不同的块之间是否允许重叠等。Pinecone 的这篇博客 <a href="https://www.pinecone.io/learn/chunking-strategies/" title="Chunking Strategies for LLM Applications">Chunking Strategies for LLM Applications</a> 中就给出了一些在设计分块策略时需要考虑的因素。</p><ul><li><strong>原始内容的特点</strong>：原始内容是长文 ( 博客文章、书籍等 ) 还是短文 ( 推文、即时消息等 )，是什么格式 ( HTML、Markdown、Code 还是 LaTeX 等 )，不同的内容特点可能会适用不同的分块策略；</li><li><strong>后续使用的索引方法</strong>：目前最常用的索引是对分块后的内容进行向量索引，那么不同的向量嵌入模型可能有其适用的分块大小，比如 <strong>sentence-transformer</strong> 模型比较适合对句子级别的内容进行嵌入，OpenAI 的 <strong>text-embedding-ada-002</strong> 模型比较适合的分块大小在 256~512 个标记数量；</li><li><strong>问题的长度</strong>：问题的长度需要考虑，因为需要基于问题去检索出相关的文本片段；</li><li><strong>检索出的相关内容在回复生成阶段的使用方法</strong>：如果是直接把检索出的相关内容作为 Prompt 的一部分提供给 LLM，那么 LLM 的输入长度限制在设计分块大小时就需要考虑。</li></ul><h4 id="分块实现方法"><a href="#分块实现方法" class="headerlink" title="分块实现方法"></a>分块实现方法</h4><p>那么文本分块具体如何实现？一般来说，实现文本分块的整体流程如下:</p><ol><li>将原始的长文本切分成小的语义单元，这里的语义单元通常是句子级别或者段落级别；</li><li>将这些小的语义单元融合成更大的块，直到达到设定的块大小 ( Chunk Size )，就将该块作为独立的文本片段；</li><li>迭代构建下一个文本片段，一般相邻的文本片段之间会设置重叠，以保持语义的连贯性。</li></ol><p>那如何把原始的长文本切分成小的语义单元? 最常用的是基于分割符进行切分，比如句号 ( <code>.</code>)、换行符 ( <code>\n</code> )、空格等。除了可以利用单个分割符进行简单切分，还可以定义一组分割符进行迭代切分，比如定义 <code>[&quot;\n\n&quot;, &quot;\n&quot;, &quot; &quot;, &quot;&quot;]</code> 这样一组分隔符，切分的时候先利用第一个分割符进行切分 ( 实现类似按段落切分的效果 )，第一次切分完成后，对于超过预设大小的块，继续使用后面的分割符进行切分，依此类推。这种切分方法能比较好地保持原始文本的层次结构。</p><p>对于一些结构化的文本，比如代码，Markdown，LaTeX 等文本，在进行切分的时候可能需要单独进行考虑:</p><ul><li>比如 Python 代码文件，分割符中可能就需要加入类似 <code>\nclass</code>，<code>\ndef</code> 这种来保证类和函数代码块的完整性；</li><li>比如 Markdown 文件，是通过不同层级的 Header 进行组织的，即不同数量的 # 符号，在切分时就可以通过使用特定的分割符来维持这种层级结构。</li></ul><p><strong>文本块大小的设定也是分块策略需要考虑的重要因素</strong>，太大或者太小都会影响最终回复生成的效果。文本块大小的计算方法，最常用的可以直接<strong>基于字符数进行统计 ( Character-level )</strong>，也可以<strong>基于标记数进行统计 ( Token-level )</strong>。至于如何确定合适的分块大小，这个因场景而异，很难有一个统一的标准，可以通过评估不同分块大小的效果来进行选择。</p><p>上面提到的一些分块方法在 <a href="https://python.langchain.com/docs/modules/data_connection/document_transformers/" title="LangChain">LangChain</a> 中都有相应的实现。比如下面的代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter, Language</span><br><span class="line"></span><br><span class="line"><span class="comment"># text split</span></span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">    <span class="comment"># Set a really small chunk size, just to show.</span></span><br><span class="line">    chunk_size = <span class="number">100</span>,</span><br><span class="line">    chunk_overlap  = <span class="number">20</span>,</span><br><span class="line">    length_function = <span class="built_in">len</span>,</span><br><span class="line">    add_start_index = <span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># code split</span></span><br><span class="line">python_splitter = RecursiveCharacterTextSplitter.from_language(</span><br><span class="line">            language=Language.PYTHON, </span><br><span class="line">            chunk_size=<span class="number">50</span>, </span><br><span class="line">            chunk_overlap=<span class="number">0</span>  </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># markdown split</span></span><br><span class="line">md_splitter = RecursiveCharacterTextSplitter.from_language(  </span><br><span class="line">            language=Language.MARKDOWN, </span><br><span class="line">            chunk_size=<span class="number">60</span>, </span><br><span class="line">            chunk_overlap=<span class="number">0</span>  </span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="（3）数据索引"><a href="#（3）数据索引" class="headerlink" title="（3）数据索引"></a>（3）数据索引</h3><p>经过前面的数据读取和文本分块操作后，接着就需要对处理好的数据进行索引。<strong>索引是一种数据结构，用于快速检索出与用户查询相关的文本内容</strong>。它是检索增强 LLM 的核心基础组件之一。</p><p>下面介绍几种常见的索引结构。为了说明不同的索引结构，引入节点(Node)的概念。在这里，节点就是前面步骤中对文档切分后生成的文本块(Chunk)。下面的索引结构图来自 LlamaIndex 的文档<a href="https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/index/index_guide.html" title="How Each Index Works">How Each Index Works</a>。</p><h4 id="1）链式索引"><a href="#1）链式索引" class="headerlink" title="1）链式索引"></a>1）链式索引</h4><p>链式索引<strong>通过链表的结构对文本块进行顺序索引</strong>。在后续的检索和生成阶段，可以简单地顺序遍历所有节点，也可以基于关键词进行过滤。</p><p><img src="image/_duij-g5vy_05Oc6DaHcB.webp" alt=""></p><p><img src="image/j6qrc_7jq1_757nASmPoB.webp" alt=""></p><p><img src="image/xsedk--wv0_wkOzAA_t4a.webp" alt=""></p><h4 id="2）树索引"><a href="#2）树索引" class="headerlink" title="2）树索引"></a>2）树索引</h4><p>树索引<strong>将一组节点 ( 文本块 ) 构建成具有层级的树状索引结构</strong>，其从叶节点 (原始文本块) 向上构建，<strong>每个父节点都是子节点的摘要</strong>。在检索阶段，既可以从根节点向下进行遍历，也可以直接利用根节点的信息。<strong>树索引提供了一种更高效地查询长文本块的方式，它还可以用于从文本的不同部分提取信息</strong>。与链式索引不同，树索引无需按顺序查询。</p><p><img src="image/z37i04np4y_nAtkMWj55Z.webp" alt=""></p><p><img src="image/rx_3_v6bga__UGJ5JmLRY.png" alt=""></p><h4 id="3）关键词表索引"><a href="#3）关键词表索引" class="headerlink" title="3）关键词表索引"></a>3）关键词表索引</h4><p>关键词表索引<strong>从每个节点中提取关键词，构建了每个关键词到相应节点的多对多映射，意味着每个关键词可能指向多个节点，每个节点也可能包含多个关键词</strong>。在检索阶段，可以基于用户查询中的关键词对节点进行筛选。</p><p><img src="image/36wmybb209_p1QxEE3TfW.webp" alt=""></p><p><img src="image/-s3r95515f_8Kef2oNxBn.webp" alt=""></p><h4 id="4）向量索引"><a href="#4）向量索引" class="headerlink" title="4）向量索引"></a>4）向量索引</h4><p>向量索引是<strong>当前最流行的一种索引方法</strong>。这种方法一般利用<strong>文本嵌入模型</strong> ( Text Embedding Model ) 将文本块映射成一个固定长度的向量，然后存储在<strong>向量数据库</strong>中。检索的时候，对用户查询文本采用同样的文本嵌入模型映射成向量，然后基于向量相似度计算获取最相似的一个或者多个节点。</p><p><img src="image/hvtl-j3m-w_G0N6aLK_DS.webp" alt=""></p><p><img src="image/i86iadgfwk_EvF_HDxcns.webp" alt=""></p><p>上面的表述中涉及到向量索引和检索中三个重要的概念: <strong>文本嵌入模型</strong>、<strong>相似向量检索</strong>和<strong>向量数据库</strong>。下面一一进行详细说明。</p><h5 id="文本嵌入模型"><a href="#文本嵌入模型" class="headerlink" title="文本嵌入模型"></a>文本嵌入模型</h5><p>文本嵌入模型 ( Text Embedding Model ) 将非结构化的文本转换成结构化的向量 ( Vector )，目前常用的是学习得到的<strong>稠密向量</strong>。</p><p><img src="image/0nt5dmo6g-_-wqyyXtkom.svg" alt=""></p><p>当前有很多文本嵌入模型可供选择，比如</p><ul><li>早期的 Word2Vec、GloVe 模型等，目前很少用。</li><li>基于孪生 BERT 网络预训练得到的 <a href="https://arxiv.org/abs/1908.10084" title="Sentence Transformers">Sentence Transformers</a> 模型，对句子的嵌入效果比较好</li><li>OpenAI 提供的 <a href="https://openai.com/blog/new-and-improved-embedding-model" title="text-embedding-ada-002">text-embedding-ada-002</a> 模型，嵌入效果表现不错，且可以处理最大 8191 标记长度的文本</li><li><a href="https://instructor-embedding.github.io/" title="Instructor">Instructor</a> 模型，这是一个经过指令微调的文本嵌入模型，可以根据任务(例如分类、检索、聚类、文本评估等)和领域(例如科学、金融等)，提供任务指令而生成相对定制化的文本嵌入向量，无需进行任何微调</li><li><a href="https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md" title="BGE">BGE</a>模型: 由智源研究院开源的中英文语义向量模型，目前在MTEB中英文榜单都排在第一位。</li></ul><p>下面就是评估文本嵌入模型效果的榜单 <a href="https://huggingface.co/spaces/mteb/leaderboard" title="MTEB Leaderboard">MTEB Leaderboard</a> (截止到 2023-08-18 )。值得说明的是，这些现成的文本嵌入模型没有针对特定的下游任务进行微调，所以不一定在下游任务上有足够好的表现。最好的方式一般是在下游特定的数据上重新训练或者微调自己的文本嵌入模型。</p><p><img src="image/knuloe5g5l_tCORu5NXuS.png" alt=""></p><h5 id="相似向量检索"><a href="#相似向量检索" class="headerlink" title="相似向量检索"></a>相似向量检索</h5><p>相似向量检索要解决的问题是给定一个查询向量，如何从候选向量中准确且高效地检索出与其相似的一个或多个向量。首先是<strong>相似性度量</strong>方法的选择，可以采用余弦相似度、点积、欧式距离、汉明距离等，通常情况下可以直接使用<strong>余弦相似度</strong>。其次是相似性检索算法和实现方法的选择，候选向量的数量量级、检索速度和准确性的要求、内存的限制等都是需要考虑的因素。</p><p>当候选向量的数量比较少时，比如只有几万个向量，那么 Numpy 库就可以实现相似向量检索，实现简单，准确性高，速度也很快。国外有个博主做了个简单的基准测试发现 <a href="https://www.ethanrosenthal.com/2023/04/10/nn-vs-ann/" title="Do you actually need a vector database">Do you actually need a vector database</a> ，当候选向量数量在 10 万量级以下时，通过对比 Numpy 和另一种高效的近似最近邻检索实现库 <a href="https://github.com/nmslib/hnswlib" title="Hnswlib">Hnswlib</a> ，发现在检索效率上并没有数量级的差异，但 Numpy 的实现过程更简单。</p><p><img src="image/96q18_1xbq_JktZEFKFwV.png" alt=""></p><p>下面就是使用 Numpy 的一种简单实现代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># candidate_vecs: 2D numpy array of shape N x D</span></span><br><span class="line"><span class="comment"># query_vec: 1D numpy array of shape D</span></span><br><span class="line"><span class="comment"># k: number of top k similar vectors</span></span><br><span class="line"></span><br><span class="line">sim_scores = np.dot(candidate_vecs, query_vec)</span><br><span class="line">topk_indices = np.argsort(sim_scores)[::-<span class="number">1</span>][:k]</span><br><span class="line">topk_values = sim_scores[topk_indices]</span><br></pre></td></tr></table></figure><p>对于大规模向量的相似性检索，使用 Numpy 库就不合适，需要使用更高效的实现方案。Facebook团队开源的 <a href="https://github.com/facebookresearch/faiss" title="Faiss">Faiss</a> 就是一个很好的选择。Faiss 是一个用于高效相似性搜索和向量聚类的库，它实现了在任意大小的向量集合中进行搜索的很多算法，除了可以在CPU上运行，有些算法也支持GPU加速。Faiss 包含多种相似性检索算法，具体使用哪种算法需要综合考虑数据量、检索频率、准确性和检索速度等因素。</p><p>Pinecone 的这篇博客 <a href="https://www.pinecone.io/learn/series/faiss/vector-indexes/" title="Nearest Neighbor Indexes for Similarity Search">Nearest Neighbor Indexes for Similarity Search</a> 对 Faiss 中常用的几种索引进行了详细介绍，下图是几种索引在不同维度下的定性对比:</p><p><img src="image/xi5bi0eqmz_L-AQ2yyJ6K.png" alt=""></p><h5 id="向量数据库"><a href="#向量数据库" class="headerlink" title="向量数据库"></a>向量数据库</h5><p>上面提到的基于 Numpy 和 Faiss 实现的向量相似检索方案，如果应用到实际产品中，可能还缺少一些功能，比如：</p><ul><li>数据托管和备份</li><li>数据管理，比如数据的插入、删除和更新</li><li>向量对应的原始数据和元数据的存储</li><li>可扩展性，包括垂直和水平扩展</li></ul><p>所以<strong>向量数据库</strong>应运而生。简单来说，<strong>向量数据库是一种专门用于存储、管理和查询向量数据的数据库，可以实现向量数据的相似检索、聚类等</strong>。目前比较流行的向量数据库有 <a href="https://www.pinecone.io/" title="Pinecone">Pinecone</a>、<a href="https://vespa.ai/" title="Vespa">Vespa</a>、<a href="https://weaviate.io/" title="Weaviate">Weaviate</a>、<a href="https://milvus.io/" title="Milvus">Milvus</a>、<a href="https://www.trychroma.com/" title="Chroma">Chroma</a> 、<a href="https://cloud.tencent.com/product/vdb" title="Tencent Cloud VectorDB">Tencent Cloud VectorDB</a>等，大部分都提供开源产品。</p><p>Pinecone 的这篇博客 <a href="https://www.pinecone.io/learn/vector-database/" title="What is a Vector Database">What is a Vector Database</a> 就对向量数据库的相关原理和组成进行了比较系统的介绍，下面这张图就是文章中给出的一个向量数据库常见的数据处理流程:</p><p><img src="image/fprd8zqxkt_4ZF8ZX7Qp-.png" alt=""></p><ol><li><strong>索引</strong>: 使用乘积量化 ( Product Quantization ) 、局部敏感哈希 ( LSH )、HNSW 等算法对向量进行索引，这一步将向量映射到一个数据结构，以实现更快的搜索。</li><li><strong>查询</strong>: 将查询向量和索引向量进行比较，以找到最近邻的相似向量。</li><li><strong>后处理</strong>: 有些情况下，向量数据库检索出最近邻向量后，对其进行后处理后再返回最终结果。</li></ol><p>向量数据库的使用比较简单，下面是使用 Python 操作 Pinecone 向量数据库的示例代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># install python pinecone client</span></span><br><span class="line"><span class="comment"># pip install pinecone-client</span></span><br><span class="line"><span class="keyword">import</span> pinecone </span><br><span class="line"><span class="comment"># initialize pinecone client</span></span><br><span class="line">pinecone.init(api_key=<span class="string">&quot;YOUR_API_KEY&quot;</span>, environment=<span class="string">&quot;YOUR_ENVIRONMENT&quot;</span>)</span><br><span class="line"><span class="comment"># create index </span></span><br><span class="line">pinecone.create_index(<span class="string">&quot;quickstart&quot;</span>, dimension=<span class="number">8</span>, metric=<span class="string">&quot;euclidean&quot;</span>)</span><br><span class="line"><span class="comment"># connect to the index</span></span><br><span class="line">index = pinecone.Index(<span class="string">&quot;quickstart&quot;</span>)</span><br><span class="line"><span class="comment"># Upsert sample data (5 8-dimensional vectors) </span></span><br><span class="line">index.upsert([ </span><br><span class="line">        (<span class="string">&quot;A&quot;</span>, [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]), </span><br><span class="line">        (<span class="string">&quot;B&quot;</span>, [<span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>]), </span><br><span class="line">        (<span class="string">&quot;C&quot;</span>, [<span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>]), </span><br><span class="line">        (<span class="string">&quot;D&quot;</span>, [<span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>]), </span><br><span class="line">        (<span class="string">&quot;E&quot;</span>, [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]) </span><br><span class="line">      ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># query</span></span><br><span class="line">index.query( </span><br><span class="line">      vector=[<span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>], </span><br><span class="line">      top_k=<span class="number">3</span>, </span><br><span class="line">      include_values=<span class="literal">True</span> </span><br><span class="line">      ) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Returns: </span></span><br><span class="line"><span class="comment"># &#123;&#x27;matches&#x27;: [&#123;&#x27;id&#x27;: &#x27;C&#x27;, </span></span><br><span class="line"><span class="comment">#               &#x27;score&#x27;: 0.0, </span></span><br><span class="line"><span class="comment">#               &#x27;values&#x27;: [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]&#125;, </span></span><br><span class="line"><span class="comment">#              &#123;&#x27;id&#x27;: &#x27;D&#x27;, </span></span><br><span class="line"><span class="comment">#               &#x27;score&#x27;: 0.0799999237, </span></span><br><span class="line"><span class="comment">#               &#x27;values&#x27;: [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]&#125;, </span></span><br><span class="line"><span class="comment">#              &#123;&#x27;id&#x27;: &#x27;B&#x27;, </span></span><br><span class="line"><span class="comment">#               &#x27;score&#x27;: 0.0800000429, </span></span><br><span class="line"><span class="comment">#               &#x27;values&#x27;: [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]&#125;], </span></span><br><span class="line"><span class="comment"># &#x27;namespace&#x27;: &#x27;&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># delete index </span></span><br><span class="line">pinecone.delete_index(<span class="string">&quot;quickstart&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="2-2查询和检索模块"><a href="#2-2查询和检索模块" class="headerlink" title="2.2查询和检索模块"></a>2.2查询和检索模块</h2><h3 id="（1）查询变换"><a href="#（1）查询变换" class="headerlink" title="（1）查询变换"></a>（1）查询变换</h3><p>查询文本的表达方法直接影响着检索结果，微小的文本改动都可能会得到天差万别的结果。直接用原始的查询文本进行检索在很多时候可能是简单有效的，但有时候可能需要对查询文本进行一些变换，以得到更好的检索结果，从而更可能在后续生成更好的回复结果。下面列出几种常见的查询变换方式。</p><h4 id="1）变换一-同义改写"><a href="#1）变换一-同义改写" class="headerlink" title="1）变换一: 同义改写"></a>1）变换一: 同义改写</h4><p>将原始查询改写成相同语义下不同的表达方式，改写工作可以调用 LLM 完成。比如对于这样一个原始查询:  <code>What are the approaches to Task Decomposition?</code>，可以改写成下面几种同义表达:</p><blockquote><p>How can Task Decomposition be approached?<br>What are the different methods for Task Decomposition?<br>What are the various approaches to decomposing tasks?</p></blockquote><p>对于每种查询表达，分别检索出一组相关文档，然后对所有检索结果进行去重合并，从而得到一个更大的候选相关文档集合。通过将同一个查询改写成多个同义查询，能够克服单一查询的局限，获得更丰富的检索结果集合。</p><h4 id="2）变换二-查询分解"><a href="#2）变换二-查询分解" class="headerlink" title="2）变换二: 查询分解"></a>2）变换二: 查询分解</h4><p>有相关研究表明 ( <a href="https://ofir.io/self-ask.pdf" title="self-ask">self-ask</a>，<a href="https://arxiv.org/abs/2210.03629" title="ReAct">ReAct</a> )，LLM 在回答复杂问题时，如果将复杂问题分解成相对简单的子问题，回复表现会更好。这里又可以分成<strong>单步分解</strong>和<strong>多步分解</strong>。</p><p><strong>单步分解</strong>将一个复杂查询转化为多个简单的子查询，融合每个子查询的答案作为原始复杂查询的回复。</p><p><img src="image/w9_m9smu46_PYSXf7jYDM.png" alt=""></p><p>对于<strong>多步分解</strong>，给定初始的复杂查询，会一步一步地转换成多个子查询，结合前一步的回复结果生成下一步的查询问题，直到问不出更多问题为止。最后结合每一步的回复生成最终的结果。</p><p><img src="image/pagdp_5q1__2gyCnas6Fz.png" alt=""></p><h4 id="3）变换三-HyDE"><a href="#3）变换三-HyDE" class="headerlink" title="3）变换三: HyDE"></a>3）变换三: HyDE</h4><p><a href="http://boston.lti.cs.cmu.edu/luyug/HyDE/HyDE.pdf" title="HyDE">HyDE</a>，全称叫 Hypothetical Document Embeddings，给定初始查询，<strong>首先利用 LLM 生成一个假设的文档或者回复，然后以这个假设的文档或者回复作为新的查询进行检索</strong>，而不是直接使用初始查询。这种转换在没有上下文的情况下可能会生成一个误导性的假设文档或者回复，从而可能得到一个和原始查询不相关的错误回复。下面是论文中给出的一个例子:</p><p><img src="image/3ig65l2ybq_07U_nQ5Ur7.png" alt=""></p><h3 id="（2）排序和后处理"><a href="#（2）排序和后处理" class="headerlink" title="（2）排序和后处理"></a>（2）排序和后处理</h3><p>经过前面的检索过程可能会得到很多相关文档，就需要进行筛选和排序。常用的筛选和排序策略包括：</p><ul><li>基于相似度分数进行过滤和排序</li><li>基于关键词进行过滤，比如限定包含或者不包含某些关键词</li><li>让 LLM 基于返回的相关文档及其相关性得分来重新排序</li><li>基于时间进行过滤和排序，比如只筛选最新的相关文档</li><li>基于时间对相似度进行加权，然后进行排序和筛选</li></ul><h2 id="2-3-回复生成模块"><a href="#2-3-回复生成模块" class="headerlink" title="2.3 回复生成模块"></a>2.3 回复生成模块</h2><h3 id="（1）回复生成策略"><a href="#（1）回复生成策略" class="headerlink" title="（1）回复生成策略"></a>（1）回复生成策略</h3><p>检索模块基于用户查询检索出相关的文本块，回复生成模块让 LLM 利用检索出的相关信息来生成对原始查询的回复。LlamaIndex 中有给出一些不同的回复生成策略。</p><p>一种策略是依次结合每个检索出的相关文本块，每次不断修正生成的回复。这样的话，有多少个独立的相关文本块，就会产生多少次的 LLM 调用。另一种策略是在每次 LLM 调用时，尽可能多地在 Prompt 中填充文本块。如果一个 Prompt 中填充不下，则采用类似的操作构建多个 Prompt，多个 Prompt 的调用可以采用和前一种相同的回复修正策略。</p><h3 id="（2）回复生成-Prompt-模板"><a href="#（2）回复生成-Prompt-模板" class="headerlink" title="（2）回复生成 Prompt 模板"></a>（2）回复生成 Prompt 模板</h3><p>下面是 LlamaIndex 中提供的一个生成回复的 Prompt 模板。从这个模板中可以看到，可以用一些分隔符 ( 比如 ——— ) 来区分相关信息的文本，还可以指定 LLM 是否需要结合它自己的知识来生成回复，以及当提供的相关信息没有帮助时，要不要回复等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">template = <span class="string">f&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Context information is below.</span></span><br><span class="line"><span class="string">---------------------</span></span><br><span class="line"><span class="string"><span class="subst">&#123;context_str&#125;</span></span></span><br><span class="line"><span class="string">---------------------</span></span><br><span class="line"><span class="string">Using both the context information and also using your own knowledge, answer the question: <span class="subst">&#123;query_str&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">If the context isn&#x27;t helpful, you can/don’t answer the question on your own.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>下面的 Prompt 模板让 LLM 不断修正已有的回复。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">template = <span class="string">f&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">The original question is as follows: <span class="subst">&#123;query_str&#125;</span></span></span><br><span class="line"><span class="string">We have provided an existing answer: <span class="subst">&#123;existing_answer&#125;</span></span></span><br><span class="line"><span class="string">We have the opportunity to refine the existing answer (only if needed) with some more context below.</span></span><br><span class="line"><span class="string">------------</span></span><br><span class="line"><span class="string"><span class="subst">&#123;context_str&#125;</span></span></span><br><span class="line"><span class="string">------------</span></span><br><span class="line"><span class="string">Using both the new context and your own knowledege, update or repeat the existing answer.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="3-案例分析和应用"><a href="#3-案例分析和应用" class="headerlink" title="3.案例分析和应用"></a>3.案例分析和应用</h1><h2 id="3-1-ChatGPT-检索插件"><a href="#3-1-ChatGPT-检索插件" class="headerlink" title="3.1 ChatGPT 检索插件"></a>3.1 ChatGPT 检索插件</h2><p>ChatGPT 检索插件 <a href="https://github.com/openai/chatgpt-retrieval-plugin" title="ChatGPT Retrieval Plugin">ChatGPT Retrieval Plugin</a> 是 OpenAI 官方给出的一个通过检索来增强 LLM 的范例，实现了让 ChatGPT 访问私有知识的一种途径，其在 Github 上的开源仓库短时间内获得了大量关注。下面是 ChatGPT 检索插件内部原理的一张示意图(<a href="https://techcommunity.microsoft.com/t5/azure-database-for-postgresql/openai-chatgpt-retrieval-plugin-and-postgresql-on-azure/ba-p/3826411" title="图片来源: openai-chatgpt-retrieval-plugin-and-postgresql-on-azure">图片来源: openai-chatgpt-retrieval-plugin-and-postgresql-on-azure</a>)。</p><p><img src="image/zx2o6-r03j_nikEYfs-nZ.png" alt=""></p><p>在 API 接口设计上，检索插件提供了下面几种接口:</p><ul><li><code>/upsert</code>: 该接口将上传的一个或多个文本文档，先切分成文本块，每个文本块大小在 200 个 Token，然后利用 OpenAI 的 文本嵌入模型将文本块转换成向量，最后连同原始文本和元信息存储在向量数据库中，代码仓库中实现了对几乎所有主流向量类数据库的支持。</li><li><code>/upsert-file</code>: 该接口允许上传 PDF、TXT、DOCX、PPTX 和 MD 格式的单个文件，先转换成纯文本后，后续处理流程和 <code>/upsert</code> 接口一样。</li><li><code>/query</code>: 该接口实现对给定的查询，返回和查询最相关的几个文本块，实现原理也是基于相似向量检索。用户可以在请求中通过 <code>filter</code> 参数对文档进行过滤，通过 <code>top_k</code> 参数指定返回的相关文本块数量。</li><li><code>/delete</code>: 该接口实现从向量数据库中对一个或多个文档进行删除操作。</li></ul><h2 id="3-2-LlamaIndex-和-LangChain"><a href="#3-2-LlamaIndex-和-LangChain" class="headerlink" title="3.2 LlamaIndex 和 LangChain"></a>3.2 LlamaIndex 和 LangChain</h2><p><a href="https://gpt-index.readthedocs.io/en/latest/index.html#" title="LlamaIndex">LlamaIndex</a> 是一个服务于 LLM 应用的数据框架，提供外部数据源的导入、结构化、索引、查询等功能，这篇文章的结构和内容有很大一部分是参考 LlamaIndex 的文档，文章中提到的很多模块、算法和策略，LlamaIndex 基本都有对应的实现，提供了相关的高阶和低阶 API。</p><p>LlamaIndex 主要包含以下组件和特性：</p><ul><li>数据连接器：能从多种数据源中导入数据，有个专门的项目 <a href="https://llamahub.ai/" title="Llama Hub">Llama Hub</a>，可以连接多种来源的数据</li><li>数据索引：支持对读取的数据进行多种不同的索引，便于后期的检索</li><li>查询和对话引擎：既支持单轮形式的查询交互引擎，也支持多轮形式的对话交互引擎</li><li>应用集成：可以方便地与一些流行的应用进行集成，比如 ChatGPT、LangChain、Flask、Docker等</li></ul><p>下面是 LlamaIndex 整体框架的一张示意图。</p><p><img src="image/88xh886jei_ZHkF6FyLQQ.jpeg" alt=""></p><p>除了 LlamaIndex，<a href="https://python.langchain.com/docs/get_started/introduction.html" title="LangChain">LangChain</a> 也是当前流行的一种 LLM 应用开发框架，其中也包含一些检索增强 LLM 的相关组件，不过相比较而言，LlamaIndex 更侧重于检索增强 LLM 这一相对小的领域，而 LangChain 覆盖的领域更广，比如会包含 LLM 的链式应用、Agent 的创建和管理等。下面这张图就是 LangChain 中 <a href="https://python.langchain.com/docs/modules/data_connection/" title="Retrieval">Retrieval</a> 模块的整体流程示意图，包含数据加载、变换、嵌入、向量存储和检索，整体处理流程和 LlamaIndex 是一样的。</p><p><img src="image/etz_ewki3z_QWUVCi861U.jpeg" alt=""></p><h2 id="3-3-Github-Copilot-分析"><a href="#3-3-Github-Copilot-分析" class="headerlink" title="3.3 Github Copilot 分析"></a>3.3 Github Copilot 分析</h2><p><a href="https://github.com/features/copilot" title="Github Copilot">Github Copilot</a> 是一款 AI 辅助编程工具。如果使用过就会发现，Github Copilot 可以根据代码的上下文来帮助用户自动生成或者补全代码，有时候可能刚写下类名或者函数名，又或者写完函数注释，Copilot 就给出了生成好的代码，并且很多时候可能就是我们想要实现的代码。由于 Github Copilot 没有开源，网上有人对其 VSCode 插件进行了逆向分析，比如 <a href="https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals" title="copilot internals">copilot internals</a> 和 <a href="https://github.com/mengjian-github/copilot-analysis" title="copilot analysis">copilot analysis</a>，让我们可以对 Copilot 的内部实现有个大概的了解。</p><p>简单来说，<strong>Github Copilot 插件会收集用户在 VSCode 编程环境中的多种上下文信息构造 Prompt，然后把构造好的 Prompt 发送给代码生成模型 ( 比如 Codex )，得到补全后的代码，显示在编辑器中</strong>。如何检索出相关的上下文信息 ( Context ) 就是其中很重要的一个环节。Github Copilot 算是检索增强 LLM 在 AI 辅助编程方向的一个应用。</p><p>需要说明的是，上面提到的两份逆向分析是几个月之前做的，Github Copilpot 目前可能已经做了很多的更新和迭代，另外分析是原作者阅读理解逆向后的代码得到的，所以可能会产生一些理解上的偏差。而下面的内容是我结合那两份分析产生的，因此有些地方可能是不准确甚至是错误的，但不妨碍我们通过 Copilot 这个例子来理解上下文信息对增强 LLM 输出结果的重要性，以及学习一些上下文相关信息检索的实践思路。</p><p>下面是一个 Prompt 的示例，可以看到包含前缀代码信息 ( prefix )，后缀代码信息 ( suffix )，生成模式 ( isFimEnabled )，以及 Prompt 不同组成元素的起始位置信息 ( promptElementRanges )。</p><p><img src="image/zl450wocuu_X6MX8Y_7BL.png" alt=""></p><p>抛开代码生成模型本身的效果不谈，Prompt 构造的好坏很大程度上会影响代码补全的效果，而上下文相关信息 ( Context ) 的提取和构成很大程度上又决定了 Prompt 构造的好坏。让我们来看一下 Github Copilot 的 Prompt 构造中有关上下文相关信息抽取的一些关键思路和实现。</p><p>Copilot 的 Prompt 包含不同类型的相关信息，包括</p><ul><li><code>BeforeCursor</code>：光标前的内容</li><li><code>AfterCursor</code>：光标后的内容</li><li><code>SimilarFile</code>：与当前文件相似度较高的代码片段</li><li><code>ImportedFile</code> ：import 依赖</li><li><code>LanguageMarker</code>：文件开头的语言标记</li><li><code>PathMarker</code>：文件的相对路径信息</li></ul><p>其中相似代码片段的抽取，会先获取最近访问过的多份同种语言的文件，作为抽取相似代码片段的候选文档。然后设定窗口大小 ( 比如默认为 60 行 ) 和步长 ( 比如默认为 1 行 )，以滑动窗口的方式将候选文档切分成代码块。接着计算每个切分后的代码块和当前文件的相似度，最后保留相似度较高的几个代码块。这里当前文件的获取是从当前光标往前截取窗口大小的内容，相似度的度量采用的是 <strong>Jaccard 系数</strong>，具体来说，会对代码块中的每一行进行分词，过滤常见的代码关键字 ( 比如 if, then, else, for 这些)，得到一个标记 ( Token ) 集合，然后就可以在当前代码块和候选代码块的 Token 集合之间计算 Jaccard 相似度。在 Copilot 的场景下，这种相似度的计算方式简单有效。<br>$J(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}$<br>上面的一篇分析文章中将 Prompt 的组成总结成下面的一张图。</p><p><img src="image/-7odlfea82_ZAUMsQpq0k.png" alt=""></p><p>构造好 Prompt 后，Copilot 还会判断是否有必要发起请求，代码生成模型的计算是非常耗费算力的，因此有必要过滤一些不必要的请求。其中一个判断是利用简单的线性回归模型对 Prompt 进行打分，当分数低于某个阈值时，请求就不会发出。这个线性回归模型利用的特征包括像代码语言、上一次代码补全建议是否被采纳或拒绝、上一次采纳或拒绝距现在的时长、光标左边的字符等。通过分析模型的权重，原作者给出了一些观察：</p><ul><li>一些编程语言的权重相对于其他语言权重要更高 ( php &gt; js &gt; python &gt; rust &gt; … )，PHP 权重最高，果然 <strong>PHP是世界上最好的语言</strong>( ^ _^ )。</li><li>右半边括号 ( 比如 <code>)</code>，<code>]</code> ) 的权重要低于左半边括号，这是符合逻辑的。</li></ul><p>通过对 Github Copilot 这个编程辅助工具的分析可以看到：</p><ul><li><strong>检索增强 LLM 的思路和技术在 Github Copilot 的实现中发挥着重要作用</strong></li><li>上下文相关信息 ( Context ) 可以是一个广义概念，可以是相关的文本或者代码片段，也可以是文件路径、相关依赖等，每个场景都可以定义其特定的上下文元素</li><li>相似性的度量和相似检索方法可以因场景而异，不一定所有场景都需要用余弦相似度，都需要通过向量相似检索的方式找出相关文档，比如 Copilot 的实现中就利用简单的 Jaccard 系数来计算分词后 Token 集合的相似度，简单高效。</li></ul><h2 id="3-4-文档和知识库的检索与问答"><a href="#3-4-文档和知识库的检索与问答" class="headerlink" title="3.4 文档和知识库的检索与问答"></a>3.4 文档和知识库的检索与问答</h2><p>检索增强 LLM 技术的一个典型应用是知识库或者文档问答，比如针对企业内部知识库或者一些文档的检索与问答等。这个应用方向目前已经出现了很多商业化和开源的产品。比如 <a href="https://www.mendable.ai/" title="Mendable">Mendable</a> 就是一款商业产品，能提供基于文档的 AI 检索和问答能力。上面提到的 LlamaIndex 和 LangChain 项目官方文档的检索能力就是由 Mendable 提供的。下面就是一张使用截图，可以看到 Mendable 除了会给出生成的回复，也会附上参考链接。</p><p><img src="image/uepwk88u4-_r-vwUmM-d1.png" alt=""></p><p>除了商业产品，也有很多类似的开源产品。比如</p><ul><li><a href="https://github.com/danswer-ai/danswer" title="Danswer">Danswer</a>: 提供针对企业内部文档的问答功能，能实现多种来源的数据导入，支持传统的检索和基于 LLM 的问答，能智能识别用户的搜索意图，从而采用不同的检索策略，支持用户和文档的权限管理，以及支持Docker部署等</li><li><a href="https://www.pandagpt.io/" title="PandaGPT">PandaGPT</a>: 支持用户上传文件，然后可以针对文件内容进行提问</li><li><a href="https://fastgpt.run/" title="FastGPT">FastGPT</a>: 一个开源的基于 LLM 的 AI 知识库问答平台</li><li><a href="https://github.com/StanGirard/quivr" title="Quivr">Quivr</a>: 这个开源项目能实现用户对个人文件或者知识库的检索和问答，期望成为用户的「第二大脑」</li><li><a href="https://github.com/guangzhengli/ChatFiles" title="ChatFiles">ChatFiles</a>: 又一个基于 LLM 的文档问答开源项目</li></ul><p>下面这张图是 ChatFiles 项目的技术架构图，可以发现这类项目的基本模块和架构都很类似，基本都遵从检索增强 LLM 的思路，这类知识库问答应用几乎成为 LLM 领域的 <strong>Hello World</strong> 应用了。</p><p><img src="image/4x9fc4i_0r_bzpIRKAkpG.png" alt=""></p><h1 id="4-参考"><a href="#4-参考" class="headerlink" title="4.参考"></a>4.参考</h1><ol><li><a href="https://github.com/openai/chatgpt-retrieval-plugin" title="ChatGPT Retrieval Plugin">ChatGPT Retrieval Plugin</a> #project</li><li><a href="https://arxiv.org/abs/2212.10496?ref=mattboegner.com" title="Hypothetical Document Embeddings">Hypothetical Document Embeddings</a> #paper</li><li><a href="https://mattboegner.com/knowledge-retrieval-architecture-for-llms/" title="Knowledge Retrieval Architecture for LLM’s (2023)">Knowledge Retrieval Architecture for LLM’s (2023)</a> #blog</li><li><a href="https://www.pinecone.io/learn/chunking-strategies/" title="Chunking Strategies for LLM Applications">Chunking Strategies for LLM Applications</a> #blog</li><li><a href="https://python.langchain.com/docs/modules/data_connection/document_transformers/" title="LangChain Document Transformers">LangChain Document Transformers</a> #doc</li><li><a href="https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/index/index_guide.html" title="LlamaIndex Index Guide">LlamaIndex Index Guide</a> #doc</li><li><a href="https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/augmented-language-models/" title="Full stack LLM Bootcamp: Augmented Language Models">Full stack LLM Bootcamp: Augmented Language Models</a> #course</li><li><a href="https://www.pinecone.io/learn/series/faiss/vector-indexes/" title="Pinecone: vector indexes in faiss">Pinecone: vector indexes in faiss</a> #blog</li><li><a href="https://www.pinecone.io/learn/vector-database/" title="Pinecone: what is a vector database">Pinecone: what is a vector database</a> #blog</li><li><a href="https://blog.reachsumit.com/posts/2023/03/llm-for-text-ranking/" title="Zero and Few Shot Text Retrieval and Ranking Using Large Language Models">Zero and Few Shot Text Retrieval and Ranking Using Large Language Models</a> #blog</li><li><a href="https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals" title="copilot internals">copilot internals</a> #blog</li><li><a href="https://github.com/mengjian-github/copilot-analysis" title="copilot analysis">copilot analysis</a> #blog</li><li><a href="https://www.youtube.com/watch?v=A3iqOJHBQhM\&amp;ab_channel=LlamaIndex" title="Discover LlamaIndex: Key Components to Build QA Systems">Discover LlamaIndex: Key Components to Build QA Systems</a> #video</li><li><a href="https://wangzwhu.github.io/home/file/acmmm-t-part3-ann.pdf" title="Billion scale approximate nearest neighbor search">Billion scale approximate nearest neighbor search</a> #slide</li><li><a href="https://acl2023-retrieval-lm.github.io/" title="ACL 2023 Tutorial: Retrieval based LM">ACL 2023 Tutorial: Retrieval based LM</a> #slide</li><li><a href="https://www.pinecone.io/blog/why-use-retrieval-instead-of-larger-context/" title="Pinecone: why use retrieval instead of larger context">Pinecone: why use retrieval instead of larger context</a> #blog</li><li><a href="https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM" title="RETA-LLM">RETA-LLM</a> #project</li><li><a href="https://www.youtube.com/watch?v=njzB6fm0U8g\&amp;ab_channel=LlamaIndex" title="Document Metadata and Local Models for Better, Faster Retrieval">Document Metadata and Local Models for Better, Faster Retrieval</a> #video</li></ol>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLMs公开课 - 6.文本理解和生成大模型</title>
      <link href="/llms/llms_course/6.%E6%96%87%E6%9C%AC%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/llms/llms_course/6.%E6%96%87%E6%9C%AC%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h1><h2 id="1-1-NLP的主要应用"><a href="#1-1-NLP的主要应用" class="headerlink" title="1.1 NLP的主要应用"></a>1.1 NLP的主要应用</h2><p>NLP的主要应用主要分为两类：<strong>自然语言理解(NLU)</strong>和<strong>自然语言生成(NLG)</strong>。 &#x20;</p><ul><li><strong>信息检索</strong>是NLU非常有代表性的应用；</li><li><strong>文本生成</strong>是NLG一个代表性例子；</li><li><strong>机器问答</strong>综合了自然语言理解和自然语言生成两个任务。</li></ul><p>在这三种任务中，大模型都带来了一定的变革。</p><p><img src="image/image_-SAdnB-JPD.png" alt=""></p><h2 id="1-2-信息检索"><a href="#1-2-信息检索" class="headerlink" title="1.2 信息检索"></a>1.2 信息检索</h2><p>信息检索是非常古老、非常经典的任务。在这一方面，大模型可以帮助机器来提供更加智能、更加准确的搜索结果。</p><p><img src="image/image_89Mw0WZw8M.png" alt=""></p><h2 id="1-3-机器问答"><a href="#1-3-机器问答" class="headerlink" title="1.3 机器问答"></a>1.3 机器问答</h2><p>问机器一些问题，希望机器能提供我们想要的答案。<strong>传统的机器问答方法是基于模板、或者基于知识库的，这样使得它的问答范围受限</strong>。</p><p>但现在大模型允许机器回答更加复杂的问题，从下面的例子中，列出一些最先进的大模型可以回答的问题。可以看到，即使它背后没有一个知识库去支撑它搜索相关的知识，大模型里面蕴含的知识也足以帮助机器回答上述问题。</p><p><img src="image/image_vFAP46ZWa3.png" alt=""></p><h2 id="1-4文本生成"><a href="#1-4文本生成" class="headerlink" title="1.4文本生成"></a>1.4文本生成</h2><p>利用大模型可以帮助机器生成更加流畅、自然的文本。</p><p><img src="image/image_BY6TOK02AS.png" alt=""></p><h1 id="2-信息检索"><a href="#2-信息检索" class="headerlink" title="2.信息检索"></a>2.信息检索</h1><h2 id="2-1-背景"><a href="#2-1-背景" class="headerlink" title="2.1 背景"></a>2.1 背景</h2><p>信息以爆炸的形式增加，用户对信息检索的需求也是在急剧增长。</p><p>可以看到全球的信息用户数量也非常庞大。 &#x20;</p><p>自动检索：<strong>根据用户的查询，从海量的文本信息中提炼出少量的与用户需求高度相关的文档</strong>，反馈给用户。</p><p><img src="https://img-blog.csdnimg.cn/552a214fc0e744ee93a0b37f9036404a.png" alt=""></p><p>信息检索有很多典型的应用，比如搜索引擎、问答系统和智能写作等。</p><h2 id="2-2-IR定义和评价"><a href="#2-2-IR定义和评价" class="headerlink" title="2.2 IR定义和评价"></a>2.2 IR定义和评价</h2><h3 id="（1）IR定义"><a href="#（1）IR定义" class="headerlink" title="（1）IR定义"></a>（1）IR定义</h3><p>首先来看下如何定义信息检索(IR)任务。</p><ul><li>给定一个<code>query</code> $q$</li><li>给定一个文档库 $D = \{\cdots,d_i,\cdots \}$</li><li>IR系统计算相关系数得分$f(q,d_i)$，然后根据该得分进行排序</li></ul><p>一个典型的IR系统分为两个阶段：检索和重排阶段。</p><ul><li>在检索阶段，针对整个文档库，从中找到相关文档的子集，它<strong>重视的检索速度和相关文档的召回率</strong>；</li><li>在重排序阶段针对上一步得到的少量文档进行精排，看重的是<strong>性能和效果</strong>。</li></ul><p><img src="image/image_8dp8clk9s2.png" alt=""></p><h3 id="（2）IR评价指标"><a href="#（2）IR评价指标" class="headerlink" title="（2）IR评价指标"></a>（2）IR评价指标</h3><p>IR中常用的三个指标是<code>MRR@k</code>、<code>MAP@k</code>和<code>NDCG@k</code>。后面的<code>@k</code>表示在评测中，只要考虑top K个排序的结果。</p><h4 id="MRR-Mean-Reciprocal-Rank"><a href="#MRR-Mean-Reciprocal-Rank" class="headerlink" title="MRR (Mean Reciprocal Rank)"></a>MRR (Mean Reciprocal Rank)</h4><p>MRR是平均倒数排名，给定一个待评测的查询集合 <code>Q</code>，MRR只会考虑哪个查询排名<strong>最靠前的第一个相关文档的位置</strong>。 &#x20;</p><script type="math/tex; mode=display">M R R=\frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\operatorname{rank}_{i}}</script><p>比如说查询集合中一个有三个查询：cat、torus和virus。这三个查询排在首位的相关文档位置，分别是第3位、第2位和第1位。那么对它们取倒数之后就是<code>1/3</code>、<code>1/2</code>和<code>1</code>。对它们求均值之后得到<code>0.61</code>，就是MRR评测的结果。</p><script type="math/tex; mode=display">M R R=(1 / 3+1 / 2+1) / 3=0.61</script><p><img src="image/image_OJgNUWQ2X3.png" alt=""></p><h4 id="MAP-Mean-Average-Precision"><a href="#MAP-Mean-Average-Precision" class="headerlink" title="MAP (Mean Average Precision)"></a>MAP (Mean Average Precision)</h4><p>MAP，<strong>一组查询平均准确率的均值，它会考虑所有相关文档</strong>。这里也举个例子，这个查询集合中一共有两个查询，它们分别有4篇和5篇相关文档。 &#x20;</p><p>在query1中，这四篇相关文档都被成功地召回了，它们被召回的位置分别是第1位、2位、4位和7位。同样对它们取倒数排名，计算均值之后得到0.83。 &#x20;</p><p>在query2中，五篇中只成功召回了3篇，位置是1,3和5。那么计算它们的倒数分数，求均值得到0.45。 &#x20;</p><p>接着对这两个查询的分数相加，再求平均，得到0.64。才是最终MAP的得分。</p><p><img src="image/image_3o1YFOjutf.png" alt=""></p><h4 id="NDCG-Normalized-Discounted-Cumulative-Gain"><a href="#NDCG-Normalized-Discounted-Cumulative-Gain" class="headerlink" title="NDCG (Normalized Discounted Cumulative Gain)"></a>NDCG (Normalized Discounted Cumulative Gain)</h4><p>NDCG，<strong>归一化的折损累积增益</strong>，该<strong>指标是商业的搜索引擎/推荐系统中最常用的评价指标</strong>。它会将文档设置成不同的相关等级，相关程度越高，等级越高。 &#x20;</p><p>它的计算方式为：用待评测的排序列表的DCG分数，除以IDCG的分数。IDCG的分数就是一个理想状态下列表的真实排序方式；DCG的计算公式如下图所示。</p><p><img src="image/image_h1e8B6K41f.png" alt=""></p><p>也看一个具体的例子，针对一个query抽回的五篇文档，分别有不同的相关等级 $ rel_i  $。 &#x20;</p><p>会计算它的增益和折损后的增益，最后再求和就是DCG的分数。</p><p><img src="image/image_sQQC_YHcLo.png" alt=""></p><h2 id="2-3-传统方法"><a href="#2-3-传统方法" class="headerlink" title="2.3 传统方法"></a>2.3 传统方法</h2><h3 id="（1）BM25"><a href="#（1）BM25" class="headerlink" title="（1）BM25"></a>（1）BM25</h3><h4 id="BM25-Best-Matching-25"><a href="#BM25-Best-Matching-25" class="headerlink" title="BM25 (Best Matching 25)"></a>BM25 (Best Matching 25)</h4><p>给定一个查询，其中包含相应的单词，BM25会计算<strong>该查询与每一篇文档的匹配分数</strong>。</p><p><img src="image/image_6Z-TYb1vtD.png" alt=""></p><h4 id="TF-Term-Frequency"><a href="#TF-Term-Frequency" class="headerlink" title="TF (Term Frequency)"></a>TF (Term Frequency)</h4><p>TF就是<strong>词频</strong>，为查询中每个单词在文档中出现的频率。</p><p><img src="image/image_swNUF-CYvB.png" alt=""></p><h4 id="IDF-Inverse-Document-Frequency"><a href="#IDF-Inverse-Document-Frequency" class="headerlink" title="IDF (Inverse Document Frequency)"></a>IDF (Inverse Document Frequency)</h4><p>而IDF是<strong>逆文档频率</strong>，评估查询单词的稀有程度。如果一个文档在所有文档中都出现，那么它的IDF分数反而很低。</p><p><img src="image/image_VcOCEG0Gt7.png" alt=""></p><h3 id="（2）缺点"><a href="#（2）缺点" class="headerlink" title="（2）缺点"></a>（2）缺点</h3><p>那么这种基于词汇匹配的算法存在两方面的问题。</p><p>首先是<strong>词汇失配</strong>的问题，因为人类会使用不同的单词来表达同一个意思。&#x20;</p><p><img src="image/image__0glioEdhZ.png" alt=""></p><p>其次是<strong>语义失配</strong>问题，可能即使文档和词汇有很高的匹配率，但描述的含义却完全不一样。</p><p><img src="image/image_bBXUB4iJAs.png" alt=""></p><h2 id="2-4-神经网络方法"><a href="#2-4-神经网络方法" class="headerlink" title="2.4 神经网络方法"></a>2.4 神经网络方法</h2><h3 id="（1）简介"><a href="#（1）简介" class="headerlink" title="（1）简介"></a>（1）简介</h3><p>神经网络IR<strong>使用神经网络将用户的查询和文档库的中的文档投射到同一个向量空间，然后计算两者的相关性分数</strong>，从而避免了传统IR中的词汇失配合语义失配的问题。</p><p><img src="image/image_u4HWHk82C4.png" alt=""></p><p>从性能上来说，Neural IR的方法尤其是基于大预训练语言模型的方法，它的检索性能远远超越了传统IR的方法。也可以看到Neural IR的研究热度是逐年增加的。</p><p><img src="image/image_CP7TFf0go6.png" alt=""></p><p>通常会在<strong>重排序阶段</strong>采用左边的<strong>Cross-Encoder的大模型架构</strong>，它会将查询和问答进行词汇级别的拼接，然后进行一个精细地交互式建模，生成一个<code>查询-文档</code>的共同表示，然后产生相关性分数。这种建模方式的好处是比较精细，达到的检索性能也较好，但缺点是计算代价比较高。所以一般使用在重排序阶段。</p><p>而在第一阶段，<strong>检索阶段</strong>，一般采用右边的<strong>Dual-encoder，双塔的架构</strong>，使用大模型对查询和文档分别进行编码，形成两个独立的向量，然后再去计算向量间的相似性。这样可以极大地降低计算的开销。</p><p><img src="image/image_j70AV_P8cg.png" alt=""></p><h3 id="（2）Cross-Encoder架构"><a href="#（2）Cross-Encoder架构" class="headerlink" title="（2）Cross-Encoder架构"></a>（2）Cross-Encoder架构</h3><p><strong>会先把查询和文档进行拼接，然后一起喂给大模型</strong>。这里以BERT为例，拼接完之后，经过多层transformer的建模之后，把最后一层的CLS token作为<code>查询-文档</code>的共同表示。经过一个NLP的投射变成一个标量的分数，可以作为<code>查询-文档</code>相关性的分数。</p><p>在训练该大模型的时候，训练数据的形式是每个查询配一个相关文档，和至少一篇的不相关文档。</p><p>然后采用常见的Ranking Loss，比如这里的Pairwise hinge loss，为相关文档和查询分配更高的分数。</p><p><img src="image/image_x1UKCSqUvV.png" alt=""></p><p>这里分别展示了以BERT和T5作为bacakbone的重排序结果，可以看到相比传统的IR方法，基于大模型的方法可以达到更出色的重排序效果。并且随着模型参数量的增加，重排序的性能也会持续地增强。</p><blockquote><p>Dai, Zhuyun, et al. SIGIR 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. &#x20;<br>Nogueira Rodrigo, et al. EMNLP 2020. Document Ranking with a Pretrained Sequence-to-Sequence Model.</p></blockquote><p><img src="image/image_vzWLAp2xgC.png" alt=""></p><h3 id="（3）Dual-Encoder架构"><a href="#（3）Dual-Encoder架构" class="headerlink" title="（3）Dual-Encoder架构"></a>（3）Dual-Encoder架构</h3><p>这里以DPR为例，它使用两个独立的Encoder分别对查询和文档进行编码，然后用类似softmax这种NLL损失来训练模型。</p><blockquote><p>Karpukhin Vladimir, et al. EMNLP 2020. Dense Passage Retrieval for Open-Domain Question Answering</p></blockquote><p><img src="image/image_wZARrDchyH.png" alt=""></p><p>Dual-Encoder架构的好处是，因为是<strong>独立编码</strong>，所以可以提前计算缓存整个文档库的编码。然后只需要计算用户的新查询编码，接着使用一些最近邻搜索的工具，比如<code>faiss</code>，去找出最相近的<code>k</code>个文档。</p><blockquote><p><a href="https://github.com/facebookresearch/faiss" title="https://github.com/facebookresearch/faiss">https://github.com/facebookresearch/faiss</a></p></blockquote><p><img src="image/image_aZFHYPpCJp.png" alt=""></p><p>在检索性能方法，在第一阶段检索时，以BERT、T5作为backbone的效果。在使用1K训练数据的情况下，它的效果已经超过了BM25，同时随着训练数据的增加，大模型的性能也会增加。同样模型的大小增加，效果也越好。</p><blockquote><p>Karpukhin Vladimir, et al. EMNLP 2020. Dense Passage Retrieval for Open-Domain Question Answering. &#x20;<br>Ni, Jianmo, et al. arXiv 2022. Large dual encoders are generalizable retrievers</p></blockquote><p><img src="image/image_8OrmCprbfy.png" alt=""></p><h2 id="2-5-前沿热点"><a href="#2-5-前沿热点" class="headerlink" title="2.5 前沿热点"></a>2.5 前沿热点</h2><p>本小节介绍几种比较常见的基于大模型的Neural IR架构，和IR领域比较前沿的研究热点。</p><h3 id="（1）Negative-enhanced-Fine-tuning"><a href="#（1）Negative-enhanced-Fine-tuning" class="headerlink" title="（1）Negative-enhanced Fine-tuning"></a>（1）Negative-enhanced Fine-tuning</h3><p>首先有相当一部分工作是在研究<strong>如何在微调阶段去挖掘更好的负例</strong>，目前几种常见的训练负例有上图这么几种。 &#x20;</p><ul><li><code>In-bach negative</code>：在训练中同一个batch的正例可以作为其他query的一个负例。 &#x20;</li><li><code>Random negative</code>：随机地从文档中进行采样。 &#x20;</li><li><code>BM25的负例</code>：先用BM25针对每个query抽回一些top k的文档，然后删除掉相关文档，剩下的就是不相关的。</li></ul><p>在In-batch空间中，它们的分布是非常不一样的， 因此它最大对大模型检索的性能影响也是比较大的。&#x20;</p><p><img src="image/image_b-Oc0fVC4m.png" alt=""></p><p>下面介绍一篇工作，它在训练过程中使用模型本身去挖掘更难的负样本，从而获得更好的性能。</p><h4 id="ANCE-Approximate-nearest-neighbor-Negative-Contrastive-Learning"><a href="#ANCE-Approximate-nearest-neighbor-Negative-Contrastive-Learning" class="headerlink" title="ANCE (Approximate nearest neighbor Negative Contrastive Learning)"></a>ANCE (Approximate nearest neighbor Negative Contrastive Learning)</h4><p>该方法称为ANCE，它会在模型的训练过程中（图中的绿线）去异步地维护Inferencer的程序，然后每隔k步，去把最新的模型拿过来推理一下，把那些排名靠前的难负样本抽回来，加到下一轮的训练过程中，这样不断地迭代刷新。</p><blockquote><p>Xiong et al. ICLR 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval.</p></blockquote><p><img src="image/image_ovqsMyDQF6.png" alt=""></p><h4 id="RocketQA-NAACL-2021"><a href="#RocketQA-NAACL-2021" class="headerlink" title="RocketQA (NAACL 2021)"></a>RocketQA (NAACL 2021)</h4><p>还有一类方法，比如RocketQA，它建模<strong>更精细的Cross-Encoder来帮助Dual-Encoder去过滤难负例</strong>，然后加到Dual-Encoder的训练中，这样交叠学习，从而提升Dual-Encoder第一阶段检索的性能。</p><blockquote><p>Qu Yingqi, et al. NAACL 2021. RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering.</p></blockquote><p><img src="image/image_I43oqyB8zG.png" alt=""></p><h3 id="（2）IR-oriented-Pretraining"><a href="#（2）IR-oriented-Pretraining" class="headerlink" title="（2）IR-oriented Pretraining"></a>（2）IR-oriented Pretraining</h3><p>上面是在微调阶段的一个研究热点，第二个研究热点集中在<strong>大规模的预训练阶段</strong>。</p><h4 id="SEED-Encoder-EMNLP-2021"><a href="#SEED-Encoder-EMNLP-2021" class="headerlink" title="SEED-Encoder (EMNLP 2021)"></a>SEED-Encoder (EMNLP 2021)</h4><p>SEED-Encoder，它通过在<strong>预训练阶段为Encoder配置一个较弱的Decoder，来促使下面的Encoder对文本形成一个更好的表示</strong>。它主要的调整，第一个在于Encoder和Decoder之间的连接，第二个在于限制Decoder的Span。这些操作的目地在于<strong>让CLS的表示足够强</strong>，这个模型在预训练的时候只能通过CLS token来重建出原文本。CLS表现能力的增强，对IR是非常有帮助的。</p><blockquote><p>Lu Shuqi, et al. EMNLP 2021. Less is More: Pre-train a Strong Text Encoder for Dense Retrieval Using a Weak Decoder.</p></blockquote><p><img src="image/image_qeiB-U12bJ.png" alt=""></p><h4 id="ICT-Inverse-Cloze-Task"><a href="#ICT-Inverse-Cloze-Task" class="headerlink" title="ICT (Inverse Cloze Task)"></a>ICT (Inverse Cloze Task)</h4><p>ICT，是在预训练的数据上去做一些操作，比如它会针对预训练的文本，随机地抽取文本中任意的一个句子，把这个句子作为我们的查询，剩下的虚线的文本框，作为查询的一个正例。这样就构建出来在微调阶段才能有的数据，接着它再用In-batch negative来配合着进行提前的预训练。</p><blockquote><p>Chang Wei-Cheng, et al. ICLR 2021. Pre-training Tasks for Embedding-based Large-scale Retrieval</p></blockquote><p><img src="image/image_zKer11IoUo.png" alt=""></p><h3 id="（3）Few-Zero-Shot-IR"><a href="#（3）Few-Zero-Shot-IR" class="headerlink" title="（3）Few/Zero-Shot IR"></a>（3）Few/Zero-Shot IR</h3><p>现在越来越多的工作开始关注到Few-shot IR领域，因为在现实生活中，有很多检索场景，都是少样本的场景。这些场景缺乏大规模的监督数据，比如长尾的网页搜索、 涉及隐私的个人检索/企业检索、人工标注昂贵的医学/法律等专业领域的检索。</p><h4 id="Weak-supervision-generation"><a href="#Weak-supervision-generation" class="headerlink" title="Weak supervision generation"></a>Weak supervision generation</h4><p>在这些领域，有一部分工作在研究如何用<strong>弱监督的数据去取代监督的数据来训练大模型</strong>。比如下图列了三种不同弱监督数据来源。有文档的标题与文本的正文、网页中的锚文本对、还有的直接用大模型去根据文本生成一个query，这样通过大模型生成数据。</p><p><img src="image/image_8odTYGHswj.png" alt=""></p><p>但由于刚才提到的弱监督数据是没有经过人工质量检测，不可避免会存在不同程度噪音。因此也涌现了一类工作，去研究如何针对弱监督数据进行去噪学习。比如ReinfoSelect。</p><blockquote><p>Kaitao Zhang, et al. WWW 2020. Selective weak supervision for neural information retrieval.</p></blockquote><p><img src="image/image_pbF2kzfEgI.png" alt=""></p><h4 id="（4）其他"><a href="#（4）其他" class="headerlink" title="（4）其他"></a>（4）其他</h4><p>还有两个有意思的研究方向，</p><ul><li>一个是<strong>对话式IR</strong>，针对用户会同时提多个问题，且后面的问题与前面的问题有关联。 &#x20;</li><li>另一个方向是<strong>使用大模型去检索长文本</strong>。因为长文本情况下，模型需要考虑的问题比较多，比如长距离依赖。</li></ul><p><img src="image/image_8SazZfgbGO.png" alt=""></p><h1 id="3-QA"><a href="#3-QA" class="headerlink" title="3.QA"></a>3.QA</h1><p>QA分为很多种：</p><ul><li>机器阅读理解：阅读特定的文本并回答问题</li><li>开放领域QA：搜索并阅读相关文档以回答问题</li><li>基于知识的QA：根据知识图谱回答问题</li><li>对话式QA：根据对话历史回答问题</li></ul><p>这里主要介绍前面两种。机器阅读理解是在检索到相关文档后，让机器代替人类去从相关文档中抽取答案的过程。</p><h2 id="3-1-机器阅读理解"><a href="#3-1-机器阅读理解" class="headerlink" title="3.1 机器阅读理解"></a>3.1 机器阅读理解</h2><h3 id="（1）RC定义与类型"><a href="#（1）RC定义与类型" class="headerlink" title="（1）RC定义与类型"></a>（1）RC定义与类型</h3><p>阅读理解的定义：首先会有一篇文章，以及对应的题目，通过理解题目的含义来回答问题。</p><p>阅读理解的形式有很多种。 &#x20;</p><ul><li>有<strong>完形填空</strong>，通过挖掉句子中某些词，希望模型能正确输出被挖掉的词。</li><li>多选：</li><li>抽取式的阅读理解，它的答案隐藏在文章中，让机器去预测问题的答案实际上是文章中的某个词/短语。</li></ul><p>从机器阅读理解的数据集类型可以看到它的发展。</p><h3 id="（2）Traditional-Pipeline"><a href="#（2）Traditional-Pipeline" class="headerlink" title="（2）Traditional Pipeline"></a>（2）Traditional Pipeline</h3><p>介绍阅读理解领域一些经典的方法。</p><p>在大模型出来之前，机器阅读理解经典的框架是这样的。</p><blockquote><p>Seo et al., Bidirectional Attention Flow for Machine Comprehension, In Proceedings of ICLR 2017</p></blockquote><p>它是一个四层的结构：</p><ol><li>首先<strong>对文档和问题分别进行编码，得分文档和问题的向量集合</strong>。</li><li>然后<strong>分别处理这些向量集合</strong>，同时包括一些注意力，得分文档和问题的汇聚向量表示。</li><li>接着基于从文档到问题/从问题到文档的<strong>交互得到融合问题和文档的向量</strong></li><li>最后喂给线性层进行<strong>预测</strong>。</li></ol><p><img src="image/image_bifZ2B9LaT.png" alt=""></p><p>BiDAF就是遵循了上面的框架实现的模型。</p><blockquote><p>Seo et al., Bidirectional Attention Flow for Machine Comprehension, In Proceedings of ICLR 2017</p></blockquote><p><img src="image/image_lDvuimXgzq.png" alt=""></p><p>这些设计很复杂，并且迁移性不好。</p><h3 id="（3）Big-model-based-Methods"><a href="#（3）Big-model-based-Methods" class="headerlink" title="（3）Big-model-based Methods"></a>（3）Big-model-based Methods</h3><p>有了大模型之后，<strong>只需要用一个大模型就可以替代上面的前三层</strong>。</p><p><img src="image/image_mT9xp5pK9x.png" alt=""></p><p>这里给出了BERT刚出来时非常简单的实现问答系统的示例。</p><p>将问题和上下文的连接提供给BERT。获得问题感知上下文表示，以预测答案的开始/结束</p><p>直接拼接问题和文档，作为BERT的输入，然后用<code>[CLS]</code>进行分类得到最终的答案。</p><p><img src="image/image_IfbzIman_9.png" alt=""></p><p>大模型的好处除了在于简化阅读理解的Pipeline之外，还有另一个好处是可以<strong>统一不同问答系统的形式</strong>。</p><p>可以统一成<code>text-to-text</code>的形式，比如抽取式QA可以看成给定输入，直接输出答案。</p><blockquote><p>Khashabi et al., UNIFIEDQA: Crossing Format Boundaries with a Single QA System, Findings of EMNLP 2020</p></blockquote><p><img src="image/image_MXQ6hzmZ02.png" alt=""></p><h2 id="3-2-开放式QA"><a href="#3-2-开放式QA" class="headerlink" title="3.2 开放式QA"></a>3.2 开放式QA</h2><p>开放式QA假设的是<strong>没有给出相关的文章，模型必须自己去寻找相关的文章</strong>。比如从维基百科中去寻找相关文章。开放式QA最终的目标是<strong>建立一个端到端的QA系统</strong>，只需要喂给它问题就能得到答案。</p><p>开放式QA有两种类型：生成式和检索式。</p><h3 id="（1）生成式QA"><a href="#（1）生成式QA" class="headerlink" title="（1）生成式QA"></a>（1）生成式QA</h3><p>生成式的方法就是<strong>用大模型内所存储的知识，直接去回答问题</strong>。</p><blockquote><p>Roberts et al., How Much Knowledge Can You Pack Into the Parameters of a Language Model?, EMNLP 2020</p></blockquote><p><img src="image/image_TetNy1S42Z.png" alt=""></p><h3 id="（2）检索式QA"><a href="#（2）检索式QA" class="headerlink" title="（2）检索式QA"></a>（2）检索式QA</h3><p>第二种是基于检索的方法，通常由两部分组成：<code>Document retriever</code>和<code>Document reader。</code></p><p>分别用于检索出相关文章以及从相关文章中找出对应答案。&#x20;</p><p><img src="image/image_AjToZN5Amo.png" alt=""></p><h3 id="（3）REALM"><a href="#（3）REALM" class="headerlink" title="（3）REALM"></a>（3）REALM</h3><p>在大模型流行起来后一个非常重要的方向是<strong>如何用检索来辅助大模型的预训练过程</strong>。 &#x20;</p><p>让大模型在下游的机器问答环节中表现得更好。 &#x20;</p><p>REALM这篇工作它在<strong>模型的预训练过程中加入了一个检索的任务，让大模型把预训练当成一个开放式QA的任务，在预训练的时候，同时训练大模型和知识检索器</strong>。然后在下游的任务中直接用检索器进行检索，从而能够达到更好的表现。</p><p><img src="image/image_Ikt_sHQLjY.png" alt=""></p><p>它具体是如何做的呢？首先在预训练语料库中有一个样本，比如遮盖了pyramidion(金字塔)这样一个词。然后把预训练的过程看作是一个问答的过程， 要去回答这个问题需要在知识库中进行一些检索。把该样本当成一个问题，然后让神经检索器去进行一些检索。再把检索到的相关文章和该问题一起输入到大模型中，希望大模型根据这些文章为找到问题的答案。</p><p><img src="image/image_wd1Ocx41gE.png" alt=""></p><p>在下游的微调过程中，就可以用相同的Pipeline，给定一个问题，用前面预训练好的检索器检索相关的文章，然后通过相关的文章来回答问题。</p><p><img src="image/image_qpwEJJS0Od.png" alt=""></p><h3 id="（4）WebGPT"><a href="#（4）WebGPT" class="headerlink" title="（4）WebGPT"></a>（4）WebGPT</h3><p>WebGPT比前面介绍的模型更强大，在于它不限定只能在维基百科中寻找答案，而是可以<strong>直接在搜索引擎上去寻找相关的文章，然后回答问题</strong>。</p><ul><li>将文档检索外包给微软必应网络搜索API</li><li>利用无监督的预训练，通过微调GPT-3来实现高质量的文档合成</li><li>创建一个基于文本的网页浏览环境，人类和语言模型都可以与之交互</li></ul><p>训练前让很多标注人员给定一些问题，让他们用基于文本的检索器去寻找答案。并记录了标注人员每一步的操作。比如可以去搜索，点击每个链接，把有用的句子摘录出来，然后 继续寻找下一个相关的内容。</p><p>用这些记录的行为去微调GPT-3，希望GPT-3能够模仿人类行为来使用浏览器。然后惊奇的发现，即使给定较少的训练数据，比如几千条，GPT-3就可以很容易地学会怎么去操控浏览器，它每次可以进行检索，记下重要的引用，再通过这些引用生成最终的问题答案。</p><p><img src="image/image_VwLyinmtYg.png" alt=""></p><h1 id="4-文本生成"><a href="#4-文本生成" class="headerlink" title="4.文本生成"></a>4.文本生成</h1><p>文本生成可以<strong>把一些非语言性的表示信息，通过模型以一种人类可以理解的语言表示处理</strong>。 &#x20;</p><p>非语言性的表示就是常说的数据，比如图片、表格、图等。我们统一把这种生成叫做<code>date-to-text</code>生成，实际上广义上还包括<code>text-to-text</code>的生成。</p><h2 id="4-1-文本生成任务"><a href="#4-1-文本生成任务" class="headerlink" title="4.1 文本生成任务"></a>4.1 文本生成任务</h2><p><img src="image/image_nxyXNBoVWZ.png" alt=""></p><ol><li><code>Data-To-Text (image, table, graph)</code> : 输入可以有很多种形式，比如说图片、表格、图等。</li><li><code>Dialogue</code>  : 模型针对用户的特定输入，给予一些回答。 &#x20;</li><li><code>Machine Translation</code>   : 机器翻译，尽可能保留语义和语法</li><li><code>Poetry Generation</code>   : 诗歌的生成，在生成诗歌的时候，不仅要求它包含某种主题，包含某些关键词，同时还要求它满足一些诗歌的格律。</li><li><code>Style Transfer</code>   : 文本风格转移，把输入文本的风格转移成所需要的风格。上面是文本风格转移中一些常见的子任务。</li><li><code>Storytelling</code> : 故事生成，在给定关键词/故事线下进行故事的生成。上面是一个简单的例子。</li></ol><p>文本生成任务中还包括总结生成的任务，输入是较长的文档，希望模型能生成较短的关于文档的摘要。</p><h2 id="4-2-神经网络文本生成"><a href="#4-2-神经网络文本生成" class="headerlink" title="4.2 神经网络文本生成"></a>4.2 神经网络文本生成</h2><h3 id="（1）语言模型"><a href="#（1）语言模型" class="headerlink" title="（1）语言模型"></a>（1）语言模型</h3><p>**基于前面<code>t-1</code><strong><strong>词生成第</strong></strong><code>t</code>**<strong>个词</strong>。</p><p><img src="image/image_RoMUqRBw3c.png" alt=""></p><p>有<strong>条件的语言建模</strong>，不仅基于已经生成的词，还基于其他输入。比如机器翻译。</p><p><img src="image/image_1x2dr-pjxs.png" alt=""></p><h3 id="（2）Seq2seq"><a href="#（2）Seq2seq" class="headerlink" title="（2）Seq2seq"></a>（2）Seq2seq</h3><p>Seq2Seq也是一种条件语言模型。&#x20;</p><p>在<strong>训练时以teacher forcing的方式进行训练，而测试时基于已生成的单词</strong>。 &#x20;</p><p>这会带来训练与测试分布的gap。</p><p><img src="image/image_EwX6sHdifJ.png" alt=""></p><p>T5也是一种seq2sqe模型，它基于Transformer实现，将所有的NLP任务统一成<code>text-to-text</code>的形式表表示。</p><p>上图左侧是Encoder部分，右侧是Decoder部分。</p><p><img src="image/image_otVGlA0kT6.png" alt=""></p><p>T5模型在清洗过的数据集上进行训练，<strong>训练时遮盖句子中的部分单词</strong>。在训练时，<strong>希望模型能通过这样的输入预测出被遮盖的部分</strong>。</p><p><img src="image/image_IZ6fwtak2D.png" alt=""></p><h3 id="（3）Autoregressive-models"><a href="#（3）Autoregressive-models" class="headerlink" title="（3）Autoregressive models"></a>（3）Autoregressive models</h3><p>语言模型分为两大类，其一是自回归生成。 &#x20;</p><p><strong>在预测时以过去的输出作为参考来生成下一个单词</strong>。</p><p><img src="image/image_NaopUwBWjH.png" alt=""></p><p><strong>GPT一系列模型就是自回归生成的典型例子</strong>。</p><p>它拿到了Transformer中的Decoder部分：</p><ul><li>GPT1认为<strong>可以通过生成式预训练来提升语言理解能力</strong>；</li><li>GPT-2认为<strong>语言模型是一种无监督的多任务学习者</strong>；</li><li>GPT3认为<strong>语言模型是少样本学习者</strong>。</li></ul><p><img src="image/image_5bME8IWKQM.png" alt=""></p><p>以GPT-2为例，<strong>它是在无标签的数据上训练的，可以根据下游具体的有标签数据进行微调</strong>。</p><p><img src="image/image_0i1Ff19zAU.png" alt=""></p><h3 id="（4）Non-autoregressive-models"><a href="#（4）Non-autoregressive-models" class="headerlink" title="（4）Non-autoregressive models"></a>（4）Non-autoregressive models</h3><p>另一类是非自回归生成。</p><p>在给定source和target的情况下，<strong>编码器会对source进行编码，在解码器生成的过程中，每个解码器之间是没有时序关系的</strong>。可以通过编码器的信息一次性地并行地生成所有的输出单词。</p><p><img src="image/image_ETU5tIYd0-.png" alt=""></p><p>在给定输入的情况下，输出只与两部分相关。</p><ol><li>输入会决定目标句子的长度  <code>m</code>；</li><li>在生成当成单词的时候只与 <code>z</code>和 <code>x</code>相关， <code>x</code>是输入的表示， <code>z</code>是计算得到的不同 <code>x</code>和不同 <code>y</code>之间的权重关系。可以看到<code>z</code>中是没有 $y_{t-1}$ 这一项的。所以可以并行地对这些词进行生成。</li></ol><p><img src="image/image_hqByI8mzEq.png" alt=""></p><h3 id="（5）Decoding-strategy"><a href="#（5）Decoding-strategy" class="headerlink" title="（5）Decoding strategy"></a>（5）Decoding strategy</h3><h4 id="Greedy-Decoding"><a href="#Greedy-Decoding" class="headerlink" title="Greedy Decoding"></a>Greedy Decoding</h4><p>Greedy Decoding，在<strong>生成的每步中都会选择计算概率最大的单词作为输出单词</strong>。</p><p>这种方法的缺点是<strong>很容易生成重复的文本，这样可读性较差</strong>。</p><p><img src="image/image_lh-S_ia5jv.png" alt=""></p><h4 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h4><p>束搜索是另一种方法，它**在生成时的每步选择最好的<code>k</code>个局部序列。最终从这 ​<code>k</code>**<strong>个序列中选择概率最大的输出</strong>。</p><p><img src="image/image__xGaR2gaJb.png" alt=""></p><p>这两种做法在每步中都会概率最大的那个/些单词，是否有必要选择一个这样概率最大的单词呢？</p><p>实际上是每必要的，那么要怎么做呢？ 下面介绍一些基于采用的方法。</p><p><img src="image/image_n0A4VspAJV.png" alt=""></p><h4 id="Sampling-based-Decoding"><a href="#Sampling-based-Decoding" class="headerlink" title="Sampling-based Decoding"></a>Sampling-based Decoding</h4><p>这些方法按照模型计算出来单词的概率分布，<strong>按照概率随机地从词表中选择生成的单词</strong>，从而增加模型生成的多样性。</p><p>但也有可能生成无关的概率较小的单词，为了避免大量出现这种无意义的词，可以采取<code>top-n</code>和<code>top-p</code>两种方法。</p><ul><li><code>top-n</code>就是在采样的过程中局限于 <code>n</code>个最有可能的单词上进行采样。</li><li><code>top-p</code>限制采样在若干个单词上进行，这些单词满足怎样的条件呢？概率最大的这些单词概率之和要大于一个阈值 <code>p</code>。</li><li><code>sample with temperature</code> : 在最终的softmax之前，inputs除以温度洗漱 $\tau$</li></ul><p><img src="image/image_Ljy9og0UCr.png" alt=""></p><p><img src="image/image_zaUVflTga9.png" alt=""></p><h2 id="4-3-受控文本生成"><a href="#4-3-受控文本生成" class="headerlink" title="4.3 受控文本生成"></a>4.3 受控文本生成</h2><h3 id="（1）Prompt-methods"><a href="#（1）Prompt-methods" class="headerlink" title="（1）Prompt methods"></a>（1）Prompt methods</h3><p>首先**通过<code>prompt</code>**<strong>的形式来控制</strong>，比如图中在 <code>A knife</code>前面加上<code>Horror</code>来生成恐怖的描述；或者在前面加上<code>Reviews</code>来生成关于它的评价。</p><p><img src="image/image_e0BgHguA15.png" alt=""></p><p>除了可以在文本前面加一个<code>Prompt</code>，还可以在模型前加一个<code>Prefix</code>。比如增加较小的参数矩阵(Prefix)拼在Transformer前面，<strong>只对Prefix进行训练</strong>。来指导模型完成不同的任务。</p><p><img src="image/image_8e1xUDPUH9.png" alt=""></p><h3 id="（2）Modifying-probability-distribution"><a href="#（2）Modifying-probability-distribution" class="headerlink" title="（2）Modifying probability distribution"></a>（2）Modifying probability distribution</h3><p>另一种是<strong>通过修改概率分布的方法</strong>，这里会再多训练两个模型，一个<strong>生成非歧视语言的模型</strong>，另一个生成<strong>带有严重歧视的语言模型</strong>。</p><p>在<strong>文本生成时希望生成的语言贴近非歧视语言模型，而原理歧视语言模型</strong>。</p><script type="math/tex; mode=display">\tilde{P}\left(X_{t} \mid \boldsymbol{x}_{<t}\right)=\operatorname{softmax}\left(\mathbf{z}_{t}+\alpha\left(\mathbf{z}_{t}^{+}-\mathbf{z}_{t}^{-}\right)\right)</script><p><img src="image/image_LRjZjKIrxr.png" alt=""></p><h3 id="（3）Reconstructing-model-architecture"><a href="#（3）Reconstructing-model-architecture" class="headerlink" title="（3）Reconstructing model architecture"></a>（3）Reconstructing model architecture</h3><p>还有一种做法是<strong>直接修改模型结构</strong>，这里给控制信号额外增加了一系列的transfomer结构，这类transformer只负责编码控制信号。</p><p><img src="image/image_2re7Zluzzx.png" alt=""></p><h2 id="4-4-文本生成评估"><a href="#4-4-文本生成评估" class="headerlink" title="4.4 文本生成评估"></a>4.4 文本生成评估</h2><p>本小节介绍文本生成的评估方法，主要分为两类。</p><p>一类是通用的，另一类是专用的。</p><h3 id="（1）通用方法"><a href="#（1）通用方法" class="headerlink" title="（1）通用方法"></a>（1）通用方法</h3><ul><li><code>BLEU</code>：生成的文本的n-gram与tokens的text的类似度，BP是对短句的惩罚</li><li><code>PPL</code>：测试集上，有多大的概率生成sample，sample和test集上的拟合度越高，<code>ppl</code>越低</li><li><code>ROUGE</code>：基于召回计算的方法，主要解决模型生成低召回率问题。</li></ul><p><img src="image/image_8TFXRh6wCu.png" alt=""></p><p><img src="image/image_vFciIwPj1J.png" alt=""></p><h3 id="（2）其他方法"><a href="#（2）其他方法" class="headerlink" title="（2）其他方法"></a>（2）其他方法</h3><p>除了通用的方法外， 还有其他的测量矩阵。比如基于距离可以测量文本的余弦相似度。</p><p><img src="image/image_b5nfXdDJ-8.png" alt=""></p><h2 id="4-5-挑战"><a href="#4-5-挑战" class="headerlink" title="4.5 挑战"></a>4.5 挑战</h2><ul><li>生成重复的文本，然后还有seq2seq方法中的gap。</li><li>模型生成的文本往往缺乏逻辑的一致性。</li><li>在控制性方面很难同时保证语言质量与可控质量。</li><li>在评估是如何在不同模型之间统一测量标准。</li></ul><p><img src="image/image_cssangheVf.png" alt=""></p><p>在这些领域，有一部分工作在研究如何用弱监督的数据去取代监督的数据来训练大模型。比如上面列了三种不同弱监督数据来源。有文档的标题与文本的正文、网页中的锚文本对、还有的直接用大模型去根据文本生成一个query，这样通过大模型生成数据。</p>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLMs公开课 - 5.高效训练&amp;模型压缩</title>
      <link href="/llms/llms_course/5.%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"/>
      <url>/llms/llms_course/5.%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/</url>
      
        <content type="html"><![CDATA[<h1 id="1-背景介绍"><a href="#1-背景介绍" class="headerlink" title="1.背景介绍"></a>1.背景介绍</h1><h2 id="1-1-CPU-amp-GPU"><a href="#1-1-CPU-amp-GPU" class="headerlink" title="1.1 CPU &amp; GPU"></a>1.1 CPU &amp; GPU</h2><p>预训练语言模型以每年十倍的速度增大，越大的模型往往表现出更好的性能；但为了训练这些模型耗费也越来越昂贵，训练代码变得更复杂。</p><p>希望让训练过程变得更加简单，训练变得更高效，并且训练更加廉价。</p><p>首先要分析GPU内存；其次理解在多张显卡之间的合作模式是怎样的。</p><p><img src="image/image_iJFHeDFErq.png" alt=""></p><p>深度学习中最常见的矩阵乘法和向量加法适合于用GPU来计算。 &#x20;</p><p>CPU和GPU的合作方法通过CPU发送一些控制信号去控制GPU进行计算。 &#x20;</p><p><img src="image/image_hj3GbT8YLj.png" alt=""></p><p>如果想把模型的向量加法或矩阵乘法放到GPU中计算的话，需要把这些数据从CPU上拷贝到GPU上(<code>.cuda</code>)。</p><p>显卡中有哪些显存的组成。</p><h2 id="1-2-显存组成"><a href="#1-2-显存组成" class="headerlink" title="1.2 显存组成"></a>1.2 显存组成</h2><h4 id="（1）参数"><a href="#（1）参数" class="headerlink" title="（1）参数"></a>（1）参数</h4><p>为了加速模型的前向传播，需要把模型所有的参数都放到显卡中。</p><p><img src="image/image_kZdam7XmAL.png" alt=""></p><h4 id="（2）梯度"><a href="#（2）梯度" class="headerlink" title="（2）梯度"></a>（2）梯度</h4><p>在反向传播过程中，计算得到的梯度也保存到显卡中。</p><p><img src="image/image_y7R04D69wF.png" alt=""></p><h4 id="（3）中间结果"><a href="#（3）中间结果" class="headerlink" title="（3）中间结果"></a>（3）中间结果</h4><p>模型的中间计算结果，比如线性层 $y=Wx$，为了计算反向传播，需要在前向传播时在显卡中保存模型的输入(中间结果)。</p><p><img src="image/image_rRZkYTL9GR.png" alt=""></p><h4 id="（4）优化器"><a href="#（4）优化器" class="headerlink" title="（4）优化器"></a>（4）优化器</h4><p>第四部分，在显存中占大头的一部分，就是优化器，比如<code>Adam</code>，需要保存模型的梯度，和相关的历史信息<code>(m_t,v_t)</code>。它们的参数量是和梯度等数量级的。</p><p><img src="image/image_FtBMNH9bhx.png" alt=""></p><p>这四部分是预训练模型在显卡中主要的四个组成部分。 &#x20;</p><h4 id="（5）示例"><a href="#（5）示例" class="headerlink" title="（5）示例"></a>（5）示例</h4><p>一个11B参数的预训练语言模型，每个需要用float类型(FP32)来存储 &#x20;</p><script type="math/tex; mode=display">\frac{11 * 10^{9} * 4(F P 32)}{1024^{3}} \approx 40 G B</script><p>光模型参数就占用了40GB的显存。</p><h1 id="2-模型训练优化方式"><a href="#2-模型训练优化方式" class="headerlink" title="2.模型训练优化方式"></a>2.模型训练优化方式</h1><h2 id="2-1-数据并行"><a href="#2-1-数据并行" class="headerlink" title="2.1 数据并行"></a>2.1 数据并行</h2><h3 id="1）协作通信"><a href="#1）协作通信" class="headerlink" title="(1）协作通信"></a>(1）协作通信</h3><h4 id="0）参数服务器"><a href="#0）参数服务器" class="headerlink" title="0）参数服务器"></a>0）参数服务器</h4><ol><li>有一个参数服务器。</li><li>前向传播</li></ol><ul><li>在每台设备上复制该参数</li><li>每个副本处理输入的一部分。</li></ul><ol><li>反向传播</li></ol><ul><li>每个副本的梯度取平均值。</li><li>平均梯度用于更新参数服务器</li></ul><p>在数据并行过程中，有一个参数服务器，它保持了模型的参数，以及完整的数据。前向传播过程中，参数服务器上的参数会被复制到所有的显卡上，这样每张显卡上都得到了和参数服务器一样的参数。然后把数据分成三份，每张显卡用这部分数据进行前向传播&amp;反向传播，得到各自的梯度，为了让模型学到这份数据的所有知识，需要把这些梯度信息进行聚合。这里用了一个取平均操作，然后让聚合好的参数去更新模型。就能学到这三部分数据合起来完整的知识。</p><p>参数服务器可以在0号显卡上，从0号显卡把模型的参数复制到1,2,3号显卡。这就像一个广播过程；而从1,2,3号显卡上对模型的梯度进行聚合(或规约)，把规约的结果放到服务器0号显卡上。</p><p><img src="image/image_Zej5DL5gWW.png" alt=""></p><h4 id="1）Broadcast-x20"><a href="#1）Broadcast-x20" class="headerlink" title="1）Broadcast &#x20;"></a>1）Broadcast &#x20;</h4><p>广播算子做的事情就是<strong>把数据从其中的一张显卡上传到其他所有的显卡上</strong>。可以看到通过广播之后，在原本第二张显卡上的<code>in</code>这个向量广播到所有显卡上变成了<code>out</code>向量。</p><p><img src="image/image_Y3ji-ayVqk.png" alt=""></p><h4 id="2）Reduce"><a href="#2）Reduce" class="headerlink" title="2）Reduce"></a>2）Reduce</h4><p>规约(Reduce)。规约有很多种种类，可以是<strong>求和、平均、最值</strong>等。会把各张显卡上的数据进行一个规约，然后把规约得到的结果放到一张指定的显卡里面。比如这里把规约的结果放到2号显卡里面。假设规约操作是求和，那么2号显卡最终得到的<code>out=int0+in1+in2+in3</code>。</p><p><img src="image/image_Q2dAvOKAf2.png" alt=""></p><h4 id="3）All-Reduce"><a href="#3）All-Reduce" class="headerlink" title="3）All Reduce"></a>3）All Reduce</h4><p>All Reduce。比规约多了一个All。在规约的基础上，<strong>把规约得到的结果告诉所有的显卡(All)</strong>。也就是说，最后得到的结果里面，每张显卡上都会得到完全一样的<code>out=in0+in1+in2+in3</code>。</p><p><img src="image/image_HVVERl5_RM.png" alt=""></p><h4 id="4）Reduce-Scatter"><a href="#4）Reduce-Scatter" class="headerlink" title="4）Reduce Scatter"></a>4）Reduce Scatter</h4><p>Reduce Scatter。和All Reduce的相同之处在于，都会把规约得到的结果发送给所有的显卡。不同之处在于，<strong>Reduce Scatter最后每张显卡上只得到了一部分的规约结果</strong>。比如0号显卡就会得到<code>in0</code>的前<code>1/4</code>的参数＋<code>in1</code>的前<code>1/4</code>参数＋<code>in2</code>的前<code>1/4</code>参数＋<code>in3</code>的前<code>1/4</code>参数。而3号显卡会得到<code>in0</code>的最后<code>1/4</code>的参数＋<code>in1</code>的最后<code>1/4</code>参数＋<code>in2</code>的最后<code>1/4</code>参数＋<code>in3</code>的最后<code>1/4</code>参数。</p><p><img src="image/image_AV3bl7SvL_.png" alt=""></p><h4 id="5）All-Gather"><a href="#5）All-Gather" class="headerlink" title="5）All Gather"></a>5）All Gather</h4><p>收集(All Gather)，<strong>拼接每张显卡上的结果</strong>。比如<code>in0</code>拼接<code>in1</code>拼接<code>in2</code>拼接<code>in3</code>得到0号显卡的<code>out</code>，然后广播到所有显卡上。</p><p><img src="image/image_TGSaKNJtGn.png" alt=""></p><h3 id="（2）数据并行"><a href="#（2）数据并行" class="headerlink" title="（2）数据并行"></a>（2）数据并行</h3><p>可以看到数据并行有两个核心点。</p><ol><li><strong>通过把数据分成很多份</strong>，让每张显卡计算得到各自梯度之后，为了得到所有数据的知识，需要把这些梯度进行一个规约操作。</li><li>通过使用参数服务器，让规约后的梯度去更新参数服务器上的参数。然后通过广播的操作，让每张显卡上<strong>同步</strong>得到更新之后的参数。</li></ol><h3 id="（3）分布式数据并行"><a href="#（3）分布式数据并行" class="headerlink" title="（3）分布式数据并行"></a>（3）分布式数据并行</h3><p>而分布式参数并行对此进行了优化，<strong>舍弃了专门的参数服务器</strong>，让每张显卡各自去完成参数的更新，保证它们参数更新之后的结果一致。</p><p>具体来说，初始时，每张显卡上都有一个相同的模型参数，得到了一部分数据。通过前向传播&amp;反向传播得到各自的梯度信息，然后对梯度信息进行一个规约。为了让每张显卡都得到相同的梯度信息，使用All Reduce，它会把规约结果告诉所有的显卡。这样，每一张显卡上都能得到完整的规约之后的梯度，每张显卡都有一样的参数，就可以分别通过模型的优化器进行更新。每轮更新之后，既然参数一样，梯度一样，优化器之前的历史信息一样，那么更新之后，各张显卡上的参数也会保持一致。</p><p><img src="image/image_n__Rw74uPL.png" alt=""></p><p>带来的显存上的优化</p><p>中间结果是一个和<code>batch</code>乘以句子长度和模型维度相关的显存占用。在使用数据并存的时候，把一批数据分成了很多份，让每张显卡只处理其中的一部分数据。每张显卡上所处理的<code>batch</code>大小就降低到了原来的显卡数量(n)分之一。通过把输入的维度进行了降低，那么模型整体的中间结果量也会进行降低。</p><p>缺点：数据较少时，参数，梯度，优化器都会保存到显卡上。</p><p><img src="image/image_iUyIpzpaFZ.png" alt=""></p><h2 id="2-2模型并行"><a href="#2-2模型并行" class="headerlink" title="2.2模型并行"></a>2.2模型并行</h2><p>一张显卡上无法存放模型的所有参数，那么就想办法把一个模型分成很多个小的部分。</p><p>比如针对线性层矩阵乘法的例子，假设有一个<code>3×2</code>的矩阵。它乘上一个 <code>2×1</code>的向量，那么本质上可以把它的结果分成三部分。</p><p>这里的 <code>3×2</code>的矩阵就是线性层中的参数 <code>W</code>，向量就是线性层的输入。可以通过矩阵乘法的性质，把模型的参数横向切成很多份(n)，最后得到线性层的结果就是很多个这样小的矩阵乘上线性层的输入，最后把结果进行拼接。</p><p>通过这样的方式，线性层的参数就可以划分到多张显卡上。同时需要保证多张显卡上模型的输入是一样的。那么就不能使用数据并行的方式对数据进行划分。</p><script type="math/tex; mode=display">\left[\begin{array}{ll}1 & 2 \\ 3 & 4 \\ 5 & 6\end{array}\right]\left[\begin{array}{l}x \\ y\end{array}\right]=\left[\begin{array}{c}1 x+2 y \\ 0 \\ 0\end{array}\right]+\left[\begin{array}{c}0 \\ 3 x+4 y \\ 0\end{array}\right]+\left[\begin{array}{c}0 \\ 0 \\ 5 x+6 y\end{array}\right]</script><script type="math/tex; mode=display">\begin{aligned} \mathbf{y}_{A} & =W_{A \times B} \mathbf{x}_{B} \\ & =\left[W_{\frac{A}{n} \times B}^{(1)} ; W_{\frac{A}{n} \times B}^{(2)} ; \cdots ; W_{\frac{A}{n} \times B}^{(n)}\right] \mathbf{x}_{B} \\ & =\left[W_{\frac{A}{n} \times B}^{(1)} \mathbf{x}_{B} ; W_{\frac{A}{n} \times B}^{(2)} \mathbf{x}_{B} ; \cdots ; W_{\frac{A}{n} \times B}^{(n)} \mathbf{x}_{B}\right]\end{aligned}</script><p><strong>需要保证每张显卡上的输入是一样的，是同样一批数据</strong>，<strong>这里对线性层参数进行划分</strong>。每张显卡上得到线性层参数矩阵的一小部分，通过这一小部分参数和数据进行矩阵乘法，就得到了很多个子结果。这里通过All Gather收集算子进行拼接，然后广播给所有的显卡。</p><p>这样，每张显卡上只需要保存原来的N分之一的模型参数，N是显卡数量。由于只保留了这么一小部分参数，梯度也只需要保留这么多，同时优化器也只需要保持同样级别的参数量。但模型计算的中间结果没有减少，这也是该方法的一个弊端。当batch size很大的时候，仍然会出现显存溢出的问题。</p><p><img src="image/image_Fr2DQZv1oj.png" alt=""></p><h2 id="2-3-ZeRO"><a href="#2-3-ZeRO" class="headerlink" title="2.3 ZeRO"></a>2.3 ZeRO</h2><p>Zero Redundancy优化器是基于<strong>数据并行</strong>建立的一套框架，在数据并行中需要对模型的梯度进行规约。为了保证每轮迭代之后每张显卡上的参数仍然是一致的。就让每张显卡都得到了规约后的参数。然后每张显卡各自进行更新。</p><p>可以发现每张显卡用的是同样的一批数据，和同样的一批梯度去进行参数更新。那么它们各自去进行参数优化，是不是就带来了计算上的重复和冗余。</p><p>为了消除这样的冗余，那么<strong>每张显卡只获得一部分的梯度，然后只更新一部分参数</strong>。这样多张显卡通过合作的方式来更新模型的完整参数。</p><p><img src="image/image_Ma5KyykqyG.png" alt=""></p><h3 id="（1）ZeRO-Stage-1-x20"><a href="#（1）ZeRO-Stage-1-x20" class="headerlink" title="（1）ZeRO-Stage 1 &#x20;"></a>（1）ZeRO-Stage 1 &#x20;</h3><p>具体来说，由于是基于数据并行的架构，因此<strong>每张显卡上保存了完整的模型参数</strong>。有一部分数据，通过前向传播&amp;反向传播得到各自的梯度。之后在规约的时候，不是使用All Reduce的方式，而是使用<strong>Reduce Scatter</strong>让每张显卡得到一部分reduce的结果。这样让<strong>每张显卡上得到的部分梯度去更新对应的部分模型参数，最后通过收集的操作All Gather将每张显卡分工合作之后的结果告诉所有的显卡</strong>。这样，每张显卡上得到了完全一样的参数和一致的结果。</p><p><img src="image/image_wLbzyqvGlW.png" alt=""></p><h3 id="（2）ZeRO-Stage-2-x20"><a href="#（2）ZeRO-Stage-2-x20" class="headerlink" title="（2）ZeRO-Stage 2 &#x20;"></a>（2）ZeRO-Stage 2 &#x20;</h3><p>在第2阶段中，进行了一个优化。在第1阶段中，需要在反向传播得到所有梯度之后，对梯度进行Reduce Scatter，然后让每张显卡上各得到一部分规约后的梯度<code>Gradient*</code>。 原来的梯度就不需要保存在显卡上了。<strong>在第1阶段，在反向传播结束之后，才把这个梯度移除</strong>。那可以在反向传播的过程中先把<code>Gradient*</code>算出来，然后把之前一步的<code>Gradient</code>删掉。</p><p><img src="image/image_S3573Un-4H.png" alt=""></p><h3 id="（3）ZeRO-Stage-3"><a href="#（3）ZeRO-Stage-3" class="headerlink" title="（3）ZeRO-Stage 3"></a>（3）ZeRO-Stage 3</h3><p>在第3阶段，<strong>对模型的参数进一步划分</strong>。因为每张显卡上只保留了一部分梯度去进行参数更新，参数更新也只更新一部分的模型参数。这样，<strong>实际上每张显卡可以只保存它自己参数更新所负责的那一部分参数</strong>。在FP\&amp;BP的过程中，需要的时候，把模型的参数进行一个All Gather的操作， 用完之后，就可以将参数从显卡中释放。</p><p>注意：反向传播也需要模型完整的参数</p><p><img src="image/image_kXpY1CXu7H.png" alt=""></p><h3 id="（4）总结"><a href="#（4）总结" class="headerlink" title="（4）总结"></a>（4）总结</h3><p>比较一下这三个阶段的显存占比：</p><p><img src="image/image_nuslQPD0jv.png" alt=""></p><p>在第1阶段中，每张显卡只需要处理一部分的模型梯度，优化器降低到了原来的显卡数分之一，同时把中间结果的量也降低到原来的卡数分之一； &#x20;</p><p>第2阶段中，进一步地把模型的梯度划分提前，把Reduce Scatter提前到了反向传播的过程中，实际上不需要保留完整的梯度。 &#x20;</p><p>第3阶段中，进一步地划分参数。</p><p>通过这三部分的优化，显卡上的四大组成部分：参数、梯度、优化器和中间结果都得到了划分，每张显卡只需要保持自己的那部分参数。</p><h2 id="2-4-Pipeline并行"><a href="#2-4-Pipeline并行" class="headerlink" title="2.4 Pipeline并行"></a>2.4 Pipeline并行</h2><p>与模型的并行方法有类似之处，模型并行的方法通过把线性层分成很多个小的矩阵，然后把这些小的矩阵分到各张显卡上。 &#x20;</p><p>而对流水线的并行方法，<strong>把模型的不同层分给不同的显卡</strong>。比如有一个三层的Transformer，可以把Transformer的第一层分到第一张显卡上；第二层分到第二张显卡上，等等。 &#x20;</p><p>进行前向传播的过程中，需要在第一张显卡上完成第一层的模型计算，然后把计算结果告诉第二张显卡，第二章显卡进行计算，再把计算结果传给下一张显卡。 &#x20;</p><p>可以看到，这样的方法，显存占比都得到了划分，因为每张显卡上只保留了某些层的参数，也只用保留对应的梯度。虽然没有使用数据并行的方法，但模型层数变少了，这样中间结果也得到了减少。 &#x20;</p><p>但这种方法存在的弊端在于，0号显卡计算的时候，1号和2号显卡实际上处于空闲的状态。</p><p><img src="image/image_lOIrlGft7h.png" alt=""></p><h2 id="2-5-优化技术细节"><a href="#2-5-优化技术细节" class="headerlink" title="2.5 优化技术细节"></a>2.5 优化技术细节</h2><h3 id="（1）混合精度"><a href="#（1）混合精度" class="headerlink" title="（1）混合精度"></a>（1）混合精度</h3><p>比如C语言中有<code>float</code>类型、<code>double</code>类型和<code>long double</code>类型。数值表示范围依次增大。</p><p><code>double</code>类型比<code>float</code>类型有更大的表示范围和更高的有效位精度，但是<code>double</code>类型的计算会更慢。 &#x20;</p><p>同理<code>FP16</code>和<code>FP32</code>是一样的，前者的数值表示范围和有效位数更小，同时计算会更快。 &#x20;</p><p>在**一般模型的训练中，可能使用<code>FP32</code>**<strong>作为默认训练参数的表示</strong>。实际上，模型的参数一般不会超过千这个数量级，那么完全可以使用<code>FP16</code>。</p><p>那能否从<code>FP32</code>转到<code>FP16</code>得到运行速度上的提升呢？其实会面临一个问题，在参数更新的时候，<code>权重=梯度*学习率</code>，一般学习率是比较小的：<code>1e-5</code>、<code>1e-3</code>等。而<code>FP16</code>能表示的最小值，是<code>1e-5</code>数量级的数，假如梯度乘上学习率低于<code>FP16</code>的表示范围，那么参数更新量就会产生丢失(下溢)。</p><p>那么既然<code>FP32</code>能达到出更高的表示范围，<strong>可以把</strong>**<code>FP16</code><strong><strong>的梯度乘上学习率得到的参数更新量表示为</strong></strong><code>FP32</code>**，但模型的参数是更低精度的<code>FP16</code>。那无法直接把参数更新量加到模型参数上，<strong>此时需要在优化器上额外保留单精度(**</strong><code>FP32</code><strong>**)的一个参数。</strong></p><p><img src="image/image_CJEG0f2EID.png" alt=""></p><p>在一般的模型训练中，模型会有<code>FP32</code>的参数和<code>FP32</code>的梯度，然后优化器会使用<code>FP32</code>的梯度进行参数优化。</p><p>而在混合精度训练中，为了加速模型的前向传播&amp;反向传播，模型中会使用半精度(<code>FP16</code>)的参数，和半精度的梯度，把梯度传到优化器里进行优化器的更新。<strong>同时把优化器的更新量保存为</strong>**<code>FP32</code><strong><strong>类型，把这个</strong></strong><code>FP32</code><strong><strong>类型通过优化器里临时创建的</strong></strong><code>FP32</code>**<strong>参数进行累积，之后转回到FP16的参数来与模型进行计算。</strong></p><h3 id="（2）Offloading"><a href="#（2）Offloading" class="headerlink" title="（2）Offloading"></a>（2）Offloading</h3><p>以Adam为例，<strong>优化器的参数量会是模型参数量两倍的关系</strong>，显然它是一个显存占用的大头。能否把它从显卡中移除呢？</p><p><img src="image/image_DgKmRdhBmk.png" alt=""></p><p>其实是可以的，<strong>可以把它从显卡上移到CPU上</strong>。 &#x20;</p><p>这样需要先把模型参数的梯度从显卡中传给CPU，在CPU上进行优化器的优化，将优化的结果传回显卡上。在使用了ZeRO3梯度优化之后，参数划分为显卡数分之一，通过把一张显卡绑定到多张CPU上，就可以让每张CPU上的计算量足够低，能让CPU不成为模型训练的瓶颈。</p><h3 id="（3）Overlapping"><a href="#（3）Overlapping" class="headerlink" title="（3）Overlapping"></a>（3）Overlapping</h3><p>通信的计算的重叠。在GPU中的内存操作一般是<strong>异步的</strong>，<strong>可以提前给内存发送一个请求，可以去进行其他的计算，其他计算完成之后，对那个内存请求进行接收</strong>。</p><p>在模型前向传播过程中，需要把Layer1的参数通过Gather操作，然后对Layer2的参数进行优化。在获得完Layer1参数之后，在Layer1前向传播计算过程中，异步地把Layer2参数的获得进行提前。在Layer1前向传播计算完之后，Layer2的参数也已经获得，那么就可以马上进行Layer2前向传播计算。</p><p><img src="image/image_x0Z1otLirj.png" alt=""></p><h3 id="（4）Checkpointing"><a href="#（4）Checkpointing" class="headerlink" title="（4）Checkpointing"></a>（4）Checkpointing</h3><p>Checkpointing就是检查点，就像单机游戏中的存档。</p><p>为了支持模型的反向传播，需要把模型计算的所有中间结果保持在显卡中，是否可以通过存档的方式进行优化。</p><p>即<strong>不把所有结果都保持到显卡中，而只保持一定的存档点</strong>。</p><p><img src="image/image_962n-kMp6b.png" alt=""></p><p>以Transformer为例，只保留Transformer大层的输入作为检查点，在反向传播过程中，那么如何为大层中的线性层梯度进行计算。此时可以通过<strong>重计算</strong>，就是说通过Transformer每个大层的输入，在反向传播过程中，重新对它进行一个前向的传播。临时得到每个大层里面所有线性层的输入，那么得到了中间结果，就可以进行反向传播。 &#x20;</p><p>完成了这一层的反向传播之后，就可以把检查点和临时重计算的中间结果从显存中清理掉。这样就不需要保存那么多中间结果。</p><h2 id="2-6-BMTrain——使用介绍"><a href="#2-6-BMTrain——使用介绍" class="headerlink" title="2.6 BMTrain——使用介绍"></a>2.6 BMTrain——使用介绍</h2><p>本小节介绍BMTrain性能上的提升。</p><p><img src="image/image_HNYBj_eL7i.png" alt=""></p><p>据说可以使用更少的机器，达到更快的速度。</p><p><a href="https://colab.research.google.com/drive/1H-T7PmTjdcgwYUFfMikfxZ4_bMWRKu8h?usp=sharing" title="bmtrain_demo.ipynb - Colaboratory (google.com)">bmtrain_demo.ipynb - Colaboratory (google.com)</a></p><p><img src="image/image_8bXbGlYObE.png" alt=""></p><p>使用上也简单，替换一些包名前缀。就可以用到前面提到的一些技术。</p><h1 id="3-模型压缩"><a href="#3-模型压缩" class="headerlink" title="3.模型压缩"></a>3.模型压缩</h1><p>背景就是大模型的规模增长非常快。</p><p><img src="image/image_IuMy2GOKk9.png" alt=""></p><p>接下来介绍模型压缩的一些技术，目的是希望把大规模的模型压缩成更小规模。 &#x20;</p><p><img src="image/image_o0NJoIrwG0.png" alt=""></p><h2 id="3-1-知识蒸馏（Knowledge-Distillation）"><a href="#3-1-知识蒸馏（Knowledge-Distillation）" class="headerlink" title="3.1 知识蒸馏（Knowledge Distillation）"></a>3.1 知识蒸馏（Knowledge Distillation）</h2><p>什么是知识 ？</p><p>这里知识指的是<strong>模型的参数本身</strong>，本质是把模型从输入映射到输出的过程。知识蒸馏就是想把这种映射能力从大模型迁移到小模型上。</p><p><img src="image/image_bOKtx3jymv.png" alt=""></p><p>soft target比gold labels提供了更多的信息</p><p>对于输入数据，会有大模型作为Teacher，它会算出当前数据的预测结果，logits。 &#x20;</p><p>同时，该数据也可以输入给一个小得多的Student模型，该模型对于数据也能给出logits，知识蒸馏想做的事情是让这两个logits尽可能地接近。</p><p><img src="image/image_6ChngCARys.png" alt=""></p><h3 id="（1）PKD"><a href="#（1）PKD" class="headerlink" title="（1）PKD"></a>（1）PKD</h3><p>第一篇关于预训练模型的知识蒸馏工作称为PKD，它是面向BERT做的知识蒸馏。</p><blockquote><p>Sun et al. Patient Knowledge Distillation for BERT Model Compression. EMNLP 2019.</p></blockquote><p>它针<strong>对传统的知识蒸馏进行改进，让student模型可以从teacher模型中间层进行学习</strong>。 &#x20;</p><p>PKD针对模型很多层都有输出，或者说隐藏状态。它想做的事情是<strong>让student模型的隐藏状态和教师的尽可能接近</strong>。而不是仅拟合最终的输出。</p><p><img src="image/image_pih5x3mhf_.png" alt=""></p><h3 id="（2）TinyBERT"><a href="#（2）TinyBERT" class="headerlink" title="（2）TinyBERT"></a>（2）TinyBERT</h3><p>还有一个非常有代表性的工作是，TinyBERT。它进一步地推广了能学习的信号。<strong>从Teacher模型中找到了更多的可用于知识蒸馏的中间表示。</strong> 比如输入的嵌入向量以及Attention矩阵。</p><blockquote><p>Jiao et al. TinyBERT: Distilling BERT for Natural Language Understanding. Findings of EMNLP 2020</p></blockquote><p><img src="image/image_7_KbPnl6sM.png" alt=""></p><h2 id="3-2-模型剪枝"><a href="#3-2-模型剪枝" class="headerlink" title="3.2 模型剪枝"></a>3.2 模型剪枝</h2><p>这里剪枝做的事情，比如对于参数矩阵<code>W</code>，可能有很多元素非常接近于0。那么是否可以把这些参数丢掉。 &#x20;</p><p>核心是<strong>去除参数冗余部分</strong>，去除的依据是根据重要性，重要性最直观的依据是看元素绝对值大小，如果非常接近于0，那么就认为它不重要。</p><p>剪枝分为<strong>结构化剪枝</strong>和<strong>非结构化剪枝</strong>。 &#x20;</p><p>现在比较有用的是结构化剪枝，它考虑一次性删除矩阵中的一行/一列/一块。这样删掉之后矩阵还是一个比较规整的形状，从而比较利于并行化计算。</p><p><img src="image/image_ZX4JQKdfAn.png" alt=""></p><p>权重剪枝效果</p><ul><li>30-40%的权值可以被丢弃而不影响BERT的普适性(剪枝预训练)</li><li>对下游任务进行微调不会改变其性质(剪枝下游)</li></ul><p><img src="image/image_4lcwv3ArA5.png" alt=""></p><p>注意力剪枝（结构化）</p><ul><li>切除一个头</li><li>定义注意头的重要性分数</li></ul><script type="math/tex; mode=display">I_{h}=\mathbb{E}_{x \sim X}\left|\operatorname{Att}_{h}(x)^{T} \frac{\partial \mathcal{L}(x)}{\partial \operatorname{Att}_{h}(x)}\right|</script><p>针对注意力中的冗余。如果把某个注意力head丢掉，观察对与机器翻译和语言理解任务上的影响，从图中可以看到，这种做法不一定会对模型造成负面的影响，甚至很多时候还带来结果的提升。</p><p><img src="image/image_JFX7_N7uH_.png" alt=""></p><p>在不同的模型上迭代地剪枝头(蓝线)</p><p><img src="image/image_-C3dqehV4P.png" alt=""></p><ul><li>层剪枝(结构化)</li><li>将dropout从权重扩展到层</li><li>训练：随机dropout层</li><li>测试：选择任意深度的sub-network</li></ul><p><img src="image/image_YQW30KDnGC.png" alt=""></p><h2 id="3-3-模型量化"><a href="#3-3-模型量化" class="headerlink" title="3.3 模型量化"></a>3.3 模型量化</h2><p>标准的神经网络数值计算是浮点计算，那么表示的位数相对多一些。观察发现，神经网络其实不需要这么高的精度，所以可以把浮点的表示转换成定精度的表示。</p><p><img src="image/image_ZVhlgGJNQM.png" alt=""></p><p>随着位数的降低，准确率的变化：</p><p><img src="image/image_-_SeUJMrWM.png" alt=""></p><h2 id="3-4-其他方法"><a href="#3-4-其他方法" class="headerlink" title="3.4 其他方法"></a>3.4 其他方法</h2><h3 id="（1）权重共享"><a href="#（1）权重共享" class="headerlink" title="（1）权重共享"></a>（1）权重共享</h3><p>ALBERT：两种参数缩减技术&#x20;</p><ul><li>将大的词表向量分解为两个小矩阵&#x20;</li><li>跨层参数共享</li></ul><blockquote><p>Lan et al. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. ICLR 2020.</p></blockquote><p><img src="image/image_jAJhCjUWew.png" alt=""></p><h3 id="（2）低阶近似（Low-rank-Approximation）"><a href="#（2）低阶近似（Low-rank-Approximation）" class="headerlink" title="（2）低阶近似（Low-rank Approximation）"></a>（2）低阶近似（Low-rank Approximation）</h3><script type="math/tex; mode=display">\begin{array}{c}D=U \Sigma V^{\top} \in \mathbb{R}^{m \times n}, \quad m \geq n \quad \Sigma=: \operatorname{diag}\left(\sigma_{1}, \ldots, \sigma_{m}\right) \\ \widehat{D}^{*}=U_{1} \Sigma_{1} V_{1}^{\top}\end{array}</script><p>难以直接进行低秩近似</p><p>分解输入矩阵</p><p><img src="image/image_4iyN7F-eNd.png" alt=""></p><h3 id="（3）Architecture-Search"><a href="#（3）Architecture-Search" class="headerlink" title="（3）Architecture Search"></a>（3）Architecture Search</h3><p>Transformer架构是否是完美的？</p><ul><li>基于Transformer的神经结构搜索</li><li>预定义几个简单模块</li><li>对每个架构进行几个小时的训练</li></ul><blockquote><p>So et al. Primer: Searching for Efficient Transformersfor Language Modeling. NeurIPS 2021.</p></blockquote><p><img src="image/image_GQLZ5yGD_y.png" alt=""></p><p>两种高效的架构</p><p><img src="image/image_ItShAgwLyr.png" alt=""></p><h1 id="4-BMCook"><a href="#4-BMCook" class="headerlink" title="4.BMCook"></a>4.BMCook</h1><p>与现有的压缩工具包相比，BMCook支持所有主流的PLM加速方法</p><p><img src="image/image_rEGvhlD4vf.png" alt=""></p><ul><li>用几行代码实现不同的压缩方法</li><li>压缩方法可以以任何方式组合到极端加速</li></ul><p><img src="image/image_l1adDQ9qSe.png" alt=""></p><ul><li>BMCook的核心：模型压缩配置文件</li><li>用几行代码实现多种方法</li></ul><p><img src="image/image_fzXxIxQE1G.png" alt=""></p><p>蒸馏配置，支持MSE和CE损耗</p><p><img src="image/image_2ql7-vdv7T.png" alt=""></p><p>模型剪枝配置，支持非结构化剪枝</p><p><img src="image/image_2npDUZd_rI.png" alt=""></p><p>模型量化配置，更换所有线性模块</p><p><img src="image/image_klIoW-zp-r.png" alt=""></p><h1 id="5-BMInf"><a href="#5-BMInf" class="headerlink" title="5.BMInf"></a>5.BMInf</h1><p>BMInf是OpenBMB发布的第一个工具包。</p><p>Github repo: <a href="https://github.com/OpenBMB/BMInf" title="https://github.com/OpenBMB/BMInf">https://github.com/OpenBMB/BMInf</a></p><p>主要的目的是能在便宜的GPU，比如GTX 1060上，也能运行起来大模型。</p><p>消费级显卡运行大模型困难：</p><ol><li>高内存占用；</li><li>计算能力；</li></ol><h2 id="5-1-深入理解Transformer"><a href="#5-1-深入理解Transformer" class="headerlink" title="5.1 深入理解Transformer"></a>5.1 深入理解Transformer</h2><p>来深入分析模型，看如何优化模型。</p><p><img src="image/image_lfoHGz1KNm.png" alt=""></p><p>Transformer模型中主要的就是<strong>线性层</strong>，比如对于CMP-2中90%的参数都是在线性层中。</p><p>所以先来针对线性层。<strong>在允许一些精度损失的前提下，来优化线性层的运算效率</strong>。</p><blockquote><p><a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" title="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/</a></p></blockquote><p><img src="image/image_igof4x4AGk.png" alt=""></p><p>目前常用的是<code>FP32</code>，但目前模型比较大，为了降低开销，逐渐在训练过程中引入<code>FP16</code>。</p><blockquote><p><code>FP16</code>示例：1.001， -1.001<br><code>FP8</code>示例：1.0， 1.25， 1.5</p></blockquote><p><code>INT8</code>：范围更小，但更准确</p><p><img src="image/image_EqH9HU_k8o.png" alt=""></p><p>为了进一步降低开销，有没有可能使用INT8来表示参数。</p><h2 id="5-2-量化"><a href="#5-2-量化" class="headerlink" title="5.2 量化"></a>5.2 量化</h2><p>使用整数来模拟浮点矩阵运算。</p><p>首先<strong>找到矩阵里面最大的那个数，然后缩放到<code>-127~127</code>，得到缩放系数。然后把浮点矩阵中所有元素除以该缩放系数</strong>，每个元素值经过四舍五入就能得到新的整数。这样可以把浮点数矩阵拆成缩放系数和一个整数矩阵。</p><p>就让能让矩阵中值从<code>FP16</code>变成了<code>INT8</code>。</p><p><img src="image/image_GHpC0ySs7e.png" alt=""></p><p>在完成了矩阵量化之后，如果用INT8来模拟矩阵乘法呢？</p><p>针对线性层来说，分别对它的<strong>输入和权重进行量化，就可以得到两个INT8的矩阵和对应的缩放系数</strong>。接着在这两个INT8的矩阵中进行矩阵乘法。这会得到一个整数结果，但该结果INT8是存不下来的，此时会用INT32来存储。同时针对缩放系数进行一个标量惩罚，得到一个新的缩放系数，然后把整数结果乘上这个新缩放系数还原成浮点数。</p><p><img src="image/image_vZFQQ83VJu.png" alt=""></p><p>但是该方法直接应用在Transformer上效果不理想。因为Transformer中矩阵太大，使用一个缩放因子有点困难。</p><p>此需要更加精细的量化方法。<strong>可以将量化的粒度从原来的整个矩阵变成一行或一列，计算单行/列的缩放系数。</strong> 这种方法能在Transformer上达到不错的效果。</p><p>使用这种方法可以使模型大小优化一半(11G)，但还是不能放到GTX 1060(6G)上。</p><p><img src="image/image_vM2Ps3NzHD.png" alt=""></p><h2 id="5-3-内存分配"><a href="#5-3-内存分配" class="headerlink" title="5.3 内存分配"></a>5.3 内存分配</h2><p>借鉴操作系统中<strong>虚拟内存</strong>机制。 &#x20;</p><p>在进行一个百亿模型推理的时候，实际上<strong>并不会同时用到这11G的参数</strong>，每次只用一部分。比如每次只计算一层，实际上只用到了这一层的参数。那些暂时不用计算的层没必要一直放到GPU上。</p><p>这种方法在<code>CUDA6</code>中被实现了。</p><p><img src="image/image_kKEU5LcgRq.png" alt=""></p><p><strong>如果能在计算一层的同时去加载另一层参</strong>数，那么理论上只需要两层，就可以让整个模型完美地运行起来。比如我们在计算第0层的时候，同时加载第1层。这样第0层计算完之后，就可以释放第0层所占的空间，去加载第1层的参数进行计算，同时加载第2层参数。</p><p><img src="image/image_IddWpXvxQr.png" alt=""></p><p>但实际操作上遇到了一些问题，</p><p>实际上<strong>传输一层参数的时间远远超过了计算该层参数所用的时间</strong>。如果只放两层参数的话，虽然占用空间小，但花费的时间反而特别长。那是否可以多放几层，来减少加载参数所用的开销。</p><p>假设<strong>一块GPU上能放n层参数，那么可以固定n-2层在GPU上，多余的2层空间用于调度</strong>。</p><p>那现在的问题是，哪些层固定？</p><p><img src="image/image_wWVZMGCERz.png" alt=""></p><p>假如两层需要从CPU加载，左边的方案是固定7,8,9，调度6和10。  右边是固定6,8,10，调度7个9。 &#x20;</p><p>这两种方法的区别在于，<strong>要加载的层之间的间隔</strong>，左边是间隔了3层，右边是间隔1层。</p><p>那么左边的方案肯定不会差于右边的，因为我们在加载完第6层之后，中间留下第7、8、9层计算的时间来加载第10层。即留给加载第10层的时间更长。</p><p>所以要<strong>尽量扩大需要加载的两层之间的间隔</strong>。</p><p><img src="image/image_1u1AB2tOUu.png" alt=""></p><h2 id="5-4-使用介绍"><a href="#5-4-使用介绍" class="headerlink" title="5.4 使用介绍"></a>5.4 使用介绍</h2><p>在实现了上面的技术(BMInf包)之后，终于可以把百亿参数模型放到GTX1060上运行起来。</p><p><img src="image/image_-jhc7BI2Xa.png" alt=""></p><p>那么这么好的工具包怎么使用呢？</p><p><img src="image/image_Iv8Tr0mL2m.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLMs公开课 - 4.Prompt Tuning &amp; Delta Tuning</title>
      <link href="/llms/llms_course/4.Prompt_Tuning_Delta_Tuning/"/>
      <url>/llms/llms_course/4.Prompt_Tuning_Delta_Tuning/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Background"><a href="#1-Background" class="headerlink" title="1.Background"></a>1.Background</h1><h2 id="1-1-Fine-Tuning-BERT"><a href="#1-1-Fine-Tuning-BERT" class="headerlink" title="1.1 Fine Tuning : BERT"></a>1.1 Fine Tuning : BERT</h2><p>BERT不管输入是单句还是两句，它实际上会对每一个token产生一个表征，这些表征可以进行分类。</p><p>如果做的是token级别的分类任务，比如NER，那么这些token的表征会送到一个分类层中，来对每个token进行分类。</p><p>如果做的是句子级别的分类任务，那么一般会把这个<code>[CLS]</code> token，代表句子级别语义，送到分类层中。</p><p>也就是时候，BERT对于不同的任务，会将不同的表征送到分类层中去，比从零学习的神经网络效果会好很多。</p><p><img src="image/image_p8wsox93QP.png" alt=""></p><p><strong>BERT : 关系抽取</strong></p><p><img src="image/image_E78ftluugl.png" alt=""></p><p><img src="image/image_qByq6KRfRp.png" alt=""></p><ol><li>用 [CLS] 进行分类</li><li>Mention Pooling， 实体之间的所有token，concat到一起，然后再接分类层。</li><li>Mention Pooling， 区分不同实体，在embedding加入Token type embedding；最后实体之间的所有token，concat到一起，然后再接分类层。考虑位置信息。</li><li>Entity Markers : 词表中增加特殊的字符</li></ol><p>可以看到这些做法是非常经验性的，并且没有那么直观，最后我们都需要去训练一个分类器，即需要随机初始化一个分类层。将得到的表征喂给它，再和模型一起微调训练。</p><p>这种方式越来不不适应于我们现在的时代。</p><h2 id="1-2-GPT"><a href="#1-2-GPT" class="headerlink" title="1.2 GPT"></a>1.2 GPT</h2><p>GPT是一个生成模型，生成第<code>n</code>个token取决于前 <code>n-1</code> token的概率。</p><p>将最后一个隐藏状态馈送到线性输出层</p><script type="math/tex; mode=display">P\left(y \mid x^{1}, \ldots, x^{m}\right)=\operatorname{softmax}\left(h_{l}^{m} W_{y}\right)</script><p><img src="image/image_WOO679LsJ4.png" alt=""></p><h2 id="1-3-T5"><a href="#1-3-T5" class="headerlink" title="1.3 T5"></a>1.3 T5</h2><ul><li>Encoder-decoder架构，11B参数</li><li>通过简单的演示将任务转换为<code>seq2seq</code>方式</li><li>解码器经过训练以输出所需的Tokens</li></ul><p>假如要做情感分类任务，此时并不是输出0或1这种没有含义的表示。而是直接输出一个可以代表情感的单词，比如positive，来代替分类。</p><p>这种做法的好处就是，把所有任务的范式统一成一个训练的框架，即seq2seq。</p><p><img src="image/image_RJTEE8_pls.png" alt=""></p><p>比如上图示一个QNLI的数据，它由一个question和一个sentence组成。将它们喂给T5的时候，会进行一些处理，比如会说<code>qnli question是什么</code>，<code>sentence是什么</code>。然后原来的target是0，现在改成了具体的单词entailment。</p><p>即解码器被训练来输出需要的单词，而并不是我们所需要的那个类。通过这些简单的demonstration就把这些任务映射成了seq2seq之后，T5模型就可以训练了，不再需要额外的分类层让它重新训练了。</p><p>这种做法表明了一种趋势。</p><h2 id="1-4-GPT-3"><a href="#1-4-GPT-3" class="headerlink" title="1.4 GPT-3"></a>1.4 GPT-3</h2><ul><li>175B参数的大模型</li><li>参数量太大，微调很困难，采用prompts策略，应用到下游任务</li></ul><p><img src="image/image_tPO44xIpy-.png" alt=""></p><h2 id="1-5-An-Irreversible-Trend"><a href="#1-5-An-Irreversible-Trend" class="headerlink" title="1.5 An Irreversible Trend"></a>1.5 An Irreversible Trend</h2><h3 id="（1）Model-Scaling"><a href="#（1）Model-Scaling" class="headerlink" title="（1）Model Scaling"></a>（1）Model Scaling</h3><ul><li>更大的PLM往往会带来更好的性能</li><li>更好的自然语言理解能力</li><li>更好的自然语言生成质量</li><li>更好的持续学习新知识的能力</li></ul><p><img src="image/image_sLcs5uS5yp.png" alt=""></p><h3 id="（2）Difficult-Tuning"><a href="#（2）Difficult-Tuning" class="headerlink" title="（2）Difficult Tuning"></a>（2）Difficult Tuning</h3><ul><li>主要方式：Fine-tuning</li><li>更新所有参数困难</li><li>为不同的任务保留单独的实例，占用存储空太大</li><li>泛化不良，监督不足</li><li>导致在研究中很少使用大规模PLM</li></ul><h2 id="1-6-Effective-Model-Adaptation"><a href="#1-6-Effective-Model-Adaptation" class="headerlink" title="1.6 Effective Model Adaptation"></a>1.6 Effective Model Adaptation</h2><p>有两种方式高效使用大模型：</p><ul><li><strong>任务和数据方面</strong>：通过缩小模型微调和预训练之间的差距，使用<strong>Prompt-learning</strong>来增强少量学习能力</li><li><strong>优化方面</strong>：使用<strong>Delta Tuning</strong>来微调具有数十亿参数的模型，并优化一小部分参数。用小参数的优化去驱动大模型</li></ul><p><img src="image/image_x4IBRZ98Hs.png" alt=""></p><h1 id="2-Prompt-learning"><a href="#2-Prompt-learning" class="headerlink" title="2.Prompt learning"></a>2.Prompt learning</h1><h2 id="2-1-基本组成与流程介绍"><a href="#2-1-基本组成与流程介绍" class="headerlink" title="2.1 基本组成与流程介绍"></a>2.1 基本组成与流程介绍</h2><h3 id="（1）Prompt-learning"><a href="#（1）Prompt-learning" class="headerlink" title="（1）Prompt-learning"></a>（1）Prompt-learning</h3><ul><li>使用encoder作为PLMs的基本编码器</li><li>Fine-tuning为特定任务添加额外的神经网络</li><li>微调所有参数</li><li>pre-training和fine-tuning之间存在差距。<strong>pre-training以mask的方式进行训练，而fine-tuning以QA的方式进行微调，存在差距</strong>。</li></ul><p><img src="image/image_3p4ZwvVkRD.png" alt=""></p><h3 id="（2）Template-vs-Verbalizer"><a href="#（2）Template-vs-Verbalizer" class="headerlink" title="（2）Template vs Verbalizer"></a>（2）Template vs Verbalizer</h3><ul><li>用 <code>[MASK]</code> 位置添加额外的上下文（Template）</li><li>使用标签标记单词（Verbalizer）</li><li>弥补pre-training and fine-tuning差距</li></ul><p><img src="image/image_cGom5ZcmFq.png" alt=""></p><p>对于一个输入的实例，给它加一句话叫<code>it was [mask]</code>，即一个prompt，同时也给它保证成一个和预训练任务一样的形式。比如预训练中的MLM任务，这里也用mask的形式，让模型去预测该mask位置的单词。这里会预测出和预训练中一样的东西，即单词的概率分布。然后根据它子在整个词表上的分布，只去抽取其中想要的词。</p><p>比如说是一个情感分类任务，那么可能会有一个正类和负类。那么对于正类，就有good或wonderful等这种词来代表正类；而bad或terrible这种词来代表负类。</p><p>这里额外增加的这个上下文(<code>it wat [mask]</code>)称之为<strong>模板(template)</strong>；把标签映射到标签单词的映射器称为<strong>verbalizer</strong>；</p><p>这种做法还有一个好处是，<strong>不再需要考虑各种任务之间的区别</strong>。同样一套数据，根据prompt设置的不同，或者verbalizer选择的不同，那么可以把不同的任务看成是不同的分类。</p><p>这样就可以把所有的分类，甚至是生成任务都可以通过prompt重新组织成同样一个范</p><h3 id="（3）Template-：情绪分类"><a href="#（3）Template-：情绪分类" class="headerlink" title="（3）Template ：情绪分类"></a>（3）Template ：情绪分类</h3><h4 id="使用模板提示"><a href="#使用模板提示" class="headerlink" title="使用模板提示"></a>使用模板提示</h4><p>首先有一个输入<code>x = &#39;I love this moive&#39;</code>。然后给它包一个prompt，变成<code>[x] Overall, it was a [z] movie</code>。这里<code>[z]</code>就是要预测的答案。最终经过prompt之后的数据变成了<code>x&#39;=&#39;I love this moive. Overall it was a [z] movie.&#39;</code>。</p><p><img src="image/image_jcXgLQl-0S.png" alt=""></p><h4 id="预测答案"><a href="#预测答案" class="headerlink" title="预测答案"></a>预测答案</h4><p>此时模型会输出一个词表上的概率分布，但只选择需要的概率最大的标签单词，假设这里时<code>fantastic</code>。</p><p><img src="image/image_P9gyRrO2Du.png" alt=""></p><h4 id="使用Verbalizer将答案映射到类标签"><a href="#使用Verbalizer将答案映射到类标签" class="headerlink" title="使用Verbalizer将答案映射到类标签"></a>使用Verbalizer将答案映射到类标签</h4><p>比如认为<code>fantastic</code>是一个positive的类。</p><p>这样就通过prompt-learning完成情感分类的pipeline。</p><p><img src="image/image_rzB6BpepV7.png" alt=""></p><h3 id="（4）Prompt-learning-：注意事项"><a href="#（4）Prompt-learning-：注意事项" class="headerlink" title="（4）Prompt-learning ：注意事项"></a>（4）Prompt-learning ：注意事项</h3><p>预训练模型：</p><ul><li>Auto-regressive (GPT-1, GPT-2, GPT-3; OPT…)&#x20;</li><li>Masked Language Modeling (BERT, RoBERTa, DeBERTa) &#x20;</li><li>Encoder-Decoder (T5, BART)</li></ul><p>模板（Template）：</p><ul><li>Manually Design &#x20;</li><li>Auto Generation &#x20;</li><li>Textual or Continuous…</li></ul><p>用言语表达（Verbalizer）：</p><ul><li>Manually Design &#x20;</li><li>Expanding by external knowledge…</li></ul><h2 id="2-2-PTM选取"><a href="#2-2-PTM选取" class="headerlink" title="2.2 PTM选取"></a>2.2 PTM选取</h2><h3 id="（1）生成式模型"><a href="#（1）生成式模型" class="headerlink" title="（1）生成式模型"></a>（1）生成式模型</h3><p>Auto-regressive (GPT-1, GPT-2, GPT-3; OPT…)&#x20;</p><p><strong>一般的MASK放在最后，需要最后一个词</strong>，不一定适用于特别长的文本。但是现在几乎超大级别的模型，都是用这种自回归的方式去训练的。这种训练方式非常适用于超大模型。</p><p><img src="image/image_QS6bX02gwE.png" alt=""></p><h3 id="（2）MLM：分类模型，语言理解"><a href="#（2）MLM：分类模型，语言理解" class="headerlink" title="（2）MLM：分类模型，语言理解"></a>（2）MLM：分类模型，语言理解</h3><p>Masked Language Modeling (BERT, RoBERTa, DeBERTa) &#x20;</p><p>如果要做理解任务或简单的分类任务，可能更好的办法用一个BERT或RoBERTa。</p><p><strong>MASK位置在中间，会把前后的上下文attention</strong>。</p><p><img src="image/image_Jc33C_Pv-8.png" alt=""></p><h3 id="（3）Encoder-Decoder：T5"><a href="#（3）Encoder-Decoder：T5" class="headerlink" title="（3）Encoder-Decoder：T5"></a>（3）Encoder-Decoder：T5</h3><p>Encoder-Decoder (T5, BART)</p><p>然后像T5模型，实际上在训练的时候，它已经有了一些所谓的比较简单的prompt。</p><p>但没有做的事情是，详细地指明这个prompt可以长什么样。也没有说如果最后生成了那些单词之后，还可不可以做进一步地处理。</p><p>T5模型有一个好处是比较通用，没有说像自回归模型那样那么不擅长做理解，又不像BERT模型那样不擅长做生成。</p><p><img src="image/image_Apbm8US_cR.png" alt=""></p><h2 id="2-3-Template构造"><a href="#2-3-Template构造" class="headerlink" title="2.3 Template构造"></a>2.3 Template构造</h2><h3 id="（1）根据任务特点人为设计"><a href="#（1）根据任务特点人为设计" class="headerlink" title="（1）根据任务特点人为设计"></a>（1）根据任务特点人为设计</h3><p>考虑任务的特性是什么，比如关系抽取、文本分类、对话等等，我们要考虑任务的特性来构造不同的模板，此时可能需要个人的先验知识。</p><p>示例，利用人类的先验知识。对于不同的任务，确实可以利用人类的先验知识来设定不同的模板。</p><p><img src="image/image_hGhHpk8eHl.png" alt=""></p><p>TL；DR：to long, don’t reading</p><h4 id="实体关系任务Template"><a href="#实体关系任务Template" class="headerlink" title="实体关系任务Template"></a><strong>实体关系任务Template</strong></h4><ul><li>复制模板中的实体</li><li>预测细粒度实体类型</li><li>汲取世界知识</li></ul><p>假设输入是<code>London is one of the biggest cities in the world.</code>。假设要加一个模板，可以把<code>London</code>复制到模板中去，然后接上<code>is a [mask]</code>，来问模型<code>London</code>是什么类别。</p><p>这样对于每个输入，该模板开头的单词都不一样，表示不同的实体。这样来完成实体分类，从而达到抽取世界知识的效果。</p><p>通过这种做法，在少样本/零样本任务上表现特别好。</p><p><img src="image/image_kVKeV1Av5Y.png" alt=""></p><h4 id="逻辑增强Template"><a href="#逻辑增强Template" class="headerlink" title="逻辑增强Template"></a><strong>逻辑增强Template</strong></h4><p>人为定义的规则，加入分类任务中</p><p>也可以让模板变得非常复杂，这个例子中要抽取<code>Mark Twain</code>和<code>Langdon</code>的关系。</p><p>这里设计<code>prompt</code>的时候加入了一些人为定制的规则，如果要保证实体之间关系的类别，首先要保证它们实体本身类别的正确性。这样会带来额外一些制约，从而提升最终关系抽取分类的准确度。比如上图中的<code>x&#39;s parent was y</code>，必须要保证x和y都是person。</p><p><img src="image/image_aB5Pk6AQKN.png" alt=""></p><h3 id="（2）结构化，与规则相结合"><a href="#（2）结构化，与规则相结合" class="headerlink" title="（2）结构化，与规则相结合"></a>（2）结构化，与规则相结合</h3><ul><li>所有提示符的<strong>键值对</strong></li><li>将不同的任务组织成结构化的格式</li></ul><p>提醒模型应该做什么。通过这种提醒，加上训练去微调模型，在模型内部做成一个区分，而且是不同维度上的区分。</p><p>首先定义一个<code>[Format]</code>表示格式是怎样的，然后定义一个<code>[Task]</code>表示数据集是怎么的，接着是<code>[Domain]</code>表示领域；然后是<code>[Question]</code>和<code>[Passage]</code>。</p><p><img src="image/image_GRMF_Zwxil.png" alt=""></p><p><strong>多个Template</strong></p><ul><li>为输入实例使用多个不同的提示</li><li>降低即时工程成本</li><li>稳定任务性能</li></ul><p>方法</p><ul><li>均匀平均</li><li>加权平均</li></ul><p><img src="image/image_o4dxrl0Z7D.png" alt=""></p><h3 id="（3）自动生成与搜索优化"><a href="#（3）自动生成与搜索优化" class="headerlink" title="（3）自动生成与搜索优化"></a>（3）自动生成与搜索优化</h3><h4 id="基于现有单词的梯度搜索提示"><a href="#基于现有单词的梯度搜索提示" class="headerlink" title="基于现有单词的梯度搜索提示"></a>基于现有单词的梯度搜索提示</h4><blockquote><p>AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts. 2020</p></blockquote><p>这里给定输入后，定义了一些触发单词，然后定义一个prompt模板，其中每个单词都是由mask来初始化，通过最大化后验标签的概率来优化这些prompt的嵌入，然后从这些触发单词中找到和优化后的嵌入所对应的单词当成prompt。这会导致最后生成的模板看起来没有什么具体含义(语义不通)，但是它就是有效的，甚至比人类定义的prompt更加有效。</p><p>这带给我们一些启示，通过prompt的目的是触发想要的单词，实际上这并不一定需要人类的直觉来定义。也就是说，<strong>对人类来说是最好的，对模型不一定是最好的</strong>。</p><p><img src="image/image_VvoBnz9olA.png" alt=""></p><h4 id="使用encoder-decoder模型生成prompts"><a href="#使用encoder-decoder模型生成prompts" class="headerlink" title="使用encoder-decoder模型生成prompts"></a>使用encoder-decoder模型生成prompts</h4><blockquote><p>LM-BFF: Making Pre-trained Language Models Better Few-shot Learners. 2021</p></blockquote><p>利用额外的模型来生成prompt，比如对于一些情感分析类数据直接喂给T5，然后看哪些prompt加上这些数据后得到的准确度最高。选择最高的作为最终的模板。</p><p><img src="image/image_4sPvzUfwV6.png" alt=""></p><h3 id="（4）连续提示优化"><a href="#（4）连续提示优化" class="headerlink" title="（4）连续提示优化"></a>（4）连续提示优化</h3><ul><li>通过优化连续提示，生成NLU模型</li><li>P-tuning v1：prompts 输入层(与重新参数化)</li><li>P-tuning v2：prompts 每一层(如前缀微调)</li></ul><blockquote><p>P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</p></blockquote><p>也可以通过特殊的字符来产生prompt</p><p><img src="image/image_gV3S2mU__c.png" alt=""></p><h2 id="2-4-Verbalizer构造"><a href="#2-4-Verbalizer构造" class="headerlink" title="2.4 Verbalizer构造"></a>2.4 Verbalizer构造</h2><p><strong>Verbalizer就是把标签映射成标签单词的过程。</strong></p><p>可以把标签定义为一个或多个词，如果是多个词的话， 那么就求这些词概率的(加权)平均值。然后比较类别之间的概率。</p><p>Verbalizer</p><ul><li>映射：answer → 不固定标签</li><li>Tokens : 预训练语言模型词汇表中的一个或多个Tokens&#x20;</li><li>Chunks : 由多个符号组成的词块</li><li>Sentence : 任意长度的句子</li></ul><p>Construction &#x20;</p><ul><li>Hand-crafted &#x20;</li><li>Auto-generation</li></ul><h3 id="（1）人工构造Verbalizer"><a href="#（1）人工构造Verbalizer" class="headerlink" title="（1）人工构造Verbalizer"></a>（1）人工构造Verbalizer</h3><ul><li>人工设计与人类的先验知识</li><li>从一个初始的标签词开始，释义和扩展</li><li>从一个初始的标签词开始，使用外部知识并扩展</li><li>用多个Tokens分解标签</li><li>虚拟Tokens 和优化标签嵌入</li></ul><p>任务和相应的语言表达方法的例子</p><p><img src="image/image_MXsFDLcqD7.png" alt=""></p><h4 id="Knowledgeable-Prompting"><a href="#Knowledgeable-Prompting" class="headerlink" title="Knowledgeable Prompting"></a><strong>Knowledgeable Prompting</strong></h4><ul><li>标签 → 单词</li><li>使用外部知识扩展标签词</li></ul><blockquote><p>Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification. 2021</p></blockquote><p>比如有一个问题：速度与加速度之间的关系是什么？ 然后加一个模板，<code>xx question</code>。这个<code>MASK</code>会预测认为定义的标签单词的概率，比如数学(mathematics)、运动(basketbal)和它们的同义词。</p><p>接着定义一个verbalizer，先给定标签，比如这里是科学(SCIENCE)。然后用一个知识库去扩充它，接着去掉噪音词，再去选择最终需要的单词。</p><p><img src="image/image_usWIY9OBNK.png" alt=""></p><h4 id="Virtual-Tokens-as-Label-Words"><a href="#Virtual-Tokens-as-Label-Words" class="headerlink" title="Virtual Tokens as Label Words"></a><strong>Virtual Tokens as Label Words</strong></h4><ul><li>将 [MASK] tokens 的隐藏状态投影到嵌入空间并学习原型</li><li>学习到的原型构成了语言表达器，并将PLM输出映射到相应的标签。</li></ul><blockquote><p>Prototypical Verbalizer for Prompt-based Few-shot Tuning. 2021</p></blockquote><p>除了用有意义的文本来构建之外，还可以用无意义的虚拟单词来表示标签单词。比如对于每个类别对应MASK的隐藏状态进行聚类，让不同的类别学到不同的簇，用这些簇中间的嵌入来表示最终的标签词。</p><p><img src="image/image_LFF1mLFO2_.png" alt=""></p><h2 id="2-5训练新范式"><a href="#2-5训练新范式" class="headerlink" title="2.5训练新范式"></a>2.5训练新范式</h2><p>训练模型的演变</p><ol><li>传统： 随机初始化后从零训练</li><li>BERT之后： 预训练-微调</li><li>T5： 基于文本-文本格式的预训练-微调</li><li>GPT： 预训练然后使用prompt\&amp;in-context实现零/少样本学习</li></ol><p>Prompt-learning 引入了新的学习策略</p><ul><li>pre-training，prompting，优化所有参数(中型模型，few-shot设置)</li><li>pre-training，添加soft prompts，冻结模型和优化prompt embeddings (delta tuning perspective)</li><li>pre-training与prompted data，zero-shot推理(Instruction tuning和T0)</li></ul><h3 id="（1）Pre-trained-Prompt-Tuning"><a href="#（1）Pre-trained-Prompt-Tuning" class="headerlink" title="（1）Pre-trained Prompt Tuning"></a>（1）Pre-trained Prompt Tuning</h3><ul><li>向输入层加入soft prompts (embeddings)</li><li><strong>模型规模</strong></li><li>与11B PLM条件下的微调结果相当</li><li>本质上是一种<strong>参数高效(delta tuning)</strong> 方法</li></ul><p><img src="image/image_XuRvlBxLxr.png" alt=""></p><p>给预训练注入Prompts</p><ul><li>完整数据：fine-tuning和prompt-tuning是可比较的</li><li>数据少：只有tuning prompts性能较差</li><li>vanilla prompt tuning不能在低数据情况下有效推广</li><li>在预训练中注入soft prompts，提高prompt tuning的泛化性</li></ul><h3 id="（2）多任务预训练和人工prompts"><a href="#（2）多任务预训练和人工prompts" class="headerlink" title="（2）多任务预训练和人工prompts"></a>（2）多任务预训练和人工prompts</h3><ul><li>微调一个130B PLM与提示60个任务</li><li>大幅提高zero-shot 能力</li></ul><p><img src="image/image_w-2YLpihad.png" alt=""></p><p>使用人工编写的prompts来训练encoder-decoder模型</p><p><img src="image/image_1RiIPD0m2h.png" alt=""></p><p>对未见过的任务进行zero-shot(绿色)。在1300亿的模型上去训练60个任务，为每个任务收集一些prompt，然后可以在未见过的任务上进行推理。</p><p><img src="image/image_LhJK_3Wz5b.png" alt=""></p><h2 id="2-6-应用"><a href="#2-6-应用" class="headerlink" title="2.6 应用"></a>2.6 应用</h2><p>已知的应用：</p><ul><li>大多数NLP任务：NLU，生成，信息抽取，QA，翻译，…</li><li>具有位置相关性的任务可能比较困难，例如序列标记</li></ul><p>视觉，多模态，生物医药</p><ul><li>可以把输入加一些<code>soft token</code>，然后加上人工定义的医学领域的<code>prompt</code>，这样哪怕是小模型也可以在生物医学领域表现得特别好。</li><li>可以应用到多模态上，本质上是训练图片和文本之间的理解。首先给图片中对象画个框，然后给定颜色，然后在文本中问，比如这个女人被框到了什么颜色里。让模型预测颜色是什么样的，从而让模型建立颜色和文字的理解。</li></ul><p><img src="image/image_6HTZl5ppS9.png" alt=""></p><h1 id="3-Delta-Tuning"><a href="#3-Delta-Tuning" class="headerlink" title="3.Delta Tuning"></a>3.Delta Tuning</h1><p>和prompt-learning不同，delta tuning是从另一个角度来高效地微调模型。  思想是<strong>模型绝大部分参数不变，只微调一小部分模型，就能驱动大模型</strong>。</p><p><img src="image/image_EWQ0BF__6x.png" alt=""></p><p><strong>delta tuning</strong>，对于每个任务只优化小部分参数，称之为<strong>delta对象</strong>，它们可能有各种各样的结构。这些delta对象代表解决任务能力的参数化表示。实际上这些参数所占空间很小，那么就没有资源压力。</p><p>实际上要考虑的地方也有很多，比如<strong>模型的选择、delta对象如何设计</strong>等等。</p><p>为什么参数高效的微调是有用的？</p><ul><li>实际上在过去是不可能实现的，因为过去所有的网络参数都是随机初始化的。因为<strong>有了预训练之后</strong>，有了大模型之后，才能用delta tuning的范式。</li><li>因为大模型通过无监督的方式学习了<strong>统一知识</strong>，很多人认为在下游任务的微调中，只是把这个统一知识激发出来。即在下游微调任务中，并没有学习更多的知识，而是激发已经学到的知识。</li></ul><p>delta tuing中的delta是什么？</p><ul><li><strong>Addition-based （增量式）</strong>：新插入模型原来不存在的参数，然后只训练这些额外插入的参数。</li><li><strong>Specification-based （指定式）</strong>：指定模型哪些参数可以训练，哪些固定。</li><li><strong>Reparameterization-based （重参数化式）</strong>：用低维子空间参数来重参数化原来存在的参数。</li></ul><p><img src="image/image_VRPRFz1NRj.png" alt=""></p><h2 id="3-1-Addition-based-增量式"><a href="#3-1-Addition-based-增量式" class="headerlink" title="3.1 Addition-based (增量式)"></a>3.1 Addition-based (增量式)</h2><ul><li>为Transformer层增加小的adapter(下图右边的网络模块)</li><li>实际上是<strong>一个简单的双层神经网络，先缩小再非线性</strong>，再还原：$h \leftarrow f\left(h W_{d}\right) W_{u}+h$（还有残差连接）</li><li>固定其他参数，只微调这些adapter</li><li>可训练参数只有整个模型的<code>0.5%~8%</code></li></ul><p>这样<strong>可以达到和全参数模型几乎相同的效果</strong>。</p><p><img src="image/image_z83QGb_dDb.png" alt=""></p><p>增量式的方法还有一种，叫做prefix-tuning，它和prompt有些联系。</p><ul><li>Addition在线性层，layernorm之前加的，</li><li>refix-tuning在每层的隐藏状态前增加soft token，然后只优化这些soft token。</li></ul><p><img src="image/image_T5u4qlSb7M.png" alt=""></p><h2 id="3-2-Specification-based-指定式"><a href="#3-2-Specification-based-指定式" class="headerlink" title="3.2 Specification-based (指定式)"></a>3.2 Specification-based (指定式)</h2><p>这里介绍一种名为BitFit的方法，它只是去<strong>微调所有偏置(bias)</strong>，也能达到和全参数微调差不多的效果(简单任务)。</p><p><img src="image/image_WvkrFbi0R6.png" alt=""></p><h2 id="3-3-Reparameterization-based-重参数化"><a href="#3-3-Reparameterization-based-重参数化" class="headerlink" title="3.3 Reparameterization-based (重参数化)"></a>3.3 Reparameterization-based (重参数化)</h2><p>重参数方法认为<strong>优化过程可以在低维的空间完成</strong>，将120个任务的优化压缩到低维的子空间里。比如在一个五维的空间中训练，然后还原到原来的参数里。此时可以发现在低维空间找到的解，可以在120个任务上表现的很好。</p><p><img src="image/image_aaXoJBcFV7.png" alt=""></p><p><strong>LoRA</strong>认为<strong>要优化的矩阵本质上是低秩的</strong>，虽然实际上并不是低秩的，但可以强行做低秩分解，比如$1000 \times 1000$分解为 $1000 \times 2$和 $2 \times 1000$的，这样可以减少很多计算量。</p><p><img src="image/image_IW00UibPD7.png" alt=""></p><p>这些重参数化的方法本质上是有一些联系的，就是说<strong>它们都基于相似的减少，模型的优化可以用很少代价来完成</strong>。可以把它映射到一个低维或低秩的过程，用一个很简单的过程去完成这个模型的优化。</p><blockquote><p>[1] Intrinsic dimensionality explains the effectiveness of language model tuning, 2020.<br>[2] LoRA: Low-Rank Adaptation of Large Langauge Models, 2021.<br>[3] Exploring low-dimensional intrinsic task subspace via prompt tuning, 2021.</p></blockquote><p><img src="image/image_5Q_gH2KyL4.png" alt=""></p><h2 id="3-4-统一tuing"><a href="#3-4-统一tuing" class="headerlink" title="3.4 统一tuing"></a>3.4 统一tuing</h2><p>这种联系可以扩展到更多的方法，最近有人建立了一种统一框架，把这三种方式联系起来。</p><p><img src="image/image_RobluIfuas.png" alt=""></p><p>认为它们本质上可能在做同一件事情。</p><p><img src="image/image_8mR8zASNPJ.png" alt=""></p><p>实际上它们都是<strong>固定大模型参数，只微调很小部分的delta对象</strong>。因此可以推导出更加通用的delta tuning变体。</p><p><img src="image/image_rywWjh1QC2.png" alt=""></p><p>在100多个NLP任务上进行了实验表明，delta tuning确实效果比较好，比如LoRA(LR)在100多个任务上只微调了0.38%的参数就能达到平均和全参数微调(FT)差不多的效果。</p><p><img src="image/image_nQPquW561i.png" alt=""></p><p>然后还可以发现不同的任务适用于不同的结构，那么是否存在一个最优结构呢。</p><p>比如可以<strong>用自动机器学习的方法来搜索这个结构</strong>，在每个位置设定一个开关，表示使用哪种delta tuning方式。这样就能找到一个比较稀疏的解，能让模型的效果特别好。</p><p><img src="image/image_puGgsEImHi.png" alt=""></p><p>下图横轴表示参数量的稀疏程度(由多变少)，纵轴代表准确率。当参数量变少到万分之一的时候，其他的delta tuning方法都有显著的下降，而通过自动搜索方法得到的解它的效果和全参数微调还是保持相差不大。</p><p><img src="image/image_qwt7P0Y5Y7.png" alt=""></p><p>通过自动搜索的方法用更少的参数去探索一种极限。同时delta tuning还具备非常好的<strong>可迁移性</strong>，这几种delta tuning在不同的任务上得到的图像差不多。</p><p><img src="image/image_E1NfAST3zc.png" alt=""></p><h2 id="3-5-总结"><a href="#3-5-总结" class="headerlink" title="3.5 总结"></a>3.5 总结</h2><ul><li><strong>delta tuning在超大规模的模型上非常高效</strong></li><li>它的结构随着模型的增加变得越发不重要</li></ul><h1 id="4-OpenPrompt"><a href="#4-OpenPrompt" class="headerlink" title="4.OpenPrompt"></a>4.OpenPrompt</h1><h2 id="4-1-OpenPrompt"><a href="#4-1-OpenPrompt" class="headerlink" title="4.1 OpenPrompt"></a>4.1 OpenPrompt</h2><p><img src="image/image_HZnAwG8ezq.png" alt=""></p><p>Prompt其实可以自定义很多不同的Template/verbalizer，比如一个普通的情感分类任务，模板可能是<code>it was__</code>。 &#x20;</p><p>模板可能不同，mask位置可能不同，verbalizer也可能不同。</p><p>之前通常将模板写死到代码中，不方便我们尝试不同的模板，也无法灵活地找到mask的位置。 &#x20;<br>OpenPrompt工具包的目的是解决上面所说的问题，<strong>定义统一的prompt tuning范式</strong>，使用不同的模板，定义不同的verbalizer，去实现不同的任务。</p><p><img src="image/image_qYyVPhC-Vp.png" alt=""></p><p>上图是API交互。<code>PromptDataset</code>的输出是一个<code>Tempate</code>，包裹上输入之后，被<code>PromptTokenizer</code>分词成可以输入模型的数据。<code>PromptModel</code>把该输入中的soft token转换成<code>TemplateEmbeddings</code>，再输入预训练模型(PLM)，最后把mask的位置的输出抽出来，送给<code>Verbalizer</code>进行预测。 &#x20;</p><p>除此之外，通过<code>PromptTrainer</code>类提供了不同的训练方式。</p><p>下面简单看一下如何使用OpenPrompt。</p><ol><li>定义一个任务</li><li>选择预训练模型</li><li>定义一个Template</li><li>定义一个Verbalizer</li><li>定义一个PromptModel</li><li>训练并推理</li></ol><p>一些Template的例子： &#x20;</p><p><img src="image/image_Qyx2aFdBee.png" alt=""></p><p>实施各种快速学习管道 （灰线是暂时没有出现的方法）</p><ul><li>修改单独的模块和创建新的方法&#x20;</li><li>将现有方法应用于其他场景</li></ul><p><img src="image/image_VHhBqyni_M.png" alt=""></p><h2 id="4-2-OpenPrompt-demo"><a href="#4-2-OpenPrompt-demo" class="headerlink" title="4.2 OpenPrompt demo"></a>4.2 OpenPrompt demo</h2><p>下面用一个实例来进行演示。&#x20;</p><p><a href="https://colab.research.google.com/drive/1bQnMvui8Zb6EwNXWiC3DYKr8AH0IEbQa#scrollTo=j1r0_pLwaqtZ" title="OpenPrompt Demo - Colaboratory (google.com)">OpenPrompt Demo - Colaboratory (google.com)</a></p><h4 id="（1）安装包"><a href="#（1）安装包" class="headerlink" title="（1）安装包"></a>（1）安装包</h4><p>首先安装需要的包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">!pip install transformers --quiet</span><br><span class="line">!pip install datasets==2.0 --quiet</span><br><span class="line">!pip install openprompt --quiet</span><br><span class="line">!pip install torch --quiet</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="（2）加载数据集"><a href="#（2）加载数据集" class="headerlink" title="（2）加载数据集"></a>（2）加载数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">raw_dataset = load_dataset(<span class="string">&#x27;super_glue&#x27;</span>, <span class="string">&#x27;cb&#x27;</span>, cache_dir=<span class="string">&quot;../datasets/.cache/huggingface_datasets&quot;</span>)</span><br><span class="line">raw_dataset[<span class="string">&#x27;train&#x27;</span>][<span class="number">0</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;premise&#x27;: &#x27;It was a complex language. Not written down but handed down. One might say it was peeled down.&#x27;,</span><br><span class="line"> &#x27;hypothesis&#x27;: &#x27;the language was peeled down&#x27;,</span><br><span class="line"> &#x27;idx&#x27;: 0,</span><br><span class="line"> &#x27;label&#x27;: 0&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>并查看样本。</p><h4 id="（3）加载模型和tokenizer"><a href="#（3）加载模型和tokenizer" class="headerlink" title="（3）加载模型和tokenizer"></a>（3）加载模型和tokenizer</h4><p>下面加载模型和分词器：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from openprompt.plms import load_plm</span><br><span class="line">plm, tokenizer, model_config, WrapperClass = load_plm(&quot;t5&quot;, &quot;t5-base&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="（4）构造输入"><a href="#（4）构造输入" class="headerlink" title="（4）构造输入"></a>（4）构造输入</h4><p><strong>构建输入</strong>，将原始数据集处理成OpenPrompt可以使用的格式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from openprompt.data_utils import InputExample</span><br><span class="line"></span><br><span class="line">dataset = &#123;&#125;</span><br><span class="line">for split in [&#x27;train&#x27;, &#x27;validation&#x27;, &#x27;test&#x27;]:</span><br><span class="line">    dataset[split] = []</span><br><span class="line">    for data in raw_dataset[split]:</span><br><span class="line">        input_example = InputExample(text_a = data[&#x27;premise&#x27;], text_b = data[&#x27;hypothesis&#x27;], label=int(data[&#x27;label&#x27;]), guid=data[&#x27;idx&#x27;])</span><br><span class="line">        dataset[split].append(input_example)</span><br><span class="line">print(dataset[&#x27;train&#x27;][0])</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;guid&quot;: 0,</span><br><span class="line">  &quot;label&quot;: 0,</span><br><span class="line">  &quot;meta&quot;: &#123;&#125;,</span><br><span class="line">  &quot;text_a&quot;: &quot;It was a complex language. Not written down but handed down. One might say it was peeled down.&quot;,</span><br><span class="line">  &quot;text_b&quot;: &quot;the language was peeled down&quot;,</span><br><span class="line">  &quot;tgt_text&quot;: null</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到，有一部分叫<code>text_a</code>，另一部分输入叫<code>text_b</code>。还有刚才提到的<code>meta</code>信息。下面</p><p><strong>定义模板文本</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from openprompt.prompts import ManualTemplate</span><br><span class="line">template_text = &#x27;&#123;&quot;placeholder&quot;:&quot;text_a&quot;&#125; Deduction: &#123;&quot;placeholder&quot;:&quot;text_b&quot;&#125;. Is it correct? &#123;&quot;mask&quot;&#125;.&#x27;</span><br><span class="line">mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>模板定义如上所示，在mask位置输出我们想要的答案。</p><p><strong>使用标记器包装器类对wrapped_example进行标记</strong></p><p>为了更好地理解模板包裹了什么，我们看一个例子</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wrapped_example = mytemplate.wrap_one_example(dataset[&#x27;train&#x27;][0])</span><br><span class="line">wrapped_example</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[[&#123;&#x27;text&#x27;: &#x27;It was a complex language. Not written down but handed down. One might say it was peeled down.&#x27;,</span><br><span class="line">   &#x27;loss_ids&#x27;: 0,</span><br><span class="line">   &#x27;shortenable_ids&#x27;: 1&#125;,</span><br><span class="line">  &#123;&#x27;text&#x27;: &#x27; Deduction:&#x27;, &#x27;loss_ids&#x27;: 0, &#x27;shortenable_ids&#x27;: 0&#125;,</span><br><span class="line">  &#123;&#x27;text&#x27;: &#x27; the language was peeled down&#x27;,</span><br><span class="line">   &#x27;loss_ids&#x27;: 0,</span><br><span class="line">   &#x27;shortenable_ids&#x27;: 1&#125;,</span><br><span class="line">  &#123;&#x27;text&#x27;: &#x27;. Is it correct?&#x27;, &#x27;loss_ids&#x27;: 0, &#x27;shortenable_ids&#x27;: 0&#125;,</span><br><span class="line">  &#123;&#x27;text&#x27;: &#x27;&lt;mask&gt;&#x27;, &#x27;loss_ids&#x27;: 1, &#x27;shortenable_ids&#x27;: 0&#125;,</span><br><span class="line">  &#123;&#x27;text&#x27;: &#x27;.&#x27;, &#x27;loss_ids&#x27;: 0, &#x27;shortenable_ids&#x27;: 0&#125;],</span><br><span class="line"> &#123;&#x27;guid&#x27;: 0, &#x27;label&#x27;: 0&#125;]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>shortenable_ids</code>表示是否可压缩，<code>loss_ids</code>表示是否需要计算损失。</p><p>接下来处理这样的输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wrapped_t5tokenizer = WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=&quot;head&quot;)</span><br><span class="line"># or</span><br><span class="line">from openprompt.plms import T5TokenizerWrapper</span><br><span class="line">wrapped_t5tokenizer= T5TokenizerWrapper(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=&quot;head&quot;)</span><br><span class="line"></span><br><span class="line"># You can see what a tokenized example looks like by</span><br><span class="line">tokenized_example = wrapped_t5tokenizer.tokenize_one_example(wrapped_example, teacher_forcing=False)</span><br><span class="line">print(tokenized_example)</span><br><span class="line">print(tokenizer.convert_ids_to_tokens(tokenized_example[&#x27;input_ids&#x27;]))</span><br><span class="line">print(tokenizer.convert_ids_to_tokens(tokenized_example[&#x27;decoder_input_ids&#x27;]))</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;input_ids&#x27;: [94, 47, 3, 9, 1561, 1612, 5, 933, 1545, 323, 68, 14014, 323, 5, 555, 429, 497, 34, 47, 158, 400, 26, 323, 5, 374, 8291, 10, 8, 1612, 47, 158, 400, 26, 323, 3, 5, 27, 7, 34, 2024, 58, 32099, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &#x27;attention_mask&#x27;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &#x27;decoder_input_ids&#x27;: [0, 32099, 0], &#x27;loss_ids&#x27;: [0, 1, 0]&#125;</span><br><span class="line">[&#x27;▁It&#x27;, &#x27;▁was&#x27;, &#x27;▁&#x27;, &#x27;a&#x27;, &#x27;▁complex&#x27;, &#x27;▁language&#x27;, &#x27;.&#x27;, &#x27;▁Not&#x27;, &#x27;▁written&#x27;, &#x27;▁down&#x27;, &#x27;▁but&#x27;, &#x27;▁handed&#x27;, &#x27;▁down&#x27;, &#x27;.&#x27;, &#x27;▁One&#x27;, &#x27;▁might&#x27;, &#x27;▁say&#x27;, &#x27;▁it&#x27;, &#x27;▁was&#x27;, &#x27;▁pe&#x27;, &#x27;ele&#x27;, &#x27;d&#x27;, &#x27;▁down&#x27;, &#x27;.&#x27;, &#x27;▁De&#x27;, &#x27;duction&#x27;, &#x27;:&#x27;, &#x27;▁the&#x27;, &#x27;▁language&#x27;, &#x27;▁was&#x27;, &#x27;▁pe&#x27;, &#x27;ele&#x27;, &#x27;d&#x27;, &#x27;▁down&#x27;, &#x27;▁&#x27;, &#x27;.&#x27;, &#x27;▁I&#x27;, &#x27;s&#x27;, &#x27;▁it&#x27;, &#x27;▁correct&#x27;, &#x27;?&#x27;, &#x27;&lt;extra_id_0&gt;&#x27;, &#x27;▁&#x27;, &#x27;.&#x27;, &#x27;&lt;/s&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;]</span><br><span class="line">[&#x27;&lt;pad&gt;&#x27;, &#x27;&lt;extra_id_0&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这样对整个数据集进行处理：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model_inputs = &#123;&#125;</span><br><span class="line">for split in [&#x27;train&#x27;, &#x27;validation&#x27;, &#x27;test&#x27;]:</span><br><span class="line">    model_inputs[split] = []</span><br><span class="line">    for sample in dataset[split]:</span><br><span class="line">        tokenized_example = wrapped_t5tokenizer.tokenize_one_example(mytemplate.wrap_one_example(sample), teacher_forcing=False)</span><br><span class="line">        model_inputs[split].append(tokenized_example)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="（5）构造dataloader"><a href="#（5）构造dataloader" class="headerlink" title="（5）构造dataloader"></a>（5）构造dataloader</h4><p>dataloader对象是一个可迭代的对象，迭代它将为模型的每次前向传递提供输入张量。&#x20;</p><p>下面构建数据加载器：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from openprompt import PromptDataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = PromptDataLoader(dataset=dataset[&quot;train&quot;], template=mytemplate, tokenizer=tokenizer,</span><br><span class="line">    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,</span><br><span class="line">    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,</span><br><span class="line">    truncate_method=&quot;head&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="（6）构建Verbalizer"><a href="#（6）构建Verbalizer" class="headerlink" title="（6）构建Verbalizer"></a>（6）构建Verbalizer</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from openprompt.prompts import ManualVerbalizer</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># for example the verbalizer contains multiple label words in each class</span><br><span class="line">myverbalizer = ManualVerbalizer(tokenizer, num_classes=3,</span><br><span class="line">                        label_words=[[&quot;yes&quot;], [&quot;no&quot;], [&quot;maybe&quot;]])</span><br><span class="line"></span><br><span class="line">print(myverbalizer.label_words_ids)</span><br><span class="line">logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm, and</span><br><span class="line">print(myverbalizer.process_logits(logits))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这里指定了三个标签单词，分别对应三种类别。下面看verbalizer加工后的形状：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[[4273]],</span><br><span class="line"></span><br><span class="line">        [[ 150]],</span><br><span class="line"></span><br><span class="line">        [[2087]]])</span><br><span class="line">tensor([[-2.6867, -0.1306, -2.9124],</span><br><span class="line">        [-0.6579, -0.8735, -2.7400]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="（7）分类Pipeline"><a href="#（7）分类Pipeline" class="headerlink" title="（7）分类Pipeline"></a>（7）分类Pipeline</h4><p>下面定义一个分类Pipeline。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from openprompt import PromptForClassification</span><br><span class="line"></span><br><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line">print(&quot;GPU enabled? &#123;&#125;&quot;.format(use_cuda))</span><br><span class="line">prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)</span><br><span class="line">if use_cuda:</span><br><span class="line">    prompt_model=  prompt_model.cuda()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="（8）GPU训练"><a href="#（8）GPU训练" class="headerlink" title="（8）GPU训练"></a>（8）GPU训练</h4><p>把模型移到GPU上。在GPU上进行训练：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># Now the training is standard</span><br><span class="line">from transformers import  AdamW, get_linear_schedule_with_warmup</span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line">no_decay = [&#x27;bias&#x27;, &#x27;LayerNorm.weight&#x27;]</span><br><span class="line"># it&#x27;s always good practice to set no decay to biase and LayerNorm parameters</span><br><span class="line">optimizer_grouped_parameters = [</span><br><span class="line">    &#123;&#x27;params&#x27;: [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], &#x27;weight_decay&#x27;: 0.01&#125;,</span><br><span class="line">    &#123;&#x27;params&#x27;: [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], &#x27;weight_decay&#x27;: 0.0&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)</span><br><span class="line"></span><br><span class="line">for epoch in range(5):</span><br><span class="line">    tot_loss = 0</span><br><span class="line">    for step, inputs in enumerate(train_dataloader):</span><br><span class="line">        if use_cuda:</span><br><span class="line">            inputs = inputs.cuda()</span><br><span class="line">        logits = prompt_model(inputs)</span><br><span class="line">        labels = inputs[&#x27;label&#x27;]</span><br><span class="line">        loss = loss_func(logits, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        tot_loss += loss.item()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        if step %100 ==1:</span><br><span class="line">            print(&quot;Epoch &#123;&#125;, average loss: &#123;&#125;&quot;.format(epoch, tot_loss/(step+1)), flush=True)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0, average loss: 0.6918223202228546</span><br><span class="line">Epoch 1, average loss: 0.21019931323826313</span><br><span class="line">Epoch 2, average loss: 0.0998007245361805</span><br><span class="line">Epoch 3, average loss: 0.0021352323819883168</span><br><span class="line">Epoch 4, average loss: 0.00015113733388716355</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="（9）评估模型"><a href="#（9）评估模型" class="headerlink" title="（9）评估模型"></a>（9）评估模型</h4><p>最后我们评估一下模型效果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">validation_dataloader = PromptDataLoader(dataset=dataset[&quot;validation&quot;], template=mytemplate, tokenizer=tokenizer,</span><br><span class="line">    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,</span><br><span class="line">    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,</span><br><span class="line">    truncate_method=&quot;head&quot;)</span><br><span class="line"></span><br><span class="line">allpreds = []</span><br><span class="line">alllabels = []</span><br><span class="line">for step, inputs in enumerate(validation_dataloader):</span><br><span class="line">    if use_cuda:</span><br><span class="line">        inputs = inputs.cuda()</span><br><span class="line">    logits = prompt_model(inputs)</span><br><span class="line">    labels = inputs[&#x27;label&#x27;]</span><br><span class="line">    alllabels.extend(labels.cpu().tolist())</span><br><span class="line">    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())</span><br><span class="line"></span><br><span class="line">acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)</span><br><span class="line">print(acc)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0.9107142857142857</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="5-OpenDelta介绍"><a href="#5-OpenDelta介绍" class="headerlink" title="5.OpenDelta介绍"></a>5.OpenDelta介绍</h1><p>下面介绍OpenDelta工具，用于delta tuning，它的特点有：</p><ul><li>干净：不需要编辑backonePTM的代码。 &#x20;</li><li>简单：从全模型tuning迁移到delta-tuning只需要3行代码。 &#x20;</li><li>可持续：外部库的进化不需要更新。</li><li>可扩展：各种ptm可以共享相同的delta-tuning代码。 &#x20;</li><li>灵活：能够应用delta-tuning到(几乎)任何位置。&#x20;</li></ul><p><img src="image/image_-OAUwkIeph.png" alt=""></p><p>非常少的修改：</p><p><img src="image/image_9s3Ai-JhRE.png" alt=""></p><p>支持非常多的模型。</p><p>还是来看一个实例吧。</p><p>首先安装需要的包。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!pip install transformers --quiet</span><br><span class="line">!pip install datasets==2.0 --quiet</span><br><span class="line">!pip install opendelta==0.2.2 --quiet</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在开头载入需要用到的包：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from dataclasses import dataclass, field</span><br><span class="line">from typing import Optional, List</span><br><span class="line">from transformers import Seq2SeqTrainingArguments, TrainerCallback </span><br><span class="line">from datasets import load_dataset, load_metric, concatenate_datasets</span><br><span class="line">import transformers</span><br><span class="line">from transformers import (</span><br><span class="line">    AutoConfig,</span><br><span class="line">    AutoModelForSeq2SeqLM,</span><br><span class="line">    AutoTokenizer,</span><br><span class="line">    HfArgumentParser,</span><br><span class="line">    MBartTokenizer,</span><br><span class="line">    default_data_collator,</span><br><span class="line">    set_seed,</span><br><span class="line">)</span><br><span class="line">from datasets import load_dataset</span><br><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line">import random</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>定义模型的参数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">@dataclass</span><br><span class="line">class ModelArguments:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model_name_or_path: str = field(</span><br><span class="line">        metadata=&#123;&quot;help&quot;: &quot;Path to pretrained model or model identifier from huggingface.co/models&quot;&#125;</span><br><span class="line">    )</span><br><span class="line">    config_name: Optional[str] = field(</span><br><span class="line">        default=None, metadata=&#123;&quot;help&quot;: &quot;Pretrained config name or path if not the same as model_name&quot;&#125;</span><br><span class="line">    )</span><br><span class="line">    tokenizer_name: Optional[str] = field(</span><br><span class="line">        default=None, metadata=&#123;&quot;help&quot;: &quot;Pretrained tokenizer name or path if not the same as model_name&quot;&#125;</span><br><span class="line">    )</span><br><span class="line">    cache_dir: Optional[str] = field(</span><br><span class="line">        default=None,</span><br><span class="line">        metadata=&#123;&quot;help&quot;: &quot;Where to store the pretrained models downloaded from huggingface.co&quot;&#125;,</span><br><span class="line">    )</span><br><span class="line">    use_fast_tokenizer: bool = field(</span><br><span class="line">        default=True,</span><br><span class="line">        metadata=&#123;&quot;help&quot;: &quot;Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.&quot;&#125;,</span><br><span class="line">    )</span><br><span class="line">    model_revision: str = field(</span><br><span class="line">        default=&quot;main&quot;,</span><br><span class="line">        metadata=&#123;&quot;help&quot;: &quot;The specific model version to use (can be a branch name, tag name or commit id).&quot;&#125;,</span><br><span class="line">    )</span><br><span class="line">    use_auth_token: bool = field(</span><br><span class="line">        default=False,</span><br><span class="line">        metadata=&#123;</span><br><span class="line">            &quot;help&quot;: &quot;Will use the token generated when running `transformers-cli login` (necessary to use this script &quot;</span><br><span class="line">            &quot;with private models).&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">model_args = ModelArguments(model_name_or_path=&quot;t5-large&quot;, )</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>使用传统的方式加载模型：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">config = AutoConfig.from_pretrained(</span><br><span class="line">    model_args.config_name if model_args.config_name else model_args.model_name_or_path,</span><br><span class="line">    cache_dir=model_args.cache_dir,</span><br><span class="line">    revision=model_args.model_revision,</span><br><span class="line">    use_auth_token=True if model_args.use_auth_token else None,</span><br><span class="line">)</span><br><span class="line">config.dropout_rate = 0.0</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(</span><br><span class="line">    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,</span><br><span class="line">    cache_dir=model_args.cache_dir,</span><br><span class="line">    use_fast=model_args.use_fast_tokenizer,</span><br><span class="line">    revision=model_args.model_revision,</span><br><span class="line">    use_auth_token=True if model_args.use_auth_token else None,</span><br><span class="line">)</span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    from_tf=bool(&quot;.ckpt&quot; in model_args.model_name_or_path),</span><br><span class="line">    config=config,</span><br><span class="line">    cache_dir=model_args.cache_dir,</span><br><span class="line">    revision=model_args.model_revision,</span><br><span class="line">    use_auth_token=True if model_args.use_auth_token else None,</span><br><span class="line">)</span><br><span class="line">model.resize_token_embeddings(len(tokenizer))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>下面演示一下opendelta提供的可视化功能：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from opendelta import Visualization</span><br><span class="line">Visualization(model).structure_graph();</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="image/image_BAKFvyUQLJ.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">├── shared(Embedding),lm_head(Linear) weight:[<span class="number">32100</span>, <span class="number">1024</span>]</span><br><span class="line">├── encoder (T5Stack)</span><br><span class="line">│   ├── embed_tokens (Embedding) weight:[<span class="number">32100</span>, <span class="number">1024</span>]</span><br><span class="line">│   ├── block (ModuleList)</span><br><span class="line">│   │   ├── <span class="number">0</span> (T5Block)</span><br><span class="line">│   │   │   └── layer (ModuleList)</span><br><span class="line">│   │   │       ├── <span class="number">0</span> (T5LayerSelfAttention)</span><br><span class="line">│   │   │       │   ├── SelfAttention (T5Attention)</span><br><span class="line">│   │   │       │   │   ├── q,k,v,o(Linear) weight:[<span class="number">1024</span>, <span class="number">1024</span>]</span><br><span class="line">│   │   │       │   │   └── relative_attention_bias (Embedding) weight:[<span class="number">32</span>, <span class="number">16</span>]</span><br><span class="line">│   │   │       │   └── layer_norm (T5LayerNorm) weight:[<span class="number">1024</span>]</span><br><span class="line">│   │   │       └── <span class="number">1</span> (T5LayerFF)</span><br><span class="line">│   │   │           ├── DenseReluDense (T5DenseActDense)</span><br><span class="line">│   │   │           │   ├── wi (Linear) weight:[<span class="number">4096</span>, <span class="number">1024</span>]</span><br><span class="line">│   │   │           │   └── wo (Linear) weight:[<span class="number">1024</span>, <span class="number">4096</span>]</span><br><span class="line">│   │   │           └── layer_norm (T5LayerNorm) weight:[<span class="number">1024</span>]</span><br><span class="line">│   │   └── <span class="number">1</span>-<span class="number">23</span>(T5Block)</span><br><span class="line">│   │       └── layer (ModuleList)</span><br><span class="line">│   │           ├── <span class="number">0</span> (T5LayerSelfAttention)</span><br><span class="line">│   │           │   ├── SelfAttention (T5Attention)</span><br><span class="line">│   │           │   │   └── q,k,v,o(Linear) weight:[<span class="number">1024</span>, <span class="number">1024</span>]</span><br><span class="line">│   │           │   └── layer_norm (T5LayerNorm) weight:[<span class="number">1024</span>]</span><br><span class="line">│   │           └── <span class="number">1</span> (T5LayerFF)</span><br><span class="line">│   │               ├── DenseReluDense (T5DenseActDense)</span><br><span class="line">│   │               │   ├── wi (Linear) weight:[<span class="number">4096</span>, <span class="number">1024</span>]</span><br><span class="line">│   │               │   └── wo (Linear) weight:[<span class="number">1024</span>, <span class="number">4096</span>]</span><br><span class="line">│   │               └── layer_norm (T5LayerNorm) weight:[<span class="number">1024</span>]</span><br><span class="line">│   └── final_layer_norm (T5LayerNorm) weight:[<span class="number">1024</span>]</span><br><span class="line">└── decoder (T5Stack)</span><br><span class="line">    ├── embed_tokens (Embedding) weight:[<span class="number">32100</span>, <span class="number">1024</span>]</span><br><span class="line">    ├── block (ModuleList)</span><br><span class="line">    │   ├── <span class="number">0</span> (T5Block)</span><br><span class="line">    │   │   └── layer (ModuleList)</span><br><span class="line">    │   │       ├── <span class="number">0</span> (T5LayerSelfAttention)</span><br><span class="line">    │   │       │   ├── SelfAttention (T5Attention)</span><br><span class="line">    │   │       │   │   ├── q,k,v,o(Linear) weight:[<span class="number">1024</span>, <span class="number">1024</span>]</span><br><span class="line">    │   │       │   │   └── relative_attention_bias (Embedding) weight:[<span class="number">32</span>, <span class="number">16</span>]</span><br><span class="line">    │   │       │   └── layer_norm (T5LayerNorm) weight:[<span class="number">1024</span>]</span><br><span class="line">    │   │       ├── <span class="number">1</span> (T5LayerCrossAttention)</span><br><span class="line">    │   │       │   ├── EncDecAttention (T5Attention)</span><br><span class="line">    │   │       │   │   └── q,k,v,o(Linear) weight:[<span class="number">1024</span>, <span class="number">1024</span>]</span><br><span class="line">    │   │       │   └── layer_norm (T5LayerNorm) weight:[<span class="number">1024</span>]</span><br><span class="line">    │   │       └── <span class="number">2</span> (T5LayerFF)</span><br><span class="line">    │   │           ├── DenseReluDense (T5DenseActDense)</span><br><span class="line">    │   │           │   ├── wi (Linear) weight:[<span class="number">4096</span>, <span class="number">1024</span>]</span><br><span class="line">    │   │           │   └── wo (Linear) weight:[<span class="number">1024</span>, <span class="number">4096</span>]</span><br><span class="line">    │   │           └── layer_norm (T5LayerNorm) weight:[<span class="number">1024</span>]</span><br><span class="line">    │   └── <span class="number">1</span>-<span class="number">23</span>(T5Block)</span><br><span class="line">    │       └── layer (ModuleList)</span><br><span class="line">    │           ├── <span class="number">0</span> (T5LayerSelfAttention)</span><br><span class="line">    │           │   ├── SelfAttention (T5Attention)</span><br><span class="line">    │           │   │   └── q,k,v,o(Linear) weight:[<span class="number">1024</span>, <span class="number">1024</span>]</span><br><span class="line">    │           │   └── layer_norm (T5LayerNorm) weight:[<span class="number">1024</span>]</span><br><span class="line">    │           ├── <span class="number">1</span> (T5LayerCrossAttention)</span><br><span class="line">    │           │   ├── EncDecAttention (T5Attention)</span><br><span class="line">    │           │   │   └── q,k,v,o(Linear) weight:[<span class="number">1024</span>, <span class="number">1024</span>]</span><br><span class="line">    │           │   └── layer_norm (T5LayerNorm) weight:[<span class="number">1024</span>]</span><br><span class="line">    │           └── <span class="number">2</span> (T5LayerFF)</span><br><span class="line">    │               ├── DenseReluDense (T5DenseActDense)</span><br><span class="line">    │               │   ├── wi (Linear) weight:[<span class="number">4096</span>, <span class="number">1024</span>]</span><br><span class="line">    │               │   └── wo (Linear) weight:[<span class="number">1024</span>, <span class="number">4096</span>]</span><br><span class="line">    │               └── layer_norm (T5LayerNorm) weight:[<span class="number">1024</span>]</span><br><span class="line">    └── final_layer_norm (T5LayerNorm) weight:[<span class="number">1024</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>下面演示同一个backbone(T5)加上不同delta：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from opendelta import AutoDeltaConfig, AutoDeltaModel</span><br><span class="line"></span><br><span class="line">delta_model_spelling = AutoDeltaModel.from_finetuned(&quot;thunlp/Spelling_Correction_T5_LRAdapter_demo&quot;, backbone_model=model)</span><br><span class="line">delta_model_spelling.detach()</span><br><span class="line"></span><br><span class="line">delta_model_topic = AutoDeltaModel.from_finetuned(&quot;thunlp/Question_Topic_T5-large_Compacter&quot;, backbone_model=model)</span><br><span class="line">delta_model_topic.detach()</span><br><span class="line"></span><br><span class="line">delta_model_fact = AutoDeltaModel.from_finetuned(&quot;thunlp/FactQA_T5-large_Adapter&quot;, backbone_model=model)</span><br><span class="line">delta_model_fact.detach()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>下面定义多任务服务函数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def multitask_serving(input_text):</span><br><span class="line">  # 首先进行拼写改错</span><br><span class="line">    input_ids = tokenizer(input_text, return_tensors=&quot;pt&quot;).input_ids#.cuda()</span><br><span class="line">    delta_model_spelling.attach()</span><br><span class="line">    answers_ids =model.generate(input_ids=input_ids, max_length=20, num_beams=4)</span><br><span class="line">    input_text = tokenizer.decode(answers_ids[0], skip_special_tokens=True)</span><br><span class="line">    print(&quot;Correct Spelling: &#123;&#125;&quot;.format(input_text))</span><br><span class="line">    delta_model_spelling.detach()</span><br><span class="line">  # 然后传入主题分类模型</span><br><span class="line">    delta_model_topic.attach()</span><br><span class="line">    input_ids = tokenizer(input_text, return_tensors=&quot;pt&quot;).input_ids#.cuda()</span><br><span class="line">    answers_ids =model.generate(input_ids=input_ids, max_length=20, num_beams=4)</span><br><span class="line">    topic = tokenizer.decode(answers_ids[0], skip_special_tokens=True)</span><br><span class="line">    delta_model_topic.detach()</span><br><span class="line">    print(&quot;Question Topic: &#123;&#125;&quot;.format(topic))</span><br><span class="line">  # 最后做问答</span><br><span class="line">    delta_model_fact.attach()</span><br><span class="line">    input_ids = tokenizer(input_text, return_tensors=&quot;pt&quot;).input_ids#.cuda()</span><br><span class="line">    answers_ids =model.generate(input_ids=input_ids, max_length=20, num_beams=4)</span><br><span class="line">    input_text = tokenizer.decode(answers_ids[0], skip_special_tokens=True)</span><br><span class="line">    delta_model_fact.detach()</span><br><span class="line">    print(&quot;Question Answer: &#123;&#125;&quot;.format(input_text))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>多个任务的切换通过先<code>attach</code>再<code>detach</code>。</p><p>这里展示两个例子：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">multitask_serving(&quot;When was Beiiing olymp#ic heldd ?&quot;)</span><br><span class="line">multitask_serving(&quot;What the commmon career of Newton ad eintesin?&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Correct Spelling: When was Beijing Olympic held?</span><br><span class="line">Question Topic: The question&#x27;s topic is sports.</span><br><span class="line">Question Answer: 2008</span><br><span class="line">Correct Spelling: What was the common career of Newton and Einstein?</span><br><span class="line">Question Topic: The question&#x27;s topic is science.</span><br><span class="line">Question Answer: Physicists</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到拼写模型把修正后的输入给了主题模型和问答模型。</p><p>如果我们想把这个预训练模型回退到没有加delta模型的模型，只要执行<code>detach</code>即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">delta_model_spelling.detach()</span><br><span class="line">delta_model_topic.detach()</span><br><span class="line">delta_model_fact.detach()</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLMs公开课 - 3.Transformer基础</title>
      <link href="/llms/llms_course/3.Transformer%E5%9F%BA%E7%A1%80/"/>
      <url>/llms/llms_course/3.Transformer%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Transformer"><a href="#1-Transformer" class="headerlink" title="1.Transformer"></a>1.Transformer</h1><h2 id="1-1-注意力机制"><a href="#1-1-注意力机制" class="headerlink" title="1.1 注意力机制"></a>1.1 注意力机制</h2><h3 id="（1）Seq2Seq注意力"><a href="#（1）Seq2Seq注意力" class="headerlink" title="（1）Seq2Seq注意力"></a>（1）Seq2Seq注意力</h3><p>传统的Seq2Seq序列模型存在信息瓶颈的问题</p><ul><li>源句子编码的单向量需要捕获远举子的所有信息</li><li>单向量限制了编码器的表示能力：信息瓶颈</li></ul><p><img src="image/image_lOTpq5OeIz.png" alt=""></p><p>注意力机制：</p><ul><li>注意力提供了瓶颈问题的解决方案；</li><li>核心思想：<strong>在解码器的每一步，专注于源序列的特定部分</strong>。</li></ul><h3 id="（2）Seq2Seq注意力机制"><a href="#（2）Seq2Seq注意力机制" class="headerlink" title="（2）Seq2Seq注意力机制"></a>（2）Seq2Seq注意力机制</h3><ol><li>Encoder隐藏状态：$h_{1}, h_{2} \ldots, h_{N} \in \mathbb{R}^{h}$</li><li>Decoder在$t$时刻的隐藏状态：$s_{t} \in \mathbb{R}^{h}$</li><li>在$t$时刻，计算注意力分数 $e_t$ ： $e^{t}=\left[s_{t}^{T} h_{1}, \ldots, s_{t}^{T} h_{N}\right] \in \mathbb{R}^{N}$</li><li>使用softmax得到注意力分布$\alpha_t$ ：$\alpha^{t}=\operatorname{softmax}\left(e^{t}\right) \in \mathbb{R}^{N}$</li><li>利用注意力分布计算编码器隐藏状态的加权和作为注意力输出 ： $o_{t}=\sum_{i=1}^{N} \alpha_{i}^{t} h_{i} \in \mathbb{R}^{h}$</li><li>连接注意力输出和解码器隐藏状态来预测单词 ：$\left[o_{t} ; s_{t}\right] \in \mathbb{R}^{2 h}$</li></ol><p><img src="image/image_9UqnbcFKUt.png" alt=""></p><p><img src="image/image_3w0mAdKSFz.png" alt=""></p><p><img src="image/image_Pf--D-VHXF.png" alt=""></p><p><img src="image/image_nfdploqSzi.png" alt=""></p><p><img src="image/image_zorAic9xaq.png" alt=""></p><h3 id="（3）注意力机制的变体"><a href="#（3）注意力机制的变体" class="headerlink" title="（3）注意力机制的变体"></a>（3）注意力机制的变体</h3><p>不同注意力分数的计算，有不同的变体，$\mathrm{e} \in \mathbb{R}^{N}$</p><h4 id="Additive-attention"><a href="#Additive-attention" class="headerlink" title="Additive attention"></a>Additive attention</h4><script type="math/tex; mode=display">e_{i}=v^{T} \tanh \left(W_{1} h_{i}+W_{2} s\right) \in \mathbb{R}</script><p>其中，$W_{1} \in \mathbb{R}^{d_{3} \times d_{1}}, W_{2} \in \mathbb{R}^{d_{3} \times d_{2}}$是权重矩阵，$v \in \mathbb{R}^{d_{3}}$是权重向量。</p><h4 id="Basic-dot-product-attention"><a href="#Basic-dot-product-attention" class="headerlink" title="Basic dot-product attention"></a>Basic dot-product attention</h4><script type="math/tex; mode=display">e_{i}=s^{T} h_{i} \in \mathbb{R}</script><p>假设向量$d_1=d_2$</p><h4 id="Multiplicative-attention"><a href="#Multiplicative-attention" class="headerlink" title="Multiplicative attention"></a>Multiplicative attention</h4><script type="math/tex; mode=display">e_{i}=s^{T} W h_{i} \in \mathbb{R}, \quad W \in \mathbb{R}^{d_{2} \times d_{1}}</script><h3 id="（4）注意力通用定义"><a href="#（4）注意力通用定义" class="headerlink" title="（4）注意力通用定义"></a>（4）注意力通用定义</h3><p>给定一个query向量和一组value向量，注意力技术根据query计算值的加权和</p><p>根据查询，<strong>加权和是值的选择性汇总</strong>。可以通过注意机制获得任意一组表征的固定大小的表征。</p><p>数学表示：</p><ul><li>如果存在value向量$\boldsymbol{h}_{1}, \boldsymbol{h}_{2} \ldots, \boldsymbol{h}_{N} \in \mathbb{R}^{d_{1}}$，query向量$\mathbf{s} \in \mathbb{R}^{d_{2}}$</li><li>根据注意力分数$\mathbf{e} \in \mathbb{R}^{N}$，计算得到注意力输出$\mathbf{o} \in \mathbb{R}^{d_{1}}$</li></ul><script type="math/tex; mode=display">\boldsymbol{\alpha}=\operatorname{softmax}(\boldsymbol{e}) \in \mathbb{R}^{N}</script><script type="math/tex; mode=display">\boldsymbol{o}=\sum_{i=1}^{N} \alpha_{i} \boldsymbol{h}_{i} \in \mathbb{R}^{h}</script><ul><li>有几种不同的方法来计算的注意力分数$\mathbf{e} \in \mathbb{R}^{N}$</li></ul><h3 id="（5）注意力机制的特点"><a href="#（5）注意力机制的特点" class="headerlink" title="（5）注意力机制的特点"></a>（5）注意力机制的特点</h3><p><strong>注意力解决Seq2Seq瓶颈问题</strong>，解码器可以直接查看全部encoder输出</p><p><strong>注意力有助于消除梯度问题</strong></p><p>注意力提供了一些可解释性，可以通过注意图找出解码器关注的是什么；注意力允许网络对齐相关的单词</p><p><img src="image/image_k0fVtzGdSd.png" alt=""></p><h2 id="1-2-Transformer结构"><a href="#1-2-Transformer结构" class="headerlink" title="1.2 Transformer结构"></a>1.2 Transformer结构</h2><h3 id="（1）总览"><a href="#（1）总览" class="headerlink" title="（1）总览"></a>（1）总览</h3><ul><li>架构：Encoder-Decoder &#x20;</li><li>输入：byte pair encoding +  positional encoding &#x20;</li><li>模型：多个Encoder-Decoder  堆叠</li><li>输出：翻译单词的概率</li><li>损失函数：标准的交叉熵损失</li></ul><p><img src="image/image_RJZXjK6jv5.png" alt=""></p><h3 id="（2）Input-Encoding"><a href="#（2）Input-Encoding" class="headerlink" title="（2）Input Encoding"></a>（2）Input Encoding</h3><p>输入：byte pair encoding +  positional encoding &#x20;</p><h4 id="Byte-Pair-Encoding（BPE）"><a href="#Byte-Pair-Encoding（BPE）" class="headerlink" title="Byte Pair Encoding（BPE）"></a>Byte Pair Encoding（BPE）</h4><p>一种分词算法。从字符词汇开始；将最常见的n-gram转换为新的n-gram。</p><p>之前使用空格，之类的切分，词表必定不会包含所有单词，所以BPE将单词切分为更小的词元。</p><p>通过将稀有词和未知词编码为子词单元序列来解决OOV (out of vocabulary)问题</p><ul><li>在上面的例子中，OOV单词“最低”将被分割成“最低”。</li><li>“low”和“lowest”之间的关系可以概括为“smart”和“smartest”。</li></ul><p>原始词汇表：</p><p><img src="image/image_DQLE30zO78.png" alt=""></p><p>1、将各个字母添加到词表中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l, o, w, e, r, n, w, s, t, i, d</span><br></pre></td></tr></table></figure><p>2、将频次为9的<code>es</code>对添加进去</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l, o, w, e, r, n, w, s, t, i, d,  es</span><br></pre></td></tr></table></figure><p>3、因为<code>s</code>不单独出现，是和<code>es</code>一起出现，删除<code>s</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l, o, w, e, r, n, w, t, i, d, es</span><br></pre></td></tr></table></figure><p>4、将频次为9的<code>est</code>对添加进去，且<code>es</code>不是单独出现，和<code>est</code>一起出现，删除<code>es</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l, o, w, e, r, n, w, t, i, d,  est</span><br></pre></td></tr></table></figure><p>循环这个过程，直到达到词表的大小要求即可。</p><h4 id="Positional-Encoding（PE）"><a href="#Positional-Encoding（PE）" class="headerlink" title="Positional Encoding（PE）"></a>Positional Encoding（PE）</h4><p>Transformer block对位置不同的相同单词不敏感；添加位置编码 <strong>，以便相同的单词在不同位置具有不同的表示</strong></p><script type="math/tex; mode=display">P E_{(p o s, 2 i)}=\sin \left(\right. pos \left./ 10000^{2 i / d}\right)</script><script type="math/tex; mode=display">P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d}\right)</script><p>其中，$i$为词嵌入索引，范围为$[0, d/2]$</p><h4 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h4><p><code>Input = BPE + PE</code></p><p><img src="image/image_kx2cPK0Lx-.png" alt=""></p><h3 id="（3）Transformer-Block"><a href="#（3）Transformer-Block" class="headerlink" title="（3）Transformer Block"></a>（3）Transformer Block</h3><h4 id="Dot-Product-Attention"><a href="#Dot-Product-Attention" class="headerlink" title="Dot-Product Attention"></a>Dot-Product Attention</h4><p>输入：</p><ul><li>一个query向量$q$，一组键值对 $(k, v)$</li><li>$q$、k向量维度为$d_k$</li><li>$v$向量维度为$d_v$</li></ul><p>输出：</p><ul><li><strong>输出是</strong>$v$<strong>向量的加权和</strong></li><li>每个值的权重由查询和对应键的点积计算： $A(q, K, V)=\sum_{i} \frac{e^{q \cdot k_{i}}}{\sum_{j} e^{q \cdot k_{j}}} v_{i}$</li><li>堆叠多个query为一个矩阵Q： $A(Q, K, V)=\operatorname{softmax}\left(Q K^{T}\right) V$</li></ul><p>图示：</p><script type="math/tex; mode=display">A(Q, K, V)=\operatorname{softmax}\left(Q K^{T}\right) V</script><p><img src="image/image_j5eM7EbHEZ.png" alt=""></p><h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><p>点积注意力问题：</p><ul><li><strong>如果</strong>$d_k$<strong>维度过大，则</strong>$q^T \cdot k$** 的方差也会变得很大**​</li><li>经过softmax后的注意力分布变得会很尖锐，梯度会变得很小</li></ul><p>解决方法：</p><ul><li>带有缩放的点积注意力：$A(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V$</li></ul><p><img src="image/image_KUP56Iuwgd.png" alt=""></p><h4 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h4><p>让词向量自己选择彼此</p><p>Q, K, V是从一个句子的单词向量中得到的</p><p><img src="image/image_jrF26UA9es.png" alt=""></p><h4 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h4><p>不同的head：相同的计算，不同的参数</p><p>连接所有输出并馈送到线性层</p><script type="math/tex; mode=display">\operatorname{head}_{i}=\mathrm{A}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)</script><script type="math/tex; mode=display">\operatorname{MultiHead}(Q, K, V)=\operatorname{Concat}\left(\right. head _{1}, \ldots, head \left._{h}\right) W^{O}</script><p><img src="image/image_lOixttIaq0.png" alt=""></p><h4 id="Encoder-Block"><a href="#Encoder-Block" class="headerlink" title="Encoder Block"></a>Encoder Block</h4><p>在每一层中，Q, K, V与前一层的输出相同</p><p><img src="image/image_tzT2xy9QOk.png" alt=""></p><h4 id="Decoder-Block"><a href="#Decoder-Block" class="headerlink" title="Decoder Block"></a>Decoder Block</h4><p><strong>Mask self-attention</strong>：单词只能看前面的单词，mask用于遮掩Encoder输出的未来的信息</p><p><strong>Encoder-Decoder 注意力</strong>：query来自解码器，而key和value来自编码器</p><p><img src="image/image_DcLkbzuVqc.png" alt=""></p><h3 id="（4）Trick"><a href="#（4）Trick" class="headerlink" title="（4）Trick"></a>（4）Trick</h3><ul><li>残差连接</li><li>层归一化：将输入向量变化为均值为0，方差为1的向量</li><li>标签平滑</li><li>ADAM优化器</li><li>在加入残差之前，在每一层的训练中Dropout</li><li>带波束搜索(beam search)和长度惩罚(length penalties)的自回归解码</li></ul><h3 id="（5）优缺点"><a href="#（5）优缺点" class="headerlink" title="（5）优缺点"></a>（5）优缺点</h3><p>优点</p><ul><li>Transformer是一个强大的模型，在许多NLP任务中被证明是有效的</li><li>Transformer适合<strong>并行化</strong></li><li>证明了<strong>注意机制</strong>的有效性</li><li>它还提供了对最近NLP进展，如BERT和GPT</li></ul><p>缺点：</p><ul><li>架构难以优化，对模型修改敏感</li><li>每层注意力计算的复杂度高$O(n^2)$，对输入文本长度有要求，最大不能超过512</li></ul><h1 id="2-PLM（Pretrained-Language-Models）"><a href="#2-PLM（Pretrained-Language-Models）" class="headerlink" title="2.PLM（Pretrained Language Models）"></a>2.PLM（Pretrained Language Models）</h1><h2 id="2-1-语言模型"><a href="#2-1-语言模型" class="headerlink" title="2.1 语言模型"></a>2.1 语言模型</h2><p>语言建模是<strong>预测即将出现的单词</strong>的任务，计算即将到来的单词K的条件概率。</p><script type="math/tex; mode=display">P\left(w_{n} \mid w_{1}, w_{2}, \cdots, w_{n-1}\right)</script><p><img src="image/image_8asp79WWux.png" alt=""></p><p><strong>语言建模</strong>：最基本和最重要的NLP任务</p><ul><li>包含多种语言理解知识，如语言知识和事实知识</li><li>只需要纯文本，不需要任何人工注释</li></ul><p>通过语言模型学习到的语言知识可以很容易地转移到其他NLP任务中</p><p>NLP迁移学习有三种代表性模型</p><ul><li>Word2vec</li><li>Pre-trained RNN</li><li>GPT &amp; BERT</li></ul><h2 id="2-2-PLMs（Pre-trained-Langue-Models）"><a href="#2-2-PLMs（Pre-trained-Langue-Models）" class="headerlink" title="2.2 PLMs（Pre-trained Langue Models）"></a>2.2 PLMs（Pre-trained Langue Models）</h2><p>PLM：对其他NLP任务具有强大可移植性的语言模型。word2vec是第一个PLM，如今的PLM都是基于Transformer的模型。</p><p>主要有两个分支：</p><p>1、<strong>Feature-based 方法</strong></p><ul><li>基于特征的方法中最具有代表性的模型是word2vec</li><li>使用PLM的输出作为下游模型的输入</li></ul><p>2、<strong>Fine-tuning 方法</strong></p><ul><li>最具代表性的微调方法模型是BERT</li><li>语言模型也是下游模型，其参数将被更新。</li></ul><h3 id="（1）GPT"><a href="#（1）GPT" class="headerlink" title="（1）GPT"></a>（1）GPT</h3><p><a href="https://wdndev.github.io/paper_reading/2.5.GPT_GPT-2_GPT-3/" title="论文精读 GPT、GPT-2、GPT-3 | 37.2° Blog (wdndev.github.io)">论文精读 GPT、GPT-2、GPT-3 | 37.2° Blog (wdndev.github.io)</a></p><p>受Transformer在不同NLP任务中的成功启发，GPT是第一个基于Transformer预训练PLM的工作；</p><p>在自然语言处理领域，有很多各式各样的的任务，如问答，文本分类等。然而，现有的无标签文本非常多，而有标签的文本很少，这使得在这些有标签文本训练一个好的分辨模型很难，因为数据集太少。因此GPT第一个版本主要就是为了解决这个问题而提出的一套针对语言模型的预训练方法，使得大量的无标签数据能够被使用，并且对预训练好的模型加以微调使其适用于许多的下游任务。</p><p>在微调时，构造与子任务相关的输入，从而之只需要很少改变模型架构。</p><p>GPT = Transformer + left-to-right LM</p><p>GPT在下游任务上fine-tuned</p><p><img src="image/image_Voqt2gbfQz.png" alt=""></p><h3 id="（2）BERT"><a href="#（2）BERT" class="headerlink" title="（2）BERT"></a>（2）BERT</h3><p><a href="https://wdndev.github.io/paper_reading/2.2.BERT/" title="论文精读 BERT | 37.2° Blog (wdndev.github.io)">论文精读 BERT | 37.2° Blog (wdndev.github.io)</a></p><p>BERT的出现使得我们能够在一个大的数据集上面训练好一个比较深的神经网络，然后应用在很多的NLP任务上面，这样既简化了NLP任务的训练，又提升了它的性能，所以BERT和它之后的一系列工作使得自然语言处理在过去三年中有了质的飞跃。</p><p><strong>输入</strong>：</p><p><img src="image/image_LAzTpzdBR0.png" alt=""></p><h2 id="2-3-Masked-LM的应用"><a href="#2-3-Masked-LM的应用" class="headerlink" title="2.3 Masked LM的应用"></a>2.3 Masked LM的应用</h2><p><strong>基本思想</strong>：<strong>使用双向的信息去预测目标token</strong></p><p>将不同域的对象一起输入，并根据输入的对象预测目标对象</p><h3 id="（1）跨语言LM预训练"><a href="#（1）跨语言LM预训练" class="headerlink" title="（1）跨语言LM预训练"></a>（1）跨语言LM预训练</h3><p>Translation Language Modeling (TLM)</p><p>TLM目标将<strong>MLM扩展到平行句对</strong>(例如，英语-法语)；为了预测一个被屏蔽的英语单词，该模型可以同时关注英语句子及其法语翻译，并鼓励对齐英语和法语表示</p><p>翻译语言建模(TLM)的目标是利用并行数据改进跨语言语言模型的预训练</p><p><img src="image/image_K1a6TVZJ2-.png" alt=""></p><h3 id="（2）跨模态LM预训练"><a href="#（2）跨模态LM预训练" class="headerlink" title="（2）跨模态LM预训练"></a>（2）跨模态LM预训练</h3><p>自动语音识别(ASR)的视频和文本对</p><p>通过使用预训练模型将分层向量量化应用于视频衍生的特征，生成一系列“视觉词”</p><p>鼓励模型关注视频中的高级语义和较长时间动态</p><h2 id="2-5-PLMs前沿技术"><a href="#2-5-PLMs前沿技术" class="headerlink" title="2.5 PLMs前沿技术"></a>2.5 PLMs前沿技术</h2><h3 id="（1）GPT-3"><a href="#（1）GPT-3" class="headerlink" title="（1）GPT-3"></a>（1）GPT-3</h3><p><a href="https://wdndev.github.io/paper_reading/2.5.GPT_GPT-2_GPT-3/" title="论文精读 GPT、GPT-2、GPT-3 | 37.2° Blog (wdndev.github.io)">论文精读 GPT、GPT-2、GPT-3 | 37.2° Blog (wdndev.github.io)</a></p><p>GPT-3：大规模的PLM</p><p><img src="image/image_pQTUKd17hu.png" alt=""></p><h3 id="（2）T5"><a href="#（2）T5" class="headerlink" title="（2）T5"></a>（2）T5</h3><p>Encoder-Decoder架构</p><p><strong>将所有NLP任务重新构建为统一的文本到文本格式</strong>，其中输入和输出始终是文本字符串</p><p><img src="image/image_x9y_A0bKby.png" alt=""></p><h3 id="（3）MoE"><a href="#（3）MoE" class="headerlink" title="（3）MoE"></a>（3）MoE</h3><p>加强Encoder-Decoder与MoE(Mixture of Experts)数十亿的参数</p><p>Gshard 600B参数</p><p>Switch Transformer 1571b参数</p><p><img src="image/image_ShhrpNkfiz.png" alt=""></p><h1 id="3-Transformers-API教程"><a href="#3-Transformers-API教程" class="headerlink" title="3.Transformers API教程"></a>3.Transformers API教程</h1><p><a href="https://www.zhihu.com/column/c_1400131016443506688" title="transformers 教程 - 知乎 (zhihu.com)">transformers 教程 - 知乎 (zhihu.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLMs公开课 - 2.神经网络基础</title>
      <link href="/llms/llms_course/2.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
      <url>/llms/llms_course/2.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h1 id="1-神经网络组成部分"><a href="#1-神经网络组成部分" class="headerlink" title="1.神经网络组成部分"></a>1.神经网络组成部分</h1><h2 id="1-1-神经网络"><a href="#1-1-神经网络" class="headerlink" title="1.1 神经网络"></a>1.1 神经网络</h2><h3 id="（1）神经元"><a href="#（1）神经元" class="headerlink" title="（1）神经元"></a>（1）神经元</h3><p>人工神经网络：灵感来自于大脑中的生物神经网络</p><p><img src="image/image__NxRK7zPme.png" alt=""></p><p>神经元是一个具有输入和一个输出和参数$w$,$b$的计算单元</p><script type="math/tex; mode=display">h_{\boldsymbol{w}, b}(\boldsymbol{x})=f\left(\boldsymbol{w}^{T} \boldsymbol{x}+b\right)</script><p><img src="image/image_bNaJM_N3DJ.png" alt=""></p><h3 id="（2）单层神经网络"><a href="#（2）单层神经网络" class="headerlink" title="（2）单层神经网络"></a>（2）单层神经网络</h3><p><img src="image/image_ECZpwvVfsU.png" alt=""></p><h3 id="（3）多层神经网络"><a href="#（3）多层神经网络" class="headerlink" title="（3）多层神经网络"></a>（3）多层神经网络</h3><p><img src="image/image_PAPMasGhUC.png" alt=""></p><script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{h}_{1}=f\left(\boldsymbol{W}_{1} \boldsymbol{x}+\boldsymbol{b}_{1}\right) \\ \boldsymbol{h}_{2}=f\left(\boldsymbol{W}_{2} \boldsymbol{h}_{1}+\boldsymbol{b}_{2}\right) \\ \boldsymbol{h}_{3}=f\left(\boldsymbol{W}_{3} \boldsymbol{h}_{2}+\boldsymbol{b}_{3}\right)\end{array}</script><h2 id="1-2-激活函数"><a href="#1-2-激活函数" class="headerlink" title="1.2 激活函数"></a>1.2 激活函数</h2><p>如果神经网络中只存在线性运算的话，那么多层的神经网络其实可以被转化为单层的神经网络；所以我们<strong>使用非线性的激活函数，防止多层的神经网络塌缩成单一的神经网络</strong></p><h3 id="（1）Sigmoid"><a href="#（1）Sigmoid" class="headerlink" title="（1）Sigmoid"></a>（1）Sigmoid</h3><script type="math/tex; mode=display">f(z)=\frac{1}{1+e^{-z}}</script><p><img src="image/image_jYT7IJKsdX.png" alt=""></p><h3 id="（2）Tanh"><a href="#（2）Tanh" class="headerlink" title="（2）Tanh"></a>（2）Tanh</h3><script type="math/tex; mode=display">f(z)=\tanh (z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}</script><p><img src="image/image_4ilK8ZuhsJ.png" alt=""></p><h3 id="（3）ReLU"><a href="#（3）ReLU" class="headerlink" title="（3）ReLU"></a>（3）ReLU</h3><script type="math/tex; mode=display">f(z)=\max (z, 0)</script><p><img src="image/image_UUiuULC_AM.png" alt=""></p><h2 id="1-3-输出层"><a href="#1-3-输出层" class="headerlink" title="1.3 输出层"></a>1.3 输出层</h2><p>增加若干个隐层可以提高网络的表达能力，如果想要得到我们想要的输出结果，就需要添加网络的最后一层，即<strong>输出层</strong></p><p><img src="image/image_5gyI-jI34X.png" alt=""></p><h1 id="2-训练方式"><a href="#2-训练方式" class="headerlink" title="2.训练方式"></a>2.训练方式</h1><h2 id="2-1-训练目标"><a href="#2-1-训练目标" class="headerlink" title="2.1 训练目标"></a>2.1 训练目标</h2><h3 id="（1）均方根误差"><a href="#（1）均方根误差" class="headerlink" title="（1）均方根误差"></a>（1）均方根误差</h3><script type="math/tex; mode=display">\min _{\theta} J(\theta)=\min _{\theta} \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-F_{\theta}\left(x_{i}\right)\right)^{2}</script><p>其中，$\theta$是神经网络参数</p><h3 id="（2）交叉熵"><a href="#（2）交叉熵" class="headerlink" title="（2）交叉熵"></a>（2）交叉熵</h3><script type="math/tex; mode=display">\min _{\theta} J(\theta)=\min _{\theta}-\frac{1}{N} \sum_{i=1}^{N} \log P_{\operatorname{model}}\left(F_{\theta}\left(x_{i}\right)=y_{i}\right)</script><p>其中，$\theta$是神经网络参数</p><p><img src="image/image_GKxgwNr-h9.png" alt=""></p><h2 id="2-2-随机梯度下降"><a href="#2-2-随机梯度下降" class="headerlink" title="2.2 随机梯度下降"></a>2.2 随机梯度下降</h2><p>更新规则：</p><script type="math/tex; mode=display">\theta^{\text {new }}=\theta^{\text {old }}-\alpha \nabla_{\theta} \mathrm{J}(\theta)</script><p>其中，$ \alpha<br>  $是学习率</p><h3 id="（1）梯度"><a href="#（1）梯度" class="headerlink" title="（1）梯度"></a>（1）梯度</h3><p>给定$n$个输入，$m$个输出的函数：</p><script type="math/tex; mode=display">\mathrm{F}(\boldsymbol{x})=\left[F_{1}\left(x_{1}, x_{2} \ldots x_{n}\right), F_{2}\left(x_{1}, x_{2} \ldots x_{n}\right) \ldots F_{m}\left(x_{1}, x_{2} \ldots x_{n}\right)\right]</script><p>则输出为$m\times n$的雅可比矩阵</p><script type="math/tex; mode=display">\frac{\partial \mathrm{F}}{\partial \boldsymbol{x}}=\left[\begin{array}{ccc}\frac{\partial \mathrm{F}_{1}}{\partial x_{1}} & \cdots & \frac{\partial \mathrm{F}_{1}}{\partial x_{n}} \\ \vdots & \ddots & \vdots \\ \frac{\partial \mathrm{F}_{\mathrm{m}}}{\partial x_{1}} & \cdots & \frac{\partial \mathrm{F}_{\mathrm{m}}}{\partial x_{n}}\end{array}\right]</script><p>其中，$\left(\frac{\partial \mathrm{F}}{\partial x}\right)_{i j}=\frac{\partial \mathrm{F}_{\mathrm{i}}}{\partial x_{j}}$表示第i个输出对第j个输入求梯度。</p><h3 id="（2）链式求导法则"><a href="#（2）链式求导法则" class="headerlink" title="（2）链式求导法则"></a>（2）链式求导法则</h3><p>给定$s=\boldsymbol{u}^{T} \boldsymbol{h}, \boldsymbol{h}=f(\boldsymbol{z}), \boldsymbol{z}=\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b}$，求$\frac{\partial s}{\partial \boldsymbol{b}}$</p><p><img src="image/image_dKGXUk91WH.png" alt=""></p><h2 id="2-3-反向传播"><a href="#2-3-反向传播" class="headerlink" title="2.3 反向传播"></a>2.3 反向传播</h2><h3 id="（1）计算图"><a href="#（1）计算图" class="headerlink" title="（1）计算图"></a>（1）计算图</h3><p>计算图：将神经网路的传播以图的形式表示。</p><ul><li>源节点：输入</li><li>内部节点：操作</li><li>边传递操作：结果</li></ul><script type="math/tex; mode=display">\begin{array}{c}s=\boldsymbol{u}^{T} \boldsymbol{h} ~,~ \boldsymbol{h}=f(\mathbf{z}) ~,~ \boldsymbol{z}=\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b} ~,~ \boldsymbol{x} \text { input }\end{array}</script><p><img src="image/image_54Urqu6Td1.png" alt=""></p><p>梯度回传：沿着边往回走，沿着梯度传递</p><p><img src="image/image_I3J-0G7nTo.png" alt=""></p><h3 id="（2）单个结点"><a href="#（2）单个结点" class="headerlink" title="（2）单个结点"></a>（2）单个结点</h3><p>节点接收到一个“上游梯度”</p><p>目标是传递正确的“下游梯度”</p><p>每个节点都有一个局部梯度（ local gradient ），输出相对于输入的梯度</p><script type="math/tex; mode=display">[downstream gradient] = [upstream gradient] \times[local gradient]</script><p><img src="image/image_apPpKuwUIC.png" alt=""></p><h3 id="（3）示例"><a href="#（3）示例" class="headerlink" title="（3）示例"></a>（3）示例</h3><p>函数：$\begin{array}{c}f(x, y, z)=(x+y) \max (y, z) <del>,</del> x=1, y=2, z=0\end{array}$</p><p>前向传播：</p><script type="math/tex; mode=display">\begin{array}{c}a=x+y=3 \\ \mathrm{~b}=\max (y, z)=2 \\ f=a b=6\end{array}</script><p>本地梯度（Local gradients）：</p><script type="math/tex; mode=display">\begin{array}{c}\frac{\partial a}{\partial x}=1, \frac{\partial a}{\partial y}=1 \\ \frac{\partial b}{\partial y}=\mathbf{1}(y>z)=1, \frac{\partial b}{\partial z}=\mathbf{1}(z>y)=0 \\ \frac{\partial f}{\partial a}=b=2, \frac{\partial f}{\partial b}=a=3\end{array}</script><p>初始计算图：</p><p><img src="image/image_EFudVV7m60.png" alt=""></p><p>回传第一步：</p><p><img src="image/image_S_FBO1kQYh.png" alt=""></p><p>回传第二步（<code>*</code>）：</p><script type="math/tex; mode=display">\frac{\partial f}{\partial a}=b=2, \frac{\partial f}{\partial b}=a=3</script><p><img src="image/image_qNcJUPWb5O.png" alt=""></p><p>回传第三步（<code>max</code>）：</p><script type="math/tex; mode=display">\frac{\partial b}{\partial y}=\mathbf{1}(y>z)=1, \frac{\partial b}{\partial z}=\mathbf{1}(z>y)=0</script><p><img src="image/image_irUw2z3RYm.png" alt=""></p><p>回传第四步（<code>+</code>）：</p><script type="math/tex; mode=display">\frac{\partial a}{\partial x}=1, \frac{\partial a}{\partial y}=1</script><p><img src="image/image_b4Whzp5kWz.png" alt=""></p><p>计算最终梯度：</p><p><img src="image/image_o7bVAnP6Vm.png" alt=""></p><h1 id="3-词表示：Word2Vec"><a href="#3-词表示：Word2Vec" class="headerlink" title="3.词表示：Word2Vec"></a>3.词表示：Word2Vec</h1><p>Word2Vec：可以学到一些语义内涵，捕捉到语言学上的一些规律</p><h2 id="3-1-Word2Vec"><a href="#3-1-Word2Vec" class="headerlink" title="3.1 Word2Vec"></a>3.1 Word2Vec</h2><p>Word2vec使用浅层神经网络将单词与分布式表示相关联</p><p>它可以捕获许多语言规则，例如:</p><p><img src="image/image_O6ap5NPKxO.png" alt=""></p><p>Word2vec可以利用两种架构来生成单词的分布式表示：</p><ul><li>Continuous bag-of-words (<code>CBOW</code>) &#x20;</li><li>Continuous <code>skip-gram</code></li></ul><p><img src="image/image_SNsI3Dluiu.png" alt=""></p><h2 id="3-2-滑动窗口"><a href="#3-2-滑动窗口" class="headerlink" title="3.2 滑动窗口"></a>3.2 滑动窗口</h2><p>Word2vec使用一个固定大小的滑动窗口沿着句子移动</p><ul><li>在每个窗口中，中间的单词是目标单词，其他单词是上下文单词</li><li>给定上下文单词，CBOW预测目标单词的概率</li><li>当给定目标词时，skip-gram预测上下文词的概率</li></ul><p>滑动窗口大小为5</p><p><img src="image/image_7Ra8EInZ_r.png" alt=""></p><h2 id="3-3-CBOW（Continuous-Bag-of-Words）"><a href="#3-3-CBOW（Continuous-Bag-of-Words）" class="headerlink" title="3.3 CBOW（Continuous Bag-of-Words）"></a>3.3 CBOW（Continuous Bag-of-Words）</h2><p>在CBOW架构中，<strong>该模型给出一个周围上下文词的窗口来预测目标词</strong></p><ul><li>根据词袋假设：上下文词的顺序不影响预测</li><li>假设窗口大小为5，<code>Never too late to learn</code></li></ul><script type="math/tex; mode=display">P( late \mid[ never, too, to, learn ])</script><p><img src="image/image_W9ppLbPlmg.png" alt=""></p><h2 id="3-4-Continuous-Skip-Gram"><a href="#3-4-Continuous-Skip-Gram" class="headerlink" title="3.4 Continuous Skip-Gram"></a>3.4 Continuous Skip-Gram</h2><p>在skip-gram架构中，该模型从目标词中预测上下文词</p><p>假设窗口大小为5，<code>Never too late to learn</code></p><script type="math/tex; mode=display">P([ too, late ] \mid Never ), P([ Never, late, to ] \mid too ), \ldots</script><p>Skip-gram每步预测一个上下文词，训练样本为:</p><script type="math/tex; mode=display">\begin{array}{l}P(\text { too } \mid \text { Never }), P(\text { late } \mid \text { Never }), P(\text { Never } \mid \text { too }), P(\text { late } \mid \text { too }), P(\text { to } \mid \text { too }), \ldots\end{array}</script><p><img src="image/image_VOuM7rA6og.png" alt=""></p><h2 id="3-5-Softmax存在问题"><a href="#3-5-Softmax存在问题" class="headerlink" title="3.5 Softmax存在问题"></a>3.5 Softmax存在问题</h2><p>当词汇量很大的时候</p><ul><li>Softmax对所有单词的每一步都依赖于大量的模型参数，这在计算上是不切实际的</li><li>我们需要提高计算效率</li></ul><p>事实上，在word2vec中我们<strong>并不需要一个完整的概率模型</strong>；word2vec主要有两种改进方法:</p><ul><li>负采样</li><li>分层softmax</li></ul><h2 id="3-6-负采样"><a href="#3-6-负采样" class="headerlink" title="3.6 负采样"></a>3.6 负采样</h2><p>当词汇表非常大，这意味着模型每一步都有大量的权重需要更新</p><p>负抽样的思想是，<strong>每一步只更新一小部分权重</strong></p><p>既然有词汇表并且知道上下文单词，<strong>可以按概率选择几个不在上下文单词列表中的单词</strong>：</p><script type="math/tex; mode=display">P\left(w_{i}\right)=\frac{f\left(w_{i}\right)^{3 / 4}}{\sum_{j=1}^{V} f\left(w_{j}\right)^{3 / 4}}</script><p>其中，$f(w_i)$为$w_i$的频次，$3/4$为经验值</p><p>相比于$\frac{f\left(w_{i}\right)}{\sum_{j=1}^{V} f\left(w_{j}\right)}$，这可以增加低频词出现的概率。</p><p>假设我们只选取4个负采样词：</p><p><img src="image/image_Hnyk433Vfj.png" alt=""></p><p>然后我们可以计算损失，并优化每一步的权重(不是所有的权重)</p><ul><li>假设有一个大小为300×10,000的权重矩阵，输出大小为5</li><li>只需要更新300×5权重，这只占所有权重的0.05%</li></ul><h2 id="3-7-其他一些细节"><a href="#3-7-其他一些细节" class="headerlink" title="3.7 其他一些细节"></a>3.7 其他一些细节</h2><h3 id="（1）Sub-Sampling"><a href="#（1）Sub-Sampling" class="headerlink" title="（1）Sub-Sampling"></a>（1）Sub-Sampling</h3><p><strong>罕见的单词可能更有可能携带不同的信息</strong>，据此，Sub-Sampling有概率地丢弃单词:</p><script type="math/tex; mode=display">1-\sqrt{t / f(w)}</script><p>其中，$f(w)$为单词频率，$t$是一个可调节的阈值吗</p><h3 id="（2）Soft-sliding-window"><a href="#（2）Soft-sliding-window" class="headerlink" title="（2）Soft sliding window"></a>（2）Soft sliding window</h3><p><strong>滑动窗口应该给较远的单词分配较少的权重</strong></p><p>将滑动窗口最大的定义为 $S_{max}$，实际的滑动窗口大小在1和$S_{max}$之间随机选择</p><p>因此，那些靠近目标单词的单词更有可能出现在窗口中</p><h1 id="4-通用神经网络"><a href="#4-通用神经网络" class="headerlink" title="4.通用神经网络"></a>4.通用神经网络</h1><h2 id="4-1-RNN"><a href="#4-1-RNN" class="headerlink" title="4.1 RNN"></a>4.1 RNN</h2><h3 id="（1）顺序记忆"><a href="#（1）顺序记忆" class="headerlink" title="（1）顺序记忆"></a>（1）顺序记忆</h3><p>RNN的关键概念：<strong>处理序列数据时的顺序存储器</strong></p><p>定义：一种让大脑更容易识别序列模式的机制</p><p>RNN递归地更新序列内存以建模序列数据</p><h3 id="（2）RNN"><a href="#（2）RNN" class="headerlink" title="（2）RNN"></a>（2）RNN</h3><p><img src="image/image_w1ezicWNjH.png" alt=""></p><h3 id="（3）RNN单元"><a href="#（3）RNN单元" class="headerlink" title="（3）RNN单元"></a>（3）RNN单元</h3><script type="math/tex; mode=display">\begin{array}{c}h_{i}=\tanh \left(W_{x} x_{i}+W_{h} h_{i-1}+b\right) \\ y_{i}=F\left(h_{i}\right)\end{array}</script><p><img src="image/image_rTnzEUTLns.png" alt=""></p><h3 id="（4）RNN语言模型"><a href="#（4）RNN语言模型" class="headerlink" title="（4）RNN语言模型"></a>（4）RNN语言模型</h3><p>$W_h$参数是共享的</p><p><img src="image/image_eNI1RlG6kX.png" alt=""></p><h3 id="（5）优缺点"><a href="#（5）优缺点" class="headerlink" title="（5）优缺点"></a>（5）优缺点</h3><p>优点：</p><ul><li>可以处理任何长度的输入</li><li>模型尺寸不增加较长的输入</li><li>跨时间步共享权重</li><li>从许多后退步骤计算步骤</li></ul><p>缺点：</p><ul><li>循环计算速度慢</li><li>在实践中，很难从许多步骤中获取信息</li></ul><h3 id="（6）梯度问题"><a href="#（6）梯度问题" class="headerlink" title="（6）梯度问题"></a>（6）梯度问题</h3><p>RNN链比较长，<strong>容易出现梯度消失或爆炸</strong></p><script type="math/tex; mode=display">h_{i}=\tanh \left(W_{x} x_{i}+W_{h} h_{i-1}+b\right)</script><script type="math/tex; mode=display">\Delta w_{1}=\frac{\partial \text { Loss }}{\partial w_{2}}=\frac{\partial \text { Loss }}{\partial h_{n}} \frac{\partial h_{n}}{\partial h_{n-1}} \frac{\partial h_{n-1}}{\partial h_{n-2}} \ldots \frac{\partial h_{3}}{\partial h_{2}} \frac{\partial h_{2}}{\partial w_{2}}</script><p><img src="image/image_jh4cUUbDUP.png" alt=""></p><h3 id="（7）RNN变种"><a href="#（7）RNN变种" class="headerlink" title="（7）RNN变种"></a>（7）RNN变种</h3><p>梯度消失问题的主要解决方案是<strong>在递归中使用更复杂的隐单元计算</strong></p><ul><li>GRU</li><li>LSTM</li></ul><p>主要思想：<strong>保持记忆，捕捉远距离的依赖</strong></p><h2 id="4-2-GRU（Gated-Recurrent-Unit）"><a href="#4-2-GRU（Gated-Recurrent-Unit）" class="headerlink" title="4.2 GRU（Gated Recurrent Unit）"></a>4.2 GRU（Gated Recurrent Unit）</h2><p>Vanilla RNN在下一个时间步直接计算隐藏层：</p><script type="math/tex; mode=display">h_{i}=\tanh \left(W_{x} x_{i}+W_{h} h_{i-1}+b\right)</script><p>在原始RNN中，增加门控机制，主要用于平衡过去的信息和输入之间的影响。主要有两个门控单元：</p><p><strong>更新门</strong>（update gate）：$z_{i}=\sigma\left(W_{x}^{(z)} x_{i}+W_{h}^{(z)} h_{i-1}+b^{(z)}\right)$</p><p><strong>重置门</strong>（reset gate）：$r_{i}=\sigma\left(W_{x}^{(r)} x_{i}+W_{h}^{(r)} h_{i-1}+b^{(r)}\right)$</p><p><strong>新的激活输出</strong> $\tilde{h}_{i}$：$\tilde{h}_{i}=\tanh \left(W_{x} x_{i}+r_{i} * W_{h} h_{i-1}+b\right)$</p><p>最后的隐藏单元输出$h_i$：$h_{i}=z_{i} <em> h_{i-1}+\left(1-z_{i}\right) </em> \tilde{h}_{i}$</p><p><img src="image/image_UuRQ4Gbe5J.png" alt=""></p><p><strong>示例</strong></p><p><img src="image/image_03sgvWVbLS.png" alt=""></p><p>如果重置门$r_i$ 接近于0</p><script type="math/tex; mode=display">\tilde{h}_{i} \approx \tanh \left(W_{x} x_{i}+0 * W_{h} h_{i-1}+b\right)</script><script type="math/tex; mode=display">\tilde{h}_{i} \approx \tanh \left(W_{x} x_{i}+b\right)</script><p>忽略先前的隐藏状态，这表明当前的激活与过去无关。例如，在一篇新文章的开头，过去的信息对于当前的激活是无用的。</p><p>更新门$z_i$控制与当前激活相比，过去的状态有多少是重要的。</p><p>如果$z_i$接近于1，然后可以通过许多时间步骤复制该单元中的信息!</p><script type="math/tex; mode=display">h_{i}=1 * h_{i-1}+(1-1) * \tilde{h}_{i}=h_{i-1}</script><p>如果$z_i$接近于0，然后将信息放入该单元并完全取代历史信息</p><h2 id="4-3-LSTM（Long-Short-Term-Memory-network）"><a href="#4-3-LSTM（Long-Short-Term-Memory-network）" class="headerlink" title="4.3 LSTM（Long Short-Term Memory network）"></a>4.3 LSTM（Long Short-Term Memory network）</h2><p>LSTM是一种特殊的RNN，能够像GRU一样学习长期依赖关系；</p><p><img src="image/image_-OJvYpjxzb.png" alt=""></p><h3 id="（1）状态单元-C-t"><a href="#（1）状态单元-C-t" class="headerlink" title="（1）状态单元 $C_t$"></a>（1）状态单元 $C_t$</h3><p>LSTM的关键是单元状态$C_t$</p><p><img src="image/image_52RFkl7c4P.png" alt=""></p><ul><li>用于捕获长期依赖的额外向量</li><li>直接贯穿整个链条，只有少量的线性交互作用</li><li>易于删除或添加信息到细胞状态</li></ul><h3 id="（2）遗忘门-f-t"><a href="#（2）遗忘门-f-t" class="headerlink" title="（2）遗忘门$f_t$"></a>（2）遗忘门$f_t$</h3><p>遗忘门：<strong>决定从状态单元中丢弃哪些信息</strong></p><script type="math/tex; mode=display">f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right)</script><p><img src="image/image_ObJdJBjn40.png" alt=""></p><p>其中，$\left[h_{t-1}, x_{t}\right]$为拼接向量</p><p>如果$f_{t}=0$，则直接遗忘过去的信息。</p><h3 id="（3）输入门-i-t"><a href="#（3）输入门-i-t" class="headerlink" title="（3）输入门 $i_t$"></a>（3）输入门 $i_t$</h3><p><strong>输入门</strong>：决定在单元状态中存储什么信息；</p><p>输入门$i_t$和新的候选状态信息 $\tilde{C}_{t}$</p><script type="math/tex; mode=display">i_{t}=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right)</script><script type="math/tex; mode=display">\tilde{C}_{t}=\tanh \left(W_{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right)</script><p><img src="image/image_98gJRgc1z0.png" alt=""></p><p>更新就的状态信息 $C_{t-1}$，结合前两步的结果</p><script type="math/tex; mode=display">C_{t}=f_{t} * C_{t-1}+i_{t} * \tilde{C}_{t}</script><p><img src="image/image_YYSR_WORpx.png" alt=""></p><h3 id="（4）输出门-o-t"><a href="#（4）输出门-o-t" class="headerlink" title="（4）输出门$o_t$"></a>（4）输出门$o_t$</h3><p>输出门：<strong>决定输出什么信息</strong></p><p>为特定的单词表示调整句子信息</p><script type="math/tex; mode=display">o_{t}=\sigma\left(W_{o}\left[h_{t-1}, x_{t}\right]+b_{o}\right)</script><script type="math/tex; mode=display">h_{t}=o_{t} * \tanh \left(C_{t}\right)</script><p><img src="image/image_H-BrfXKh1N.png" alt=""></p><p>功能强大，特别是当堆叠和更深层时(每个隐藏层已经由深层内部网络计算)</p><p>如果你有大量的数据，非常有用</p><h2 id="4-4-双向RNN"><a href="#4-4-双向RNN" class="headerlink" title="4.4 双向RNN"></a>4.4 双向RNN</h2><p>在传统的RNN中，当前状态只捕获过去的信息</p><script type="math/tex; mode=display">h_{t}=f\left(x_{t-1}, \ldots, x_{2}, x_{1}\right)</script><p>问题：在很多应用中，我们希望输出$y_t$依赖于整个输入序列</p><p><img src="image/image_LP3RQkv82Z.png" alt=""></p><h2 id="4-5-CNN"><a href="#4-5-CNN" class="headerlink" title="4.5 CNN"></a>4.5 CNN</h2><p><img src="image/image_KxVW2W4Fx_.png" alt=""></p><p>RNN vs CNN</p><p><img src="image/image_J9fDDFZpmY.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>滑动窗口（Slide Window）</title>
      <link href="/dsa/confusing_topics/3.slide_window/"/>
      <url>/dsa/confusing_topics/3.slide_window/</url>
      
        <content type="html"><![CDATA[<h2 id="1-滑动窗口算法"><a href="#1-滑动窗口算法" class="headerlink" title="1.滑动窗口算法"></a>1.滑动窗口算法</h2><h3 id="1-1-滑动窗口介绍"><a href="#1-1-滑动窗口介绍" class="headerlink" title="1.1 滑动窗口介绍"></a>1.1 滑动窗口介绍</h3><blockquote><p><strong>滑动窗口算法（Sliding Window）</strong>：在给定数组 / 字符串上维护一个固定长度或不定长度的窗口。可以对窗口进行滑动操作、缩放操作，以及维护最优解操作。</p></blockquote><ul><li><strong>滑动操作</strong>：窗口可按照一定方向进行移动。最常见的是向右侧移动。</li><li><strong>缩放操作</strong>：对于不定长度的窗口，可以从左侧缩小窗口长度，也可以从右侧增大窗口长度。</li></ul><p>滑动窗口<strong>利用了双指针中的快慢指针技巧</strong>，可以将滑动窗口看做是快慢指针两个指针中间的区间，也可以将滑动窗口看做是快慢指针的一种特殊形式。</p><p><img src="image/image_lhMAn5AD2-.png" alt=""></p><h3 id="1-2-适用范围"><a href="#1-2-适用范围" class="headerlink" title="1.2 适用范围"></a>1.2 适用范围</h3><p>滑动窗口算法一般用来解决一些查找满足一定条件的连续区间的性质（长度等）的问题。该算法可以将一部分问题中的嵌套循环转变为一个单循环，因此它可以减少时间复杂度。</p><p>按照窗口长度的固定情况，可以将滑动窗口题目分为以下两种：</p><ul><li><strong>固定长度窗口</strong>：窗口大小是固定的。</li><li><strong>不定长度窗口</strong>：窗口大小是不固定的。<ul><li>求解最大的满足条件的窗口。</li><li>求解最小的满足条件的窗口。</li></ul></li></ul><h2 id="2-固定长度的滑动窗口"><a href="#2-固定长度的滑动窗口" class="headerlink" title="2.固定长度的滑动窗口"></a>2.固定长度的滑动窗口</h2><blockquote><p><strong>固定长度滑动窗口算法（Fixed Length Sliding Window）</strong>：在给定数组 / 字符串上维护一个固定长度的窗口。可以对窗口进行滑动操作、缩放操作，以及维护最优解操作。</p></blockquote><p><img src="image/image_cNPm2PrPxJ.png" alt=""></p><h3 id="2-1-算法步骤"><a href="#2-1-算法步骤" class="headerlink" title="2.1 算法步骤"></a>2.1 算法步骤</h3><p>假设窗口的固定大小为 $window_size$。</p><ol><li>使用两个指针 $left$、$right$。初始时，$left$、$right$ 都指向序列的第一个元素，即：$left = 0$，$right = 0$，区间 $[left, right]$ 被称为一个「窗口」。</li><li>当窗口未达到 $window_{}size$ 大小时，不断移动 $right$，先将数组前 $window_{}size$ 个元素填入窗口中，即 <code>window.append(nums[right])</code>。</li><li>当窗口达到 $window_{}size$ 大小时，即满足 <code>right - left + 1 &gt;= window_size</code> 时，判断窗口内的连续元素是否满足题目限定的条件。<ol><li>如果满足，再根据要求更新最优解。</li><li>然后向右移动 $left$，从而缩小窗口长度，即 <code>left += 1</code>，使得窗口大小始终保持为 $window_{}size$。</li></ol></li><li>向右移动 $right$，将元素填入窗口中，即 <code>window.append(nums[right])</code>。</li><li>重复 $2 \sim 4$ 步，直到 $right$ 到达数组末尾。</li></ol><h3 id="2-2-代码模板"><a href="#2-2-代码模板" class="headerlink" title="2.2 代码模板"></a>2.2 代码模板</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">left = <span class="number">0</span></span><br><span class="line">right = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> right &lt; <span class="built_in">len</span>(nums):</span><br><span class="line">    window.append(nums[right])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 超过窗口大小时，缩小窗口，维护窗口中始终为 window_size 的长度</span></span><br><span class="line">    <span class="keyword">if</span> right - left + <span class="number">1</span> &gt;= window_size:</span><br><span class="line">        <span class="comment"># ... 维护答案</span></span><br><span class="line">        window.popleft()</span><br><span class="line">        left += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 向右侧增大窗口</span></span><br><span class="line">    right += <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="3-不定长度滑动窗口"><a href="#3-不定长度滑动窗口" class="headerlink" title="3.不定长度滑动窗口"></a>3.不定长度滑动窗口</h2><blockquote><p><strong>不定长度滑动窗口算法（Sliding Window）</strong>：在给定数组 / 字符串上维护一个不定长度的窗口。可以对窗口进行滑动操作、缩放操作，以及维护最优解操作。</p></blockquote><p><img src="image/image_20rEOZRRjB.png" alt=""></p><h3 id="3-1-算法步骤"><a href="#3-1-算法步骤" class="headerlink" title="3.1 算法步骤"></a>3.1 算法步骤</h3><ol><li>使用两个指针 $left$、$right$。初始时，$left$、$right$ 都指向序列的第一个元素。即：$left = 0$，$right = 0$，区间 $[left, right]$ 被称为一个「窗口」。</li><li>将区间最右侧元素添加入窗口中，即 <code>window.add(s[right])</code>。</li><li>然后向右移动 $right$，从而增大窗口长度，即 <code>right += 1</code>。直到窗口中的连续元素满足要求。</li><li>此时，停止增加窗口大小。转向不断将左侧元素移出窗口，即 <code>window.popleft(s[left])</code>。</li><li>然后向右移动 $left$，从而缩小窗口长度，即 <code>left += 1</code>。直到窗口中的连续元素不再满足要求。</li><li>重复 2 ~ 5 步，直到 $right$ 到达序列末尾。</li></ol><h3 id="3-2-代码模板"><a href="#3-2-代码模板" class="headerlink" title="3.2 代码模板"></a>3.2 代码模板</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">left = <span class="number">0</span></span><br><span class="line">right = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> right &lt; <span class="built_in">len</span>(nums):</span><br><span class="line">    window.append(nums[right])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> 窗口需要缩小:</span><br><span class="line">        <span class="comment"># ... 可维护答案</span></span><br><span class="line">        window.popleft()</span><br><span class="line">        left += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 向右侧增大窗口</span></span><br><span class="line">    right += <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="4-实战题目"><a href="#4-实战题目" class="headerlink" title="4.实战题目"></a>4.实战题目</h2><h3 id="4-1-大小为k且平均值大于等于阈值的子数组数目"><a href="#4-1-大小为k且平均值大于等于阈值的子数组数目" class="headerlink" title="4.1 大小为k且平均值大于等于阈值的子数组数目"></a>4.1 大小为k且平均值大于等于阈值的子数组数目</h3><p><a href="https://leetcode.cn/problems/number-of-sub-arrays-of-size-k-and-average-greater-than-or-equal-to-threshold/description/" title="1343. 大小为 K 且平均值大于等于阈值的子数组数目 - 力扣（LeetCode）">1343. 大小为 K 且平均值大于等于阈值的子数组数目 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数数组 arr 和两个整数 k 和 threshold 。</span><br><span class="line"></span><br><span class="line">请你返回长度为 k 且平均值大于等于 threshold 的子数组数目。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：arr = [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">8</span>], k = <span class="number">3</span>, threshold = <span class="number">4</span></span><br><span class="line">输出：<span class="number">3</span></span><br><span class="line">解释：子数组 [<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>],[<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>] 和 [<span class="number">5</span>,<span class="number">5</span>,<span class="number">8</span>] 的平均值分别为 <span class="number">4</span>，<span class="number">5</span> 和 <span class="number">6</span> 。其他长度为 <span class="number">3</span> 的子数组的平均值都小于 <span class="number">4</span> （threshold 的值)。</span><br></pre></td></tr></table></figure><ol><li>$ans$ 用来维护答案数目。$window\underline{}sum$ 用来维护窗口中元素的和。</li><li>$left$ 、$right$ 都指向序列的第一个元素，即：$left = 0$，$right = 0$。</li><li>向右移动 $right$，先将 $k$ 个元素填入窗口中，即 <code>window_sum += arr[right]</code>。</li><li>当窗口元素个数为 $k$ 时，即满足 <code>right - left + 1 &gt;= k</code> 时，判断窗口内的元素和平均值是否大于等于阈值 $threshold$。<ol><li>如果满足，则答案数目加 $1$。</li><li>然后向右移动 $left$，从而缩小窗口长度，即 <code>left += 1</code>，使得窗口大小始终保持为 $k$。</li></ol></li><li>重复 $3 \sim 4$ 步，直到 $right$ 到达数组末尾。</li><li>最后输出答案数目。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">numOfSubarrays</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; arr, <span class="type">int</span> k, <span class="type">int</span> threshold)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// int slide_windows </span></span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> window_sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (right &lt; arr.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            window_sum += arr[right];</span><br><span class="line">            <span class="keyword">if</span> (right - left + <span class="number">1</span> &gt;= k) &#123;</span><br><span class="line">                <span class="keyword">if</span> (window_sum &gt;= k * threshold) &#123;</span><br><span class="line">                    ans++;</span><br><span class="line">                &#125;</span><br><span class="line">                window_sum -= arr[left];</span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            right++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="4-2-无重复字符的最长子串"><a href="#4-2-无重复字符的最长子串" class="headerlink" title="4.2 无重复字符的最长子串"></a>4.2 无重复字符的最长子串</h3><p><a href="https://leetcode.cn/problems/longest-substring-without-repeating-characters/description/" title="3. 无重复字符的最长子串 - 力扣（LeetCode）">3. 无重复字符的最长子串 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。</span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">输入: s = <span class="string">&quot;abcabcbb&quot;</span></span><br><span class="line">输出: <span class="number">3</span> </span><br><span class="line">解释: 因为无重复字符的最长子串是 <span class="string">&quot;abc&quot;</span>，所以其长度为 <span class="number">3</span>。</span><br></pre></td></tr></table></figure><ol><li>一开始，$left$、$right$ 都指向 $0$。</li><li>向右移动 $right$，将最右侧元素加入当前窗口和 $window\underline{}sum$ 中。</li><li>如果 $window\underline{}sum \ge target$，则不断右移 $left$，缩小滑动窗口长度，并更新窗口和的最小值，直到 $window\underline{}sum &lt; target$。</li><li>然后继续右移 $right$，直到 $right \ge len(nums)$ 结束。</li><li>输出窗口和的最小值作为答案。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 滑动窗口 + hash</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 哈希集合，记录每个字符是否出现过</span></span><br><span class="line">        std::unordered_set&lt;<span class="type">char</span>&gt; occ;</span><br><span class="line">        <span class="type">int</span> n = s.<span class="built_in">size</span>();</span><br><span class="line">        <span class="comment">// 初始化右指针，初始值为-1，还没开始移动</span></span><br><span class="line">        <span class="type">int</span> right = <span class="number">-1</span>;</span><br><span class="line">        <span class="comment">// 最长子串的长度</span></span><br><span class="line">        <span class="type">int</span> max_len = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开始遍历左指针位置</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> left = <span class="number">0</span>; left &lt; n; left++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (left != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">// 左指针向右移动一格，一处一个字符</span></span><br><span class="line">                occ.<span class="built_in">erase</span>(s[left - <span class="number">1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 不断移动右指针，直到出现重复的字符，或是到达最后一个字符</span></span><br><span class="line">            <span class="keyword">while</span> (right + <span class="number">1</span> &lt; n &amp;&amp; !occ.<span class="built_in">count</span>(s[right + <span class="number">1</span>])) &#123;</span><br><span class="line">                occ.<span class="built_in">insert</span>(s[right + <span class="number">1</span>]);</span><br><span class="line">                right++;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 更新max_len</span></span><br><span class="line">            max_len = std::<span class="built_in">max</span>(max_len, right - left + <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_len;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="4-3-长度最小的子数组"><a href="#4-3-长度最小的子数组" class="headerlink" title="4.3 长度最小的子数组"></a>4.3 长度最小的子数组</h3><p><a href="https://leetcode.cn/problems/minimum-size-subarray-sum/description/" title="209. 长度最小的子数组 - 力扣（LeetCode）">209. 长度最小的子数组 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">给定一个含有 n 个正整数的数组和一个正整数 target 。</span><br><span class="line"></span><br><span class="line">找出该数组中满足其总和大于等于 target 的长度最小的 连续子数组 [numsl, numsl+<span class="number">1</span>, ..., numsr<span class="number">-1</span>, numsr] ，并返回其长度。如果不存在符合条件的子数组，返回 <span class="number">0</span> 。</span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：target = <span class="number">7</span>, nums = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>]</span><br><span class="line">输出：<span class="number">2</span></span><br><span class="line">解释：子数组 [<span class="number">4</span>,<span class="number">3</span>] 是该条件下的长度最小的子数组。</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>用滑动窗口来记录连续子数组的和，设定两个指针：$left$、$right$，分别指向滑动窗口的左右边界，保证窗口中的和刚好大于等于 $target$。</p><ol><li>一开始，$left$、$right$ 都指向 $0$。</li><li>向右移动 $right$，将最右侧元素加入当前窗口和 $window\underline{}sum$ 中。</li><li>如果 $window\underline{}sum \ge target$，则不断右移 $left$，缩小滑动窗口长度，并更新窗口和的最小值，直到 $window\underline{}sum &lt; target$。</li><li>然后继续右移 $right$，直到 $right \ge len(nums)$ 结束。</li><li>输出窗口和的最小值作为答案。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 滑动窗口</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minSubArrayLen</span><span class="params">(<span class="type">int</span> target, vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.<span class="built_in">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> window_sum = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> ans = nums.<span class="built_in">size</span>() + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (right &lt; nums.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            window_sum += nums[right];</span><br><span class="line">            <span class="comment">// 如果连续数组之和超过target</span></span><br><span class="line">            <span class="comment">// 更新ans为right - left + 1</span></span><br><span class="line">            <span class="comment">// 从sum中移除 nums[left]</span></span><br><span class="line">            <span class="comment">// left后移一位</span></span><br><span class="line">            <span class="keyword">while</span> (window_sum &gt;= target) &#123;</span><br><span class="line">                ans = std::<span class="built_in">min</span>(ans, right - left + <span class="number">1</span>);</span><br><span class="line">                window_sum -= nums[left];</span><br><span class="line">                left++;</span><br><span class="line">            &#125; </span><br><span class="line">            right++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (ans == nums.<span class="built_in">size</span>() + <span class="number">1</span>) &#123;</span><br><span class="line">            ans = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans == nums.<span class="built_in">size</span>() + <span class="number">1</span> ? <span class="number">0</span> : ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 暴力解法，两层循环, 超出时间限制</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minSubArrayLen_nonono</span><span class="params">(<span class="type">int</span> target, vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> ans = INT_MAX;</span><br><span class="line">        <span class="comment">// 对于每个开始下标i，需要找到大于或等于i的最小下标j</span></span><br><span class="line">        <span class="comment">// 使得 nums[i]到nums[j]的元素之和大于或等于s</span></span><br><span class="line">        <span class="comment">// 最后更新数组的最小长度，j - i + 1</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = i; j &lt; n; j++) &#123;</span><br><span class="line">                sum += nums[j];</span><br><span class="line">                <span class="keyword">if</span> (sum &gt;= target) &#123;</span><br><span class="line">                    ans = std::<span class="built_in">min</span>(ans, j - i + <span class="number">1</span>);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans == INT_MAX ? <span class="number">0</span> : ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="4-4-乘积小于k的子数组"><a href="#4-4-乘积小于k的子数组" class="headerlink" title="4.4 乘积小于k的子数组"></a>4.4 乘积小于k的子数组</h3><p><a href="https://leetcode.cn/problems/subarray-product-less-than-k/" title="713. 乘积小于 K 的子数组 - 力扣（LeetCode）">713. 乘积小于 K 的子数组 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数数组 nums 和一个整数 k ，请你返回子数组内所有元素的乘积严格小于 k 的连续子数组的数目。</span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：nums = [<span class="number">10</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">6</span>], k = <span class="number">100</span></span><br><span class="line">输出：<span class="number">8</span></span><br><span class="line">解释：<span class="number">8</span> 个乘积小于 <span class="number">100</span> 的子数组分别为：[<span class="number">10</span>]、[<span class="number">5</span>]、[<span class="number">2</span>],、[<span class="number">6</span>]、[<span class="number">10</span>,<span class="number">5</span>]、[<span class="number">5</span>,<span class="number">2</span>]、[<span class="number">2</span>,<span class="number">6</span>]、[<span class="number">5</span>,<span class="number">2</span>,<span class="number">6</span>]。</span><br><span class="line">需要注意的是 [<span class="number">10</span>,<span class="number">5</span>,<span class="number">2</span>] 并不是乘积小于 <span class="number">100</span> 的子数组。</span><br></pre></td></tr></table></figure><ol><li>设定两个指针：$left$、$right$，分别指向滑动窗口的左右边界，保证窗口内所有数的乘积 $window_{}product$ 都小于 $k$。使用 $window_product$ 记录窗口中的乘积值，使用 $count$ 记录符合要求的子数组个数。</li><li>一开始，$left$、$right$ 都指向 $0$。</li><li>向右移动 $right$，将最右侧元素加入当前子数组乘积 $window_{}product$ 中。</li><li>如果 $window_{}product \ge k$，则不断右移 $left$，缩小滑动窗口长度，并更新当前乘积值 $window_{}product$  直到 $window_{}product &lt; k$。</li><li>记录累积答案个数加 $1$，继续右移 $right$，直到 $right \ge len(nums)$ 结束。</li><li>输出累积答案个数。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">numSubarrayProductLessThanK</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (k &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> window_product = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (right &lt; nums.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            window_product = nums[right];</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (window_product &gt;= k) &#123;</span><br><span class="line">                window_product /= nums[left];</span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line">            count += (right - left + <span class="number">1</span>);</span><br><span class="line">            right++;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="4-5-子数组最大平均数Ⅰ"><a href="#4-5-子数组最大平均数Ⅰ" class="headerlink" title="4.5 子数组最大平均数Ⅰ"></a>4.5 子数组最大平均数Ⅰ</h3><p><a href="https://leetcode.cn/problems/maximum-average-subarray-i/" title="643. 子数组最大平均数 I - 力扣（LeetCode）">643. 子数组最大平均数 I - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">给你一个由 n 个元素组成的整数数组 nums 和一个整数 k 。</span><br><span class="line"></span><br><span class="line">请你找出平均数最大且 长度为 k 的连续子数组，并输出该最大平均数。</span><br><span class="line"></span><br><span class="line">任何误差小于 <span class="number">10</span><span class="number">-5</span> 的答案都将被视为正确答案。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：nums = [<span class="number">1</span>,<span class="number">12</span>,<span class="number">-5</span>,<span class="number">-6</span>,<span class="number">50</span>,<span class="number">3</span>], k = <span class="number">4</span></span><br><span class="line">输出：<span class="number">12.75</span></span><br><span class="line">解释：最大平均数 (<span class="number">12</span><span class="number">-5</span><span class="number">-6</span>+<span class="number">50</span>)/<span class="number">4</span> = <span class="number">51</span>/<span class="number">4</span> = <span class="number">12.75</span></span><br></pre></td></tr></table></figure><p>固定窗口大小，</p><ol><li>$max_sum$ 用来维护子数组最大和，初始值为负无穷。$window_sum$ 用来维护窗口中元素的和。</li><li>$left$ 、$right$ 都指向序列的第一个元素，即：<code>left = 0</code>，<code>right = 0</code>。</li><li>向右移动 $right$，先将 $k$ 个元素填入窗口中。</li><li>当窗口元素个数为 $k$ 时，即：$right - left + 1 &gt;= k$ 时，计算窗口内的和，并维护子数组最大和。</li><li>然后向右移动 $left$，从而缩小窗口长度，即 <code>left += 1</code>，使得窗口大小始终保持为 $k$。</li><li>重复 $4 \sim 5$ 步，直到 $right$ 到达数组末尾。</li><li>最后输出答案 $ans$。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">findMaxAverage</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> windows_sum = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> max_sum = INT_MIN;</span><br><span class="line">        <span class="keyword">while</span> (right &lt; nums.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            windows_sum += nums[right];</span><br><span class="line">            <span class="keyword">if</span> (right - left + <span class="number">1</span> &gt;= k) &#123;</span><br><span class="line">                max_sum = std::<span class="built_in">max</span>(max_sum, windows_sum);</span><br><span class="line">                windows_sum -= nums[left];</span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            right++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (<span class="type">double</span>)max_sum/k;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="4-6-最长连续递序列"><a href="#4-6-最长连续递序列" class="headerlink" title="4.6 最长连续递序列"></a>4.6 最长连续递序列</h3><p><a href="https://leetcode.cn/problems/longest-continuous-increasing-subsequence/" title="674. 最长连续递增序列 - 力扣（LeetCode）">674. 最长连续递增序列 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">给定一个未经排序的整数数组，找到最长且 连续递增的子序列，并返回该序列的长度。</span><br><span class="line"></span><br><span class="line">连续递增的子序列 可以由两个下标 l 和 r（l &lt; r）确定，如果对于每个 l &lt;= i &lt; r，都有 nums[i] &lt; nums[i + <span class="number">1</span>] ，那么子序列 [nums[l], nums[l + <span class="number">1</span>], ..., nums[r - <span class="number">1</span>], nums[r]] 就是连续递增子序列。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：nums = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">7</span>]</span><br><span class="line">输出：<span class="number">3</span></span><br><span class="line">解释：最长连续递增序列是 [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>], 长度为<span class="number">3</span>。</span><br><span class="line">尽管 [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>] 也是升序的子序列, 但它不是连续的，因为 <span class="number">5</span> 和 <span class="number">7</span> 在原数组里被 <span class="number">4</span> 隔开。 </span><br></pre></td></tr></table></figure><p>1、动态规划</p><ul><li>状态定义：<code>dp[i]</code> , 以<code>nums[i]</code>结尾的最长且连续递增子序列长度</li><li>状态转移方程：<code>dp[i] = dp[i - 1] + 1</code>， <code>nums[i - 1] &lt; nums[i]</code></li></ul><p>2、滑动窗口，不定长度</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1、动态规划</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findLengthOfLCIS1</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> size = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="comment">// 注意，初始化为1，而不是0</span></span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(size, <span class="number">1</span>)</span></span>;</span><br><span class="line">        <span class="type">int</span> max_len = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; size; i++) &#123;</span><br><span class="line">            <span class="comment">// 不连续递增子序列</span></span><br><span class="line">            <span class="keyword">if</span> (nums[i] &gt; nums[i - <span class="number">1</span>]) &#123;</span><br><span class="line">                dp[i] = dp[i - <span class="number">1</span>] + <span class="number">1</span>;</span><br><span class="line">                max_len = std::<span class="built_in">max</span>(dp[i], max_len);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max_len;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.滑动窗口</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findLengthOfLCIS</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> size = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> window_len = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> max_len = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (right &lt; size) &#123;</span><br><span class="line">            window_len++;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果不满足连续递增序列，将left移动到窗口最右侧，重置当前窗口长度</span></span><br><span class="line">            <span class="keyword">if</span> (right &gt; <span class="number">0</span> &amp;&amp; nums[right - <span class="number">1</span>] &gt;= nums[right]) &#123;</span><br><span class="line">                left = right;</span><br><span class="line">                window_len = <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            max_len = std::<span class="built_in">max</span>(max_len, window_len);</span><br><span class="line">            right++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_len;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="4-7-最大连续1的个数Ⅲ"><a href="#4-7-最大连续1的个数Ⅲ" class="headerlink" title="4.7 最大连续1的个数Ⅲ"></a>4.7 最大连续1的个数Ⅲ</h3><p><a href="https://leetcode.cn/problems/max-consecutive-ones-iii/description/" title="1004. 最大连续1的个数 III - 力扣（LeetCode）">1004. 最大连续1的个数 III - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">给定一个二进制数组 nums 和一个整数 k，如果可以翻转最多 k 个 <span class="number">0</span> ，则返回 数组中连续 <span class="number">1</span> 的最大个数 。</span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：nums = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>], K = <span class="number">2</span></span><br><span class="line">输出：<span class="number">6</span></span><br><span class="line">解释：[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>, <span class="number">1</span> ,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>, <span class="number">1</span> ]</span><br><span class="line">粗体数字从 <span class="number">0</span> 翻转到 <span class="number">1</span>，最长的子数组长度为 <span class="number">6</span>。</span><br></pre></td></tr></table></figure><p>不定滑动窗口</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">longestOnes</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> max_count = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> zero_count = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (right &lt; nums.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            <span class="comment">// 统计0的个数</span></span><br><span class="line">            <span class="keyword">if</span> (nums[right] == <span class="number">0</span>) &#123;</span><br><span class="line">                zero_count++;</span><br><span class="line">            &#125; </span><br><span class="line">            right++;</span><br><span class="line">            <span class="comment">// 如果0的个数超过k时，将left开始右移，缩小滑动窗口范围，并减小0元素个数</span></span><br><span class="line">            <span class="comment">// 同时维护 max_count</span></span><br><span class="line">            <span class="keyword">if</span> (zero_count &gt; k) &#123;</span><br><span class="line">               <span class="keyword">if</span> (nums[left] == <span class="number">0</span>) &#123;</span><br><span class="line">                    zero_count--;</span><br><span class="line">               &#125;</span><br><span class="line">               left++;</span><br><span class="line">            &#125;</span><br><span class="line">            max_count = std::<span class="built_in">max</span>(max_count, right - left);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_count;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLMs公开课 - 1.NLP&amp;大模型基础</title>
      <link href="/llms/llms_course/1.NLP_%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/"/>
      <url>/llms/llms_course/1.NLP_%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h1 id="1-自然语言处理"><a href="#1-自然语言处理" class="headerlink" title="1.自然语言处理"></a>1.自然语言处理</h1><h2 id="1-1-基础与应用"><a href="#1-1-基础与应用" class="headerlink" title="1.1 基础与应用"></a>1.1 基础与应用</h2><h3 id="（1）图灵测试"><a href="#（1）图灵测试" class="headerlink" title="（1）图灵测试"></a>（1）图灵测试</h3><p>原名：Imitation Game</p><p>采用一种行为注意的手段，尝试定义人工智能是不是具备人类智能的水平</p><ul><li>1997年，人工智能在象棋方面战胜人类</li><li>2011年，IBM Watson DeepQA在问答节目上战胜所有人类。</li><li>2016年，Alpha go 在围棋方面战胜人类</li></ul><h3 id="（2）NLP任务"><a href="#（2）NLP任务" class="headerlink" title="（2）NLP任务"></a>（2）NLP任务</h3><p>基础任务：</p><ul><li>词性标注：</li><li>命名实体识别：</li><li>共指消解：用代词代替实体</li><li>句法关系：互相依存关系</li><li>中文自动分词：</li></ul><h2 id="1-2-词表示"><a href="#1-2-词表示" class="headerlink" title="1.2 词表示"></a>1.2 词表示</h2><h3 id="（1）词表示"><a href="#（1）词表示" class="headerlink" title="（1）词表示"></a>（1）词表示</h3><p>目的：<strong>将单词转换为机器可以理解的符号</strong></p><p>目标：</p><ul><li>词之间相似度的计算</li><li>词之间语义的关系</li></ul><h3 id="（2）用一组相关的词表示"><a href="#（2）用一组相关的词表示" class="headerlink" title="（2）用一组相关的词表示"></a>（2）用一组相关的词表示</h3><p><strong>近义词，上位词</strong>；</p><p>问题：</p><ol><li>词语之间的较小差异无法区分；</li><li>词义会发生变化，出现新的词义；</li><li>主观性的问题，受限于词典的标注；</li><li>数据稀疏问题；</li><li>大量的人工去构建、维护词典；</li></ol><h3 id="（3）one-hot表示"><a href="#（3）one-hot表示" class="headerlink" title="（3）one-hot表示"></a>（3）one-hot表示</h3><p>把每个词表示成独立的符号；</p><p>和词表一样长的向量去找一维跟这个词相对应，整个向量的维度跟词表的长度是相当的；</p><p>用来<strong>表示文档时非常有效</strong>，能较好地完成两个文档之间的相似度计算；</p><p>但是，在表示词的时候会有问题：<strong>会假设词根词之间的向量任意之间都是正交的</strong>，导致任意两个词之间进行相似度计算都是0.</p><script type="math/tex; mode=display">similarity ( star, sun )=\left(v_{\text {star }}, v_{\text {sun }}\right)=0</script><h3 id="（4）上下文表示"><a href="#（4）上下文表示" class="headerlink" title="（4）上下文表示"></a>（4）上下文表示</h3><p>一个词的词义由他经常出现在的位置的上下文有密切的关系</p><p>任何一个词都可以用他出现的维度或者重要性去进行表示，可以得到关于<strong>每一个词的稠密向量</strong>，就可以在这个空间里面利用稠密向量来计算两个词之间的相似度</p><p>问题：</p><ol><li>词表变大，存储需求也会变大</li><li>有些词出现频度特别少，上下文少，这种方法不好表示</li></ol><h3 id="（5）词嵌入"><a href="#（5）词嵌入" class="headerlink" title="（5）词嵌入"></a>（5）词嵌入</h3><p>建立低维的稠密的向量空间，尝试把每一个词都学到这个空间里，用这个空间里的某一个位置所对应的向量来表示这个词，</p><p>在这个空间里我们可以<strong>自动的学习出来词与词之间可能存在的相对比较稳定的一些关系</strong></p><p><img src="image/image_ggQ0XvgPsy.png" alt=""></p><h2 id="1-3-语言模型"><a href="#1-3-语言模型" class="headerlink" title="1.3 语言模型"></a>1.3 语言模型</h2><p>目的：根据前文，预测下一个词</p><ol><li>计算一个词的序列成为一句合法的话的概率，<strong>联合概率</strong>：$P(W)=P\left(w_{1}, w_{2}, \cdots, w_{n}\right)$</li><li>根据前面说过的话，预测下一个词是什么，<strong>条件概率</strong>：$P\left(w_{n} \mid w_{1}, w_{2}, \cdots, w_{n-1}\right)$</li></ol><p><img src="image/image_cfrPHap7uB.png" alt=""></p><p>基本假设：一个未来的词只会受到他前面的词的影响</p><script type="math/tex; mode=display">\begin{array}{l}P(\text { Never }, \text { too }, \text { late }, \text { to }, \text { learn })= \\ \quad P(\text { Never }) \times P(\text { too } \mid \text { Never }) \times P(\text { late } \mid \text { Never }, \text { too }) \times \\ \quad P(\text { to } \mid \text { Never }, \text { too, late }) \times P(\text { learn } \mid \text { Never }, \text { too, late, to })\end{array}</script><script type="math/tex; mode=display">P( learn \mid Never, too, late, to )=\frac{P(\text { Never }, \text { too }, \text { late }, \text { to }, \text { learn })}{P(\text { Never }, \text { too }, \text { late }, \text { to })}</script><p>语言模型：一个句子的联合概率=里面的每一个词基于前面已经出现的词的条件概率之积</p><script type="math/tex; mode=display">P\left(w_{1}, w_{2}, \cdots, w_{n}\right)=\prod_{i} P\left(w_{i} \mid w_{1}, w_{2}, \cdots, w_{i-1}\right)</script><h2 id="1-4-N-gram-Model"><a href="#1-4-N-gram-Model" class="headerlink" title="1.4 N-gram Model"></a>1.4 N-gram Model</h2><p><strong>每一个词是一个单独的符号</strong></p><p><code>4-gram</code>只会考虑相邻的4个词，也就是前面出现的三个词来预测下一个词</p><script type="math/tex; mode=display">P\left(w_{j} \mid\right. never to late to )=\frac{\text { count }\left(\text { too late to } w_{j}\right)}{\text { count }(\text { too late to })}</script><p><code>Bigram</code>就是<code>2-gram</code>，考虑连续出现的两个词，相当于只考虑前面出现的一个词，预测下一个词是什么</p><p><code>Trigram</code>就是<code>3-gram</code></p><p>在一个大规模数据里统计连续出现的序列的频度，在深度学习出现之前一个非常重要的技术</p><p>遵守<strong>Markov的假设</strong>，只考虑前面的有限的几个词</p><script type="math/tex; mode=display">P\left(w_{1}, w_{2}, \cdots, w_{n}\right) \approx \prod_{i} P\left(w_{i} \mid w_{i-k}, \cdots, w_{i-1}\right)</script><script type="math/tex; mode=display">P\left(w_{i} \mid w_{1}, w_{2}, \cdots, w_{i-1}\right) \approx P\left(w_{i} \mid w_{i-k}, \cdots, w_{i-1}\right)</script><p><strong>问题：</strong></p><ol><li>考虑的长度通常短，N多是2或者3，那上下文是1或2</li><li>背后还是会<strong>假设所有词相互之间都是独立的</strong>，上下文基于符号去做统计，不能理解词与词之间的相似度造成了什么</li></ol><h2 id="1-5-神经语言模型"><a href="#1-5-神经语言模型" class="headerlink" title="1.5 神经语言模型"></a>1.5 神经语言模型</h2><p><strong>每一个词是一个低维的向量</strong></p><p>用分布式的表示建构前文和当前词的预测条件概率</p><ol><li>把词表示成低维的向量</li><li>把低维的向量拼在一起，形成一个更高的上下文的向量</li><li>经过非线性的转换，用向量去预测下一个词是什么</li></ol><p>通过对上下文的表示完成。</p><p><img src="image/image_iGWRwsBfGU.png" alt=""></p><p>N-gram Model中每一个词是一个单独的符号，在Neural language Model中每一个词会被表示为一个向量。</p><p>相似的词会有一个相似的向量，就有可能在语境中发挥相似的作用。</p><h1 id="2-大模型基础"><a href="#2-大模型基础" class="headerlink" title="2.大模型基础"></a>2.大模型基础</h1><h2 id="2-1-大模型之旅"><a href="#2-1-大模型之旅" class="headerlink" title="2.1 大模型之旅"></a>2.1 大模型之旅</h2><p><img src="image/image_FWDm0QAdl1.png" alt=""></p><h3 id="（1）预训练大模型PLM"><a href="#（1）预训练大模型PLM" class="headerlink" title="（1）预训练大模型PLM"></a>（1）预训练大模型PLM</h3><p>GLUE上预训练的语言模型的结果优于人类的表现，反映了语言理解的能力</p><p><img src="image/image_HU7nzIs7I1.png" alt=""></p><h3 id="（2）大模型的特点"><a href="#（2）大模型的特点" class="headerlink" title="（2）大模型的特点"></a>（2）大模型的特点</h3><p>2018年以后，预训练大模型有以下三个特点：</p><ol><li>参数量越来越大</li><li>数据越来越多</li><li>计算资源越来越大</li></ol><p><img src="image/image_pbvSxCsrDQ.png" alt=""></p><p>近两年来，参数尺度以每年10倍左右的速度增长；数据量也随之增长，计算成本也越来越高</p><p><img src="image/image_JHCVjojnnM.png" alt=""></p><p>注:M-millions, b -billion。最后一列训练时间是使用单个NVIDIA V100 GPU训练的估计时间</p><h2 id="2-2-大模型背后的范式"><a href="#2-2-大模型背后的范式" class="headerlink" title="2.2 大模型背后的范式"></a>2.2 大模型背后的范式</h2><h3 id="（1）预训练-微调"><a href="#（1）预训练-微调" class="headerlink" title="（1）预训练 + 微调"></a>（1）预训练 + 微调</h3><p>在<strong>预训练</strong>阶段，预训练的语言模型从大规模的未标记数据中获取丰富的知识</p><p>然后，我们可以使用特定任务的训练数据对预训练的语言模型进行<strong>微调</strong>，以适应预训练的知识</p><p><img src="image/image_Ebv4EMmNWq.png" alt=""></p><p>预训练和微调的基本范例可以追溯到<strong>迁移学习</strong></p><p>人类可以应用以前学到的知识来更快地处理新问题，我们希望机器也有类似的能力</p><p><img src="image/image_YsYjUqgq4J.png" alt=""></p><p>迁移学习使用“预训练，然后微调”的框架来实现“知识获取，然后知识转移”。</p><p>在预训练模型的后续工作中，使用了特征-表征-迁移和参数-迁移</p><h3 id="（2）词嵌入Word2Vec"><a href="#（2）词嵌入Word2Vec" class="headerlink" title="（2）词嵌入Word2Vec"></a>（2）词嵌入Word2Vec</h3><p>Word2Vec使用两种主要的技术：CBOW（Continuous Bag of Words）和Skip-gram。两者均通过优化一个神经网络来训练词向量，但目标函数略有不同。</p><p><strong>CBOW (Continuous Bag of Words)</strong>：CBOW模型预测的是目标词（中心词），而根据的是上下文词（周围的词）。具体来说，给定一个词的上下文，CBOW试图预测该词。</p><p><strong>Skip-gram</strong>：Skip-gram与CBOW恰好相反。它的输入是中心词，输出则是上下文词。换句话说，它根据某个词来预测其周围的词。</p><p><img src="image/image_xEmsEBZnB9.png" alt=""></p><h3 id="（3）解决一词多义：ELMo"><a href="#（3）解决一词多义：ELMo" class="headerlink" title="（3）解决一词多义：ELMo"></a>（3）解决一词多义：ELMo</h3><ul><li>使用RNN对大规模未标记数据进行语言建模</li><li>使用预训练的RNN生成<strong>上下文词嵌入</strong></li></ul><p>特定于任务的模型</p><p><img src="image/image_0RLD-ilQKV.png" alt=""></p><h3 id="（4）Transformer"><a href="#（4）Transformer" class="headerlink" title="（4）Transformer"></a>（4）Transformer</h3><p>在Transformer的基础上，开发了一系列深度预训练模型，取代了更强大的浅层RNN</p><p><img src="image/image_Ua9819JN7h.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数组双指针（Double Pointer）</title>
      <link href="/dsa/confusing_topics/2.double_pointer/"/>
      <url>/dsa/confusing_topics/2.double_pointer/</url>
      
        <content type="html"><![CDATA[<h2 id="1-双指针简介"><a href="#1-双指针简介" class="headerlink" title="1.双指针简介"></a>1.双指针简介</h2><blockquote><p><strong>双指针（Two Pointers）</strong>：指的是在遍历元素的过程中，不是使用单个指针进行访问，而是使用两个指针进行访问，从而达到相应的目的。如果两个指针方向相反，则称为「<strong>对撞指针</strong>」。如果两个指针方向相同，则称为「<strong>快慢指针</strong>」。如果两个指针分别属于不同的数组 / 链表，则称为「<strong>分离双指针</strong>」。</p></blockquote><p>在数组的区间问题上，暴力算法的时间复杂度往往是 $O(n^2)$。而双指针利用了区间「单调性」的性质，可以将时间复杂度降到 $O(n)$。</p><h2 id="2-对撞指针"><a href="#2-对撞指针" class="headerlink" title="2.对撞指针"></a>2.对撞指针</h2><blockquote><p><strong>对撞指针</strong>：指的是两个指针 $left$、$right$ 分别指向序列第一个元素和最后一个元素，然后 $left$ 指针不断递增，$right$ 不断递减，直到两个指针的值相撞（即 $left == right$），或者满足其他要求的特殊条件为止。</p></blockquote><p><img src="image/image_LnHBqnOPL9.png" alt=""></p><h3 id="2-1-对撞指针求解步骤"><a href="#2-1-对撞指针求解步骤" class="headerlink" title="2.1 对撞指针求解步骤"></a>2.1 对撞指针求解步骤</h3><ol><li>使用两个指针 $left$，$right$。$left$ 指向序列第一个元素，即：$left = 0$，$right$ 指向序列最后一个元素，即：$right = len(nums) - 1$。</li><li>在循环体中将左右指针相向移动，当满足一定条件时，将左指针右移，$left += 1$。当满足另外一定条件时，将右指针左移，$right -= 1$。</li><li>直到两指针相撞（即 $left == right$），或者满足其他要求的特殊条件时，跳出循环体。</li></ol><p>伪代码模板：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> left &lt; right:</span><br><span class="line">    <span class="keyword">if</span> 满足要求的特殊条件:</span><br><span class="line">        <span class="keyword">return</span> 符合条件的值 </span><br><span class="line">    <span class="keyword">elif</span> 一定条件 <span class="number">1</span>:</span><br><span class="line">        left += <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> 一定条件 <span class="number">2</span>:</span><br><span class="line">        right -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> 没找到 或 找到对应值</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-2-对撞指针适用范围"><a href="#2-2-对撞指针适用范围" class="headerlink" title="2.2 对撞指针适用范围"></a>2.2 对撞指针适用范围</h3><p>对撞指针一般用来解决有序数组或者字符串问题：</p><ul><li>查找有序数组中满足某些约束条件的一组元素问题：比如<strong>二分查找、数字之和</strong>等问题。</li><li><strong>字符串反转问题</strong>：反转字符串、回文数、颠倒二进制等问题。</li></ul><h2 id="3-快慢指针"><a href="#3-快慢指针" class="headerlink" title="3.快慢指针"></a>3.快慢指针</h2><blockquote><p><strong>快慢指针</strong>：指的是两个指针从同一侧开始遍历序列，且移动的步长一个快一个慢。移动快的指针被称为 「快指针（fast）」，移动慢的指针被称为「慢指针（slow）」。<strong>两个指针以不同速度、不同策略移动</strong>，直到快指针移动到数组尾端，或者两指针相交，或者满足其他特殊条件时为止。</p></blockquote><p><img src="image/image_sFN9lWgk18.png" alt=""></p><h3 id="3-1-快慢指针求解步骤"><a href="#3-1-快慢指针求解步骤" class="headerlink" title="3.1 快慢指针求解步骤"></a>3.1 快慢指针求解步骤</h3><ol><li>使用两个指针 $slow$、$fast$。$slow$ 一般指向序列第一个元素，即：$slow = 0$，$fast$ 一般指向序列第二个元素，即：$fast = 1$。</li><li>在循环体中将左右指针向右移动。当满足一定条件时，将慢指针右移，即 $slow += 1$。当满足另外一定条件时（也可能不需要满足条件），将快指针右移，即 $fast += 1$。</li><li>到快指针移动到数组尾端（即 $fast == len(nums) - 1$），或者两指针相交，或者满足其他特殊条件时跳出循环体。</li></ol><p>伪代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">slow = <span class="number">0</span></span><br><span class="line">fast = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> 没有遍历完：</span><br><span class="line">    <span class="keyword">if</span> 满足要求的特殊条件:</span><br><span class="line">        slow += <span class="number">1</span></span><br><span class="line">    fast += <span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> 合适的值</span><br></pre></td></tr></table></figure><h3 id="3-2-快慢指针使用范围"><a href="#3-2-快慢指针使用范围" class="headerlink" title="3.2 快慢指针使用范围"></a>3.2 快慢指针使用范围</h3><p>快慢指针一般用于处理<strong>数组中的移动、删除元素问题</strong>，或者链表中的<strong>判断是否有环、长度问题</strong>。关于链表相关的双指针做法我们到链表章节再详细讲解。</p><h2 id="4-分离双指针"><a href="#4-分离双指针" class="headerlink" title="4.分离双指针"></a>4.分离双指针</h2><blockquote><p><strong>分离双指针</strong>：两个指针分别属于不同的数组，两个指针分别在两个数组中移动。</p></blockquote><p><img src="image/image_HU-RW5G7B6.png" alt=""></p><h3 id="4-1-分离双指针求解步骤"><a href="#4-1-分离双指针求解步骤" class="headerlink" title="4.1 分离双指针求解步骤"></a>4.1 分离双指针求解步骤</h3><ol><li>使用两个指针 $left\underline{}1$、$left\underline{}2$。$left\underline{}1$ 指向第一个数组的第一个元素，即：$left\underline{}1 = 0$，$left\underline{}2$ 指向第二个数组的第一个元素，即：$left\underline{}2 = 0$。</li><li>当满足一定条件时，两个指针同时右移，即 $left\underline{}1 += 1$、$left\underline{}2 += 1$。</li><li>当满足另外一定条件时，将 $left\underline{}1$ 指针右移，即 $left\underline{}1 += 1$。</li><li>当满足其他一定条件时，将 $left\underline{}2$ 指针右移，即 $left\underline{}2 += 1$。</li><li>当其中一个数组遍历完时或者满足其他特殊条件时跳出循环体。</li></ol><p>伪代码模板：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">left_1 = <span class="number">0</span></span><br><span class="line">left_2 = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> left_1 &lt; <span class="built_in">len</span>(nums1) <span class="keyword">and</span> left_2 &lt; <span class="built_in">len</span>(nums2):</span><br><span class="line">    <span class="keyword">if</span> 一定条件 <span class="number">1</span>:</span><br><span class="line">        left_1 += <span class="number">1</span></span><br><span class="line">        left_2 += <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> 一定条件 <span class="number">2</span>:</span><br><span class="line">        left_1 += <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> 一定条件 <span class="number">3</span>:</span><br><span class="line">        left_2 += <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="4-2-分离双指针使用范围"><a href="#4-2-分离双指针使用范围" class="headerlink" title="4.2 分离双指针使用范围"></a>4.2 分离双指针使用范围</h3><p>分离双指针一般用于处理<strong>有序数组合并，求交集、并集</strong>问题。</p><h2 id="5-实战题目"><a href="#5-实战题目" class="headerlink" title="5.实战题目"></a>5.实战题目</h2><h3 id="5-1-两数之和Ⅱ-输入有序数组"><a href="#5-1-两数之和Ⅱ-输入有序数组" class="headerlink" title="5.1 两数之和Ⅱ-输入有序数组"></a>5.1 两数之和Ⅱ-输入有序数组</h3><p><a href="https://leetcode.cn/problems/two-sum-ii-input-array-is-sorted/description/" title="167. 两数之和 II - 输入有序数组 - 力扣（LeetCode）">167. 两数之和 II - 输入有序数组 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">给你一个下标从 <span class="number">1</span> 开始的整数数组 numbers ，该数组已按 非递减顺序排列  ，请你从数组中找出满足相加之和等于目标数 target 的两个数。如果设这两个数分别是 numbers[index1] 和 numbers[index2] ，则 <span class="number">1</span> &lt;= index1 &lt; index2 &lt;= numbers.length 。</span><br><span class="line"></span><br><span class="line">以长度为 <span class="number">2</span> 的整数数组 [index1, index2] 的形式返回这两个整数的下标 index1 和 index2。</span><br><span class="line"></span><br><span class="line">你可以假设每个输入 只对应唯一的答案 ，而且你 不可以 重复使用相同的元素。</span><br><span class="line"></span><br><span class="line">你所设计的解决方案必须只使用常量级的额外空间。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：numbers = [<span class="number">2</span>,<span class="number">7</span>,<span class="number">11</span>,<span class="number">15</span>], target = <span class="number">9</span></span><br><span class="line">输出：[<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">解释：<span class="number">2</span> 与 <span class="number">7</span> 之和等于目标数 <span class="number">9</span> 。因此 index1 = <span class="number">1</span>, index2 = <span class="number">2</span> 。返回 [<span class="number">1</span>, <span class="number">2</span>] 。</span><br></pre></td></tr></table></figure><p>双指针 - 对撞指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">twoSum</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; numbers, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = numbers.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="type">int</span> sum = numbers[left] + numbers[right];</span><br><span class="line">            <span class="keyword">if</span> (sum == target) &#123;</span><br><span class="line">                <span class="keyword">return</span> &#123;left + <span class="number">1</span>, right + <span class="number">1</span>&#125;;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (sum &gt; target) &#123;</span><br><span class="line">                right--;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="number">-1</span>, <span class="number">-1</span>&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="5-2-验证回文串"><a href="#5-2-验证回文串" class="headerlink" title="5.2 验证回文串"></a>5.2 验证回文串</h3><p><a href="https://leetcode.cn/problems/valid-palindrome/" title="125. 验证回文串 - 力扣（LeetCode）">125. 验证回文串 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">如果在将所有大写字符转换为小写字符、并移除所有非字母数字字符之后，短语正着读和反着读都一样。则可以认为该短语是一个 回文串 。</span><br><span class="line"></span><br><span class="line">字母和数字都属于字母数字字符。</span><br><span class="line"></span><br><span class="line">给你一个字符串 s，如果它是 回文串 ，返回 <span class="literal">true</span> ；否则，返回 <span class="literal">false</span> 。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入: s = <span class="string">&quot;A man, a plan, a canal: Panama&quot;</span></span><br><span class="line">输出：<span class="literal">true</span></span><br><span class="line">解释：<span class="string">&quot;amanaplanacanalpanama&quot;</span> 是回文串。</span><br></pre></td></tr></table></figure><p>双指针 - 对撞指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isPalindrome</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = s.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">while</span> (left &lt; right &amp;&amp; !(<span class="built_in">isdigit</span>(s[left]) || <span class="built_in">isalpha</span>(s[left]))) &#123;</span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">while</span> (left &lt; right &amp;&amp; !(<span class="built_in">isdigit</span>(s[right]) || <span class="built_in">isalpha</span>(s[right]))) &#123;</span><br><span class="line">                right--;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">tolower</span>(s[left]) == <span class="built_in">tolower</span>(s[right])) &#123;</span><br><span class="line">                left++;</span><br><span class="line">                right--;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="5-3-盛最多水的容器"><a href="#5-3-盛最多水的容器" class="headerlink" title="5.3 盛最多水的容器"></a>5.3 盛最多水的容器</h3><p><a href="https://leetcode.cn/problems/container-with-most-water/description/" title="11. 盛最多水的容器 - 力扣（LeetCode）">11. 盛最多水的容器 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, <span class="number">0</span>) 和 (i, height[i]) 。</span><br><span class="line"></span><br><span class="line">找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。</span><br><span class="line"></span><br><span class="line">返回容器可以储存的最大水量。</span><br><span class="line"></span><br><span class="line">输入：[<span class="number">1</span>,<span class="number">8</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">8</span>,<span class="number">3</span>,<span class="number">7</span>]</span><br><span class="line">输出：<span class="number">49</span> </span><br><span class="line">解释：图中垂直线代表输入数组 [<span class="number">1</span>,<span class="number">8</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">8</span>,<span class="number">3</span>,<span class="number">7</span>]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 <span class="number">49</span>。</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="image/image_wPAjw49QpT.png" alt=""></p><p>从示例中可以看出，如果确定好左右两端的直线，容纳的水量是由「左右两端直线中较低直线的高度 * 两端直线之间的距离」所决定的。所以我们应该使得「」，这样才能使盛水面积尽可能的大。</p><p>可以使用对撞指针求解。移动较低直线所在的指针位置，从而得到不同的高度和面积，最终获取其中最大的面积。具体做法如下：</p><ol><li>使用两个指针 $left$，$right$。$left$ 指向数组开始位置，$right$ 指向数组结束位置。</li><li>计算 $left$ 和 $right$ 所构成的面积值，同时维护更新最大面积值。</li><li>判断 $left$ 和 $right$ 的高度值大小。<ol><li>如果 $left$ 指向的直线高度比较低，则将 $left$ 指针右移。</li><li>如果 $right$ 指向的直线高度比较低，则将 $right$ 指针左移。</li></ol></li><li>如果遇到 $left == right$，跳出循环，最后返回最大的面积。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1.枚举： left bar x, right bar y, (x-y)*height_diff</span></span><br><span class="line">    <span class="comment">// 超出时间限制</span></span><br><span class="line">    <span class="comment">// O(n^2)</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">maxArea1</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; height)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> max_water = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; height.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = i + <span class="number">1</span>; j &lt; height.<span class="built_in">size</span>(); j++) &#123;</span><br><span class="line">                <span class="type">int</span> tmp = std::<span class="built_in">min</span>(height[i], height[j]) * (j - i);</span><br><span class="line">                max_water = std::<span class="built_in">max</span>(max_water, tmp);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_water;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.双指针</span></span><br><span class="line">    <span class="comment">// left和right两个指针，那么bar的高度小，那个往里面移动</span></span><br><span class="line">    <span class="comment">// O(n)</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">maxArea</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; height)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = height.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> max_water = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="type">int</span> tmp = std::<span class="built_in">min</span>(height[left], height[right]) * (right - left);</span><br><span class="line">            max_water = std::<span class="built_in">max</span>(max_water, tmp);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (height[left] &lt; height[right]) &#123;</span><br><span class="line">                left++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                right--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_water;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="5-4-删除有序数组中的重复项"><a href="#5-4-删除有序数组中的重复项" class="headerlink" title="5.4 删除有序数组中的重复项"></a>5.4 删除有序数组中的重复项</h3><p><a href="https://leetcode.cn/problems/remove-duplicates-from-sorted-array/description/" title="26. 删除有序数组中的重复项 - 力扣（LeetCode）">26. 删除有序数组中的重复项 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">给你一个 非严格递增排列 的数组 nums ，请你 原地 删除重复出现的元素，使每个元素 只出现一次 ，返回删除后数组的新长度。元素的 相对顺序 应该保持 一致 。然后返回 nums 中唯一元素的个数。</span><br><span class="line"></span><br><span class="line">考虑 nums 的唯一元素的数量为 k ，你需要做以下事情确保你的题解可以被通过：</span><br><span class="line"></span><br><span class="line">更改数组 nums ，使 nums 的前 k 个元素包含唯一元素，并按照它们最初在 nums 中出现的顺序排列。nums 的其余元素与 nums 的大小不重要。</span><br><span class="line">返回 k 。</span><br><span class="line">判题标准:</span><br><span class="line"></span><br><span class="line">系统会用下面的代码来测试你的题解:</span><br><span class="line"></span><br><span class="line"><span class="type">int</span>[] nums = [...]; <span class="comment">// 输入数组</span></span><br><span class="line"><span class="type">int</span>[] expectedNums = [...]; <span class="comment">// 长度正确的期望答案</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> k = <span class="built_in">removeDuplicates</span>(nums); <span class="comment">// 调用</span></span><br><span class="line"></span><br><span class="line">assert k == expectedNums.length;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; k; i++) &#123;</span><br><span class="line">    assert nums[i] == expectedNums[i];</span><br><span class="line">&#125;</span><br><span class="line">如果所有断言都通过，那么您的题解将被 通过。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：nums = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">输出：<span class="number">2</span>, nums = [<span class="number">1</span>,<span class="number">2</span>,_]</span><br><span class="line">解释：函数应该返回新的长度 <span class="number">2</span> ，并且原数组 nums 的前两个元素被修改为 <span class="number">1</span>, <span class="number">2</span> 。不需要考虑数组中超出新长度后面的元素。</span><br></pre></td></tr></table></figure><p>快慢指针</p><p>因为数组是有序的，那么重复的元素一定会相邻。</p><p>删除重复元素，实际上就是将不重复的元素移到数组左侧。考虑使用双指针。具体算法如下：</p><ol><li>定义两个快慢指针 $slow$，$fast$。其中 $slow$ 指向去除重复元素后的数组的末尾位置。$fast$ 指向当前元素。</li><li>令 $slow$ 在后， $fast$ 在前。令 $slow = 0$，$fast = 1$。</li><li>比较 $slow$ 位置上元素值和 $fast$ 位置上元素值是否相等。<ul><li>如果不相等，则将 $slow$ 右移一位，将 $fast$ 指向位置的元素复制到 $slow$ 位置上。</li></ul></li><li>将 $fast$ 右移 $1$ 位。</li><li>重复上述 $3 \sim 4$ 步，直到 $fast$ 等于数组长度。</li><li>返回 $slow + 1$ 即为新数组长度。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">removeDuplicates</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.<span class="built_in">size</span>() &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> nums.<span class="built_in">size</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> slow = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> fast = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (fast &lt; nums.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[slow] != nums[fast]) &#123;</span><br><span class="line">                slow++;</span><br><span class="line">                nums[slow] = nums[fast];</span><br><span class="line">            &#125;</span><br><span class="line">            fast++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> slow + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="5-5-两个数组的交集"><a href="#5-5-两个数组的交集" class="headerlink" title="5.5 两个数组的交集"></a>5.5 两个数组的交集</h3><p><a href="https://leetcode.cn/problems/intersection-of-two-arrays/description/" title="349. 两个数组的交集 - 力扣（LeetCode）">349. 两个数组的交集 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">给定两个数组 nums1 和 nums2 ，返回 它们的交集 。输出结果中的每个元素一定是 唯一 的。我们可以 不考虑输出结果的顺序 。</span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：nums1 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], nums2 = [<span class="number">2</span>,<span class="number">2</span>]</span><br><span class="line">输出：[<span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>分离双指针</p><ol><li>对数组 $nums1$、$nums2$ 先排序。</li><li>使用两个指针 $left\underline{}1$、$left\underline{}2$。$left\underline{}1$ 指向第一个数组的第一个元素，即：$left\underline{}1 = 0$，$left\underline{}2$ 指向第二个数组的第一个元素，即：$left\underline{}2 = 0$。</li><li>如果 $nums1[left\underline{}1] == nums2[left\underline{}2]$，则将其加入答案数组（注意去重），并将 $left\underline{}1$ 和 $left\underline{}2$ 右移。</li><li>如果 $nums1[left\underline{}1] &lt; nums2[left\underline{}2]$，则将 $left\underline{}1$ 右移。</li><li>如果 $nums1[left\underline{}1] &gt; nums2[left\underline{}2]$，则将 $left\underline{}2$ 右移。</li><li>最后返回答案数组。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">intersection</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums1, vector&lt;<span class="type">int</span>&gt;&amp; nums2)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 排序</span></span><br><span class="line">        std::<span class="built_in">sort</span>(nums1.<span class="built_in">begin</span>(), nums1.<span class="built_in">end</span>());</span><br><span class="line">        std::<span class="built_in">sort</span>(nums2.<span class="built_in">begin</span>(), nums2.<span class="built_in">end</span>());</span><br><span class="line"></span><br><span class="line">        std::vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> idx_1 = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> idx_2 = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (idx_1 &lt; nums1.<span class="built_in">size</span>() &amp;&amp; idx_2 &lt; nums2.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums1[idx_1] == nums2[idx_2]) &#123;</span><br><span class="line">                <span class="comment">// 保证加入元素的唯一性</span></span><br><span class="line">                <span class="keyword">if</span> (!ans.<span class="built_in">size</span>() || nums1[idx_1] != ans.<span class="built_in">back</span>()) &#123;</span><br><span class="line">                    ans.<span class="built_in">push_back</span>(nums1[idx_1]);</span><br><span class="line">                &#125;</span><br><span class="line">                idx_1++;</span><br><span class="line">                idx_2++;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums1[idx_1] &lt; nums2[idx_2]) &#123;</span><br><span class="line">                idx_1++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                idx_2++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="5-6-反转字符串"><a href="#5-6-反转字符串" class="headerlink" title="5.6 反转字符串"></a>5.6 反转字符串</h3><p><a href="https://leetcode.cn/problems/reverse-string/" title="344. 反转字符串 - 力扣（LeetCode）">344. 反转字符串 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">编写一个函数，其作用是将输入的字符串反转过来。输入字符串以字符数组 s 的形式给出。</span><br><span class="line"></span><br><span class="line">不要给另外的数组分配额外的空间，你必须原地修改输入数组、使用 <span class="built_in">O</span>(<span class="number">1</span>) 的额外空间解决这一问题。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：s = [<span class="string">&quot;h&quot;</span>,<span class="string">&quot;e&quot;</span>,<span class="string">&quot;l&quot;</span>,<span class="string">&quot;l&quot;</span>,<span class="string">&quot;o&quot;</span>]</span><br><span class="line">输出：[<span class="string">&quot;o&quot;</span>,<span class="string">&quot;l&quot;</span>,<span class="string">&quot;l&quot;</span>,<span class="string">&quot;e&quot;</span>,<span class="string">&quot;h&quot;</span>]</span><br></pre></td></tr></table></figure><p>对撞指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">reverseString</span><span class="params">(vector&lt;<span class="type">char</span>&gt;&amp; s)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = s.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="built_in">swap</span>(s[left], s[right]);</span><br><span class="line">            left++;</span><br><span class="line">            right--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="5-7-反转字符串中的元音字母"><a href="#5-7-反转字符串中的元音字母" class="headerlink" title="5.7 反转字符串中的元音字母"></a>5.7 反转字符串中的元音字母</h3><p><a href="https://leetcode.cn/problems/reverse-vowels-of-a-string/description/" title="345. 反转字符串中的元音字母 - 力扣（LeetCode）">345. 反转字符串中的元音字母 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">给你一个字符串 s ，仅反转字符串中的所有元音字母，并返回结果字符串。</span><br><span class="line"></span><br><span class="line">元音字母包括 <span class="string">&#x27;a&#x27;</span>、<span class="string">&#x27;e&#x27;</span>、<span class="string">&#x27;i&#x27;</span>、<span class="string">&#x27;o&#x27;</span>、<span class="string">&#x27;u&#x27;</span>，且可能以大小写两种形式出现不止一次。</span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：s = <span class="string">&quot;hello&quot;</span></span><br><span class="line">输出：<span class="string">&quot;holle&quot;</span></span><br></pre></td></tr></table></figure><p>对撞指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">string <span class="title">reverseVowels</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = s.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">while</span> (left &lt; right &amp;&amp;!<span class="built_in">isVowel</span>(s[left])) &#123;</span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">while</span> (left &lt; right &amp;&amp;!<span class="built_in">isVowel</span>(s[right])) &#123;</span><br><span class="line">                right--;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">swap</span>(s[left], s[right]);</span><br><span class="line">            left++;</span><br><span class="line">            right--;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> s;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isVowel</span><span class="params">(<span class="type">char</span> ch)</span> </span>&#123;</span><br><span class="line">        <span class="type">bool</span> lower = ch == <span class="string">&#x27;a&#x27;</span> || ch == <span class="string">&#x27;e&#x27;</span> || ch == <span class="string">&#x27;i&#x27;</span> || ch == <span class="string">&#x27;o&#x27;</span> || ch == <span class="string">&#x27;u&#x27;</span>;</span><br><span class="line">        <span class="type">bool</span> upper = ch == <span class="string">&#x27;A&#x27;</span> || ch == <span class="string">&#x27;E&#x27;</span> || ch == <span class="string">&#x27;I&#x27;</span> || ch == <span class="string">&#x27;O&#x27;</span> || ch == <span class="string">&#x27;U&#x27;</span>;</span><br><span class="line">        <span class="keyword">return</span> lower || upper;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="5-8-三数之和"><a href="#5-8-三数之和" class="headerlink" title="5.8 三数之和"></a>5.8 三数之和</h3><p><a href="https://leetcode.cn/problems/3sum/description/" title="15. 三数之和 - 力扣（LeetCode）">15. 三数之和 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数数组 nums ，判断是否存在三元组 [nums[i], nums[j], nums[k]] 满足 i != j、i != k 且 j != k ，同时还满足 nums[i] + nums[j] + nums[k] == <span class="number">0</span> 。请</span><br><span class="line"></span><br><span class="line">你返回所有和为 <span class="number">0</span> 且不重复的三元组。</span><br><span class="line"></span><br><span class="line">注意：答案中不可以包含重复的三元组。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：nums = [<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">-1</span>,<span class="number">-4</span>]</span><br><span class="line">输出：[[<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">2</span>],[<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>]]</span><br><span class="line">解释：</span><br><span class="line">nums[<span class="number">0</span>] + nums[<span class="number">1</span>] + nums[<span class="number">2</span>] = (<span class="number">-1</span>) + <span class="number">0</span> + <span class="number">1</span> = <span class="number">0</span> 。</span><br><span class="line">nums[<span class="number">1</span>] + nums[<span class="number">2</span>] + nums[<span class="number">4</span>] = <span class="number">0</span> + <span class="number">1</span> + (<span class="number">-1</span>) = <span class="number">0</span> 。</span><br><span class="line">nums[<span class="number">0</span>] + nums[<span class="number">3</span>] + nums[<span class="number">4</span>] = (<span class="number">-1</span>) + <span class="number">2</span> + (<span class="number">-1</span>) = <span class="number">0</span> 。</span><br><span class="line">不同的三元组是 [<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>] 和 [<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">2</span>] 。</span><br><span class="line">注意，输出的顺序和三元组的顺序并不重要。</span><br></pre></td></tr></table></figure><p>对撞指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">threeSum</span>(vector&lt;<span class="type">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        <span class="type">int</span> size = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (size &lt; <span class="number">3</span>)</span><br><span class="line">            <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; res;</span><br><span class="line">        <span class="comment">// 排序</span></span><br><span class="line">        std::<span class="built_in">sort</span>(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>());</span><br><span class="line">        <span class="comment">// 固定第一个数，转化为求两数之和</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果第一个数为正数，因为是递增的，后面你的数不可能为0了</span></span><br><span class="line">            <span class="keyword">if</span> (nums[i] &gt; <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">return</span> res;</span><br><span class="line">            <span class="comment">// 去重，如果被选过了，跳过</span></span><br><span class="line">            <span class="keyword">if</span> (i &gt; <span class="number">0</span> &amp;&amp; nums[i] == nums[i<span class="number">-1</span>])</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="comment">// 双指针在nums[i]后面的区间中寻找和为0-nums[i]的另外两个数</span></span><br><span class="line">            <span class="type">int</span> left = i + <span class="number">1</span>;</span><br><span class="line">            <span class="type">int</span> right = size - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">while</span>(left &lt; right)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 两数之和太大，右指针左移</span></span><br><span class="line">                <span class="keyword">if</span> (nums[left] + nums[right] &gt; -nums[i])</span><br><span class="line">                    right--;</span><br><span class="line">                <span class="comment">// 两数之和太小，左指针右移</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(nums[left] + nums[right] &lt; -nums[i])</span><br><span class="line">                    left++;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 找到一个和为零的三元组，添加到结果中，左右指针内缩，继续寻找</span></span><br><span class="line">                    res.<span class="built_in">push_back</span>(std::vector&lt;<span class="type">int</span>&gt;&#123;nums[i], nums[left], nums[right]&#125;);</span><br><span class="line">                    left++;</span><br><span class="line">                    right--;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 去重：第二个数和第三个数也不重复选取</span></span><br><span class="line">                    <span class="comment">// 例如：[-4,1,1,1,2,3,3,3], i=0, left=1, right=5</span></span><br><span class="line">                    <span class="keyword">while</span> (left &lt; right &amp;&amp; nums[left] == nums[left<span class="number">-1</span>])  left++;</span><br><span class="line">                    <span class="keyword">while</span> (left &lt; right &amp;&amp; nums[right] == nums[right+<span class="number">1</span>])    right--;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="5-9-移除元素"><a href="#5-9-移除元素" class="headerlink" title="5.9 移除元素"></a>5.9 移除元素</h3><p><a href="https://leetcode.cn/problems/remove-element/" title="27. 移除元素 - 力扣（LeetCode）">27. 移除元素 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">给你一个数组 nums 和一个值 val，你需要 原地 移除所有数值等于 val 的元素，并返回移除后数组的新长度。</span><br><span class="line"></span><br><span class="line">不要使用额外的数组空间，你必须仅使用 <span class="built_in">O</span>(<span class="number">1</span>) 额外空间并 原地 修改输入数组。</span><br><span class="line"></span><br><span class="line">元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">说明:</span><br><span class="line"></span><br><span class="line">为什么返回数值是整数，但输出的答案是数组呢?</span><br><span class="line"></span><br><span class="line">请注意，输入数组是以「引用」方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。</span><br><span class="line"></span><br><span class="line">你可以想象内部操作如下:</span><br><span class="line"></span><br><span class="line"><span class="comment">// nums 是以“引用”方式传递的。也就是说，不对实参作任何拷贝</span></span><br><span class="line"><span class="type">int</span> len = <span class="built_in">removeElement</span>(nums, val);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在函数里修改输入数组对于调用者是可见的。</span></span><br><span class="line"><span class="comment">// 根据你的函数返回的长度, 它会打印出数组中 该长度范围内 的所有元素。</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</span><br><span class="line">    <span class="built_in">print</span>(nums[i]);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：nums = [<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>], val = <span class="number">3</span></span><br><span class="line">输出：<span class="number">2</span>, nums = [<span class="number">2</span>,<span class="number">2</span>]</span><br><span class="line">解释：函数应该返回新的长度 <span class="number">2</span>, 并且 nums 中的前两个元素均为 <span class="number">2</span>。你不需要考虑数组中超出新长度后面的元素。例如，函数返回的新长度为 <span class="number">2</span> ，而 nums = [<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>] 或 nums = [<span class="number">2</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>]，也会被视作正确答案。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 一次遍历</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">removeElement1</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> len = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; nums.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[i] != val) &#123;</span><br><span class="line">                nums[len] = nums[i];</span><br><span class="line">                len++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> len;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 对撞指针</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">removeElement</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> start = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> end = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">while</span>(start &lt; end) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[start] == val) &#123;</span><br><span class="line">                nums[start] = nums[end - <span class="number">1</span>];</span><br><span class="line">                end--;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                start++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> start;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="5-10-删除有序数组中的重复项Ⅱ"><a href="#5-10-删除有序数组中的重复项Ⅱ" class="headerlink" title="5.10 删除有序数组中的重复项Ⅱ"></a>5.10 删除有序数组中的重复项Ⅱ</h3><p><a href="https://leetcode.cn/problems/remove-duplicates-from-sorted-array-ii/description/" title="80. 删除有序数组中的重复项 II - 力扣（LeetCode）">80. 删除有序数组中的重复项 II - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">给你一个有序数组 nums ，请你 原地 删除重复出现的元素，使得出现次数超过两次的元素只出现两次 ，返回删除后数组的新长度。</span><br><span class="line"></span><br><span class="line">不要使用额外的数组空间，你必须在 原地 修改输入数组 并在使用 <span class="built_in">O</span>(<span class="number">1</span>) 额外空间的条件下完成。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">说明：</span><br><span class="line"></span><br><span class="line">为什么返回数值是整数，但输出的答案是数组呢？</span><br><span class="line"></span><br><span class="line">请注意，输入数组是以「引用」方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。</span><br><span class="line"></span><br><span class="line">你可以想象内部操作如下:</span><br><span class="line"></span><br><span class="line"><span class="comment">// nums 是以“引用”方式传递的。也就是说，不对实参做任何拷贝</span></span><br><span class="line"><span class="type">int</span> len = <span class="built_in">removeDuplicates</span>(nums);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在函数里修改输入数组对于调用者是可见的。</span></span><br><span class="line"><span class="comment">// 根据你的函数返回的长度, 它会打印出数组中 该长度范围内 的所有元素。</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</span><br><span class="line">    <span class="built_in">print</span>(nums[i]);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：nums = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">输出：<span class="number">5</span>, nums = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">解释：函数应返回新长度 length = <span class="number">5</span>, 并且原数组的前五个元素被修改为 <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>。 不需要考虑数组中超出新长度后面的元素。</span><br></pre></td></tr></table></figure><p>快慢指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 一次遍历</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">removeDuplicates1</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.<span class="built_in">size</span>() &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> nums.<span class="built_in">size</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> idx = <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">2</span>; i &lt; nums.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[i]!= nums[idx - <span class="number">2</span>]) &#123;</span><br><span class="line">                nums[idx] = nums[i];</span><br><span class="line">                idx++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> idx;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 快慢指针</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">removeDuplicates</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.<span class="built_in">size</span>() &lt;= <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> nums.<span class="built_in">size</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> slow = <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> fast = <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">while</span> (fast &lt; nums.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[slow - <span class="number">2</span>] != nums[fast]) &#123;</span><br><span class="line">                nums[slow] = nums[fast];</span><br><span class="line">                slow++;</span><br><span class="line">            &#125;</span><br><span class="line">            fast++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> slow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="5-11-长按键入"><a href="#5-11-长按键入" class="headerlink" title="5.11 长按键入"></a>5.11 长按键入</h3><p><a href="https://leetcode.cn/problems/long-pressed-name/description/" title="925. 长按键入 - 力扣（LeetCode）">925. 长按键入 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">你的朋友正在使用键盘输入他的名字 name。偶尔，在键入字符 c 时，按键可能会被长按，而字符可能被输入 <span class="number">1</span> 次或多次。</span><br><span class="line"></span><br><span class="line">你将会检查键盘输入的字符 typed。如果它对应的可能是你的朋友的名字（其中一些字符可能被长按），那么就返回 True。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line"></span><br><span class="line">输入：name = <span class="string">&quot;alex&quot;</span>, typed = <span class="string">&quot;aaleex&quot;</span></span><br><span class="line">输出：<span class="literal">true</span></span><br><span class="line">解释：<span class="string">&#x27;alex&#x27;</span> 中的 <span class="string">&#x27;a&#x27;</span> 和 <span class="string">&#x27;e&#x27;</span> 被长按。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>分离双指针</p><p>在typed中匹配name，同时考虑字符重复问题，以及不匹配问题。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 分离双指针</span></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isLongPressedName</span><span class="params">(string name, string typed)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> idx_name = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> idx_typed = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (idx_name &lt; name.<span class="built_in">length</span>() &amp;&amp; idx_typed &lt; typed.<span class="built_in">length</span>()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (name[idx_name] == typed[idx_typed]) &#123;</span><br><span class="line">                idx_name++;</span><br><span class="line">                idx_typed++;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (typed[idx_typed] == typed[idx_typed - <span class="number">1</span>]) &#123;</span><br><span class="line">                <span class="comment">// typed出现重复元素，后移</span></span><br><span class="line">                idx_typed++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// typed出现不匹配元素，多余的，直接返回false</span></span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 过滤type末尾的元素</span></span><br><span class="line">        <span class="keyword">while</span> ((<span class="number">0</span> &lt; idx_typed &amp;&amp; idx_typed &lt; typed.<span class="built_in">length</span>())</span><br><span class="line">              || (typed[idx_typed] == typed[idx_typed - <span class="number">1</span>])) &#123;</span><br><span class="line">            idx_typed++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (idx_name == name.<span class="built_in">length</span>() &amp;&amp; idx_typed == typed.<span class="built_in">length</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 ZeRO</title>
      <link href="/paper_reading/4.1.ZeRO/"/>
      <url>/paper_reading/4.1.ZeRO/</url>
      
        <content type="html"><![CDATA[<p>ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</p><ul><li>论文链接：<a href="https://arxiv.org/abs/1910.02054" title="ZeRO: Memory Optimizations Toward Training Trillion Parameter Models">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></li><li>代码链接： <a href="https://github.com/microsoft/DeepSpeed" title="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></li><li>李沐讲解：<a href="https://www.bilibili.com/video/BV1tY411g7ZT" title="https://www.bilibili.com/video/BV1tY411g7ZT">https://www.bilibili.com/video/BV1tY411g7ZT</a></li></ul><h2 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1.摘要"></a>1.摘要</h2><p>ZeRO 是一种用于大规模深度学习模型的内存优化解决方案，通过消除数据和模型并行训练中的内存冗余，同时保持了低通信量和高计算粒度。</p><p>实现了ZeRO，可以训练超过100B参数的模型，在400个GPUs上实现超线性加速。</p><p><strong>模型训练的挑战</strong>：传统的 DP 方法不能减少每个设备的内存使用量，导致在当时 32G 显存的 GPU 上无法训练超过 14 亿个参数的模型</p><p><strong>现有的方法</strong>：流水线并行，模型并行， CPU offloading，在功能性，内存使用，<strong>计算通信效率之间做出了取舍</strong></p><p>作者提到<strong>模型并行（张量并行）最有潜力</strong>，需要将模型在垂直方向上进行切分，并将每个层的计算和参数在多个设备之间进行划分，当时的 11B 参数的 T5，8.3B 的 Megatron-LM 都使用了模型并行。但是<strong>这需要大量的层间通信</strong>，虽然在单个节点内表现良好，作者在两个 DGX-2 节点上使用 Megatron-LM 测试了 40B 的模型，每个 V100 GPU 的计算性能仅为 5 TFLOPS（仅为5%的理论峰值）。</p><p>作者发现内存消耗主要集中两个部分：</p><ol><li>模型状态：优化器，梯度，参数</li><li>residual states：激活值，buffer，内存碎片</li></ol><p>针对这两个部分提出了 ZeRO（Zero Redundancy Optimizer）</p><h2 id="2-引论（ZeRO总结）"><a href="#2-引论（ZeRO总结）" class="headerlink" title="2.引论（ZeRO总结）"></a>2.引论（ZeRO总结）</h2><h3 id="2-1优化模型状态内存"><a href="#2-1优化模型状态内存" class="headerlink" title="2.1优化模型状态内存"></a>2.1优化模型状态内存</h3><p><strong>数据并行 （DP）不足</strong>：不需要频繁地通信，但需要在每个进程中复制整个模型状态，浪费内存</p><p><strong>模型并行（MP）不足</strong>：对模型状态分区以高效率使用内存，但会导致过细的计算粒度，需要频繁通信</p><p>整体来看，上述方法需要在整个训练过程中静态地维护所有模型状态，但在训练时不总是需要所有的模型状态</p><p>改进：</p><p>提出了 ZeRO-DP，ZeRO-powered DP ，通过<strong>分区模型状态而不是复制来消除 DP 时的内存冗余，并使用动态通信调度策略优化计算通信效率</strong>，提出了三个优化阶段</p><ul><li><strong>优化器分区（Pos）</strong>：内存减少4倍，与DP具有相同的通信量</li><li><strong>梯度分区（Pos+g）</strong>：内存减少8倍，与DP具有相同的通信量</li><li><strong>模型参数分区（Pos+g+p）</strong>：内存减少量与 DP 的份数成线性关系。如在 64 个 GPU 上拆分将减少64倍内存。通信仅增加 50%</li></ul><p>图中 $Ψ$ 表示模型参数量，K 表示优化器状态的内存倍数，Nd 表示 DP 份数。在该示例中，假设基于 Adam 优化器的混合精度训练，$Ψ=7.5B，Nd=64，K=12$，其中的详细计算方法后面会介绍：</p><p><img src="image/image_RbDm1rZNtr.png" alt=""></p><p>作者在这里计算了通过上述优化在 1024 张 GPU 上训练万亿参数模型，使用 fp16，需要 16TB 空间存储优化器状态，梯度和参数，每张卡占用 16GB 显存</p><h3 id="2-2-优化-residual-states-内存"><a href="#2-2-优化-residual-states-内存" class="headerlink" title="2.2 优化 residual states 内存"></a>2.2 优化 residual states 内存</h3><p>为了优化激活值，buffer，内存碎片的占用，提出了** ZeRO-R**，有以下几个优化点</p><ul><li>使用对激活值的 checkpointing 来节省内存，还对激活值切片，根据需要将激活数据转移至 CPU 来优化激活值的内存占用</li><li>ZeRO-R 定义了适当的临时缓冲区大小，使内存和计算效率平衡</li><li>根据不同 tensor 的生命周期，主动管理内存，预防内存碎片化</li></ul><p>综上，<strong>ZeRO 主要是由 ZeRO-DP 和 ZeRO-R 两种优化结合</strong></p><h3 id="2-3-ZeRO-搭配模型并行（MP）"><a href="#2-3-ZeRO-搭配模型并行（MP）" class="headerlink" title="2.3 ZeRO 搭配模型并行（MP）"></a>2.3 ZeRO 搭配模型并行（MP）</h3><p>虽然使用 ZeRO 的策略后，MP 的策略变得不那么重要了，MP 在使用时还需要修改模型，相比 DP，有诸多限制</p><p>但在激活内存占用非常大时，这时 ZeRO-R 的策略也不能满足训练优化，可以搭配 MP 减少激活内存占用。并且，在结合 ZeRO 和 MP 时，理论上可以优化到 $Nd * Nm$（MP 份数） 倍的内存占用</p><p>在小模型的情况下，单独使用 DP 会导致 batch size 过大可能无法收敛，使用 MP 可以在加速的同时减小 batch size 到合适的值，帮助模型收敛</p><p>作者进行了一些上述工作的实验，得出如下结论</p><p><img src="image/image_N5ExZA9B-r.png" alt=""></p><blockquote><p>X轴不同大小的模型</p></blockquote><ul><li>模型大小：将 Megatron 与 MP 相结合，ZeRO-100B 可以高效地运行 1700 亿参数的模型，而单独使用 Megatron 等现有系统在 40 亿参数以上的规模上无法高效扩展。相比于SOTA，模型大小增加了8倍以上</li><li>训练速度：改进的内存效率提高了吞吐量和训练速度。通过在 400 台 Nvidia V100 GPU 上运行，ZeRO 可以以每个 GPU 38 TFlops 的速度训练 1000 亿参数的模型，总性能超过 15 Petaflops。与 SOTA 相比，对于相同模型大小，训练速度提高了 10 倍以上</li><li>可扩展性：当使用64-400个GPU时，性能呈现超线性加速，即当GPU数量加倍时，性能增加了一倍以上。这是 ZeRO-DP 的特性，它随着DP度数的增加减少了模型状态的内存占用，使得每个GPU能够承载更大的 <code>batch_size</code>，从而提高性能</li><li>大规模训练可行性：ZeRO-100B 使得 130亿参数的模型只需重构即可训练。相比之下，现有系统（如PyTorch Distributed Data Parallel）在 14 亿参数的规模上就会出现内存不足的情况</li><li>SOTA：ZeRO 支持拥有 170 亿参数的 Turing-NLG 模型，并取得了 SOTA 的成绩</li></ul><h2 id="3-相关工作"><a href="#3-相关工作" class="headerlink" title="3.相关工作"></a>3.相关工作</h2><h3 id="3-1-DP-TP-and-PP"><a href="#3-1-DP-TP-and-PP" class="headerlink" title="3.1 DP, TP and PP"></a>3.1 DP, TP and PP</h3><h4 id="（1）DP（数据并行）"><a href="#（1）DP（数据并行）" class="headerlink" title="（1）DP（数据并行）"></a>（1）DP（数据并行）</h4><p><strong>将每批输入的训练数据都在 DP 的 worker 之间进行平分</strong>。反向传播之后，需要进行通信来规约梯度，以保证优化器在各个 worker 上可以得到相同的更新。</p><p><strong>优势</strong>：计算效率高，工程上易于实现</p><p><strong>不足</strong>：会在所有 worker 之间复制模型和优化器，因此显存效率不高。且随着并行度的提高，每个 worker 执行的计算量是恒定的。 DP 可以在小规模上实现近乎线性扩展。但是，因为在 worker 之间规约梯度的通信成本跟模型大小成正相关，所以当模型很大或通信带宽很低时，计算效率会受到限制。</p><p><strong>改进</strong>：梯度累积可以增加 batch size，在本地使用 micro-batch 进行多次正向和反向传播，在进行优化器更新之前再规约梯度，从而分摊通信成本。</p><h4 id="（2）TP（模型并行）"><a href="#（2）TP（模型并行）" class="headerlink" title="（2）TP（模型并行）"></a>（2）TP（模型并行）</h4><p><strong>在多个 worker 之间划分模型的各个层。</strong> 模型并行的计算和通信因模型结构而异，因此需要很大的工作量来实现。DeepSpeed 利用了 Megatron-LM 来构建基于 Transformer的大规模模型并行语言模型。</p><p><strong>优势</strong>：会根据 worker 数量成比例地减少显存使用，这是这三种并行模式中显存效率最高的。且可以通过在模型并行 worker 之间划分激活显存，减少显存占用。</p><p><strong>不足</strong>：每次前向和反向传播中都需要额外通信来传递激活，模型并行的计算效率很低。模型并行需要高通信带宽，并且不能很好地扩展到通信带宽受限的单个节点之外。此外，每个模型并行worker都会减少每个通信阶段之间执行的计算量，从而影响计算效率。</p><h4 id="（3）PP（流水线并行）"><a href="#（3）PP（流水线并行）" class="headerlink" title="（3）PP（流水线并行）"></a>（3）PP（流水线并行）</h4><p><strong>将模型的各层划分为可以并行处理的阶段</strong>。当一个阶段完成一个 micro-batch 的正向传播时，激活内存将被发送给流水线的下一个阶段。类似地，当下一阶段完成反向传播时，将通过流水线把梯度反向传递回来。为了确保流水线的各个阶段能并行计算，必须同时计算多个 micro-batch</p><p>在 PipeDream 的实现中，通过保留多份旧参数来隐藏流水线泡沫，而且不会过多增加 batch size，本文通过梯度累积来实现并行的方法，在相同的 batch size下可以达到与传统 DP 和模型并行相同的训练效果。</p><p><strong>优势</strong>：流水线并行减少的显存与流水线的阶段数成正比，这使模型的大小可以随 worker 的数量线性扩展，并且通过 micro-batch 可以有效减少 bubble。此外，流水线的通信量只和阶段边界的各层的激活值大小成正比，所以流水线并行的通信量最低。</p><p><strong>不足</strong>：每个 worker 必须同时存储并运行的各个 micro-batch 的激活值，导致流水线第一阶段的激活内存与单个 mirco-batch 的总激活内存大致相同。不断增加流水线大小会减少每个流水线阶段的计算量，降低计算通信效率。流水线并行还对每个阶段的负载均衡有很高的要求。此外，由于水平拆分和 micro-batch，如tied-weight 和 batch-normalization 等功能难以实现。</p><h3 id="3-2非并行化方法节省内存"><a href="#3-2非并行化方法节省内存" class="headerlink" title="3.2非并行化方法节省内存"></a>3.2非并行化方法节省内存</h3><h4 id="（1）减少激活内存"><a href="#（1）减少激活内存" class="headerlink" title="（1）减少激活内存"></a>（1）减少激活内存</h4><p>作者引用了几篇文献举例如何优化激活内存</p><h5 id="压缩内存"><a href="#压缩内存" class="headerlink" title="压缩内存"></a>压缩内存</h5><p>论文主要研究了在深度神经网络（DNNs）的训练中，GPU主内存限制导致训练更深层次的网络时出现瓶颈的问题。研究发现，<strong>主要的内存占用问题来自于中间层的输出（特征图）</strong>。为了解决这个问题，论文提出了<strong>一种DNN层特定的优化框架，通过对特定层（如卷积、ReLU、池化）进行编码和解码，以显著降低GPU主内存的压力</strong>。其核心方法是<strong>在两个时间点之间存储特征图的编码表示，并在反向传播时解码，而在前向传播时使用完整的特征图</strong>。作者还介绍了名为Gist的系统，它采用两类层特定的编码方案（无损和有损），利用DNN训练中现有的数值冗余，显著减少了目标特征图的内存消耗。例如，通过充分利用从池化到ReLU层的反向传播的计算特性，可以将中间特征图的存储精简至每个值仅使用1位而不是32位。通过在一流的DNN框架（CNTK）中部署这些机制，Gist在5个最先进的图像分类DNN上将内存占用降低了最多2倍，平均降低了1.8倍，仅带来4%的性能开销。此外，论文还表明，进一步的软件（例如CuDNN）和硬件（例如动态分配）优化可以进一步降低内存占用，最多可减少4.1倍</p><h5 id="激活内存-checkpoint"><a href="#激活内存-checkpoint" class="headerlink" title="激活内存 checkpoint"></a>激活内存 checkpoint</h5><p>论文提出了一种系统性方法，旨在<strong>减少深度神经网络训练的内存消耗</strong>。主要关注点是<strong>减少存储中间结果（特征图）和梯度所需的内存成本</strong>，因为与许多常见的深度架构中的中间特征图相比，参数的大小相对较小。论文<strong>使用计算图分析进行自动原地操作和内存共享优化</strong>。论文还提出了一种新的方法来以计算为代价来换取内存，用于特征图训练n层网络，成本为$O(\sqrt n)$的内存，仅需双倍的前向传播计算成本。在极端情况下，可以使用仅O(logn)的内存来训练n层网络的特征图</p><p>作者介绍了他们开发的系统 Checkmate，该系统旨在解决张量重新生成调度的最佳化问题。Checkmate 可以在合理的时间内（不到一小时）使用现成的MILP（Mixed Integer Linear Programming）求解器或使用近似算法找到接近最优的调度方案。这些调度方案可以用于加速数百万次的训练迭代。除了减少训练成本外，Checkmate还可以使现实世界中的网络能够使用比以前大约多5.1倍的输入尺寸进行训练</p><h5 id="实时内存管理"><a href="#实时内存管理" class="headerlink" title="实时内存管理"></a>实时内存管理</h5><p>作者提出了SuperNeurons 动态的GPU内存调度运行时策略，包括三种内存优化技术：<strong>Liveness Analysis（存活性分析）、Unified Tensor Pool（统一张量池）和 Cost-Aware Recomputation（成本感知的重计算）</strong>。这些技术共同有效地将网络整体的内存峰值使用量降低到各层中的最大内存使用量。此外，SuperNeurons还解决了这些内存节省技术中的性能问题。鉴于有限的GPU DRAM，SuperNeurons不仅提供了训练所需的内存，还动态分配内存用于卷积工作空间，以实现高性能的训练。SuperNeurons 能够在一个12GB的 K40c GPU上训练包含104个基本网络层的 ResNet2500</p><p>本文的 ZeRO-R 同时使用了压缩内存和 checkpoint 技术</p><h4 id="（2）CPU-Offload"><a href="#（2）CPU-Offload" class="headerlink" title="（2）CPU Offload"></a>（2）CPU Offload</h4><p>这里引用了其它两种内存卸载方法</p><h5 id="优化执行算法"><a href="#优化执行算法" class="headerlink" title="优化执行算法"></a>优化执行算法</h5><p>论文介绍了一种名为“<code>L2L</code>”的新型执行算法，它<strong>采用一种中继式执行技术</strong>。<strong>在任何给定时刻，设备内存主要仅填充了正在执行的层的占用空间</strong>。模型驻留在DRAM内存中，连接到CPU或FPGA，作为一种称为“eager param-server（EPS）”的实体。为了解决将参数传递到EPS的带宽问题，该模型采用了一种逐层执行的方式，而不是传统的小批量执行整个模型的方法。这意味着模型以多个微批次的方式执行，而不是传统的小批量方式。这种方法可以显著减少内存占用，并提高吞吐量</p><h5 id="虚拟内存-Offload"><a href="#虚拟内存-Offload" class="headerlink" title="虚拟内存 Offload"></a>虚拟内存 Offload</h5><p>论文提出了一种名为**<code>vDNN</code><strong>**的虚拟化DNN</strong>，主要思想是<em>*保守地分配GPU内存，以立即使用给定层的计算，从而大幅减少最大和平均内存使用 \</em>*。vDNN 利用分配的数据结构的数据依赖性，特别是占内存使用量大部分的中间特征映射，在 GPU 和 CPU 内存之间释放或移动这些中间数据。具体来说，如果没有进一步的重用，就积极地从 GPU 内存中释放这些特征映射，如果存在进一步的重用，但不是立即需要，就从CPU内存卸载/预取。通过 DNN 络的层间内存访问和重用模式，内存管理器智能地将正常的 DNN 计算与卸载/预取/释放 操作重叠，几乎没有性能损失</p><p>上面的文献利用计算节点的异构性，分别通过算法设计或虚拟化内存将模型状态转移到CPU内存。但是这导致有50%的时间被浪费在GPU-CPU-GPU传输。ZeRO的不同之处在于，它显著降低了内存消耗，而无需将模型状态存储到CPU内存中。在极少数情况下，ZeRO-R可能只针对非常大的模型才 offload 激活 checkpoint，以提高性能</p><h4 id="（3）Memory-Efficient-Optimizer"><a href="#（3）Memory-Efficient-Optimizer" class="headerlink" title="（3）Memory Efficient Optimizer"></a>（3）Memory Efficient Optimizer</h4><p>自适应梯度优化器如 Adagrad 和 Adam 方法在 NLP 任务中取得了不错的性能，然而这些方法为每个参数维护了二阶统计信息，因此引入了显著的内存开销，限制了可使用的模型大小以及每个小批量中的示例数量。下面是过去提出的优化内存的文献</p><h5 id="自适应学习率"><a href="#自适应学习率" class="headerlink" title="自适应学习率"></a>自适应学习率</h5><p>论文提出了一种名为 Adafactor 的自适应学习率优化算法，为了减少内存占用，仅维护每行和每列的移动平均和二阶矩的和，然后基于这些和来估计每个参数的二阶矩。这种方法在实验中表现出与传统方法相似的结果。最后，作者还提出了根据参数本身的规模来调整参数更新的方法</p><h5 id="自适应优化器"><a href="#自适应优化器" class="headerlink" title="自适应优化器"></a>自适应优化器</h5><p>作者提出了一种自适应优化算法 SM3，该算法通过使用参数的覆盖集合来减少内存需求，其中每个集合都对应一个变量，通过维护一组覆盖集合，并对每个集合的最大方差进行求和，确定每个梯度条目的学习率。通过减少内存需求实现了两倍的速度提升</p><p>上面的文献通过获取模型参数或梯度的粗粒度统计数据来减少自适应优化方法的内存消耗，这可能会对模型收敛保证产生影响。ZeRO与这些工作不同，它的优化不会改变模型优化方法或影响模型收敛，但会有效地减少每个设备的优化器状态和梯度的内存占用</p><h3 id="3-3-训练优化器"><a href="#3-3-训练优化器" class="headerlink" title="3.3 训练优化器"></a>3.3 训练优化器</h3><p>对于大型模型，自适应优化（Adaptive）方法对于达到 SOTA 性能和精度至关重要。与 SGD 相比，它以显著的内存占用为代价，维护每个模型参数和梯度的细粒度一阶和二阶统计信息。ZeRO可以将这些优化器的内存占用减少几个数量级，使这些复杂的优化方法在训练大模型时非常有效。它还允许开发和使用更复杂、内存消耗更大、收敛性更好的优化器</p><h2 id="4-优化内存占用"><a href="#4-优化内存占用" class="headerlink" title="4.优化内存占用"></a>4.优化内存占用</h2><p>前面提到内存消耗主要集中在</p><ul><li><strong>模型状态</strong>：优化器，梯度，参数</li><li><strong>Residual states</strong>：激活值，buffer，内存碎片</li></ul><p>下面展开讨论为何会这样</p><h3 id="4-1-模型状态：优化器状态，梯度与参数"><a href="#4-1-模型状态：优化器状态，梯度与参数" class="headerlink" title="4.1 模型状态：优化器状态，梯度与参数"></a>4.1 模型状态：优化器状态，梯度与参数</h3><p>在使用adam优化器训练时，Adam<strong>使用指数移动平均来计算梯度，这需要保存梯度的拷贝</strong>，以稳定更新参数。Adam还使用了自适应学习率机制，会为每个参数自动调整学习率。学习率的自适应性依赖于每个参数的梯度方差。<strong>为了计算梯度方差，就需要保存梯度的平方的移动平均值</strong>，以便在参数更新时更好地适应局部梯度的特性。</p><p>在使用<strong>混合精度训练</strong>时，**将参数和梯度存储为 <strong><strong><code>fp16</code></strong></strong>，并在前反向传播时都使用<code>fp16</code><strong>更新，但是</strong>为了反向传播结束后保证计算的精确，需要保存参数和优化器状态的<code>fp32</code>**<strong>副本</strong>，以Adam优化器为例，使用Adam对具有$Ψ$个参数的模型进行混合精度训练需要足够的内存来存储参数和梯度的<code>fp16</code>副本，内存需求分别为 $2Ψ$ 和 $2Ψ$ 字节（参数和梯度）。此外，还需要存储优化器状态：参数、动量和方差的 <code>fp32</code>副本，内存需求分别为 $4Ψ$、$4Ψ$ 和 $4Ψ$ 字节（一个w和两个状态）。文中使用 K 来表示优化器状态的内存乘数，即存储它们所需的额外内存为 $KΨ$ 字节。混合精度 Adam 的 K 值为 12。这导致了 $ 2+2+12=16 Ψ  $字节的内存需求。对于像 GPT-2 这样有15亿参数的模型，至少需要24GB的内存，远远高于 <code>3GB</code> 内存来存储 <code>fp16</code> 参数的需求</p><h3 id="4-2-Residual-内存消耗"><a href="#4-2-Residual-内存消耗" class="headerlink" title="4.2 Residual 内存消耗"></a>4.2 Residual 内存消耗</h3><p>在训练过程中，使用的激活函数会占用大量内存。以 GPT-2 模型为例，当序列长度为 1K，<code>batch_size</code>为 32时，1.5B 参数的模型需要大约 60GB 的内存.</p><p>计算公式：$激活值内存 = Transformer 层数 × hidden_dim × batch_size × seq_len × Transformer 层数$</p><p>使用激活值 checkpoint 方法可以减少激活函数内存的消耗，会增加 33% 的 recompute 开销，但可以将激活函数内存消耗降低到约 8GB 但对于更大的模型，激活函数的内存消耗仍然可能非常大。如一个拥有 1000 亿参数的 GPT-like 模型，在 <code>batch_size</code>为32 时，即使使用了激活值 checkpoint 仍需60GB的内存。此外，对于大型模型，用于存储中间结果的临时 buffer 也会占用相当大的内存。例如，对梯度 <code>all_reduce</code>或梯度计算时会将所有梯度融合到一个 flattened buffer 中，尽管梯度可以以 <code>fp16</code>存储，但 buffer 可能还是 <code>fp32</code>。对于一个具有 15 亿参数的模型，一个 flattened fp32 buffer 要占用6GB的内存</p><p>此外，<strong>内存碎片的问题也要注意，在极端情况下，内存碎片可浪费 30% 的内存</strong></p><h2 id="5-ZeRO"><a href="#5-ZeRO" class="headerlink" title="5.ZeRO"></a>5.ZeRO</h2><p>ZeRO 提出了两组优化：</p><ul><li><strong>ZeRO-DP</strong>：优化模型状态内存消耗</li><li><strong>ZeRO-R</strong>：优化 Residual 内存消耗</li></ul><h3 id="5-1-ZeRO-DP"><a href="#5-1-ZeRO-DP" class="headerlink" title="5.1 ZeRO-DP"></a>5.1 ZeRO-DP</h3><p><strong>DP</strong>：优点：计算粒度高，通信低；不足： DP 进程之间冗余存储</p><p><strong>MP</strong>：优点：通过分区模型利用内存；不足：计算粒度降低</p><p><strong>ZeRO-DP</strong> <strong>通过分区模型状态并使用动态的通信调度，同时有 DP 和 MP 的优点</strong></p><p>假设两块卡做数据并行，一个层</p><p><img src="image/image_Z6NIHTGklt.png" alt=""></p><p><strong>梯度（FP16）</strong>：两个卡维护的梯度不同，不同发送给对方，来得到全局的梯度</p><p><strong>状态（FP32）</strong>：得到梯度后，将各个部分梯度累加，得到中间状态；再将中间状态发送到权重参数W（FP16）上，同一块卡直接复制，不同卡进行发送</p><h4 id="（1）Pos-：-Optimizer-State-Partitioning"><a href="#（1）Pos-：-Optimizer-State-Partitioning" class="headerlink" title="（1）Pos ： Optimizer State Partitioning"></a>（1）Pos ： Optimizer State Partitioning</h4><p>在DP中，通过将优化器的状态分成N个分区，使得每个DP进程只更新对应的分区的优化器状态，也就是1/N的总优化器状态参数量</p><h4 id="（2）Pg-：Gradient-Partitioning"><a href="#（2）Pg-：Gradient-Partitioning" class="headerlink" title="（2）Pg ：Gradient Partitioning"></a>（2）Pg ：Gradient Partitioning</h4><p>梯度的计算被分为不同的分区，每个 DP 进程只处理和更新对应参数分区的梯度。文中还采用了一种 ucketization  策略，将同一参数分区的梯度进行分组，并一次性对整个组进行归约操作。类似于NVIDIA的 AMP 优化器中将全局梯度计算进行 bucketization 以重叠计算通信。通过在最后一个分区进行 all-reduce，以减少内存占用，实现计算通信重叠</p><h4 id="（3）Pp-：-Parameter-Partitioning"><a href="#（3）Pp-：-Parameter-Partitioning" class="headerlink" title="（3）Pp ： Parameter Partitioning"></a>（3）Pp ： Parameter Partitioning</h4><p>参数分区是在 DP 训练中减少内存消耗的一种方式。每个进程只存储与其分区相对应的参数，当需要使用到其他分区的参数进行前向和反向传播时，通过 broadcast 从相应的 DP 进程接收这些参数</p><h4 id="（4）Implication-on-Model-Size"><a href="#（4）Implication-on-Model-Size" class="headerlink" title="（4）Implication on Model Size"></a>（4）Implication on Model Size</h4><p>在 1024 DP 的情况下，搭配 Pos+g+p，可以实现 1.5 万亿参数的训练，如果只使用 DP 训练，仅能训练 1.5 Billion 参数量</p><h4 id="（5）内存消耗对比"><a href="#（5）内存消耗对比" class="headerlink" title="（5）内存消耗对比"></a>（5）内存消耗对比</h4><p><img src="image/image_9sBUSXq3Bt.png" alt=""></p><h4 id="（6）通信量分析"><a href="#（6）通信量分析" class="headerlink" title="（6）通信量分析"></a>（6）通信量分析</h4><h5 id="1）DP-通信量"><a href="#1）DP-通信量" class="headerlink" title="1）DP 通信量"></a>1）DP 通信量</h5><p>在每次反向后执行 all-reduce 平均，这种 all-reduce 会在大模型上会完全依赖通信带宽，现在的 all-reduce 通过流水线执行 reduce-scatter 再 all-gather 达到 all-reduce 的效果，会有两倍的数据通信量</p><h5 id="2）ZeRO-DP-通信量"><a href="#2）ZeRO-DP-通信量" class="headerlink" title="2）ZeRO-DP 通信量"></a>2）ZeRO-DP 通信量</h5><p><strong>Pos+g ： Zero-2</strong></p><p>因为每个进程只保存其分区的梯度，对梯度进程进行 scatter-reduce，再执行 all-gather，通信量与 DP 相同，为 $2Ψ$</p><p><strong>Pos+g+p ： Zero-3</strong></p><p>通过参数分区，每个进程只存储更新的参数。在计算前向传播结果前，每个分区的进程将权重 braodcast 到所有进程，在前向传播时，通过流水线执行 all-gather 接受其他分区的参数，以减少内存占用，前向传播后，丢弃权重。在反向传播时需要再次 all-gather，因此，总通信量为$  (Ψ * N) / N = Ψ $</p><p>综上，<strong>总通信量为 3Ψ，为 DP 的 1.5 倍</strong></p><h3 id="5-2-ZeRO-R"><a href="#5-2-ZeRO-R" class="headerlink" title="5.2 ZeRO-R"></a>5.2 ZeRO-R</h3><p>将使用的内存分为两类：</p><ul><li><strong>长期存在</strong>：前向传播时的激活值 checkpoint，反向传播时的参数梯度</li><li><strong>短期存在</strong>：前向传播时的 recompute，反向传播时的激活值梯度</li></ul><p>ZeRO-R <strong>通过将激活值 checkpoint 和 梯度 移动到预先分配的连续 buffer 中，进行实时内存碎片整理，还减少了查找空闲连续内存的时间</strong>（带宽换时间）</p><p>ZeRO-R主要用于模型并行，假设有一个层，又对应的输入，假设有2块卡。</p><p><img src="image/image_3ev3Uc-yLA.png" alt=""></p><h4 id="（1）Pa-：Partitioned-Activation-Checkpointing"><a href="#（1）Pa-：Partitioned-Activation-Checkpointing" class="headerlink" title="（1）Pa ：Partitioned Activation Checkpointing"></a>（1）Pa ：Partitioned Activation Checkpointing</h4><p>通过激活值 checkpoint 分区，消除了 MP 中的内存冗余，只有在计算中需要使用激活时，才会将激活值复制</p><p>在模型中一层完成前向完成时，会将激活值分区到所有并行进程上，如果在反向传播过程中被使用时，则使用 all-gather 重新创建激活值的 copy</p><h4 id="（2）-C-B-：Constant-Size-Buffers"><a href="#（2）-C-B-：Constant-Size-Buffers" class="headerlink" title="（2）$C_B$：Constant Size Buffers"></a>（2）$C_B$：Constant Size Buffers</h4><p>因为做了切片，需要不断通信，当机器数变多之后，一个层的切片数据就变得很多，每一次发出去的数据可能很小，在带宽很大的情况下，这样发是不划算的。</p><p>使用固定大小的Buffer，等到填满后，再发出去。</p><p>通过保持足够大的常量 buffer ，在计算之前将所有参数融合到这个单独的 buffer 中，可以加速内存读写效率.</p><h4 id="（3）-M-D-Memory-Defragmentation"><a href="#（3）-M-D-Memory-Defragmentation" class="headerlink" title="（3）$M_D$ : Memory Defragmentation"></a>（3）$M_D$ : Memory Defragmentation</h4><p>前向传播过程中通过激活值 checkpoint 只保留了部分激活值，其余的激活值会被丢弃，因为它们在后向传播时可以重新计算。同样，在后向传播过程中，参数梯度是长期存在的，而激活梯度和其他用于计算参数梯度的缓冲区是短期存在的。这种长期和短期内存的交织导致了内存碎片化的问题</p><p>文中提出了 In-place Activation Reuse 的方法，在反向传播过程中可以重复使用激活值的内存，而无需每次都重新分配内存。将不再需要的激活值内存标记为可重用，并在下一次需要相同大小内存的地方重用它们。减少了内存分配和释放的次数，减少了碎片化.</p><h4 id="（4）通信量分析"><a href="#（4）通信量分析" class="headerlink" title="（4）通信量分析"></a>（4）通信量分析</h4><p>下面通过分析分区激活值 checkpoint（Pa）与MP，DP通信量决定使用Pa还是Pa+cpu</p><p>Pa 的通信量权衡取决于模型大小、checkpoint 策略和 MP 策略。论文使用 Megatron-LM 实现的模型背景下进行分析。 在带有激活值 checkpoint 的 Megatron-LM 中，每个transformer在正向传播中执行两个大小为 $batch × seq_length × hidden_dim$ 的 all-reduce 操作用于正向传播时的重计算，另外两个 all-reduce 操作用于反向传播。每个块的总通信量为 $12 × seq length × hidden dim$，因为 all-reduce 的通信量为$  2 × message_size $。 当 ZeRO-R 对激活值 checkpoint 进行分区时，在每个激活值 checkpoint 上的反向传播的正向重新计算之前需要额外的 all-gather 操作。会检查每个transformer块的输入激活，需要一个 all-gather，因此，Pa 的通信量为 $seq_length ∗ hidden_dim$。因为 all-gather 的通信量为 <code>message_size</code>，计算 Pa 的总通信量小于 MP 原始通信量的 10%。当MP与DP结合使用时，Pa可用于将Pa通信量减少一个数量级，而模型并行通信量增加10%，并在 DP 通信成为性能瓶颈时显著提高效率。另外，Pa将激活内存消耗降低了 MP 并行度，从而允许按比例增加 <code>batch_size</code>。由于 Pa 导致 <code>batch_size</code> 增加一个数量级可能导致 DP 通信量减少一个数量级。 如果采用 Pa+cpu，分区激活值 checkpoint 将卸载到cpu，就不再需要激活内存了，与Pa相比，cpu内存之间增加了2倍的数据移动。在极端情况下，DP 通信量是主要瓶颈，因为即使使用Pa，<code>batch_size</code> 也很小，在小<code>batch_size</code>的情况下，只要 cpu 数据传输开销小于DP通信量开销，Pa+cpu 就可以通过增加 batch\\_size 来提高效率。在给定模型和硬件特性的情况下，可以利用上述分析来决定是否以及何时使用Pa还是Pa+cpu</p><h2 id="6-万亿参数训练"><a href="#6-万亿参数训练" class="headerlink" title="6.万亿参数训练"></a>6.万亿参数训练</h2><p>仅使用 DP，ZeRO能够在1024个GPU上容纳超过1万亿参数的模型。此外，如下表所示，当与 MP 结合使用时，每个DGX2节点内使用16路MP，跨节点使用64路DP，ZeRO能够在1024个GPU上运行超过1万亿参数的模型，但训练时长会超过一年，期待未来算力提升</p><p><img src="image/image_hOqphgTvhv.png" alt=""></p><h2 id="7-实验评估"><a href="#7-实验评估" class="headerlink" title="7.实验评估"></a>7.实验评估</h2><p>实施：基于 PyTorch 的 ZeRO-100B，包括Pos+g 和 ZeRO-R 中的全部优化点</p><p>硬件：由 400 个 V100 GPU（25个DGX-2节点）组成的集群，节点间通信带宽为800 Gbps</p><p>Baseline：没有MP的实验使用了 torch 的 DDP，MP 的实验使用 Megatron-LM 的 MP</p><p>ZeRO：没有MP的实验使用了 ZeRO-100B 中基于 ZeRO 的 DP 实现。MP 的实验将 ZeRO-100B 中的 ZeRO-powered DP 与 Megatron-LM 的 MP 相结合</p><p>模型：基于GPT-2的 transformer 模型，下表是参数配置</p><p><img src="image/image_mNjbLHGwaE.png" alt=""></p><h3 id="7-1-Speed-and-Model-size"><a href="#7-1-Speed-and-Model-size" class="headerlink" title="7.1 Speed and Model size"></a>7.1 Speed and Model size</h3><p>Baseline 的 Megatron MP 在模型规模增大时性能会快速下降，因为 MP 在 GPU 之间产生了高额通信量，而在超过单个节点以适应更大的模型时，每条链路（NVSwitch）的通信带宽从300GB/秒下降为12.5GB/秒，导致性能严重下降。对比之下，ZeRO-100B 会有 10 倍的训练速度提升</p><h3 id="7-2-Super-Linear-Scalability"><a href="#7-2-Super-Linear-Scalability" class="headerlink" title="7.2 Super-Linear Scalability"></a>7.2 Super-Linear Scalability</h3><p>如下图所示，使用ZeRO-100B可以实现超线性可扩展性，并通过增加 DP 并行度来提高每个GPU的吞吐量，从而适应更大的 batch_size</p><p><img src="image/image_1Qj1nuyQK6.png" alt=""></p><h3 id="7-3-Democratizing-Large-Model-Training"><a href="#7-3-Democratizing-Large-Model-Training" class="headerlink" title="7.3 Democratizing Large Model Training"></a>7.3 Democratizing Large Model Training</h3><p>下图使用128个GPU，ZeRO-100B 可以训练13B参数的模型，平均每个GPU吞吐量超过40 TFlops。相比之下，没有使用ZeRO，仅使用DP的最大可训练模型仅有 1.4B 参数，每个 GPU 的吞吐量不到 20 TFlops。此外，由于没有 MP 带来的通信开销，这些模型可以在具有较低端计算节点上进行训练，无需 NVLINK 或 NVSwitch 这种高速互联方式</p><p><img src="image/image_RB2o3B2eqt.png" alt=""></p><h3 id="7-4-Memory-and-Performance-Analysis"><a href="#7-4-Memory-and-Performance-Analysis" class="headerlink" title="7.4 Memory and Performance Analysis"></a>7.4 Memory and Performance Analysis</h3><p>作者讨论了不同优化方法对最大模型大小、内存消耗和性能的影响。作者将这些优化方法分为配置1到5（C1-C5），如下表所示，通过使用固定 <code>batch_size</code>和 MP 为16，观察启用不同ZeRO优化的可训练模型的最大尺寸</p><p><img src="image/image_Jr7EDtwCJA.png" alt=""></p><p>在最大模型尺寸方面，如 Figure 6 所示，通过使用 Pa 优化，模型大小从40B增加到了60B。而通过使用Pos+g优化，在C2的基础上，模型大小增加到了140B，这是因为与C2相比，该优化使模型状态的内存需求减半。使用C5进一步减少了激活内存，将分区激活值 checkpoint 转移到CPU内存，使模型大小增加到150B</p><p>对于每个训练迭代中 PyTorch 缓存的最大内存，如 Figure 7 所示，作者观察了40B和100B模型的情况。从C1到C2，缓存的内存大小如期减少。C2到C3的内存消耗差异取决于模型状态与激活内存的大小关系，当激活内存较大时，差异可能增加，当模型状态较大时，差异可能减小。值得注意的是，在40B模型中，从C4到C5时，缓存的内存大小没有减少，但在100B模型中有减少。这是因为100B的激活内存较大，减少不明显。作者指出，当我们处理非常大的模型时，Pa+cpu优化可用于适应更大的 <code>batch_size</code></p><p>对于不同优化设置的最佳性能，如 Figure 8 所示，性能提升与内存消耗的减少相对应。较低的内存消耗可以实现更大的 batch\\_size，从而提高性能。唯一的例外是60B参数模型在C4和C5之间的性能下降。尽管内存消耗较低，但C5会导致激活在CPU之间移动，这通常会导致性能下降，除非模型非常大以至于无法在没有C5的情况下运行，或者可以在没有C5的情况下运行的 <code>batch_size</code>很小（例如在 Figure 8 中具有170B参数的模型）。在训练过程中，Pa+cpu优化只在有益时才启用</p><p><img src="image/image_ajuHX6EIaH.png" alt=""></p><h3 id="7-5-Turing-NLG-the-SOTA-language-model-with-17B-parameters"><a href="#7-5-Turing-NLG-the-SOTA-language-model-with-17B-parameters" class="headerlink" title="7.5 Turing-NLG, the SOTA language model with 17B parameters"></a>7.5 Turing-NLG, the SOTA language model with 17B parameters</h3><p>下图展示了在300K次迭代中与之前最先进的Megatron-LM 8.3B参数模型的验证 Perplexity 对比，使用 ZeRO 训练得到的模型指标优于 Megatron-LM，此外 ZeRO100B 还实现了持续的 41.4 TFlops/GPU的吞吐量</p><p><img src="image/image_TfpquXtfIo.png" alt=""></p><h2 id="8-结论"><a href="#8-结论" class="headerlink" title="8.结论"></a>8.结论</h2><p><strong>通讯换内存</strong></p>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLMs </tag>
            
            <tag> PaperReading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLM 推理常见参数</title>
      <link href="/llms/llms_article/8.LLM%20%E6%8E%A8%E7%90%86%E5%B8%B8%E8%A7%81%E5%8F%82%E6%95%B0/"/>
      <url>/llms/llms_article/8.LLM%20%E6%8E%A8%E7%90%86%E5%B8%B8%E8%A7%81%E5%8F%82%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章参考：<a href="https://mp.weixin.qq.com/s?__biz=Mzk0ODU3MjcxNA==\&amp;mid=2247484444\&amp;idx=1\&amp;sn=da7767b359c5707a8a5c0096a5c9e48c\&amp;chksm=c364c359f4134a4f3b8321ab9cffa45deef6b3f453243d290db0fd9af643adaeb105762c6ba6\&amp;mpshare=1\&amp;scene=1\&amp;srcid=1208PVZ0tCkXwSJdQd0cLqyP\&amp;sharer_shareinfo=d9196be9eb87f381d27033be958a58c3\&amp;sharer_shareinfo_first=d9196be9eb87f381d27033be958a58c3#rd" title="LLM 推理常见参数解析 (qq.com)">LLM 推理常见参数解析 (qq.com)</a></p></blockquote><h2 id="1-引言-x20"><a href="#1-引言-x20" class="headerlink" title="1.引言 &#x20;"></a>1.引言 &#x20;</h2><p>以下图Huggingface Inference API为例（其他框架类似），这里重点介绍$top_k$,$top_p$,$temperature$,$repetition_penalty$参数，以及$greedy~search$和$beam~search$。</p><p><img src="image/image_NCb4GBVWiF.png" alt=""></p><h2 id="2-背景介绍"><a href="#2-背景介绍" class="headerlink" title="2.背景介绍"></a>2.背景介绍</h2><p>现在常见的LLM基本都是只包含<code>Transformer Decoder</code>的，每个Token在输入模型的Transformer Decoder之前，都会首先从Token Embedding（有些也叫Word Embedding）中通过查表获取对应的embedding向量，然后将embedding向量输入Transformer Decoder，并且在最后一层输出的也是同维度的embedding。在预测下一个Token时，实际只利用了上一个Token的embedding。</p><p>如下图所示，将输入“<code>a robot must obey the orders given it</code>”对应的embedding输入Transformer Decoding后，在最后的Transformer Decoder之后，每个Token对应的位置相应的也会生成一个新的embedding，然后使用最后一个Token“<code>it</code>”<strong>对应的新生成的embedding（蓝色）</strong> 来生成新的Token“<code>Okay</code>”，之后将新的Token“<code>Okay</code>”也作为输入，进一步根据“<code>Okay</code>”对应位置新生成的embedding来生成新的Token“<code>human</code>”，以此类推：</p><p><img src="image/640 (1" alt="">_5dXIUr8oeN.gif&gt;)</p><p>那么怎么<strong>根据新生成的embedding</strong>来生成下一个Token呢，如下图所示，具体来说是<strong>让新生成的embedding与Token Embeddings矩阵相乘</strong>（也就是和每个Token对应的embedding向量做内积），得到和词表中每个Token的相似性得分（<code>logits</code>），然后基于这个得分即可以选择生成新的Token（比如直接取得分最高的Token）。</p><p><img src="image/image_9fhgUNgDPH.png" alt=""></p><p>其中的Token Embeddings行数即为模型词表中Token的个数，列数即为embedding的维度，也就是每个Token对应一个embedding向量，如下图所示：</p><p><img src="image/image_x3BorCXKpc.png" alt=""></p><p>对于LLM的推理过程详情可以参考这两篇博文：</p><ul><li><a href="http://jalammar.github.io/illustrated-gpt2/" title="The Illustrated GPT-2">The Illustrated GPT-2</a></li><li><a href="http://jalammar.github.io/how-gpt3-works-visualizations-animations/" title="How GPT3 Works - Visualizations and Animations">How GPT3 Works - Visualizations and Animations</a></li></ul><h2 id="3-Greedy-Search"><a href="#3-Greedy-Search" class="headerlink" title="3.Greedy Search"></a>3.Greedy Search</h2><p>假设词表中有“<code>a</code>”,“<code>given</code>”,“<code>human</code>”,“<code>it</code>”,“<code>must</code>”,“<code>obey</code>”,“<code>Okay</code>”,“<code>orders</code>”,“<code>robot</code>”,“<code>the</code>”,“<code>.</code>”,“<code>EOS</code>”共12个Token，其中“<code>EOS</code>”表示终止Token。</p><p>GreedySearch（贪心搜索）的思路非常简单，<strong>就是每次都从相似性得分（logits）选择得分最高的Token</strong>（一般来说，都会将得分经过softmax层转换为概率分布，数值区间为<code>0~1</code>，此处为了简单，就不额外转换，不过都假设数值在<code>0~1</code>之间），直到结束。如下图所示：</p><ul><li><strong>Step1</strong>：使用最后一个Token“<code>it</code>”<strong>对应的新生成的embedding来计算相似性得分（logits）</strong>，最终“<code>Okay</code>”对应的得分0.91最高，所以选择“<code>Okay</code>”作为下一个Token。</li><li><strong>Step2</strong>：使用“<code>Okay</code>”来计算相似性得分（logits），最终“<code>human</code>”对应的得分0.89最高，所以选择“<code>human</code>”作为下一个Token。</li><li><strong>Step3</strong>：使用“<code>human</code>”来计算相似性得分（logits），最终“<code>.</code>”对应的得分0.78最高，所以选择“<code>.</code>”作为下一个Token。</li><li><strong>Step4</strong>：使用“<code>.</code>”来计算相似性得分（logits），最终“<code>EOS</code>”对应的得分0.90最高，所以终止生成。</li></ul><p><img src="image/image_8OGYRSQm1C.png" alt=""></p><p>在推理阶段模型的权重都是确定的，并且也不会有dropout等其他随机性（忽略不可抗的硬件计算误差，比如并行规约求和的累积误差等），因此<strong>如果是greedy search，则对于同一个输入，多次运行后模型的输出结果应该完全一致</strong>。</p><ul><li>这样的好处是<strong>在模型效果严格对齐时非常有必要</strong>（比如将模型从Huggingface模型转换为推理效率更高的Faster Transformer模型，并且使用Faster Transformer进行推理部署）。</li><li>这样的坏处<strong>是模型效果可能不是最优的，也会缺乏一定的多样性</strong>，比如用同样的问题问ChatGPT，其答案并不会每次都一样。至于如何增加多样性。</li></ul><h2 id="4-Beam-Search"><a href="#4-Beam-Search" class="headerlink" title="4.Beam Search"></a>4.Beam Search</h2><p>BeamSearch是GreedySearch的改进版本，<strong>其不再是每次都取得分最大的Token，而是始终保留beam_size个得分最大的序列</strong>。还是使用上面的例子。如下图所示，假设beam_size为2，也就是始终保留两个得分最大的序列：</p><p><strong>Step1</strong>：使用最后一个Token“<code>it</code>”对应的新生成的embedding来计算相似性得分（logits），最终“<code>Okay</code>”对应的得分0.91和“<code>.</code>”对应的得分0.84最高，所以选择“Okay”和“.”作为下一个Token。</p><ul><li>“a robot must obey the orders given it Okay”，对应得分0.91</li><li>“a robot must obey the orders given it .”，对应得分0.84</li></ul><p><strong>Step2</strong>：分别使用“<code>Okay</code>”和“<code>.</code>”来计算相似性得分（logits）</p><ul><li>对于“<code>Okay</code>”，最终“<code>human</code>”对应的得分0.89和“<code>the</code>”对应的得分0.72最高，对应候选序列</li><li>“a robot must obey the orders given it <strong>Okay human</strong>”，对应得分<strong>0.8099</strong></li><li>“a robot must obey the orders given it <strong>Okay the</strong>”，对应得分0.6552</li><li>对于“<code>.</code>”，最终“<code>the</code>”对应的得分0.92和“<code>EOS</code>”对应的得分0.70最高，对应候选序列</li><li>“a robot must obey the orders given it <strong>. the</strong>”，对应得分<strong>0.7728</strong></li><li>“a robot must obey the orders given it <strong>.</strong>”，对应得分0.5880</li><li><strong>从以上4个序列中选出得分最高的2个保留</strong>：</li><li>“a robot must obey the orders given it <strong>Okay human</strong>”，对应得分0.8099</li><li>“a robot must obey the orders given it <strong>. the</strong>”，对应得分0.7728</li></ul><p><strong>Step3</strong>：分别使用“<code>human</code>”和“<code>the</code>”来计算相似性得分（logits）</p><ul><li>对于“<code>human</code>”，最终“<code>.</code>”对应的得分0.78和“<code>human</code>”对应的得分0.72最高，对应候选序列</li><li>“a robot must obey the orders given it <strong>Okay human.</strong>”，对应得分<strong>0.6317</strong></li><li>“a robot must obey the orders given it <strong>Okay human human</strong>”，对应得分0.5831</li><li>对于“<code>the</code>”，最终“<code>human</code>”对应的得分0.80和“<code>robot</code>”对应的得分0.68最高，对应候选序列</li><li>“a robot must obey the orders given it <strong>. the human</strong>”，对应得分<strong>0.6128</strong></li><li>“a robot must obey the orders given it <strong>. the robot</strong>”，对应得分0.5255</li><li><strong>从以上4个序列中选出得分最高的2个保留</strong>：</li><li>“a robot must obey the orders given it <strong>Okay human.</strong>”，对应得分0.6317</li><li>“a robot must obey the orders given it <strong>. the human</strong>”，对应得分0.6128</li></ul><p><strong>Step4</strong>：分别使用“<code>.</code>”和“<code>human</code>”来计算相似性得分（logits）</p><ul><li>对于“<code>.</code>”，最终“<code>robot</code>”对应的得分0.81和“<code>EOS</code>”对应的得分0.90最高，对应候选序列</li><li>“a robot must obey the orders given it <strong>Okay human. robot</strong>”，对应得分0.5117</li><li>“a robot must obey the orders given it <strong>Okay human.</strong>”，对应得分<strong>0.5685</strong></li><li>对于“<code>human</code>”，最终“<code>must</code>”对应的得分0.68和“<code>.</code>”对应的得分0.90最高，对应候选序列</li><li>“a robot must obey the orders given it <strong>. the human must</strong>”，对应得分0.4167</li><li>“a robot must obey the orders given it <strong>. the human.</strong>”，对应得分0.5515</li><li><strong>从以上4个序列中选出概率最高的2个保留</strong>，由于此时得分最高的“a robot must obey the orders given it Okay human.”已经生成终止符Token“<code>EOS</code>”，所以可以在此终止，因为不会有其他得分更高的序列。</li></ul><p><img src="image/image_jkAJ6V7kGt.png" alt=""></p><p>由于beam search会同时保留多个序列，因此<strong>就更容易得到得分更高的序列，并且beam_size越大，获得更高得分的概率越高</strong>。然而从上面也可以看出，每个step都需要进行beam_size次前向计算（当然可以使用batch计算，但总的计算量不变），也就是计算量会扩大beam_size倍。另一方面，LLM推理中一般都会使用Key、Valuecache，这也就会进一步增大Key、Valuecache的内存占用，同时增加了Key、Valuecache管理的复杂度。这也就是在LLM推理中为什么比较少使用beam search。</p><p>与greedy search类似，虽然beam search保留了多个序列，但最终输出时还是返回的得分最大的序列，因此<strong>对于同一个输入，使用beam search，多次运行模型最终的输出依然是固定不变的</strong>。</p><h2 id="5-top-k"><a href="#5-top-k" class="headerlink" title="5.top_k"></a>5.top_k</h2><p>从上面的介绍可以看出，<strong>不管是greedysearch，还是beamsearch，对于固定输入，模型的输出是固定不变的</strong>，这就显得比较单调，为了增加模型输出的多样性，人们提出了<a href="https://arxiv.org/abs/1805.04833" title="top-k采样策略">top-k采样策略</a>，其不像greedysearch那样每次取分数最高的，而是<strong>先选出分数最高的k个，然后将其分数作为权重进行随机采样，得到下一个Token</strong>。这也就引入了随机性，每次预测的结果都可能不一样。</p><p>还是以上面的例子来介绍，如下图所示（假设<code>k=3</code>）：</p><ul><li><strong>Step1</strong>：使用最后一个Token“<code>it</code>”对应的新生成的embedding来计算相似性得分（logits），选出得分最高的3个Token：[“<code>Okay</code>”、“<code>.</code>”、“<code>EOS</code>”]，对应的权重为：<code>[0.91,0.84,0.72]</code>，使用该权重进行随机采样，获得新Token“<code>Okay</code>”。</li><li><strong>Step2</strong>：使用“<code>Okay</code>”来计算相似性得分（logits），选出得分最高的3个Token：<code>[“human”、“robot”、“the”]</code>，对应的权重为：<code>[0.89,0.65,0.72]</code>，使用该权重进行随机采样，获得新Token“<code>the</code>”，事实上，“<code>the</code>”并不是得分最高的。</li><li>以此类推，最终得到输出序列：“a robot must obey the orders given it <strong>Okay the human.</strong>”</li></ul><p><img src="image/image_HEQg4L4Aua.png" alt=""></p><p>可以看出，<strong>如果top_k=1，则对应greedysearch。</strong></p><h2 id="6-top-p"><a href="#6-top-p" class="headerlink" title="6.top_p"></a>6.top_p</h2><p>在top_k中，每次都是从k个Token中采样，但是难免会出现一些特殊的case，比如某一个Token的分数非常高，其他分数都很低，此时仍旧会有一定的概率采样到那些分数非常低的Token，导致生成输出质量变差。此时，如果k是可变的，那么就可以过滤掉分数很低的Token，在<a href="https://arxiv.org/abs/1904.09751" title="The Curious Case of Neural Text Generation">The Curious Case of Neural Text Generation</a>.中，作者提出了top_p采样，<strong>在每个step中，都对输出分数进行排序，然后将分数从大到小累加，直到累加分数大于设置的p为止，然后和top_k类似，将每个选择出来的Token的分数作为权重进行随机采样</strong>。这样，每次候选的Token个数都会因为Token分数的分布不同而不一样。</p><p>还是以上面的例子来介绍，如下图所示（假设<code>p=2.2</code>）：</p><ul><li><strong>Step1</strong>：使用最后一个Token“<code>it</code>”对应的新生成的embedding来计算相似性得分（logits），选出累积得分超过2.2的Token：<code>[“Okay”、“.”、“EOS”]</code>，对应的权重为：<code>[0.91,0.84,0.72]</code>，使用该权重进行随机采样，获得新Token“<code>Okay</code>”。</li><li><strong>Step2</strong>：使用“<code>Okay</code>”来计算相似性得分（logits），选出累积得分超过2.2的Token：<code>[“human”、“robot”、“the”]</code>，对应的权重为：<code>[0.89,0.65,0.72]</code>，使用该权重进行随机采样，获得新Token“<code>the</code>”，事实上，“<code>the</code>”并不是得分最高的。</li><li><strong>Step3</strong>：使用“<code>the</code>”来计算相似性得分（logits），选出累积得分超过2.2的Token：<code>[“human”、“obey”、“robot”、“.”]</code>，对应的权重为：<code>[0.82,0.41,0.53,0.48]</code>，使用该权重进行随机采样，获得新Token“<code>human</code>”，事实上，“<code>human</code>”并不是得分最高的，并且此时选出了4个候选Token。</li><li>以此类推，最终得到输出序列：“a robot must obey the orders given it Okay the human.”</li></ul><p><img src="image/image_IQ0gtepioS.png" alt=""></p><p>虽然从理论上讲，<strong>top_p似乎比top_k更优雅，但这两种方法在实践中都很好用。top_p也可以与top_k结合使用，这可以避免分数非常低的Token</strong>，同时提供一些动态选择的空间。</p><h2 id="7-temperature"><a href="#7-temperature" class="headerlink" title="7.temperature"></a>7.temperature</h2><p>事实上，在<strong>top_k和top_p的采样中并不是完全按照分数权重来采样的</strong>，一般采样前我们会将候选Token的得分向量经过softmax（公式如下图）转换为概率，然后按照概率分布采样。</p><script type="math/tex; mode=display">\operatorname{softmax}\left(y_{i}\right)=\frac{e^{y_{i}}}{\sum_{j=1}^{n} e^{y_{j}}}</script><p>很多时候我们想要控制采样的随机性，可以使用<strong>带有温度系数T的softmax实现</strong>，如下所示，温度系数T为大于0的任意值（Huggingface中限制<code>0.0&lt;T&lt;100.0</code>）。<strong>当</strong>**<code>T=1</code>**<strong>时，输出分布将与标准softmax输出相同。T的值越大，输出分布就越平滑，T的值越小，输出分布越陡峭</strong>。</p><ul><li>如果希望<strong>增加</strong>输出分布的<strong>随机性</strong>，可以<strong>增加</strong>参数T的值，当T为无穷大时，分布变成均匀分布，就是完全随机。</li><li>如果希望<strong>减小</strong>输出分布的<strong>随机性</strong>，可以<strong>减小</strong>参数T，当T趋近于0时，就是等价于取top1。</li></ul><script type="math/tex; mode=display">\operatorname{softmax}\left(y_{i}\right)=\frac{e^{\frac{y_{i}}{T}}}{\sum_{j=1}^{n} e^{\frac{y_{j}}{T}}}</script><p>假设得到的候选Token为：<code>[“human”、“obey”、“robot”、“EOS”]</code>，对应的分数为：<code>[0.92,0.11,0.33,0.04]</code>，则对于不同的参数t，利用上面的softmax可以得到对应的概率分布为：</p><ul><li>橙色：<code>t=1</code>，分布不变，不改变随机</li><li>蓝色：<code>t&lt;1</code>，减小随机性，并且t越小，随机性越小</li><li>红色：<code>t&gt;1</code>，增大随机性，并且t越大，随机性越大</li></ul><p><img src="image/image_Nab1ILx91y.png" alt=""></p><h2 id="8-repetition-penalty（重复惩罚）"><a href="#8-repetition-penalty（重复惩罚）" class="headerlink" title="8.repetition_penalty（重复惩罚）"></a>8.repetition_penalty（重复惩罚）</h2><p>这个选项最早是由<a href="https://arxiv.org/abs/1909.05858" title="A Conditional Transformer Language Model for Controllable Generation">A Conditional Transformer Language Model for Controllable Generation</a>中提出的，其是<strong>为了解决语言模型中重复生成的问题</strong>，即使比较大的LLM也会存在。其思想比较简单，<strong>就是记录之前已经生成过的Token，当预测下一个Token时，人为降低已经生成过的Token的分数，使其被采样到的概率降低</strong>。</p><p>如下所示，直接基于上述带温度系数T的softmax进行实现，其中的<code>g</code>表示已经生成过的Token列表，如果某个Token已经在生成过的Token列表<code>g</code>中，则对其对应的温度系数T乘上一个系数<code>θ</code>，<code>θ</code>为大于0的任意值。</p><ul><li><code>θ=1</code>，表示不进行任何惩罚</li><li><code>θ&gt;1</code>，相当于尽量避免重复</li><li><code>θ&lt;1</code>，相当于希望出现重复</li></ul><script type="math/tex; mode=display">p_{i}=\frac{\exp \left(x_{i} /(T \cdot I(i \in g))\right.}{\sum_{j} \exp \left(x_{j} /(T \cdot I(j \in g))\right.} \quad I(c)=\theta ~if~ c ~is ~True ~else ~1</script><p>还是使用上一部分的示例，假设得到的候选Token为：<code>[“human”、“obey”、“robot”、“EOS”]</code>，对应的分数为：<code>[0.92,0.11,0.33,0.04]</code>，令<code>g=[“robot”,“it”]</code>，也就是这些Token已经生成过，对应的惩罚系数<code>θ=3</code>，可以看出，“<code>robot</code>”对应的采样概率都在降低：</p><p><img src="image/image_Wy-zAii0H2.png" alt=""></p><p>如果希望鼓励出现重复，可以设置惩罚系数<code>θ&lt;1</code>，比如，令<code>θ=0.5</code>，可以看出，“<code>robot</code>”对应的采样概率都在增加：</p><p><img src="image/image_ryMAaWcm4G.png" alt=""></p><h2 id="9-总结"><a href="#9-总结" class="headerlink" title="9.总结"></a>9.总结</h2><p>通过以上的介绍，大概知道了各个参数的含义，整体来说：</p><ul><li><code>GreedySearch</code>是最简单、最直接的方式，其可以保证稳定的输出，相应的，<code>BeamSearch</code>可以进一步提升生成效果，但是代价更高，也是可以保证稳定的输出。</li><li><code>top_p</code>和<code>top_k</code>都可以用于增加模型生成结果的多样性，输出结果往往会变。</li><li>温度系数<code>temperature</code>一般用于控制随机性，<code>temperature</code>越大，随机性越强，<code>temperature</code>越小，随机性越弱。</li><li>重复惩罚<code>repetition_penalty</code>用于避免模型一直输出重复的结果，<code>repetition_penalty</code>越大，出现重复性可能越小，<code>repetition_penalty</code>越小，出现重复性可能越大。</li></ul>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大语言模型方法与实践</title>
      <link href="/llms/llms_article/7.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%96%B9%E6%B3%95%E4%B8%8E%E5%AE%9E%E8%B7%B5/"/>
      <url>/llms/llms_article/7.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%96%B9%E6%B3%95%E4%B8%8E%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<blockquote><p>崔一鸣(<a href="https://link.zhihu.com/?target=https://ymcui.com" title="https://ymcui.com">https://ymcui.com</a>)老师分享了一期《大规模预训练语言模型方法与实践》，本文为课程笔记。</p></blockquote><h2 id="1-自然语言表示发展简介"><a href="#1-自然语言表示发展简介" class="headerlink" title="1.自然语言表示发展简介"></a>1.自然语言表示发展简介</h2><p><img src="image/image_ArwyMG1C0z.png" alt=""></p><p><img src="image/image_4dWjX5XtbL.png" alt=""></p><p><img src="image/image_EdEWxMdLwa.png" alt=""></p><p><img src="image/image_d99xPXY2Qn.png" alt=""></p><p><img src="image/image_r4pPu012mb.png" alt=""></p><p><img src="image/image_54WRlO-dSm.png" alt=""></p><p><img src="image/image_L9_culNbN0.png" alt=""></p><h2 id="2-生成式预训练语言模型：GPT系列"><a href="#2-生成式预训练语言模型：GPT系列" class="headerlink" title="2.生成式预训练语言模型：GPT系列"></a>2.生成式预训练语言模型：GPT系列</h2><p><img src="image/image_BjKuzl6vPe.png" alt=""></p><h3 id="2-1-GPT"><a href="#2-1-GPT" class="headerlink" title="2.1 GPT"></a>2.1 GPT</h3><p><img src="image/image_pDGG-D6mop.png" alt=""></p><p><img src="image/image_yHOiW_Vmdm.png" alt=""></p><p><img src="image/image_YtQ3Gu8GNP.png" alt=""></p><p><img src="image/image_eZjRA_YbmZ.png" alt=""></p><h3 id="2-2-GPT-2"><a href="#2-2-GPT-2" class="headerlink" title="2.2 GPT-2"></a>2.2 GPT-2</h3><p><img src="image/image_9NHS67xWsy.png" alt=""></p><h3 id="2-3-GPT-3"><a href="#2-3-GPT-3" class="headerlink" title="2.3 GPT-3"></a>2.3 GPT-3</h3><p><img src="image/image_ET-Y1u4tA5.png" alt=""></p><p><img src="image/image__QXzI0uBxT.png" alt=""></p><h3 id="2-4-InstructGPT"><a href="#2-4-InstructGPT" class="headerlink" title="2.4 InstructGPT"></a>2.4 InstructGPT</h3><p><img src="image/image_8bqrF1_5p-.png" alt=""></p><p><img src="image/image_eqMFe5I0P2.png" alt=""></p><p><img src="image/image_rIXUgtRGVI.png" alt=""></p><p><img src="image/image_jzSBHkXRB-.png" alt=""></p><h3 id="2-5-ChatGPT（GPT-3-5）"><a href="#2-5-ChatGPT（GPT-3-5）" class="headerlink" title="2.5 ChatGPT（GPT-3.5）"></a>2.5 ChatGPT（GPT-3.5）</h3><p>2022年11月30日，OpenAI推出了划时代的“ChatGPT”</p><ul><li>ChatGPT是InstructGPT的衍生模型，可以理解和执行指令并给出详细的回复</li><li>被广泛认为是近期“大模型”兴起的开端</li><li>ChatGPT打破了预训练模型的刻板印象，展现出了极强的创造力和可能性</li></ul><p><img src="image/image_AAIE_6DRjk.png" alt=""></p><p><img src="image/image_fUz6smI4zA.png" alt=""></p><h3 id="2-6-GPT-4"><a href="#2-6-GPT-4" class="headerlink" title="2.6 GPT-4"></a>2.6 GPT-4</h3><p>2023年3月，OpenAI发布了最新一代大型多模态模型 GPT-4，是迄今为止GPT家族中能力最强的模型</p><p>主要提升：生成的文本更具创造性、支持图像输入、支持更⻓的 上下文、显著提升推理能力</p><p>其他特点&#x20;</p><ul><li>花费了6个月的时间让GPT-4更安全更符合人类偏好</li><li>相比GPT3.5能够在拒答问题上提升82%，事实性提 升40%</li></ul><p>目前Web Demo界面仅向Plus用户提供，API价格比gpt3.5-turbo高出10倍以上</p><h3 id="2-7-总结"><a href="#2-7-总结" class="headerlink" title="2.7 总结"></a>2.7 总结</h3><p>从GPT到GPT-4，模型规模逐渐变大，能力越来越强</p><p><img src="image/image__5pGFaPQWn.png" alt=""></p><h2 id="3-开源大规模预训练语言模型"><a href="#3-开源大规模预训练语言模型" class="headerlink" title="3.开源大规模预训练语言模型"></a>3.开源大规模预训练语言模型</h2><h3 id="3-1-LLaMA"><a href="#3-1-LLaMA" class="headerlink" title="3.1 LLaMA"></a>3.1 LLaMA</h3><p>Open and Efficient Foundation Language Models (Open但没完全Open的LLaMA)&#x20;</p><p>2023年2月，Meta（原Facebook）推出了LLaMA大模型，使用了1.4T token进行训练</p><p>虽然最大模型只有65B，但在相关评测任务上的效果可以媲美甚至超过千亿级大模型</p><p>被认为是近期开源大模型百花⻬放的开端之一，“羊驼”系列模型及其生态快速发展</p><p>主干模型仍然是传统的<strong>transformer decoder</strong>结构</p><p>主要技术：<strong>Pre-normalization</strong>, <strong>SwiGLU activation, Rotary Embedding (RoPE)</strong></p><p><img src="image/image_JCwB8lkvnu.png" alt=""></p><p><img src="image/image_RolZET905I.png" alt=""></p><p><img src="image/image_D2FzF1XfB1.png" alt=""></p><p><img src="image/image_KiQkB5PfgI.png" alt=""></p><p><img src="image/image_hzFxuo-OFZ.png" alt=""></p><h3 id="3-2-Alpaca"><a href="#3-2-Alpaca" class="headerlink" title="3.2 Alpaca"></a>3.2 Alpaca</h3><p>Stanford Alpaca: An Instruction-following LLaMA Model&#x20;</p><p>Alpaca是在LLaMA基础上使用52K指令数据精调的预训练模型</p><p>作者只用了不到600美元的成本训练出了该模型（数据$500 + 机器$100）</p><p>初步实验结果表明Alpaca可以达到与OpenAI text-davinci-003相匹敌的效果</p><p><img src="image/image_n43p6BIr65.png" alt=""></p><p><img src="image/image_-tFyDLWofC.png" alt=""></p><p><img src="image/image_4cPglEwkah.png" alt=""></p><h3 id="3-3-Llama-2"><a href="#3-3-Llama-2" class="headerlink" title="3.3 Llama-2"></a>3.3 Llama-2</h3><p>Llama 2: Open Foundation and Fine-Tuned Chat Models</p><p>2023年7月，Meta推出了Llama-2开源大模型，并且推出了Llama-2-Chat对话模型</p><p>虽然仍然需要填写申请表获取模型权重，但在二次分发和商用许可方面更加宽松</p><p>与一代LLaMA主要区别体现在更多的训练数据、更⻓的上下文窗口、GQA技术等</p><p><img src="image/image_vfEJtY0qOJ.png" alt=""></p><p><img src="image/image_-c3_6Hx5tE.png" alt=""></p><p><img src="image/image_mDrWJzEOd2.png" alt=""></p><p><img src="image/image_hGpRDvG4uj.png" alt=""></p><h3 id="3-4-Code-Llama"><a href="#3-4-Code-Llama" class="headerlink" title="3.4 Code Llama"></a>3.4 Code Llama</h3><p>Code Llama: Open Foundation Models for Code</p><p>2023年8月24日，Meta推出了面向代码的可商用大模型Code Llama，包含三个大小版本（7B/13B/34B）</p><p>支持多种编程语言，包括Python、C++、Java、PHP、Typescript (Javascript)、C#和Bash</p><p>亮点：</p><ul><li>免费供学术研究和商用</li><li>支持100K上下文</li><li>“神秘”34B版接近GPT-4效果</li></ul><p><img src="image/image_io0DmhNLpZ.png" alt=""></p><p><img src="image/image_-OtsUClaNZ.png" alt=""></p><p><img src="image/image_eMFKRbXHPR.png" alt=""></p><p><img src="image/image_4pYQvRleAo.png" alt=""></p><h2 id="4-中文开源大模型LLaMA-amp-Alpace"><a href="#4-中文开源大模型LLaMA-amp-Alpace" class="headerlink" title="4.中文开源大模型LLaMA &amp; Alpace"></a>4.中文开源大模型LLaMA &amp; Alpace</h2><h3 id="4-1-中文LLaMA-amp-Alpaca大模型"><a href="#4-1-中文LLaMA-amp-Alpaca大模型" class="headerlink" title="4.1 中文LLaMA &amp; Alpaca大模型"></a>4.1 中文LLaMA &amp; Alpaca大模型</h3><p>针对LLaMA进行了中文适配，扩展其词表，并进一步推出了指令精调的中文Alpaca模型（类ChatGPT模型）</p><p>支持llama.cpp, 🤗transformers, text-generation-webui, LangChain, privateGPT等生态</p><ul><li><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca" title="中文LLaMA\&amp;Alpaca大语言模型">中文LLaMA\&amp;Alpaca大语言模型</a></li><li><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2" title="中文LLaMA-2 &amp; Alpaca-2大模型二期项目">中文LLaMA-2 &amp; Alpaca-2大模型二期项目</a></li></ul><p><img src="image/image_R6AubE3SXL.png" alt=""></p><p><img src="image/image_E_TZ8CjUBr.png" alt=""></p><h3 id="4-2-模型方法"><a href="#4-2-模型方法" class="headerlink" title="4.2 模型方法"></a>4.2 模型方法</h3><h4 id="（1）中文词表扩充"><a href="#（1）中文词表扩充" class="headerlink" title="（1）中文词表扩充"></a>（1）中文词表扩充</h4><p>动机：LLaMA没有显式使用中文语料进行训练</p><p>原版LLaMA没有显式使用中文语料进行训练，词表（32K tokens）中仅包含非常少量的中文字符</p><p>经粗略统计，LLaMA词表中仅包含约700个中文字符（范围：\u4E00-\u9FFF）</p><p><img src="image/image_e5sAW_vlbZ.png" alt=""></p><p><img src="image/image_jr40CAcI73.png" alt=""></p><h4 id="（2）LoRA高效训练"><a href="#（2）LoRA高效训练" class="headerlink" title="（2）LoRA高效训练"></a>（2）LoRA高效训练</h4><p><img src="image/image_q6ueYF6aOb.png" alt=""></p><p><img src="image/image_G5LAM6SI_g.png" alt=""></p><p><img src="image/image_jKw6AV_h37.png" alt=""></p><h3 id="4-3-预训练过程：LLaMA-LLaMA-2"><a href="#4-3-预训练过程：LLaMA-LLaMA-2" class="headerlink" title="4.3 预训练过程：LLaMA/LLaMA-2"></a>4.3 预训练过程：LLaMA/LLaMA-2</h3><p>在原版LLaMA基础上，使用大规模无标注中文文本进行训练，补充基础中文语义和知识</p><p>训练目标为传统的Causal LM，即给定上文预测下一个token是什么</p><p><img src="image/image_HunLriHLF8.png" alt=""></p><h3 id="4-4-指令精调过程"><a href="#4-4-指令精调过程" class="headerlink" title="4.4 指令精调过程"></a>4.4 指令精调过程</h3><h4 id="（1）Alpaca"><a href="#（1）Alpaca" class="headerlink" title="（1）Alpaca"></a>（1）Alpaca</h4><p><img src="image/image_VM97AR03nm.png" alt=""></p><h4 id="（2）Alpaca-2"><a href="#（2）Alpaca-2" class="headerlink" title="（2）Alpaca-2"></a>（2）Alpaca-2</h4><p><img src="image/image_WCLQipJHdR.png" alt=""></p><h4 id="（3）模型对比"><a href="#（3）模型对比" class="headerlink" title="（3）模型对比"></a>（3）模型对比</h4><p><img src="image/image_S6l33C5sOv.png" alt=""></p><h3 id="4-5-解码参数对效果的影响"><a href="#4-5-解码参数对效果的影响" class="headerlink" title="4.5 解码参数对效果的影响"></a>4.5 解码参数对效果的影响</h3><p><img src="image/image_rKqiNBvjJH.png" alt=""></p><p><img src="image/image_FeQh5CrHWR.png" alt=""></p><h3 id="4-6-扩展上下文"><a href="#4-6-扩展上下文" class="headerlink" title="4.6 扩展上下文"></a>4.6 扩展上下文</h3><p><img src="image/image_HUpY-gfEzF.png" alt=""></p><p><img src="image/image_tTtvhXiyrj.png" alt=""></p><p><img src="image/image_whLq8pgSpf.png" alt=""></p><h3 id="4-7-多模态VisualCLA"><a href="#4-7-多模态VisualCLA" class="headerlink" title="4.7 多模态VisualCLA"></a>4.7 多模态VisualCLA</h3><p>VisualCLA是一个支持图像和文本输入的中文多模态大模型，基于中文Alpaca和OpenAI CLIP模型开发</p><p><img src="image/image_salcmUHPOG.png" alt=""></p><p><img src="image/image_xScATMHyQj.png" alt=""></p><h2 id="5-LLaMA生态与下游应用"><a href="#5-LLaMA生态与下游应用" class="headerlink" title="5.LLaMA生态与下游应用"></a>5.LLaMA生态与下游应用</h2><p><img src="image/image_Q0bs2hWZxq.png" alt=""></p><h3 id="5-1-llama-cpp"><a href="#5-1-llama-cpp" class="headerlink" title="5.1 llama.cpp"></a>5.1 <strong>llama.cpp</strong></h3><p>llama.cpp是一个基于C/C++的本地量化和部署大模型的工具</p><ul><li>支持CPU/GPU下快速量化和加载体验大模型，还可以搭建server与API服务</li><li>支持2~8bit量化方法，支持ARM NEON、BLAS、cuBLAS、CBLAST等加速</li><li>支持macOS/Linux/Windows等平台，支持LLaMA/Alpaca/Vicuna/GPT4All等常⻅LLM及其变体</li></ul><p><img src="image/image_ct69bhbhj4.png" alt=""></p><h3 id="5-2-LangChain"><a href="#5-2-LangChain" class="headerlink" title="5.2 LangChain"></a>5.2 <strong>LangChain</strong></h3><p>LangChain是一个用于开发语言模型驱动的应用的框架</p><ul><li>开发人员可以方便地设计与搭建问答、摘要、聊天机器人、代码理解、信息抽取等基于LLM能力的应用程序</li><li>数据感知：将语言模型连接到其他数据源</li><li>具有代理性质：允许语言模型与其环境进行交互</li></ul><h3 id="5-3-privateGPT"><a href="#5-3-privateGPT" class="headerlink" title="5.3 privateGPT"></a>5.3 <strong>privateGPT</strong></h3><p>privateGPT是一种面向离线数据的大模型交互应用</p><ul><li>基于LangChain开发的本地化文档分析与问答交互的接口，确保数据本地化和私有化</li><li>支持GGML格式（llama.cpp量化格式）模型以及GPT4All-J格式模型</li></ul><p><img src="image/image_yXbSKzPSqr.png" alt=""></p><h3 id="5-4-仿OpenAI-API调用"><a href="#5-4-仿OpenAI-API调用" class="headerlink" title="5.4 仿OpenAI API调用"></a>5.4 <strong>仿OpenAI API调用</strong></h3><p>仿OpenAI的API调用方法</p><ul><li>使用fastapi实现的简易的仿OpenAI API⻛格的服务器Demo</li><li>可以使用这个API快速搭建基于中文大模型的个人网站以及其他有趣的Web Demo</li></ul><p><img src="image/image_ftMPpazoK4.png" alt=""></p><h3 id="5-5-基于Gradio的WebUI"><a href="#5-5-基于Gradio的WebUI" class="headerlink" title="5.5 基于Gradio的WebUI"></a>5.5 <strong>基于Gradio的WebUI</strong></h3><p>以界面友好的形式与大模型进行交互，支持量化模型加载、多轮对话、流式输出返回等</p><p><img src="image/image_QxpNaXJZNV.png" alt=""></p><h2 id="6-相关资源"><a href="#6-相关资源" class="headerlink" title="6.相关资源"></a>6.相关资源</h2><ul><li>GitHub: <a href="http://anthology.hfl-rc.com" title="http://anthology.hfl-rc.com">http://anthology.hfl-rc.com</a>&#x20;</li><li>Model Hub: <a href="https://huggingface.co/HFL" title="https://huggingface.co/HFL">https://huggingface.co/HFL</a></li></ul><p><img src="image/image_qFUzFnWwgi.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二分查找（Binary Search）</title>
      <link href="/dsa/confusing_topics/1.Binary_Search/"/>
      <url>/dsa/confusing_topics/1.Binary_Search/</url>
      
        <content type="html"><![CDATA[<h2 id="1-无处不在的二分查找思想"><a href="#1-无处不在的二分查找思想" class="headerlink" title="1.无处不在的二分查找思想"></a>1.无处不在的二分查找思想</h2><p>二分查找是一种非常简单易懂的快速查找算法，生活中到处可见。</p><p>举一个例子，假设只有10个订单，订单金额分别是： 8， 11， 19， 23， 27， 33， 45， 55， 67， 98。从中查找到金额等于19元的订单。</p><p>利用二分思想，每次都与区间的中间数据比对大小，缩小查找区间的范围。</p><p><img src="image/image_nRHzt6t0qS.png" alt=""></p><p>二分查找针对的是一个<strong>有序的数据集合</strong>，查找思想有点类似<strong>分治思想</strong>。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为0。</p><h2 id="2-O-logn-惊人的查找速度"><a href="#2-O-logn-惊人的查找速度" class="headerlink" title="2.O(logn)惊人的查找速度"></a>2.<code>O(logn)</code>惊人的查找速度</h2><p>二分查找是一种非常高效的查找算法。</p><p>假设数据大小是n，每次查找后数据都会缩小为原来的一半，也就是会除以2。最坏情况下，直到查找区间被缩小为空，才停止。</p><p><img src="image/image_BSyEnICBnw.png" alt=""></p><p>可以看出来，这是一个等比数列。其中<code>n/2k=1</code>时， k的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了k次区间缩小操作，时间复杂度就是<code>O(k)</code>。通过<code>n/2k=1</code>，可以求得<code>k=log2n</code>，所以时间复杂度就是<code>O(logn)</code>。</p><h2 id="3-二分查找的实现"><a href="#3-二分查找的实现" class="headerlink" title="3.二分查找的实现"></a>3.二分查找的实现</h2><p><strong>简单</strong>的二分查找并不难写，注意这里的“简单”二字</p><p>最简单的情况就是<strong>有序数组中不存在重复元素</strong>，在其中用二分查找值等于给定值的数据。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> low = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> high = nums.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (low &lt;= high) &#123;</span><br><span class="line">        <span class="type">int</span> mid = (low + high) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums[mid] == value) &#123;</span><br><span class="line">            <span class="keyword">return</span> mid;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &lt; value) &#123;</span><br><span class="line">            low = mid + <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            high = mid - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>着重强调容易出错的三个地方：</p><ol><li><strong>循环退出条件</strong>：注意是<code>low&lt;=high</code>，而不是<code>low&lt;high</code>。</li><li><strong>mid的取值</strong>：<code>mid=(low+high)/2</code>这种写法是有问题的。因为如果low和high比较大的话，两者之和就有可能会溢出。改进的方法是将mid的计算方式写成<code>low+(high - low)/2</code>。更进一步，如果要将性能优化到极致的话，可以将这里的除以2操作转化成位运算<code>low+((high-low)&gt;&gt;1</code>)。因为相比除法运算来说，计算机处理位运算要快得多。</li><li><strong>low和high的更新</strong>：<code>low=mid+1</code>，<code>high=mid-1</code>。注意这里的+1和-1，如果直接写成<code>low=mid</code>或者<code>high=mid</code>，就可能会发生死循环。比如，当high=3， low=3时，如果a[3]不等于value，就会导致一直循环不退出。</li></ol><p>二分查找递归代码的实现</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_internally</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> low, <span class="type">int</span> high, <span class="type">int</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (low &gt; high) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> mid = low + (high - low) / <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">if</span> (nums[mid] == value) &#123;</span><br><span class="line">        <span class="keyword">return</span> mid;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &lt; value) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">bsearch_internally</span>(nums, mid + <span class="number">1</span>, high, value);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">bsearch_internally</span>(nums, low, high - <span class="number">1</span>, value);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">bsearch_internally</span>(nums, <span class="number">0</span>, nums.<span class="built_in">size</span>() - <span class="number">1</span>, value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4-二分查找变形问题"><a href="#4-二分查找变形问题" class="headerlink" title="4.二分查找变形问题"></a>4.二分查找变形问题</h2><p>注意：以下示例是从小到大排列为前提的</p><h3 id="4-1-查找第一个值等于给定值的元素"><a href="#4-1-查找第一个值等于给定值的元素" class="headerlink" title="4.1 查找第一个值等于给定值的元素"></a>4.1 查找第一个值等于给定值的元素</h3><p>比如下面这样一个有序数组，其中，<code>a[5]</code>，<code>a[6]</code>，<code>a[7]</code>的值都等于8，是重复的数据。希望查找第一个等于8的数据，也就是下标是5的元素。</p><p><img src="image/image_GUhE3sMGiy.png" alt=""></p><p>如果用简单二分查找的代码实现，首先拿8与区间的中间值<code>a[4]</code>比较， 8比6大，于是在下标5到9之间继续查找。下标5和9的中间位置是下标7， <code>a[7]</code> 正好等于8，所以代码就返回了。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_first</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> low = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> high = nums.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (low &lt;= high) &#123;</span><br><span class="line">        <span class="type">int</span> mid = low + (high - low) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums[mid] &gt; value) &#123;</span><br><span class="line">            high = mid - <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &lt; value) &#123;</span><br><span class="line">            low = mid + <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> ((mid == <span class="number">0</span>) || (nums[mid - <span class="number">1</span>] != value)) &#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                high = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>nums[mid]</code>跟要查找的value的大小关系有三种情况：大于、小于、等于。对于<code>nums[mid]&gt;value</code>的情况，需要更新<code>high= mid-1</code>；对于<code>nums[mid]&lt;value</code>的情况，需要更新<code>low=mid+1</code>。</p><p><code>nums[mid]=value</code>时，a[mid]就是要找的元素。但是，如果求解的是第一个值等于给定值的元素，当<code>nums[mid]</code>等于要查找的值时，就需要确认一下这个<code>nums[mid]</code>是不是第一个值等于给定值的元素。</p><p>重点看<code>if((mid == 0) || (nums[mid - 1] != value))</code>。如果<code>mid</code>等于0，那这个元素已经是数组的第一个元素，那它肯定是要找的；如果<code>mid</code>不等于0，但<code>nums[mid]</code>的前一个元素<code>nums[mid-1]</code>不等于value，那也说明<code>nums[mid]</code>就是要找的第一个值等于给定值的元素。</p><p>如果经过检查之后发现<code>nums[mid]</code>前面的一个元素<code>nums[mid-1]</code>也等于value，那说明此时的<code>nums[mid]</code>肯定不是要查找的第一个值等于给定值的元素。那就更新<code>high=mid-1</code>，因为要找的元素肯定出现在<code>[low,mid-1]</code>之间。</p><h3 id="4-2-查找最后一个值等于给定值的元素"><a href="#4-2-查找最后一个值等于给定值的元素" class="headerlink" title="4.2 查找最后一个值等于给定值的元素"></a>4.2 查找最后一个值等于给定值的元素</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_last</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> low = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> high = nums.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (low &lt;= high) &#123;</span><br><span class="line">        <span class="type">int</span> mid = low + (high - low) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums[mid] &gt; value) &#123;</span><br><span class="line">            high = mid - <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &lt; value) &#123;</span><br><span class="line">            low = mid + <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> ((mid == nums.<span class="built_in">size</span>() - <span class="number">1</span>) || (nums[mid + <span class="number">1</span>] != value)) &#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                low = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重点看 <code>if((mid == n -1) || (nums[mid + 1] != value))</code>。如果<code>nums[mid]</code>这个元素已经是数组中的最后一个元素了，那它肯定是要找的；如果<code>nums[mid]</code>的后一个元素<code>nums[mid+1]</code>不等于value，那也说明<code>nums[mid]</code>就是要找的最后一个值等于给定值的元素。</p><p>如果经过检查之后，发现<code>nums[mid]</code>后面的一个元素<code>nums[mid+1]</code>也等于value，那说明当前的这个<code>nums[mid]</code>并不是最后一个值等于给定值的元素。就更新<code>low=mid+1</code>，因为要找的元素肯定出现在<code>[mid+1,high]</code>之间。</p><h3 id="4-3-查找第一个大于等于给定值的元素"><a href="#4-3-查找第一个大于等于给定值的元素" class="headerlink" title="4.3 查找第一个大于等于给定值的元素"></a>4.3 查找第一个大于等于给定值的元素</h3><p>在有序数组中，<strong>查找第一个大于等于给定值的元素</strong>。比如，数组中存储的这样一个序列： 3， 4， 6， 7， 10。如果查找第一个大于等于5的元素，那就是6。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_up</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> low = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> high = nums.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (low &lt;= high) &#123;</span><br><span class="line">        <span class="type">int</span> mid = low + (high - low) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums[mid] &gt;= value) &#123;</span><br><span class="line">            <span class="keyword">if</span> ((mid == <span class="number">0</span>) || (nums[mid - <span class="number">1</span>] &lt; value)) &#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                high = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            low = mid + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果<code>nums[mid]</code>小于要查找的值value，那要查找的值肯定在<code>[mid+1,high]</code>之间，所以，更新<code>low=mid+1</code>。</p><p>对于<code>nums[mid]</code>大于等于给定值value的情况，要先看下这个<code>nums[mid]</code>是不是要找的第一个值大于等于给定值的元素。如果<code>nums[mid]</code>前面已经没有元素，或者前面一个元素小于要查找的值value，那<code>nums[mid]</code>就是我们要找的元素。</p><p>如果<code>nums[mid-1]</code>也大于等于要查找的值value，那说明要查找的元素在<code>[low,mid-1]</code>之间，所以，将high更新为<code>mid-1</code>。</p><h3 id="4-4-查找最后一个小于等于给定值的元素"><a href="#4-4-查找最后一个小于等于给定值的元素" class="headerlink" title="4.4 查找最后一个小于等于给定值的元素"></a>4.4 查找最后一个小于等于给定值的元素</h3><p>最后一种二分查找的变形问题，<strong>查找最后一个小于等于给定值的元素</strong>。比如，数组中存储了这样一组数据： 3， 5， 6， 8， 9， 10。最后一个小于等于7的元素就是6</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_down</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> low = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> high = nums.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (low &lt;= high) &#123;</span><br><span class="line">        <span class="type">int</span> mid = low + (high - low) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums[mid] &gt; value) &#123;</span><br><span class="line">            high = mid - <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> ((mid == nums.<span class="built_in">size</span>() - <span class="number">1</span>) || (nums[mid + <span class="number">1</span>] &gt; value)) &#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                low = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-二分查找的局限性"><a href="#5-二分查找的局限性" class="headerlink" title="5.二分查找的局限性"></a>5.二分查找的局限性</h2><p><strong>二分查找的时间复杂度是</strong>**<code>O(logn)</code>，查找数据的效率非常高 **。不过，并不是什么情况下都可以用二分查找，它的应用场景是有很大局限性的。</p><ol><li><strong>二分查找依赖的是顺序表结构，简单点说就是数组</strong>。二分查找只能用在<strong>数据是通过顺序表来存储的数据结构上</strong>。如果你的数据是通过其他数据结构存储的，则无法应用二分查找。</li><li><strong>二分查找针对的是有序数据</strong>。二分查找<strong>只能用在插入、删除操作不频繁</strong>，一次排序多次查找的场景中。针对动态变化的数据集合，二分查找将不再适用。</li><li><strong>数据量太小不适合二分查找</strong>。只有数据量比较大的时候，二分查找的优势才会比较明显。</li><li><strong>数据量太大也不适合二分查找</strong>。二分查找的底层需要依赖数组这种数据结构，而<strong>数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻</strong>。比如，有1GB大小的数据，如果希望用数组来存储，那就需要1G的连续内存空间。</li></ol><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6.总结"></a>6.总结</h2><p>二分查找，一种针对有序数据的高效查找算法，它的时间复杂度是<code>O(logn)</code>。</p><p>二分查找的核心思想理解起来非常简单，有点类似<strong>分治思想</strong>。即<strong>每次都通过跟区间中的中间元素对比，将待查找的区间缩小为一半，直到找到要查找的元素，或者区间被缩小为0</strong>。但是二分查找的代码实现比较容易写错。你需要着重掌握它的三个容易出错的地方：<strong>循环退出条件、 mid的取值， low和high的更新</strong>。</p><p>二分查找虽然性能比较优秀，但应用场景也比较有限。底层必须依赖数组，并且还要求数据是有序的。对于较小规模的数据查找，直接使用顺序遍历就可以了，二分查找的优势并不明显。二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。</p><p>实际上，求“值等于给定值”的二分查找确实不怎么会被用到，<strong>二分查找更适合用在“近似”查找问题</strong>，在这类问题上，二分查找的优势更加明显。</p>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型Agent技术</title>
      <link href="/llms/llms_article/6.%E5%A4%A7%E6%A8%A1%E5%9E%8BAgent%E6%8A%80%E6%9C%AF/"/>
      <url>/llms/llms_article/6.%E5%A4%A7%E6%A8%A1%E5%9E%8BAgent%E6%8A%80%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>视频链接：<a href="https://www.bilibili.com/video/BV1mC4y1g7cT" title="https://www.bilibili.com/video/BV1mC4y1g7cT">https://www.bilibili.com/video/BV1mC4y1g7cT</a><br>文字版链接：<a href="https://mp.weixin.qq.com/s/PL-QjlvVugUfmRD4g0P-qQ" title="https://mp.weixin.qq.com/s/PL-QjlvVugUfmRD4g0P-qQ">https://mp.weixin.qq.com/s/PL-QjlvVugUfmRD4g0P-qQ</a></p><p>现在全球对Agent的关注也是非常狂热的，几个月前，OpenAI 在内部就开始高度关注智能体（Agent）领域，Deep Mind的联合创始人最近也提到下一代 AI 技术走向并非是生成性 AI，而应该是交互性 AI。这种交互性 AI 在很大程度上类似提到的智能体，用户要求完成各种任务，智能体则可以对软件进行操作或者与人进行协作，完成相关的工作。</p></blockquote><p>主要包含以下内容：</p><ol><li><strong>LLM Agents综述</strong>：对从大模型到现在的智能体的技术发展做一个串讲</li><li><strong>通用智能基本原理</strong>：介绍通用智能原理和面向目标架构这个两个根本性问题</li><li><strong>面向目标架构</strong>：</li><li><strong>前瞻性分析</strong>：</li></ol><h2 id="1-LLM-Agents综述"><a href="#1-LLM-Agents综述" class="headerlink" title="1. LLM Agents综述"></a>1. LLM Agents综述</h2><p>如果你一直关注 AI 领域，你应该能看到一个清晰的技术脉络，一开始大家玩** Prompt 工程，接着是Prompt Chain或Flow，再到Agent，多Agent**，很清晰的一个脉络架构，我们也会沿着这个脉络给大家分享相关的经典工作。</p><p><img src="image/image_7LjFxktlGe.png" alt=""></p><p>回到 Agent 这个概念上，实际上，人类是这个星球上最强大的 Agent。<strong>Agent是一个能感知并自主地采取行动的实体，这里的自主性极其关键，Agent要能够实现设定的目标，其中包括具备学习和获取知识的能力以提高自身性能</strong>。</p><p>Agent 的复杂程度各不相同，一个简单的恒温器可以是一个 Agent，一个大型的国家或者一个生物群体也可能是个 Agent。感知环境、自主决策、具备行动能力，设定明确的目标和任务，适应环境及学习能力，都是 Agent 的关键特点。</p><p><img src="image/image_ZrDtpgpuLd.png" alt=""></p><p>Agent 理论在大模型时代之前已经被学术界研究了很多年，许多理论研究都试图创造出具有人类智能水平的 Agent。然而，在大模型出现之前，Agent 的技术始终面对天花板限制，无法取得实用的进步，它的本质问题还是AGI问题，反过来说，<strong>只有AGI的技术进步才能让 Agent 技术进步</strong>。</p><p><img src="image/image_34modshq0W.png" alt=""></p><p>在学术领域，最经典的案例可能是与机器人相关的研究，都涉及到了Agent 技术。在大模型时代之前，比较知名的垂直领域 Agent 的例子比如 Alphago，它有感知环境、做决策、采取行动的闭环，当时的主要研究方向还有使用强化学习打游戏的DeepMind的Agent57，后来更加通用的Gato，还有OpenAI玩“躲猫猫”的多智能体。</p><p>我们认为<strong>Agent技术是未来实现社会全面自动化的关键技术</strong>。在大模型出现之前，自动化更多的是一些偏结构化固定模式环境中通过实现固定算法流程来完成自动化任务，而大模型智能体的通用性带来了灵活性，使其可能应对人类在脑力劳动中面临的各种复杂长尾任务，进一步实现体力和脑力任务的全面自动化。</p><p>大模型和Agent技术开启了全面自动化的新时代。<strong>大模型是第一个可以自主学习并拥有广泛知识的模型，所以在大模型时代，Agent技术开始迅速发展</strong>。今天，我们可能只是在起点，我们看到的Agent还偏向于玩具，但是预计在未来几年，这个领域将产生极大的改变，它的发展速度可能会超越我们的想象，因为我们现在看到改进每天都在发生，天花板远未来到，甚至天花板可能不会再来了。</p><h3 id="1-1-Prompt工程"><a href="#1-1-Prompt工程" class="headerlink" title="1.1 Prompt工程"></a>1.1 Prompt工程</h3><p><img src="image/image_6BmEUuvPIG.png" alt=""></p><p>Prompt工程，把大模型当成一种编程语言来看待。人们通过描述角色技能、任务关键词、任务目标及任务背景，告知大模型需要输出的格式，并调用大模型进行输出。这种方法就是经典的把大模型当做工具来调用，可以称为<strong>工具模式</strong>。</p><p><img src="image/image_DgsZwXLnb1.png" alt=""></p><p><a href="https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor" title="https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor">https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor</a></p><h3 id="1-2-Prompt外挂"><a href="#1-2-Prompt外挂" class="headerlink" title="1.2 Prompt外挂"></a>1.2 Prompt外挂</h3><p>仅凭Prompt工程根本无法满足人们日益增长的大模型需要，鉴于大模型本身的诸多缺陷，如不能及时更新知识，上下文有限等等，人们开始给大模型加入插件，如<strong>引入向量数据库，把数据索引进向量数据库，再召回数据，再提交给大模型做Prompt工程</strong>，这样就可以使用最新的知识和比大模型里的知识更准确的知识。</p><p><img src="image/image_3-Eyi5nA4o.png" alt=""></p><p>这些还不够，人们又开启了外挂模式，尝试<strong>让 GPT 调用函数和使用工具</strong>，一系列关于工具使用的实践开始出现，ChatGPT也推出了插件体系。当人们发现大模型的推理能力很差时，开始试图让模型自身清楚地描述问题，把问题转化为 PDDL （Planning Domain Definition Language）格式的描述语言，通过调用通用规划器来解决规划问题，再把解决方案转化为可执行的动作，以更好地逻辑推理和规划等任务。</p><p><img src="image/image_9-bKZi4alr.png" alt=""></p><p>此外，大模型虽然具备一定的推理能力和思考能力，在很多推理任务上依然力不从心，能不能让模型自己不做规划推理，让他把问题描述清楚，转化成一个 PDDL 的一个关于规划描述的语言，然后使用通用的规划器去做规划，再转化成动作执行，这就把大模型作为一个中转器，把规划器当做了一个外挂。</p><p>我们可能会思考，大模型或许真的就是我们以前想象的那样，会达到人类智慧水平的普适性机器么？显然从各项评测来看还有很多任务做不到，更何况这些任务评测本身的覆盖度也不够完备。</p><p><img src="image/image_dnBFfXPoGs.png" alt=""></p><p>有一个经典概念被誉为”通用任务解决器”，在达特茅斯会议之后得名“<strong>GPS</strong>”，即General Problem Solver。这是由赫伯特·西蒙（Herbert Simon）和艾伦·纽维尔（Allen Newell）在早期提出的概念，他们尝试寻找可用于解决数学问题的通用解决方案。这套理念其实很简洁，可以看作是早期的面向目标架构。它的<strong>主要内容是将目标状态列出，然后在解空间中搜索可以将初始状态转化为目标状态的操作组合，这样的组合便是问题的答案</strong>。</p><p><img src="image/image_sJCQyrUSnO.png" alt=""></p><h3 id="1-3-分解与组合"><a href="#1-3-分解与组合" class="headerlink" title="1.3 分解与组合"></a>1.3 分解与组合</h3><p>然而，目前我们发现，在通用人工智能（AGI）的漫长旅途中，大模型虽显强大，仍<strong>存在着显著的技术天花板</strong>。许多人开始探索如何挖掘大模型在大任务执行能力上的可能性，<strong>其中一个基本策略就是能够分解和组合</strong>。例如，经典的 MapReduce 模式可以将一个大型文本进行摘要，因为它的上下文有限，一种解决办法是扩大 context 的范围。另一个解决方案是，在有限的 context 中，先将文本拆分成小片段，对每个片段进行摘要，然后再将其组合，从而得出结果。</p><p><img src="image/image_fZ5J-gEust.png" alt=""></p><p>大家也发现大模型直接给出答案似乎并不靠谱，那么是否可以让它像人类一样，一步一步思考呢？毕竟，人类在解决问题时，也是逐渐构建解决方案，而并非立即给出答案。因此，开始出现了一系列的尝试解法，比如<strong>思维链、多思维链、思维树和思维图</strong>等。</p><p><img src="image/image_0TUOLnNhqx.png" alt=""></p><p><strong>思维链（Chain of Thought，CoT）</strong>，它<strong>要求模型展示其思考过程，而非仅给出答案</strong>。这可以通过两种方式实现：</p><ol><li>一种是<strong>具体说明</strong>，即要求模型详细地、一步步地思考；</li><li>另一种是<strong>示例说明</strong>，即通过给定问题和答案的同时，提供思考过程。</li></ol><p>这样，当询问模型时，模型会模仿此过程，逐渐思考并给出答案。再往后，我们发现一个CoT有时可能出现错误，然后开始尝试让它发散，<strong>尝试多种思路来解决问题，然后投票选择最佳答案</strong>，这就是<strong>CoT-SC</strong>了。</p><p><img src="image/image_TWgky_AX_E.png" alt=""></p><p><img src="image/image_9Rnb691WlS.png" alt=""></p><p>在这过程中，<strong>这种发散的方法也有局限性</strong>，例如24点问题，它不能很好地解决，那么<strong>就会尝试把这个问题进行垂直分解，分成三步来做，每一步分解成多个子问题，类似于动态规划的做法，就好像把一个大任务拆解成了三个小的子任务，然后再一步一步地去实现它</strong>。</p><p><img src="image/image_NJPkHG378v.png" alt=""></p><p>这就是<strong>思维树（ToT， Tree of Thought）</strong>的一个主要思路，它会<strong>根据当前的问题分解出多个可能，然后每一个树节点就是父节点的一个子问题，逐层扩散，遍布整个解空间，一些节点就直接会发现不合适而终止掉，达到了有效剪枝的作用</strong>。然而 ToT 的方式也存在问题，<strong>对于一些需要分解后再整合的问题</strong>，比如排序问题，排序你可能需要分解和排序，然后再merge，就不行了。</p><p><img src="image/image_jgA4w0YicL.png" alt=""></p><p>为了解决这个问题，一种名为<strong>思维图（Graph of Tree，GoT）</strong>的方法被提出。这种思维图<strong>既可以分解，也可以合并</strong>。</p><p><img src="image/image_yumwNgccHL.png" alt=""></p><p>2023年9月26日，清华姚期智团队又提出了更新的方法——<strong>累计推理</strong>，在24点问题上成功率已经达到98%的SOTA。他们方式很接近主流 Agent 的实现方式，具备一定的通用性。它首先会提出一个初步的想法，然后再对这个想法进行验证，看这个提案是否合适。如果提案合适，就将它添加到图的下一个节点，每一步都基于已经建立的图节点进行下一个思考节点的创建，这样发散、合并或删除直到达到最终目标状态，完备性和灵活性大大增强。</p><h3 id="1-4-反馈"><a href="#1-4-反馈" class="headerlink" title="1.4 反馈"></a>1.4 反馈</h3><p>上述的讨论主要是任务分解和组合，他们尽管强大，<strong>却不能与外界进行互动</strong>，这就不得不讲到反馈机制了。反馈是整个控制论的基石，也是动物体从诞生之初就具备的基本能力。</p><p><img src="image/image_7LKIZunU04.png" alt=""></p><p>最经典的方法实际就是 <strong>ReACT</strong>，这个方法非常经典，基本把智能体最核心的能力圈出来了，当然它也有它的缺陷，将在后面讨论为什么还会有 Agent 更多的复杂技术以克服它的不足。<strong>ReACT让大模型先进行思考，思考完再进行行动，然后根据行动的结果再进行观察，再进行思考，这样一步一步循环下去。</strong> 这种行为模式基本上就是人类这样的智能体主要模式。</p><p><strong>ChatGPT的代码解释器主要采用的就是这种模式</strong>。首先，代码解释器能够与用户进行简单的互动，如用户的问侧和解释器的回应。当用户的问题需要外部调用时，例如询问天气情况，解释器会生成相应的代码，利用代码调用外部工具获取结果。基于这些结果，代码解释器会将信息反馈给用户，如“今天天气很好”。下图是，我们调研的ChatGPT Code Interpreter 的主要实现方式。</p><p><img src="image/image_KYrrm7q2Cg.png" alt=""></p><p>然而，我们始终觉得这样仍然不够，<strong>希望大模型在完成每一个任务后，能够积累经验，故而产生了借鉴强化学习思路的”反射”机制</strong>。反射机制能够让机器记住每一次任务的完成情况，无论效果好坏，以供未来参考，提升模型的性能。</p><p><img src="image/image_QljHWWUC5x.png" alt=""></p><p>Agent的框架都会让模型输出JSON进行函数调用，OpenAI也就推出了Funtion Calling，将外部调用内化到模型中，变成了一种原生能力。 &#x20;</p><p><img src="image/image_wGolkhGct3.png" alt=""></p><h3 id="1-5-Agent"><a href="#1-5-Agent" class="headerlink" title="1.5 Agent"></a>1.5 Agent</h3><p>今天，全世界都在关注这个领域，Agent 模式的研究和应用都在迅猛发展，作为一个”共识”可预见的未来该技术的进步将势不可挡。</p><p><img src="image/image_GRhtRak5_B.png" alt=""></p><h4 id="（1）AutoGPT"><a href="#（1）AutoGPT" class="headerlink" title="（1）AutoGPT"></a>（1）AutoGPT</h4><p>下图是AutoGPT 发布的进行中的架构图，<strong>旨在实现对任务的有效管理</strong>。<strong>生成的任务将会被加入优先级队列中，随后系统会不断从优先队列中选择优先级最高的任务进行执行，整个过程中，任何反馈都会通过记忆进行迭代优化代码</strong>。</p><p><img src="image/image_QnwSmfOn0R.png" alt=""></p><p>这个主要框架虽然相对简单，但其设计理念具有重要意义。首先，创建一个初始的计划，然后进入主循环。系统会让模型判断在当前计划下该进行何种行动，接着会执行行动。执行完毕后，结果会写入下一次循环中。如此，每次决策都会基于之前的结果、记忆和计划，从而制定出新的行动方案。</p><p><img src="image/image_uEahzCS6wX.png" alt=""></p><p>在该框架中，模型的决策过程涉及到动作选择，这也是主要的功能之一。此外，整个过程中我们主要关注的一些工具包括“Start Another Agent”以及“Task Complete”。这两个工具体现了<strong>Agent可以被调用，从而将大任务拆解为若干小任务进行处理，继而形成层次化的树状结构</strong>，这种结构与人类分工和协作的工作方式极为相似。</p><h4 id="（2）Jarvis-HuggingGPT"><a href="#（2）Jarvis-HuggingGPT" class="headerlink" title="（2）Jarvis HuggingGPT"></a>（2）Jarvis HuggingGPT</h4><p><img src="image/image_frsF7Kelx3.png" alt=""></p><p>值得一提的是，微软的贾维斯 (Jarvis)一个深度学习任务调度系统，也采用了类似思想。主要关注<strong>如何调用模型来执行各种深度学习任务</strong>，<strong>涉及到了先做计划，再选择模型，然后执行任务，获取反馈，然后进入下一轮循环等环节</strong>。</p><h4 id="（3）RecurrentGPT"><a href="#（3）RecurrentGPT" class="headerlink" title="（3）RecurrentGPT"></a>（3）RecurrentGPT</h4><p><img src="image/image_cW5obr08hp.png" alt=""></p><p>有的研究者会尝试使用大模型写小说，<strong>借鉴LSTM这个经典深度网络的思想发明RecurrentGPT</strong>，还引入了长时记忆和短时记忆机制，使模型拥有了更佳的记忆和学习功能。</p><p>在每一个时间步中，RecurrentGPT会接收上一个时间步生成的内容、最近生成内容的摘要（短期记忆），历史生成内容中和当前时间步最相关的内容（长期记忆），以及一个对下一步生成内容的梗概。</p><p><img src="image/image_yd8hlh2DhV.png" alt=""></p><h4 id="（4）Voyager"><a href="#（4）Voyager" class="headerlink" title="（4）Voyager"></a>（4）Voyager</h4><p>其他方向，我们看到<strong>把大模型视作一个虚拟世界中的智能体</strong>，如MineCraft游戏中所设定的角色。这个角色可以沿着指定的路线，完成一些在环境中探索的任务，如建房子、挖矿、打怪等。这个角色首先需要被告知怎样去执行任务，例如<strong>自动训练课程计划</strong>的使用。然后逐步的完成任务，形成自己的<strong>执行代码库、技能库</strong>等，这样就算是在以后遇到相似的任务，它都能快速调用已有的技能和经验来完成任务。某种意义上，这就是一种强化学习的方式。</p><p><img src="image/image_-qG5inTuGM.png" alt=""></p><p><img src="image/image_aPi8wOT10o.png" alt=""></p><h4 id="（5）XAgent"><a href="#（5）XAgent" class="headerlink" title="（5）XAgent"></a>（5）XAgent</h4><p>这个方向的变化真的是一日千里，2023年10月17日，清华联合面壁发布了XAgent，提出了双循环机制在效果上碾压了AutoGPT。这种机制中，外循环负责宏观规划，而内循环则负责细节的执行。</p><p><img src="image/image_kPZRtnGu2g.png" alt=""></p><p>双循环模式：</p><ul><li><strong>外循环</strong>：负责全局任务规划，将复杂任务分解为可操作的简单任务。</li><li><strong>内循环</strong>：负责局部任务执行，专注于细节。</li></ul><p>在完成各类任务的时候，它的能力也大大胜过 GPT 4</p><h3 id="1-6-Multi-Agent"><a href="#1-6-Multi-Agent" class="headerlink" title="1.6 Multi-Agent"></a>1.6 Multi-Agent</h3><h4 id="（1）斯坦福小镇"><a href="#（1）斯坦福小镇" class="headerlink" title="（1）斯坦福小镇"></a>（1）斯坦福小镇</h4><p>进一步，人们很自然地想到了多智能体（Multi-agent）模式， “斯坦福小镇”开了一个好头。在这个虚拟的小镇里，每个角色都是一个单独的智能体，每天依据制定的计划按照设定的角色去活动和做事情，当他们相遇并交谈时，他们的交谈内容会被存储在记忆数据库中，并在第二天的活动计划中被回忆和引用，<strong>这一过程中就能涌现出许多颇有趣味性的社会学现象</strong>，我们成为群体智能的涌现。</p><p><img src="image/image_CB9_impjrd.png" alt=""></p><h4 id="（2）MetaGPT"><a href="#（2）MetaGPT" class="headerlink" title="（2）MetaGPT"></a>（2）MetaGPT</h4><p>再看2023年7月份，一个被命名为MetaGPT的项目引起了广泛关注，这个项目中定义了产品经理、架构师、项目管理员、工程师和质量保证等角色，各角色之间通过相互协作，基本可以胜任完成500行左右代码的小工程了。</p><p><img src="image/image_sT5eoOHKTD.png" alt=""></p><p>Meta GPT 最有价值的思想是<strong>借鉴人类社会中的协作方式</strong>，尤其是SOP，之于Agent 设计则平平无奇，也包括观察、思考、状态管理、任务行动以及结果反馈等等必备组件。</p><p><img src="image/image_TE1Fxfu6mX.png" alt=""></p><p>两层架构设计：</p><ol><li>基础组件层，这对于代理操作和系统范围的通信至关重要；</li><li>协作层，通过关键机制（例如知识共享和工作流封装）促进代理协调</li></ol><p>在该框架内，MetaGPT中的代理能力已经得到了显著增强。由“锚代理”所引导的专门角色提示的代理实例化，为角色提供观察、思考、反思和知识积累能力。这些角色通过已经建立的订阅和发布方法与环境进行交互。</p><h4 id="（3）实在智能TARS-RPA-Agent产品"><a href="#（3）实在智能TARS-RPA-Agent产品" class="headerlink" title="（3）实在智能TARS-RPA-Agent产品"></a>（3）实在智能TARS-RPA-Agent产品</h4><p>值得一提的是，Agent 的应用方向其实非常广泛。比如 RPA 公司实在智能把 Agent 用于他们的产品调用常见桌面软件，如淘宝网、钉钉，来自动完成桌面任务。</p><p><img src="image/image_x30kuqBdNA.png" alt=""></p><h4 id="（4）Agents开源框架"><a href="#（4）Agents开源框架" class="headerlink" title="（4）Agents开源框架"></a>（4）Agents开源框架</h4><p>而任何一个 Agent 的实现，似乎共性都挺多，都需要有长短时记忆能力、工具使用能力、通信能力，甚至包括 SOP 的能力，自然而言就有人要做这样的框架了，如 agents。</p><p><img src="image/image_jzzHmDjmGn.png" alt=""></p><h3 id="1-7-简单的难题"><a href="#1-7-简单的难题" class="headerlink" title="1.7 简单的难题"></a>1.7 简单的难题</h3><p>尽管 GPT-4 等模型非常强大、Agent的发展似乎牛气冲天，它们仍然无法满足很多任务的需要，甚至一些在我们看来很简单的任务都完成不了，比如我们构造的这个任务：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">给小学生展示一下两数相加的每一步计算过程，如1135 + 78</span><br><span class="line">答：计算详细过程如下</span><br><span class="line">5+8=13， 进位1</span><br><span class="line">3+7+1=11， 进位1</span><br><span class="line">一个数已经加完，剩余数11 + 1 = 12</span><br><span class="line">结果为：1211</span><br><span class="line">下面请列出以下两数的详细计算过程：</span><br><span class="line">81728738271872871871672 + 28781729836746721</span><br></pre></td></tr></table></figure><p>尽管AI在一定程度上模仿了人脑的工作方式，但实际上，<strong>机器人和人脑在处理信息时采用的策略有很大的不同</strong>。因此，即使在未来，也需要继续改进 AI 框架，以解决这种差距。比如一个百万位数的加法任务，GPT-4囿于token数的限制是不可能完成这个任务的，但人类却可以，这恰是人类和AI需要弥补的Gap。我们进行了一些简单的试验，还没有发现大模型和Agent能搞定这个任务。其中，ChatGPT4的Code Interpreter是表现最好的，因为它调用了外部计算器，但中间的过程描述还是发生了错误。</p><p><img src="image/image_IGscHXI0u4.png" alt=""></p><p>至此，我们已经讲述了大模型到 Agent 的发展历程。接下来的时间，我们将从人类智能的视角，结合面向目标架构的理念，分析 Agent 技术的本质、存在的缺陷以及未来可能的发展方向。</p><h2 id="2-通用智能基本原理"><a href="#2-通用智能基本原理" class="headerlink" title="2. 通用智能基本原理"></a>2. 通用智能基本原理</h2><p>首先来看看这个众人熟知的认知飞轮，感知、认知、决策、行动，今天的人工智能代理更像是基于这个认知飞轮构建的。但是从本质上，人类智能远比这复杂。</p><p><img src="image/image_nTdcfOfl7O.png" alt=""></p><p>在漫长的进化历史中，生物神经网络从简单的条件反射逐渐进化到今天的主动预测，我们已经可以在大脑中构建世界模型，进行强大的推理和分析。看似繁杂的过程，实际上都发生在核心的架构上，并且逐步完善。无论是<strong>工作记忆</strong>，还是<strong>人类处理语言的能力的诞生</strong>，这些都是智能的必不可少的元素，尤其是<strong>符号能力</strong>，对人类智能的发展有着不可替代的作用。</p><p><img src="image/image_YvUKwZ1M71.png" alt=""></p><p>因此，先提出一个更为宏观的问题，<strong>智能究竟是什么</strong>？我强烈推荐这本名为《预测算法》的书，它在20年发表，那一年，GPT 3也刚刚问世，我在阅读之后，就有这样一个感觉：<strong>生成模型是战略正确的</strong>。在之前关于AGI的分享中，也提到过这个观点，<strong>智能是通过预测来解决应对世界的不确定性的</strong>，分享视频参见这里https\://www.bilibili.com/video/BV16h4y1w79A/</p><p>深入理解一下模拟的概念，当一个低等动物接触到外界的刺激，它会收缩来逃避潜在的风险。这其实是一种模拟，只不过这个模拟反射神经元对有些过于反应敏锐，它假设所有的刺激都是潜在的危险。然而，对于人类来说，我们的模拟则更为精细。我们对世界进行建模，把世界以实体、关系、属性描绘出来。然而，这也是我们认知的极限，我们只能理解一个对象化的世界，非对象化的世界我们无法理解。比如，当我们探索量子的时候，我们还常常用对事物进行对象化的方式去理解，但是发现我们的理解力有时候是有限的，因为量子世界的真相超出了人类认知能力的范围，我们智能使用低维空间的投影去推断它，就像我们无法在三维世界去想象十一维世界的样子。</p><p><img src="image/image_Wlhb2sRGTF.png" alt=""></p><p>在过去的四十年里，科学家对认知架构有很多深入的研究，并尝试据此研发出通用人工智能，但天地不仁以万物为刍狗，当前来看只有GPT系列模型距离实现通用人工智能最近，当然这些认知理论依然具有巨大的参考和指导意义。</p><p><img src="image/image_84WlAQTyyy.png" alt=""></p><p>深入地聊认知架构和智能原理之前，我们必须要聊的是绕不开的《思考快与慢》，这是一本畅销书，其后面的学术道理也十分受用。大脑中的<strong>系统1</strong>和<strong>系统2</strong>是我们所有人都熟知的，尽管在实际实现中，系统2可能由系统1涌现，但至少在表现上，我们的大脑看起来有两个系统，系统1和系统2，分别负责不同的功能。<strong>知识和情感的快速反应被称为系统1</strong>，而<strong>逻辑性强、思考速度慢的反应被称为系统2</strong>。</p><p><img src="image/image_3_G4G0jV9f.png" alt=""></p><p><strong>GWT(Global Workspace Theory，全局工作空间理论)</strong></p><p>接下来我们看看这些认知架构中，有一个叫做<strong>GWT(Global Workspace Theory，全局工作空间理论)</strong>，如下图所示：</p><p>全局工作空间理论（GWT）是认知科学家伯纳德·巴尔斯（Bernard Baars）和斯坦·富兰克林（Stan Franklin）在20世纪80年代后期提出的一种意识思维框架。它被开发出来，以定性地解释一系列有意识和无意识过程之间的匹配。GWT在建模意识和高级认知方面具有影响力，认为它们是从广泛、并行的神经过程中信息的竞争和集成流动中产生的。</p><p>系统1涵盖了神经网络的外围连接，涉及长期记忆、价值系统、感知运动控制相关的神经网络，系统2则是一个高度集中的“舞台”，人类的有意识思考，如做数学题时，脑中想象数字相加的过程，都在这个舞台上进行。这个舞台叫全局工作空间，记忆在这个舞台上被拉进来加工，然后被扔出去。LIDA (Learning Intelligent Distribution Agent) 受到多种计算范例的启发，并且实现了GWT。认知模块包括知觉关联记忆，情景记忆，意识，程序性记忆和行动选择。由 LIDA 架构控制的认知机器人和软件代理将能够进行多种学习机制。</p><p><img src="image/image_O_hwwSJafb.png" alt=""></p><p>其实在大模型Agent技术出现之前，人们就已经意识到，试图集成各种深度学习模型以实现人工普遍智能（AGI）并不够，还需要更高层次的认知模型。Lecun在思考AGI时对大模型的出现也提出过意见，<strong>它认为世界模型才是关键</strong>，但前两天新的研究却认为大模型中有世界模型。但毫无疑问的一点是，世界模型对于我们对世界的认知是非常关键的，无论大模型中是否包含世界的认知，<strong>Agent都必须对世界有准确的理解才能做出正确的决策。当模型不能正确运行时，决策就会出错；只有当世界模型构建的正确，才能选择正确的模型，进而做出正确的决策</strong>。</p><p>总结一下，系统2包含意识、思考、符号主义、逻辑推理图灵、机制结构化和模型。而系统1包含快速思考、神经网络连接主义、长期记忆、深度学习、亚符号、潜意识和非结构化数据。在构建 Agent 时，可以参考这两种系统的思维框架。在理解智能架构的概念时，我们需要从<strong>记忆空间、符号系统、世界模型构建与加工</strong>三个方向去考虑。记忆空间是基础，符号系统是思考和推理的核心，而世界模型的构建和加工则是其中最重要的环节。在现在的大模型中，如 GPT，虽然很多人认为它没有符号系统，但我们认为，其内部的注意力机制可能已经在激活流转过程中模拟了世界模型的加工过程，只是这个过程并不显式，而且无法控制，只能通过Prompt工程引导它进行，但它会经常跑偏。</p><p><img src="image/image_wdPow5AhYm.png" alt=""></p><blockquote><p>智能要素的汇合</p></blockquote><p><img src="image/image_40__SprCBq.png" alt=""></p><blockquote><p>一种通用智能架构示意</p></blockquote><p>我们通过学习掌握了对世界的知识，并针对感知数据尝试在符号系统中构建世界模型，进行预测和行动。如弹钢琴这样的行动，我们需要通过反复训练，逐渐将运动序列内化，变成肌肉记忆和反射。这些在系统2中反复出现的行为，会逐渐沉淀到系统1中。这个过程可以理解为一个“<strong>快捷通道</strong>”的形成过程，称为<strong>Shortcut</strong>。</p><p>人的视觉识别过程是一个层次性的关系，从最初级的视觉皮层一直到更高级的皮层，从简单的视觉边缘特征到线条的方向性，再到线条之间的组合，如角等更高维特征的形成，直到形成物体的感知。这些物体的概念再对应符号系统和自然语言的绑定，当图像信息经过解码过程进入符号系统后，我们的关联记忆会帮助我们召回数字等语义概念。</p><p><img src="image/image_Aiv_VTkT2f.png" alt=""></p><p>以人类做加法为例，假设我们要解决“219 + 13”的问题，这个过程可能会遇到一个看似相同的图形，比如图中有”13”和”B”的歧义。这就打破了现在很多人的想法，通常我们喜欢做前向过程，先使用一个视觉模型处理输入，然后再将其输出传递给大模型进行处理。实际上，人在理解这个场景时是一个双向过程，<strong>首先有一些直觉的特征传入到系统2，系统2会推断这是一个做加法任务，并将看似“B”的图形解释为13</strong>，这个过程称为<strong>Projection</strong>。例如，我们经常从一些像素点中识别出人脸，这就是由上至下的功效发挥作用，这是对未来人工智能代理（Agent）的一种启发。</p><p><img src="image/image_kiJ-1pMrTw.png" alt=""></p><blockquote><p>Projection示例</p></blockquote><p>另一个关键的能力是<strong>关联记忆</strong>。当我们开始观察某个物体时，比如进行加法操作时，我们的大脑并不会以固定模式运作。相反，<strong>我们的神经网络会并行运行，有的神经网络开始将加法的概念、数字的概念以及加法规则等各种信息激活，所有这些信息都会基于一个关联网络唤醒出来</strong>，这样我们就可以开始下一步的工作。接下来就是所谓的结构推理，我们会开始将这些符号结构化，例如，如果它是一个三位数，我们就会开始理解它的每一位构成整体和部分之间的关系。</p><p><img src="image/image_GrHy3_1ilu.png" alt=""></p><blockquote><p>Structure / Grammar Inference</p></blockquote><p>当我们已经理解到219 + 13是加法时，我们也会执行Structure Inference得到结构的认知A+B=C的两位数加法结构，并将219和A对应上，13和B对应上，这个过程就是Variable Binding了，我们将具体的实例与它的角色对应上了。</p><p><img src="image/image_galRR-f6QM.png" alt=""></p><p>接着我们要遵循加法规则进行运算以实现我们的目标——完成加法任务。根据我们打算完成的目标以及现在的状态，我们需要规划出达成目标所需要的具体步骤，即执行加法规则。进入到这样一个循环过程之中，我们会额外提到两个概念，即”<strong>Shortcut</strong>“和”<strong>Exception</strong>“。</p><p>那么什么是Shortcut呢？当我们初次开始书写数字时，速度往往很慢，但随着练习，我们将逐渐写得越来越快。这个过程实际上包含了一个叫做“<strong>Recoding</strong>”的过程，我们<strong>会将熟悉的操作或流程用神经元重新表示，这样就把一个复杂的操作简化为了一个子任务，通过类似于传参的方式控制一个子神经网络完成任务</strong>。比如开车，一开始，每个动作都需要集中注意力，严重依赖系统2，但是开了一段时间之后，就可以自如地进行了，这就是因为系统2的控制能力已经被沉淀到了系统1里面，称为Shortcut。</p><p><img src="image/image_Sjt_3xm_78.png" alt=""></p><blockquote><p>Action、Shortcut、Exception</p></blockquote><p>另一个重要的方面是<strong>异常处理能力</strong>，人类最强大的能力就是能够随时应对异常。譬如，你在走路时突然被绊了一跤，你首先需要应对的就是摔倒这个状况，然后再回到原来的路线上继续走。</p><p>因此，在执行加法过程中，并不是由于一个细节被中断或遇到各种异常，才开始执行加法。我们会发现，在遇到各种问题时，我们<strong>总是会奔着目标勇往直前</strong>。人是一个运作着面向目标架构的复杂过程。面向目标架构是人类智能的一个核心机制，当然并不是唯一的。有时，我们也会没有具体的目标或者说目标不是显式的，同时有一些底层的目标机制，诸如生存，这说明人的面向目标架构要复杂许多，这就是我们不得不说的智能核心的面向目标架构。</p><h2 id="3-面向目标架构"><a href="#3-面向目标架构" class="headerlink" title="3. 面向目标架构"></a>3. 面向目标架构</h2><p>我们的情绪系统其实也在解决目标问题，例如，你会因为目标无法达成而生气，因为目标可能无法达成焦虑，因为别阻碍你的目标而愤怒。显而易见，许多情绪都与目标机制有所关联。因此，这套面向目标的机制在人的智能运作中占有极其核心的地位。</p><p><img src="image/image_XYIGVZrDSG.png" alt=""></p><blockquote><p>目标驱动机制</p></blockquote><p>让我们通过一个简单的模型来描述该机制。首先，我们需要对这个世界有理解，因此我们会在脑中构建一个关于世界的模型。这个模型在结构化之后，就会变成了当前世界状态。而我们的目标是对应的一个目标世界状态。因此，人类就是在不停地消除当前状态和目标状态之间的差异，这个消除的过程就是目标驱动的过程。</p><p>在目标驱动的过程中，你开始尝试去解决这个问题，消除这个差异，你也可能有现成的解决方案，直接动用已有的解决方案执行已知的运动序列，也可能需要进行一定的思考，做出推理分析帮助你解决问题。</p><p>一旦你找到了一些执行序列，这些序列可能会变成一个子序列，子序列里有子目标。每个子目标的执行有可能是直接完成的，也可能需要进一步思考才能完成。正如我们可以看到，GPS这段代码就是在为了达成某一个目标而工作，它会遍历所有的目标，尝试让每一个目标都能够达成，一旦达成就结束。有兴趣的同学可以读一下这个代码，就是做暴力遍历找出达到目标状态的操作序列。</p><p><img src="image/image_qqn3XxqELt.png" alt=""></p><p><img src="image/image__MvmP0MS4v.png" alt=""></p><p>不过，<strong>像GPS这种理想的解决方案在现实世界中可能并不奏效，因为真实世界的解空间过于庞大</strong>，想想AlphaGo的故事就理解了，这也是为什么虽然此想法在理论上看起来很好，但在实际操作时却无法实施。</p><p>但这种思考很有启发，在Newell和Simon1972年出版的《Human Problem Solving》一书中，他们研究了人类如何解决问题，并意识到我们经常进行手段-目的分析(means-ends)</p><p>举一个例子：</p><blockquote><p>“我想把儿子送到幼儿园。我现在的状态和我想要的状态之间有什么区别？其中一个是距离。<br>是什么因素会改变距离？我的汽车。可是我的汽车坏了。要让它工作需要什么？一个新电池。<br>哪里能买到新电池？汽车修理店。我想让修理店为我安装一个新电池，但店里不知道我需要一个新电池。问题出在哪里？是沟通的问题。什么能让沟通变得容易？一部电话……以此类推。”</p></blockquote><p>在计算机领域，有很多方法都与目标机制相关。例如，<strong>过程描述语言（PDL）</strong> 就是一种经典的方法，主要用于解决机器人问题。我们可以描述世界上的对象，它们当前的状态是怎样的，目标状态是怎样的，有哪些可以采取的操作，然后我们可以基于这些操作，使用规划器寻找一个合适的运动序列来解决问题。</p><p><img src="image/image_30SuS7GleN.png" alt=""></p><blockquote><p>PDDL</p></blockquote><p>但在今天计算机领域的工程实践中，人们更多采用的是面向过程架构，无论是接口、函数、UI界面，还是组件，又或者是一个应用程序，都是以接口的形式存在的。而这个接口实质上是一种被调用的子流程，借此过程的完成，我们希望执行结果符合我们的预期，但程序并不为结果负责。它<strong>解决的是过程和流程问题</strong>，系统内没有目标的概念。</p><p><img src="image/image_NBWWfsrdM_.png" alt=""></p><blockquote><p>面向过程架构（Process Oriented Architure）</p></blockquote><p>当然，也存在一些以目标导向为核心理念的的软件工程，例如声明式编程，它只需要你描述你想要什么，而无需关心执行的过程，像HTML和SQL便是其经典例子。在这样的架构下，程序能够自行寻找达成目标的方法。</p><p><img src="image/image_t8R-NI8mmS.png" alt=""></p><blockquote><p>命令式编程 vs 声明式编程</p></blockquote><p>然而问题在于，这种面向目标的架构只能应用于垂直领域，而<strong>无法普遍应用到所有领域，只有在特定的领域内才能发挥作用，这就限制了它的应用范围</strong>。 &#x20;</p><p><img src="image/image_oQBHEXCNED.png" alt=""></p><blockquote><p>面向过程架构 vs 面向目标架构</p></blockquote><p>总的来说，尽管面向目标架构在计算机领域有一席之地，但由于其只能在特定领域发挥作用，而无法解决所有领域的问题，因此它的应用还是有所限制，更多出现在特定的DSL（领域特定语言）中，这种架构的确也发挥了巨大的作用。在软件工程的范式迁移中，我们发现<strong>面向过程架构与面向目标架构之间的重要区别点：随着人类的生产方式的变化，软件工程可能正逐步演化为智能体工程(Agent Engineering)；</strong> 以前我们主导的生产方式是人类处于中心位，AI做辅助。而未来可能会变成以 AI 为中心，人类变为辅助。由此，整个产品形态和平台的构成可能会发生这样的转变。</p><p>在这一转变中，原本由人类主导的功能开发，逐渐演变为以智能体为主要驱动力。传统的用户界面，由于其垂直的任务层级架构，每一层都需要人类逐一生成，未来这个过程可能会被智能体自主生成并改良。此外，原本只能解决有限范围的任务，未来的架构则可以解决无限域的任务。就如同头条这样的平台，它是一个信息的分发平台。那么，是否会出现新的平台模式？比如一种知识和世界模型的分发平台。以前我们只能处理大量长尾数据，在未来可能能解决大量长尾任务。以前是廉价的规模化加昂贵的个性化，以后是廉价的规模化的个性化。</p><h2 id="4-前瞻性分析"><a href="#4-前瞻性分析" class="headerlink" title="4. 前瞻性分析"></a>4. 前瞻性分析</h2><p>根据上面的分析，我们能看到 Agent 技术在未来的发展还有很大的提升空间。我认为，这些提升主要可以从几个方向开始，包括引入<strong>中央执行机构、学习能力、输入感知、输出执行、世界模型和记忆</strong>等几个方面，这些构成因素是完备非正交的，都对提升 AI 技术至关重要。</p><p><img src="image/image_0h2EqEVCV2.png" alt=""></p><h3 id="4-1-Central-Executive"><a href="#4-1-Central-Executive" class="headerlink" title="4.1 Central Executive"></a>4.1 Central Executive</h3><p>中央执行机构，这是一个核心的概念，但常常被人们忽视。现在的 Agent 只是一个规划器，它负责做规划。但实际上，这个流程中还存在很多未明确的问题，比如，<strong>是否存在一个内部加工过程</strong>，以及这个过程是否透明可控等。一种可能的解决办法是，将内部加工过程外部化，用系统2包裹起来，使每一步细粒度的思考都可以展现出来。</p><p><img src="image/image_6efpF9_-ZX.png" alt=""></p><p>其次是<strong>世界模型，</strong>现在的大模型只能输入语言，显然这样是不够的，进一步理解世界需要多模态输入。这是我们在未来需要处理的关键问题。同样地，对于时间和自身的身体运动控制的认知也需要能够输入到大模型里面去。我们观察到，无论是自动驾驶汽车、大模型Agent，还是其他的诸多智能体模型，都已经在应用这种<strong>面向目标的架构</strong>。目前的挑战在于如何在细节上加以改进，如找出此架构未能完成某些任务的原因，以及这些缺陷是源于大模型底层的子任务能力不足，还是需要对框架本身做出改进，比如增加更多的思考层次，或加入更多的内部推演等。</p><p>另一个重要的问题是<strong>宏观注意力</strong>，由于大模型的上下文限制，是否可以让模型自身主动去探索外部世界，将其精力和注意力主动地投入到解答某些具有目标性的问题上去，实现主动的注意力机制？这不仅涉及到搜索和尝试的问题，如针对一些无法思考出解决方案的情况，模型应如何去进行尝试，而且这些尝试何时能够带来进步，以及如何去寻找更为优秀的解决空间，进行推理和规划。</p><h3 id="4-2-Memory"><a href="#4-2-Memory" class="headerlink" title="4.2 Memory"></a>4.2 Memory</h3><p>值得注意的是，数学和逻辑学习也会涉及到上述问题，比如人类在很多情况下不擅长规划，那么我们是否可以利用网络和记忆机制来实现规划的功能？这其中就涉及到<strong>记忆的内化</strong>，也就是把大模型从外部世界获取的经验转化为内部参数，或者说把这些经验转化为内存。</p><p><img src="image/image_n3d4HYLcZ1.png" alt=""></p><p>目前，我们依赖的记忆机制主要是把所有的信息存储在历史记录里，然后在需要的时候进行召回。然而，这些信息并未经过整理，在一些试图<strong>整理记忆</strong>的尝试中，我们发现人类是具有这种能力的。人类在获得大量相关的知识后，不会简单地把它们堆积在脑中，因为人的神经元存储空间是有限的。相反，人脑会通过海马体进行整理，而在我们做梦时，大脑会重新构造这些相关的知识，使得记忆网络变得有序。</p><p>目前还未见到具有<strong>遗忘功能</strong>的模型，也就是删掉一些垃圾信息或错误的信息。在大模型训练过程中，产生了许多无用甚至是错误的信息，而我们在工作中只是采用了许多方式来规避这些错误的信息，但为什么不试图去删掉它们呢？如果能够将这些信息替换为有价值的信息，那将是一件有价值的事。我注意到在人工智能领域中，<strong>对于长短时记忆与工作记忆</strong>，以及它们之间的关系讨论并不深入，更常见的是，人们将长短时记忆简化为向量数据库。我想解决这个问题，尝试对这两者进行深层次的理解，并建立更完备，更正交的关系也很重要。</p><h3 id="4-3-Sensory"><a href="#4-3-Sensory" class="headerlink" title="4.3 Sensory"></a>4.3 Sensory</h3><p>当人工智能Agent融入人类生活后，它与我们的体验和经历能否成为Agent自身的存储内容？如果可以，那么在未来，我们与Agent之间的互动将会变得更加实用，更加贴近现实生活，更加有温度。</p><p><img src="image/image_aESTrhWcLW.png" alt=""></p><p>在输入的问题上，我明确地看到了<strong>多模态</strong>输入的必要性，同时，对于时间感知我认为也非常重要，时间性对于运动控制任务极其重要。引入多模态输入后，我们还要解决一个<strong>自上而下</strong>的机制问题，就是Projection启发的这个点，OCR嫁接术一定会在某类任务存在缺陷。</p><h3 id="4-4-Motor"><a href="#4-4-Motor" class="headerlink" title="4.4 Motor"></a>4.4 Motor</h3><p>在交流方式上，我认为不应仅仅依赖于语言，虽然现在的交流基本都是基于语言的，但是，语言是一个低带宽且低效的通信工具。我在想，我们能否引入一种新的沟通方式 - 类似心灵感应的方式，让Agent在隐空间通信。</p><p><img src="image/image_iKS3hRuQOo.png" alt=""></p><p>关于运动控制，当前的方式包括一些机器人应用，都比较<strong>结构化</strong>。但我认为，在未来，大模型的神经网络应该可以直接连接到运动控制的神经网络，实现层次化控制，使得运动更为流畅，甚至比人类更为灵活。</p><p>在另一方面，运动控制也应该是<strong>数据化</strong>的，而不是仅仅处于我们所说的”计划者“的层面。如果有一个命令下达，神经网络应该可以直接执行。</p><p>除此之外，还有一些<strong>亚符号的控制</strong>，在大模型直接对接神经网络时，我们应当避免通过语言来描述，因为我们可以通过这种方式得到的信息量会比通过语言描述来得多。</p><p>同时，也需要进行一些<strong>外部工具的优化</strong>，让现有的工具更适应我们的需求，比如一些愿意为了方便Agent调用进行改造的工具服务商将会在新的价值网络中占据一席之地，如一个旅游服务供应商，加入下一代Agent平台之后，Agent在完成用户旅游类任务时可能会有限调用它，并使用类似Web3的技术进行价值分配。</p><h3 id="4-5-Learning"><a href="#4-5-Learning" class="headerlink" title="4.5 Learning"></a>4.5 Learning</h3><p>任何一个产品，或者说Agent，都需要学习。学习的过程是十分重要的，尤其是模型需要学会对自身的可靠性进行判断，<strong>知道自己知道什么，更重要的是，知道自己并不知道什么</strong>，不擅长什么，这将会对模型的发展产生重大影响。关于大型模型的优化，我认为最关键的问题就在于<strong>模型需要明确自己的能力范围</strong>。有些问题，大模型不能张口就来直接给出答案，过于逞能，它应该经过仔细的思考，保证任务目标的准确达成。</p><p><img src="image/image_oz_-FIxgX5.png" alt=""></p><p>同时，我们也需要考虑模型的<strong>权威性</strong>问题。大模型可能从互联网和垃圾信息中学到很多知识，但这并不意味着它在解决问题时能提供最权威、最佳的做法。我们需要把这个模型训练到，即使是在面对垃圾信息输入时，它也能输出更好的、更有价值的解决方案。</p><p>另一方面，我们还需要考虑到<strong>模型的多样性</strong>。很多时候，为了保证任务的有效执行，往往会控制模型的温度参数，以保持其输出的稳定性。但是，在保证模型正确性的同时，我们也不应该忽略它的思维活跃度。我们应允许智能体在解决任务时有更大的解空间，以便找到最优的解决方案。</p><h3 id="4-6-World-Models"><a href="#4-6-World-Models" class="headerlink" title="4.6 World Models"></a>4.6 World Models</h3><p>关于世界模型 ，需要注意的是，尽管模型的训练数据中可能含有很多垃圾信息和错误信息，我们还需要<strong>让模型具有辨别和整理这些信息的能力</strong>，以构建一个无矛盾、统一的实体网络，这一点鲜被提及，我认为现在黯然神伤的之前做知识图谱的同学可以重点考虑一下这个方向。</p><p><img src="image/image_LupLNzojvr.png" alt=""></p><p>在此基础上，还需要让模型具备<strong>推理能力</strong>。一个优秀的智能体不应该仅仅依赖于内部推理，而应该有能力借助外部推理，当然这个外部推理可以当做工具来使用。</p><p>最后，我们还必须强化模型的<strong>内部思考机制</strong>。当调用一些有成本的接口时，模型不能只是“想到就做到”，而应该有自我觉知的能力，或者叫Mental Simulation，预判自己的行动可能会带来的结果，并在内部进行纠错，以保证行动的可靠性，这不同于Reflection是执行后根据执行结果再反思。进一步，我们可能更大的关注点应该是它在家庭生活及现实社会中的应用上，将其实现为实体化的机器人，那么<strong>动力学机制和时间性认知</strong>还是很重要的，而当前的大模型仅是一个简单的循环调用，无法实现这方面的任务。</p><p>好，以上就是我对一些方向的浅显思考。</p><p>最后，我们以伟人的一段话来结尾：Agent 技术，它是站在海岸遥望海中已经看得见桅杆尖头了的一只航船，它是立于高山之巅远看东方已见光芒四射喷薄欲出的一轮朝日，它是躁动于母腹中的快要成熟了的一个婴儿。</p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ol><li>Wikipedia Agent. <a href="https://en.wikipedia.org/wiki/Intelligent_agent" title="https://en.wikipedia.org/wiki/Intelligent_agent">https://en.wikipedia.org/wiki/Intelligent_agent</a></li><li>Intelligent Agents 综述 <a href="https://vsis-www.informatik.uni-hamburg.de/getDoc.php/publications/373/INTELLIGENT_AGENTS_v7_final.pdf" title="https://vsis-www.informatik.uni-hamburg.de/getDoc.php/publications/373/INTELLIGENT_AGENTS_v7_final.pdf">https://vsis-www.informatik.uni-hamburg.de/getDoc.php/publications/373/INTELLIGENT_AGENTS_v7_final.pdf</a></li><li>Prompt经典收集。<a href="https://github.com/f/awesome-chatgpt-prompts" title="https://github.com/f/awesome-chatgpt-prompts">https://github.com/f/awesome-chatgpt-prompts</a></li><li>LLM+P: Empowering Large Language Models with Optimal Planning Proficiency</li><li><a href="https://github.com/Cranial-XIX/llm-pddl" title="https://github.com/Cranial-XIX/llm-pddl">https://github.com/Cranial-XIX/llm-pddl</a></li><li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</li><li>Self-Consistency Improves Chain of Thought Reasoning in Language Models</li><li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</li><li>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</li><li>Cumulative Reasoning with Large Language Models</li><li>ReAct: Synergizing Reasoning and Acting in Language Models</li><li>Reflexion: Language Agents with Verbal Reinforcement Learning</li><li><a href="https://openai.com/blog/function-calling-and-other-api-updates" title="https://openai.com/blog/function-calling-and-other-api-updates">https://openai.com/blog/function-calling-and-other-api-updates</a></li><li>人大综述https\://arxiv.org/pdf/2308.11432.pdf</li><li>复旦综述 <a href="https://arxiv.org/pdf/2309.07864.pdf" title="https://arxiv.org/pdf/2309.07864.pdf">https://arxiv.org/pdf/2309.07864.pdf</a></li><li><a href="https://github.com/Significant-Gravitas/AutoGPT" title="https://github.com/Significant-Gravitas/AutoGPT">https://github.com/Significant-Gravitas/AutoGPT</a></li><li><a href="https://github.com/microsoft/JARVIS" title="https://github.com/microsoft/JARVIS">https://github.com/microsoft/JARVIS</a></li><li>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</li><li>GPT-Researcher <a href="https://github.com/assafelovic/gpt-researcher" title="https://github.com/assafelovic/gpt-researcher">https://github.com/assafelovic/gpt-researcher</a></li><li>RecurrentGPT <a href="https://arxiv.org/abs/2305.13304" title="https://arxiv.org/abs/2305.13304">https://arxiv.org/abs/2305.13304</a></li><li>Voyager <a href="https://arxiv.org/abs/2305.16291" title="https://arxiv.org/abs/2305.16291">https://arxiv.org/abs/2305.16291</a></li><li><a href="https://github.com/OpenBMB/XAgent" title="https://github.com/OpenBMB/XAgent">https://github.com/OpenBMB/XAgent</a></li><li>斯坦福小镇代码 <a href="https://github.com/joonspk-research/generative_agents" title="https://github.com/joonspk-research/generative_agents">https://github.com/joonspk-research/generative_agents</a></li><li>斯坦福小镇论文 Generative Agents: Interactive Simulacra of Human Behavior</li><li>MetaGPT代码 <a href="https://github.com/geekan/MetaGPT" title="https://github.com/geekan/MetaGPT">https://github.com/geekan/MetaGPT</a></li><li>MetaGPT论文 <a href="https://arxiv.org/pdf/2308.00352.pdf" title="https://arxiv.org/pdf/2308.00352.pdf">https://arxiv.org/pdf/2308.00352.pdf</a></li><li><a href="https://github.com/OpenBMB/ChatDev" title="https://github.com/OpenBMB/ChatDev">https://github.com/OpenBMB/ChatDev</a></li><li><a href="https://github.com/OpenBMB/AgentVerse" title="https://github.com/OpenBMB/AgentVerse">https://github.com/OpenBMB/AgentVerse</a></li><li><a href="https://arxiv.org/pdf/2307.07924.pdf" title="https://arxiv.org/pdf/2307.07924.pdf">https://arxiv.org/pdf/2307.07924.pdf</a></li><li>Agents: An Open-source Framework for Autonomous Language Agents</li><li><a href="https://lilianweng.github.io/posts/2023-06-23-agent/" title="https://lilianweng.github.io/posts/2023-06-23-agent/">https://lilianweng.github.io/posts/2023-06-23-agent/</a></li><li>Phase transitions of brain evolution that produced human language and beyond</li><li>A Review of 40 Years in Cognitive Architecture Research Core Cognitive Abilities and Practical Applications</li><li>LIDA: A Computational Model of Global Workspace Theory and Developmental Learning</li><li><a href="https://hal.science/hal-03311492/document" title="https://hal.science/hal-03311492/document">https://hal.science/hal-03311492/document</a></li><li><a href="https://ai.meta.com/blog/yann-lecun-advances-in-ai-research/" title="https://ai.meta.com/blog/yann-lecun-advances-in-ai-research/">https://ai.meta.com/blog/yann-lecun-advances-in-ai-research/</a></li><li>Projection: A Mechanism for Human-like Reasoning in Artificial Intelligence</li><li><a href="https://en.wikipedia.org/wiki/Planning_Domain_Definition_Language" title="https://en.wikipedia.org/wiki/Planning_Domain_Definition_Language">https://en.wikipedia.org/wiki/Planning_Domain_Definition_Language</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RAG（检索增强生成）技术</title>
      <link href="/llms/llms_article/5.RAG%E6%8A%80%E6%9C%AF/"/>
      <url>/llms/llms_article/5.RAG%E6%8A%80%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1.基本概念"></a>1.基本概念</h1><p><strong>检索增强 LLM ( Retrieval Augmented LLM )</strong>，简单来说，<strong>就是给 LLM 提供外部数据库，对于用户问题 ( Query )，通过一些信息检索 ( Information Retrieval, IR ) 的技术，先从外部数据库中检索出和用户问题相关的信息，然后让 LLM 结合这些相关信息来生成结果</strong>。下图是一个检索增强 LLM 的简单示意图。</p><p><img src="image/lr3r0h6wjf_IFAt46kMTh.png" alt=""></p><p>传统的信息检索工具，比如 Google/Bing 这样的搜索引擎，只有检索能力 ( <strong>Retrieval-only</strong> )，现在 LLM 通过预训练过程，将海量数据和知识嵌入到其巨大的模型参数中，具有记忆能力 ( <strong>Memory-only</strong> )。从这个角度看，检索增强 LLM 处于中间，将 LLM 和传统的信息检索相结合，通过一些信息检索技术将相关信息加载到 LLM 的工作内存 ( <strong>Working Memory</strong> ) 中，即 LLM 的上下文窗口 ( <strong>Context Window</strong> )，亦即 LLM 单次生成时能接受的最大文本输入。</p><h1 id="2-RAG解决的问题"><a href="#2-RAG解决的问题" class="headerlink" title="2.RAG解决的问题"></a>2.RAG解决的问题</h1><blockquote><p>参考资料：ACL 2023 Tutorial: Retrieval-based Language Models and Applications</p></blockquote><h3 id="（1）长尾知识："><a href="#（1）长尾知识：" class="headerlink" title="（1）长尾知识："></a>（1）长尾知识：</h3><p><strong>对于一些相对通用和大众的知识，LLM 通常能生成比较准确的结果，而对于一些长尾知识</strong>，LLM 生成的回复通常并不可靠。ICML 会议上的这篇论文 <a href="https://arxiv.org/abs/2211.08411" title="Large Language Models Struggle to Learn Long-Tail Knowledge">Large Language Models Struggle to Learn Long-Tail Knowledge</a>，就研究了 LLM 对基于事实的问答的准确性和预训练数据中相关领域文档数量的关系，发现有很强的相关性，即<strong>预训练数据中相关文档数量越多，LLM 对事实性问答的回复准确性就越高</strong>。从这个研究中可以得出一个简单的结论 ——** LLM 对长尾知识的学习能力比较弱**。下面这张图就是论文中绘制的相关性曲线。</p><p>为了提升 LLM 对长尾知识的学习能力，容易想到的是<strong>在训练数据加入更多的相关长尾知识，或者增大模型的参数量</strong>，虽然这两种方法确实都有一定的效果，上面提到的论文中也有实验数据支撑，但这<strong>两种方法是不经济的</strong>，即需要一个很大的训练数据量级和模型参数才能大幅度提升 LLM 对长尾知识的回复准确性。而通<strong>过检索的方法把相关信息在 LLM 推断时作为上下文 ( Context ) 给出</strong>，既能达到一个比较好的回复准确性，也是一种<strong>比较经济的方式</strong>。</p><h3 id="（2）私有数据"><a href="#（2）私有数据" class="headerlink" title="（2）私有数据"></a>（2）私有数据</h3><p>ChatGPT 这类通用的 LLM 预训练阶段利用的大部分都是公开的数据，<strong>不包含私有数据，因此对于一些私有领域知识是欠缺的</strong>。比如问 ChatGPT 某个企业内部相关的知识，ChatGPT 大概率是不知道或者胡编乱造。虽然可以在预训练阶段加入私有数据或者利用私有数据进行微调，但训练和迭代成本很高。此外，有研究和实践表明，<strong>通过一些特定的攻击手法，可以让 LLM 泄漏训练数据，如果训练数据中包含一些私有信息，就很可能会发生隐私信息泄露</strong>。</p><p><strong>如果把私有数据作为一个外部数据库，让 LLM 在回答基于私有数据的问题时，直接从外部数据库中检索出相关信息，再结合检索出的相关信息进行回答</strong>。这样就不用通过预训练或者微调的方法让 LLM 在参数中记住私有知识，既节省了训练或者微调成本，也一定程度上避免了私有数据的泄露风险。</p><h3 id="（3）数据新鲜度"><a href="#（3）数据新鲜度" class="headerlink" title="（3）数据新鲜度"></a>（3）数据新鲜度</h3><p>由于 LLM 中学习的知识来自于训练数据，虽然大部分知识的更新周期不会很快，但依然会有一些知识或者信息更新得很频繁。<strong>LLM 通过从预训练数据中学到的这部分信息就很容易过时</strong>。</p><p>如果<strong>把频繁更新的知识作为外部数据库，供 LLM 在必要的时候进行检索，就可以实现在不重新训练 LLM 的情况下对 LLM 的知识进行更新和拓展，从而解决 LLM 数据新鲜度的问题</strong>。</p><h3 id="（4）来源验证和可解释性"><a href="#（4）来源验证和可解释性" class="headerlink" title="（4）来源验证和可解释性"></a>（4）来源验证和可解释性</h3><p>通常情况下，LLM 生成的输出不会给出其来源，比较难解释为什么会这么生成。而<strong>通过给 LLM 提供外部数据源，让其基于检索出的相关信息进行生成，就在生成的结果和信息来源之间建立了关联，因此生成的结果就可以追溯参考来源，可解释性和可控性就大大增强</strong>。即可以知道 LLM 是基于什么相关信息来生成的回复。</p><p>利用检索来增强 LLM 的输出，其中很重要的一步是通过一些检索相关的技术从外部数据中找出相关信息片段，然后把相关信息片段作为上下文供 LLM 在生成回复时参考。有人可能会说，随着 LLM 的上下文窗口 ( <strong>Context Window</strong> ) 越来越长，检索相关信息的步骤是不是就没有必要了，直接在上下文中提供尽可能多的信息。</p><h1 id="3-RAG关键模块"><a href="#3-RAG关键模块" class="headerlink" title="3.RAG关键模块"></a>3.RAG关键模块</h1><p>为了构建检索增强 LLM 系统，需要实现的关键模块和解决的问题包括:</p><ul><li><strong>数据和索引模块</strong>：<strong>将多种来源、多种类型和格式的外部数据转换成一个统一的文档对象</strong> ( Document Object )，便于后续流程的处理和使用。文档对象除了包含原始的文本内容，一般还会携带文档的<strong>元信息 ( Metadata )</strong>，<strong>可以用于后期的检索和过滤</strong>。</li><li><strong>查询和检索模块</strong>：如何准确高效地检索出相关信息</li><li><strong>响应生成模块</strong>：如何利用检索出的相关信息来增强 LLM 的输出</li></ul><h1 id="4-几种RAG的调用模式"><a href="#4-几种RAG的调用模式" class="headerlink" title="4.几种RAG的调用模式"></a>4.几种RAG的调用模式</h1><p><img src="image/image_CKUDPzYIzu.png" alt=""></p><p><strong>模式一：</strong> 非结构化数据通过Embedding Model把非结构化数据进行embedding存到向量数据库中，然后形成Construct Prompts给到LLM。LLM返回结果给到用户。</p><p><strong>模式二：</strong> 用户提出问题，下一步把问题通过Embedding Model向量化，然后保存到长时记忆数据库（向量数据库）中，然后调用LLM完成问题的回答，接下来将大模型的回答存到长时记忆数据库中，最后返回给用户。</p><p><strong>模式三：</strong> 用户问问题，下一步把问题通过Embedding Model向量化，然后从Cache中（向量数据库）查询类似的问题和答案，返回给用户。如果没有命中，则去和LLM交互。然后把LLM的回答存到Cache中，最后把回答返回给用户。</p><p>这三种形式就是典型的RAG的调用模式。它可以解决不同类型的数据如何让大模型知道的问题，同时在性能和效率上得到了提高，解决了长时记忆的问题，幻觉问题也有很大改善。</p><h1 id="5-RAG-vs-SFT"><a href="#5-RAG-vs-SFT" class="headerlink" title="5.RAG vs. SFT"></a>5.RAG vs. SFT</h1><div class="table-container"><table><thead><tr><th></th><th>RAG</th><th>SFT传统方法</th></tr></thead><tbody><tr><td>数据</td><td>动态数据。 RAG 不断查询外部源，确保信息保持最新，而无需频繁的模型重新训练。</td><td>(相对)静态数据，并且在动态数据场景中可能很快就会过时。 SFT 也不能保证记住这些知识。</td></tr><tr><td>外部知识库</td><td>RAG 擅长利用外部资源。通过在生成响应之前从知识源检索相关信息来增强 LLM 能力。 它非常适合文档或其他结构化/非结构化数据库。</td><td>SFT 可以对 LLM 进行微调以对齐预训练学到的外部知识，但对于频繁更改的数据源来说可能不太实用。</td></tr><tr><td>模型定制</td><td>RAG 主要关注信息检索，擅长整合外部知识，但可能无法完全定制模型的行为或写作风格。</td><td>SFT 允许根据特定的语气或术语调整LLM 的行为、写作风格或特定领域的知识。</td></tr><tr><td>缓解幻觉</td><td>RAG 本质上不太容易产生幻觉，因为每个回答都建立在检索到的证据上。</td><td>SFT 可以通过将模型基于特定领域的训练数据来帮助减少幻觉。 但当面对不熟悉的输入时，它仍然可能产生幻觉。</td></tr><tr><td>透明度</td><td>RAG 系统通过将响应生成分解为不同的阶段来提供透明度，提供对数据检索的匹配度以提高对输出的信任。</td><td>SFT 就像一个黑匣子，使得响应背后的推理更加不透明。</td></tr><tr><td>相关技术</td><td>RAG 需要高效的检索策略和大型数据库相关技术。另外还需要保持外部数据源集成以及数据更新。</td><td>SFT 需要准备和整理高质量的训练数据集、定义微调目标以及相应的计算资源。</td></tr></tbody></table></div><p>与预训练或微调基础模型等传统方法相比，RAG 提供了一种经济高效的替代方法。RAG 从根本上增强了大语言模型在响应特定提示时直接访问特定数据的能力。为了说明 RAG 与其他方法的区别，请看下图。雷达图具体比较了三种不同的方法：预训练大语言模型、预训练 + 微调 LLM 、预训练 + RAG LLM。</p><p><img src="image/image_DRfFTG22sc.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLaMA系列模型架构</title>
      <link href="/llms/llms_article/3.llama%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B/"/>
      <url>/llms/llms_article/3.llama%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="1-LLama"><a href="#1-LLama" class="headerlink" title="1.LLama"></a>1.LLama</h1><h2 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h2><p>Open and Efficient Foundation Language Models (Open但没完全Open的LLaMA)&#x20;</p><p>2023年2月，Meta（原Facebook）推出了LLaMA大模型，使用了1.4T token进行训练，虽然最大模型只有65B，但在相关评测任务上的效果可以媲美甚至超过千亿级大模型，被认为是近期开源大模型百花⻬放的开端之一，“羊驼”系列模型及其生态快速发展。</p><p>LLaMA 所采用的 Transformer 结构和细节，与标准的 Transformer 架构不同的地方包括采用了<strong>前置层归一化（Pre-normalization）</strong>并使用 <strong>RMSNorm 归一化函数</strong> （Normalizing Function）、激活函数更换为<strong> SwiGLU</strong>，并使用了<strong>旋转位置嵌入（RoP）</strong>，整体 Transformer 架构与 GPT-2 类似。</p><p><img src="image/image_V-mY0ArIbu.png" alt=""></p><h2 id="1-2-RMSNorm归一化函数"><a href="#1-2-RMSNorm归一化函数" class="headerlink" title="1.2 RMSNorm归一化函数"></a>1.2 RMSNorm归一化函数</h2><p><strong>为了使得模型训练过程更加稳定</strong>，GPT-2 相较于 GPT 就引入了<strong>前置层归一化方法</strong>，将第一个层归一化移动到多头自注意力层之前，第二个层归一化也移动到了全连接层之前，同时残差连接的位置也调整到了多头自注意力层与全连接层之后。层归一化中也采用了 <strong>RMSNorm 归一化函数</strong>。 针对输入向量 aRMSNorm 函数计算公式如下</p><script type="math/tex; mode=display">R M S(a)=\sqrt{\frac{1}{n} \sum_{i=1}^{n} a_{i}^{2}}</script><script type="math/tex; mode=display">\bar{a}_{i}=\frac{a_{i}}{R M S(\boldsymbol{a})}</script><p>此外，RMSNorm 还可以引入可学习的缩放因子 $ g_<br>i  $和偏移参数 $b_i$，从而得到 $\bar{a}_{i}=\frac{a_{i}}{\operatorname{RMS}(\boldsymbol{a})} g_{i}+b_{i}$。 RMSNorm 在 HuggingFace Transformer 库中代码实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaRMSNorm</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, eps=<span class="number">1e-6</span></span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    LlamaRMSNorm is equivalent to T5LayerNorm </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span> </span><br><span class="line">    <span class="built_in">super</span>().__init__() </span><br><span class="line">    self.weight = nn.Parameter(torch.ones(hidden_size)) </span><br><span class="line">    self.variance_epsilon = eps <span class="comment"># eps 防止取倒数之后分母为 0 </span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states</span>): </span><br><span class="line">    input_dtype = hidden_states.dtype </span><br><span class="line">    variance = hidden_states.to(torch.float32).<span class="built_in">pow</span>(<span class="number">2</span>).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon) <span class="comment"># weight 是末尾乘的可训练参数, 即 g_i </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (self.weight * hidden_states).to(input_dtype)</span><br></pre></td></tr></table></figure><h2 id="1-3-SwiGLU计划函数"><a href="#1-3-SwiGLU计划函数" class="headerlink" title="1.3 SwiGLU计划函数"></a>1.3 SwiGLU计划函数</h2><p>SwiGLU激活函数是相较于 ReLU 函数在大部分评测中都有不少提升。在 LLaMA 中全连接层 使用带有 SwiGLU 激活函数的 FFN（Position-wise Feed-Forward Network）的计算公式如下：</p><script type="math/tex; mode=display">\operatorname{FFN}_{\text {SwiGLU }}\left(\boldsymbol{x}, \boldsymbol{W}, \boldsymbol{V}, \boldsymbol{W}_{2}\right)=\operatorname{SwiGLU}(\boldsymbol{x}, \boldsymbol{W}, \boldsymbol{V}) \boldsymbol{W}_{2}</script><script type="math/tex; mode=display">\operatorname{SwiGLU}(\boldsymbol{x}, \boldsymbol{W}, \boldsymbol{V})=\operatorname{Swish}_{\beta}(x \boldsymbol{W}) \otimes \boldsymbol{x} \boldsymbol{V}</script><script type="math/tex; mode=display">\operatorname{Swish}_{\beta}(\boldsymbol{x})=\boldsymbol{x} \sigma(\boldsymbol{\beta} \boldsymbol{x})</script><p>其中，$σ(x)$ 是 Sigmoid 函数。下图给出了 Swish 激活函数在参数 $β$ 不同取值下的形状。可以看 到当 $β$ 趋近于 0 时，Swish 函数趋近于线性函数 $y = x$，当 $ β  $趋近于无穷大时，Swish 函数趋近于 ReLU 函数，$β$ 取值为 1 时，Swish 函数是光滑且非单调。在 HuggingFace 的 Transformer 库中 Swish1 函数使用 silu 函数代替。</p><p><img src="image/image_3VP6MMucO_.png" alt=""></p><p>LLaMA中直接将FFN中的ReLU替换为SwiGLU，并将维度放缩为$(2/3) ⋅ 4d$</p><p><img src="image/image_GfepsrgfTq.png" alt=""></p><h2 id="1-4-旋转位置嵌入（RoPE）"><a href="#1-4-旋转位置嵌入（RoPE）" class="headerlink" title="1.4 旋转位置嵌入（RoPE）"></a>1.4 旋转位置嵌入（RoPE）</h2><p>在位置编码上，使用旋转位置嵌入（Rotary Positional Embeddings，RoPE）代替原有的绝 对位置编码。RoPE 借助了<strong>复数的思想</strong>，出发点是<strong>通过绝对位置编码的方式实现相对位置编码</strong>。其目标是通过下述运算来给 <code>q</code>，<code>k</code> 添加绝对位置信息：</p><script type="math/tex; mode=display">\tilde{\boldsymbol{q}}_{m}=f(\boldsymbol{q}, m), \tilde{\boldsymbol{k}}_{n}=f(\boldsymbol{k}, n)</script><p>经过上述操作后，$\tilde{\boldsymbol{q}}_{m}$和$\tilde{\boldsymbol{k}}_{n}$就带有位置m和n的绝对位置信息。</p><p>最终可以得到二维情况下用复数表示的 RoPE：</p><script type="math/tex; mode=display">f(\boldsymbol{q}, m)=R_{f}(\boldsymbol{q}, m) e^{i \Theta_{f}(\boldsymbol{q}, m)}=\|\boldsymbol{q}\| e^{i(\Theta(\boldsymbol{q})+m \theta)}=\boldsymbol{q} e^{i m \theta}</script><p>根据复数乘法的几何意义，上述变换实际上是对应向量旋转，所以位置向量称为“旋转式位置编 码”。还可以使用矩阵形式表示</p><script type="math/tex; mode=display">f(\boldsymbol{q}, m)=\left(\begin{array}{cc}\cos m \theta & -\sin \cos m \theta \\ \sin m \theta & \cos m \theta\end{array}\right)\left(\begin{array}{l}\boldsymbol{q}_{0} \\ \boldsymbol{q}_{1}\end{array}\right)</script><p>根据内积满足线性叠加的性质，任意偶数维的 RoPE，都可以表示为二维情形的拼接，即：</p><script type="math/tex; mode=display">f(\boldsymbol{q}, m)=\underbrace{\left(\begin{array}{ccccccc}\cos m \theta_{0} & -\sin m \theta_{0} & 0 & 0 & \cdots & 0 & 0 \\ \sin m \theta_{0} & \cos m \theta_{0} & 0 & 0 & \cdots & 0 & 0 \\ 0 & 0 & \cos m \theta_{1} & -\sin m \theta_{1} & \cdots & 0 & 0 \\ 0 & 0 & \sin m \theta_{1} & \cos m \theta_{1} & \cdots & 0 & 0 \\ \cdots & \cdots & \cdots & \cdots & \ddots & \cdots & \cdots \\ 0 & 0 & 0 & 0 & \cdots & \cos m \theta_{d / 2-1} & -\sin m \theta_{d / 2-1} \\ 0 & 0 & 0 & 0 & \cdots & \sin m \theta_{d / 2-1} & \cos m \theta_{d / 2-1}\end{array}\right)}_{\boldsymbol{R}_{d}}\left(\begin{array}{c}\boldsymbol{q}_{0} \\ \boldsymbol{q}_{1} \\ \boldsymbol{q}_{2} \\ \boldsymbol{q}_{3} \\ \cdots \\ \boldsymbol{q}_{d-2} \\ \boldsymbol{q}_{d-1}\end{array}\right)</script><p><img src="image/image_FT9IKVPWqd.png" alt=""></p><p>RoPE 在 HuggingFace Transformer 库中代码实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">precompute_freqs_cis</span>(<span class="params">dim: <span class="built_in">int</span>, end: <span class="built_in">int</span>, constant: <span class="built_in">float</span> = <span class="number">10000.0</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    计算cos和sin的值，cos值在实部，sin值在虚部，类似于 cosx+j*sinx</span></span><br><span class="line"><span class="string">    :param dim: q,k,v的最后一维，一般为emb_dim/head_num</span></span><br><span class="line"><span class="string">    :param end: 句长length</span></span><br><span class="line"><span class="string">    :param constant： 这里指10000</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    复数计算 torch.polar(a, t)输出， a*(cos(t)+j*sin(t))</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># freqs: 计算 1/(10000^(2i/d) )，将结果作为参数theta</span></span><br><span class="line">    <span class="comment"># 形式化为 [theta_0, theta_1, ..., theta_(d/2-1)]</span></span><br><span class="line">    freqs = <span class="number">1.0</span> / (constant ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>)[: (dim // <span class="number">2</span>)].<span class="built_in">float</span>() / dim)) <span class="comment"># [d/2]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算m</span></span><br><span class="line">    t = torch.arange(end, device=freqs.device)  <span class="comment"># [length]</span></span><br><span class="line">    <span class="comment"># 计算m*theta</span></span><br><span class="line">    freqs = torch.outer(t, freqs).<span class="built_in">float</span>()  <span class="comment"># [length, d/2]</span></span><br><span class="line">    <span class="comment"># freqs形式化为 [m*theta_0, m*theta_1, ..., m*theta_(d/2-1)],其中 m=0,1,...,length-1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算cos(m*theta)+j*sin(m*theta)</span></span><br><span class="line">    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  <span class="comment"># complex64</span></span><br><span class="line">    <span class="comment"># freqs_cis: [cos(m*theta_0)+j*sin(m*theta_0),  cos(m*theta_1)+j*sin(m*theta_1),), ..., cos(m*theta_(d/2-1))+j*sin(m*theta_(d/2-1))]</span></span><br><span class="line">    <span class="comment"># 其中j为虚数单位， m=0,1,...,length-1</span></span><br><span class="line">    <span class="keyword">return</span> freqs_cis <span class="comment"># [length, d/2]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reshape_for_broadcast</span>(<span class="params">freqs_cis: torch.Tensor, x: torch.Tensor</span>):</span><br><span class="line">    ndim = x.ndim</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= <span class="number">1</span> &lt; ndim</span><br><span class="line">    <span class="keyword">assert</span> freqs_cis.shape == (x.shape[<span class="number">1</span>], x.shape[-<span class="number">1</span>])</span><br><span class="line">    shape = [d <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">or</span> i == ndim - <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(x.shape)] <span class="comment"># (1, length, 1, d/2)</span></span><br><span class="line">    <span class="keyword">return</span> freqs_cis.view(*shape) <span class="comment"># [1, length, 1, d/2]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_emb</span>(<span class="params">xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor,</span>):</span><br><span class="line">    <span class="comment"># 先将xq维度变为[bs, length, head,  d/2, 2], 利用torch.view_as_complex转变为复数</span></span><br><span class="line">    <span class="comment"># xq:[q0, q1, .., q(d-1)] 转变为 xq_: [q0+j*q1, q2+j*q3, ..., q(d-2)+j*q(d-1)]</span></span><br><span class="line">    xq_ = torch.view_as_complex(xq.<span class="built_in">float</span>().reshape(*xq.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># [bs, length, head, d/2]</span></span><br><span class="line">    <span class="comment"># 同样的，xk_:[k0+j*k1, k2+j*k3, ..., k(d-2)+j*k(d-1)]</span></span><br><span class="line">    xk_ = torch.view_as_complex(xk.<span class="built_in">float</span>().reshape(*xk.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    freqs_cis = reshape_for_broadcast(freqs_cis, xq_) <span class="comment"># [1, length, 1, d/2]</span></span><br><span class="line">    <span class="comment"># 下式xq_ * freqs_cis形式化输出，以第一个为例, 如下</span></span><br><span class="line">    <span class="comment"># (q0+j*q1)(cos(m*theta_0)+j*sin(m*theta_0)) = q0*cos(m*theta_0)-q1*sin(m*theta_0) + j*(q1*cos(m*theta_0)+q0*sin(m*theta_0))</span></span><br><span class="line">    <span class="comment"># 上式的实部为q0*cos(m*theta_0)-q1*sin(m*theta_0)，虚部为q1*cos(m*theta_0)+q0*sin(m*theta_0)</span></span><br><span class="line">    <span class="comment"># 然后通过torch.view_as_real函数，取出实部和虚部，维度由[bs, length, head, d/2]变为[bs, length, head, d/2, 2]，最后一维放实部与虚部</span></span><br><span class="line">    <span class="comment"># 最后经flatten函数将维度拉平，即[bs, length, head, d]</span></span><br><span class="line">    <span class="comment"># 此时xq_out形式化为 [实部0，虚部0，实部1，虚部1，..., 实部(d/2-1), 虚部(d/2-1)]</span></span><br><span class="line">    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(<span class="number">3</span>) <span class="comment"># [bs, length, head, d]</span></span><br><span class="line">    <span class="comment"># 即为新生成的q</span></span><br><span class="line"></span><br><span class="line">    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># (bs, length, head, d)</span></span><br><span class="line">    q = torch.randn((<span class="number">2</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">32</span>))  <span class="comment"># q=[q0, q1, .., qd-1]</span></span><br><span class="line">    k = torch.randn((<span class="number">2</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">32</span>))</span><br><span class="line">    v = torch.randn((<span class="number">2</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">32</span>))</span><br><span class="line">    freqs_cis= precompute_freqs_cis(dim=<span class="number">32</span>, end=<span class="number">10</span>, constant= <span class="number">10000.0</span>)</span><br><span class="line">    <span class="comment"># print(freqs_cis.detach().numpy())</span></span><br><span class="line"></span><br><span class="line">    q_new, k_new = apply_rotary_emb(xq=q, xk=k, freqs_cis=freqs_cis)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="2-Alpaca"><a href="#2-Alpaca" class="headerlink" title="2.Alpaca"></a>2.Alpaca</h1><h2 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h2><p>Stanford Alpaca: An Instruction-following LLaMA Model</p><p>Alpaca是在<strong>LLaMA基础上使用52K指令数据精调的预训练模型</strong>，作者只用了不到600美元的成本训练出了该模型（数据$500 + 机器$100）。初步实验结果表明Alpaca可以达到与OpenAI text-davinci-003相匹敌的效果</p><h2 id="2-2-微调方法"><a href="#2-2-微调方法" class="headerlink" title="2.2 微调方法"></a>2.2 微调方法</h2><ol><li>第一步：构造175条self-instruct 种子示例任务</li><li>第二步：基于上述种子任务，利 用text-davinci-003爬取指令数据</li><li>第三步：使用爬取下来的52K指令 数据在LLaMA上进行精调，最终 得到Alpaca</li></ol><p><img src="image/image_j15EAfp-zb.png" alt=""></p><h2 id="2-3-Self-instruct数据构造"><a href="#2-3-Self-instruct数据构造" class="headerlink" title="2.3 Self-instruct数据构造"></a>2.3 Self-instruct数据构造</h2><p>首先由人工构造175条种子数据</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;seed_task_25&quot;</span><span class="punctuation">,</span> </span><br><span class="line">  <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;perfect_numbers&quot;</span><span class="punctuation">,</span> </span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Find the four smallest perfect numbers.&quot;</span><span class="punctuation">,</span> </span><br><span class="line">  <span class="attr">&quot;instances&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span> <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;6, 28, 496, and 8128”&#125;], </span></span><br><span class="line"><span class="string">  &quot;</span>is_classification<span class="string">&quot;: false</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><p>将“爬取要求”和种子数据进行适当组合，送入textdavinci-003，要求生成类似的指令数据。要求包括：提升指令多样性、包含真实数据、字数 要求、语言要求、拒绝不合适指令等</p><h2 id="2-4-指令数据格式"><a href="#2-4-指令数据格式" class="headerlink" title="2.4 指令数据格式"></a>2.4 指令数据格式</h2><ul><li><code>instruction</code>: 描述模型需要执行的指令内容</li><li><code>input</code>（可选）: 任务上下文或输入信息，例如当指令是“对文章进行总结”，则input是文章内容</li><li><code>output</code>: 由text-davinci-003生成的针对指令的回复</li></ul><p><img src="image/image_pPtBso1few.png" alt=""></p><h1 id="3-Llama-2"><a href="#3-Llama-2" class="headerlink" title="3.Llama-2"></a>3.Llama-2</h1><h2 id="3-1-简介"><a href="#3-1-简介" class="headerlink" title="3.1 简介"></a>3.1 简介</h2><p>Llama 2: Open Foundation and Fine-Tuned Chat Models&#x20;</p><p>2023年7月，Meta推出了Llama-2开源大模型，并且推出了Llama-2-Chat对话模型</p><p>与一代LLaMA主要区别体现在<strong>更多的训练数据、更⻓的上下文窗口、GQA技术</strong>等</p><p><img src="image/image_2nhkpLZZzT.png" alt=""></p><p>模型结构的变动主要是体现在<strong>GQA</strong>和<strong>FFN</strong>缩放上</p><ul><li><strong>MHA改成GQA</strong>：整体参数量会有减少</li><li><strong>FFN模块矩阵维度有扩充</strong>：增强泛化能力，整体参数量增加</li><li><strong>上下文长度是llama两倍</strong>(长度从2048-&gt;4096) 训练语料增加约 40%，体现在1.4T-&gt;2.0T的Tokens llama2-34B和llama2-70B使用了GQA，加速模型训练和推理速度</li></ul><h2 id="3-2-GQA"><a href="#3-2-GQA" class="headerlink" title="3.2 GQA"></a>3.2 GQA</h2><p>GQA和MQA都是注意力的变体，其中多个查询头关注相同的键和值头，以减少推理过程中 KV 缓存的大小，并可以显著提高推理吞吐量。</p><p>MHA、GQA、MQA的区别和联系，具体的优点如下：</p><ul><li><code>Mutil-Head Attention</code> 因为自回归模型生成回答时，需要前面生成的KV缓存起来，来加速计算。</li><li><code>Multi-Query Attention</code> 多个头之间可以共享KV对，因此速度上非常有优势，实验验证大约减少30-40%吞吐。</li><li><code>Group Query Attention</code> 没有像MQA那么极端，将query分组，组内共享KV，效果接近MQA，速度上与MQA可比较。</li></ul><p><img src="image/image_SFLwTUXOiI.png" alt=""></p><p>Llama-2中使用了8个KV映射，即GQA-8，<strong>GQA在多数任务上与MHA效果相当，且平均效果优于MQA；GQA和MQA均比MHA有更好的吞吐量</strong></p><h2 id="3-3-源码"><a href="#3-3-源码" class="headerlink" title="3.3 源码"></a>3.3 源码</h2><p><img src="image/image_WANwLmpxNK.png" alt=""></p><h1 id="4-Code-Llama"><a href="#4-Code-Llama" class="headerlink" title="4.Code Llama"></a>4.Code Llama</h1><h2 id="4-1-简介"><a href="#4-1-简介" class="headerlink" title="4.1 简介"></a>4.1 简介</h2><p>2023年8月24日，Meta推出了面向代码的可商用大模型Code Llama，包含三个大小版本（7B/13B/34B）</p><p>支持多种编程语言，包括Python、C++、Java、PHP、Typescript (Javascript)、C#和Bash</p><p>亮点：</p><ul><li>免费供学术研究和商用</li><li>支持100K上下文</li><li>“神秘”34B版接近GPT-4效果</li></ul><h2 id="4-2-模型训练流程"><a href="#4-2-模型训练流程" class="headerlink" title="4.2 模型训练流程"></a>4.2 模型训练流程</h2><p><img src="image/image_bjYO7Dnhe_.png" alt=""></p><h2 id="4-3-Code-Infilling-Task-（7B-13B-only）"><a href="#4-3-Code-Infilling-Task-（7B-13B-only）" class="headerlink" title="4.3 Code Infilling Task （7B/13B only）"></a>4.3 Code Infilling Task （7B/13B only）</h2><p>任务目标：根据代码的上下文，预测残缺部分的代码</p><p>方法：</p><ul><li>从完整的代码中选择一部分进行掩码（mask）并替换为<code>&lt;MASK&gt;</code>符号，构成上下文</li><li>利用自回归的方法，根据上下文信息预测解码出被mask的代码部分</li></ul><p><img src="image/image_PEeja7_w0_.png" alt=""></p><h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h1><p><strong>LLaMA</strong>&#x20;</p><ul><li>开源大模型繁荣发展的开端，一系列相关工作均基于LLaMA开展</li><li>模型规模7B、13B、33B、65B满足了开发者和研究者的不同需求</li></ul><p><strong>Alpaca</strong>：通过少量的指令精调赋予LLaMA指令理解与执行的能力</p><p><strong>Llama-2</strong></p><ul><li>LLaMA的二代模型，相关模型性能进一步提升，模型可商用</li><li>推出官方对⻬的Chat版本模型，采用了完整的RLHF链条</li></ul><p><strong>Code Llama</strong>：专注于代码能力的LLaMA模型，最好的模型代码能力接近GPT-4效果，模型可商用</p>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ChatGLM系列模型架构</title>
      <link href="/llms/llms_article/4.chatglm%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B/"/>
      <url>/llms/llms_article/4.chatglm%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="1-ChatGLM"><a href="#1-ChatGLM" class="headerlink" title="1.ChatGLM"></a>1.ChatGLM</h1><h2 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h2><p>主流的预训练框架主要有三种：</p><ol><li><strong>autoregressive自回归模型（AR模型）</strong>：代表作GPT。本质上是一个left-to-right的语言模型。<strong>通常用于生成式任务</strong>，在长文本生成方面取得了巨大的成功，比如自然语言生成（NLG）领域的任务：摘要、翻译或抽象问答。当扩展到十亿级别参数时，表现出了少样本学习能力。缺点是单向注意力机制，在NLU任务中，无法完全捕捉上下文的依赖关系。</li><li><strong>autoencoding自编码模型（AE模型）</strong>：代表作BERT。是<strong>通过某个降噪目标（比如MLM）训练的双向文本编码器</strong>。编码器会产出适用于NLU任务的上下文表示，但无法直接用于文本生成。</li><li><strong>encoder-decoder（Seq2seq模型）</strong>：代表作T5。采用双向注意力机制，<strong>通常用于条件生成任务</strong>，比如文本摘要、机器翻译等。</li></ol><p>三种预训练框架各有利弊，没有一种框架在以下三种领域的表现最佳：自然语言理解（NLU）、无条件生成以及条件生成。T5曾经尝试使用MTL的方式统一上述框架，然而自编码和自回归目标天然存在差异，简单的融合自然无法继承各个框架的优点。</p><p>在这个天下三分的僵持局面下，GLM诞生了。</p><p><strong>GLM模型基于autoregressive blank infilling方法，结合了上述三种预训练模型的思想</strong>。</p><h2 id="1-2-GLM预训练框架"><a href="#1-2-GLM预训练框架" class="headerlink" title="1.2 GLM预训练框架"></a>1.2 GLM预训练框架</h2><p>GLM特点</p><ol><li><strong>自编码思想</strong>：在输入文本中，随机删除连续的tokens。</li><li><strong>自回归思想</strong>：顺序重建连续tokens。在使用自回归方式预测缺失tokens时，模型既可以访问corrupted文本，又可以访问之前已经被预测的spans。</li><li><strong>span shuffling + 二维位置编码技术</strong>。</li><li>通过改变缺失spans的数量和长度，自回归空格填充目标可以为条件生成以及无条件生成任务预训练语言模型。</li></ol><h3 id="（1）自回归空格填充任务"><a href="#（1）自回归空格填充任务" class="headerlink" title="（1）自回归空格填充任务"></a>（1）自回归空格填充任务</h3><p>给定一个输入文本$x=\left[x_{1}, \ldots x_{n}\right]$，可以采样得到多个文本spans $\left\{s_{1}, \ldots s_{m}\right\}$。为了充分捕捉各spans之间的相互依赖关系，可以对spans的顺序进行随机排列，得到所有可能的排列集合$Z_m$，其中：$S_{z&lt;i}=\left[s_{z_{1}}, \ldots, s_{z_{i-1}}\right]$。所以预训练目标很清晰：</p><script type="math/tex; mode=display">\max _{\theta} \mathbb{E}_{\boldsymbol{z} \sim Z_{m}}\left[\sum_{i=1}^{m} \log p_{\theta}\left(\boldsymbol{s}_{z_{i}} \mid \boldsymbol{x}_{\text {corrupt }}, \boldsymbol{s}_{\boldsymbol{z}_{<i}}\right)\right]</script><p>GLM自回归空格填充任务的技术细节：</p><ol><li>输入$x$可以被分成两部分：Part A是被mask的文本 $x_{\text {corrupt }}$，Part B由masked spans组成。假设原始输入文本是$[x1, x2, x3, x4, x5, x6]$，采样的两个文本片段是$[x3]$以及$[x5, x6]$。那么mask后的文本序列是：$x1, x2, [M], x4, [M]$，即Part A；同时我们需要对Part B的片段进行shuffle。每个片段使用<code>[S]</code>填充在开头作为输入，使用<code>[E]</code>填充在末尾作为输出。</li><li><strong>二维位置编码</strong>：Transformer使用位置编码来标记tokens中的绝对和相对位置。在GLM中，使用二维位置编码，第一个位置id用来标记Part A中的位置，第二个位置id用来表示跨度内部的相对位置。这两个位置id会通过embedding表被投影为两个向量，最终都会被加入到输入token的embedding表达中。</li><li>观察GLM中自定义attention mask的设计，非常巧妙：<ol><li>Part A中的tokens彼此可见，但是不可见B中的任意tokens。</li><li>Part B tokens可见Part A。</li><li>Part B tokens可见B中过去的tokens，不可见B中未来的tokens。</li></ol></li><li>采样方式：文本片段的采样遵循泊松分布，重复采样，直到原始tokens中有15%被mask。</li><li>总结：模型可以自动学习双向encoder（Part A）以及单向decoder（Part B）。</li></ol><p><img src="image/image_RrfQcfLf8G.png" alt=""></p><h3 id="（2）多目标预训练"><a href="#（2）多目标预训练" class="headerlink" title="（2）多目标预训练"></a>（2）多目标预训练</h3><p>上述方法适合于NLU任务。作者希望可以训练一个既可以解决NLU任务，又具备文本生成能力的模型。因此除了空格填充目标之外，还需要增加一个生成长文本目标的任务。具体包含以下两个目标：</p><ol><li><strong>文档级别</strong>。从文档中采样一个文本片段进行mask，且片段长度为文档长度的50%～100%。这个目标用于长文本生成。</li><li><strong>句子级别</strong>。限制被mask的片段必须是完整句子。多个片段需覆盖原始tokens的15%。这个目标是用于预测完整句子或者段落的seq2seq任务。</li></ol><h3 id="（3）模型结构"><a href="#（3）模型结构" class="headerlink" title="（3）模型结构"></a>（3）模型结构</h3><p>GLM在原始single Transformer的基础上进行了一些修改：</p><ol><li>重组了LN和残差连接的顺序；</li><li>使用单个线性层对输出token进行预测；</li><li>激活函数从ReLU换成了GeLUS。</li></ol><p>但我觉得这部分的修改比较简单常见。核心和亮点还是空格填充任务的设计。</p><h3 id="（4）GLM微调"><a href="#（4）GLM微调" class="headerlink" title="（4）GLM微调"></a>（4）GLM微调</h3><p>对于下游NLU任务来说，通常会将预训练模型产出的序列或tokens表达作为输入，使用线性分类器预测label。所以预训练与微调之间存在天然不一致。</p><p>作者按照PET的方式，将下游NLU任务重新表述为空白填充的生成任务。具体来说，比如给定一个已标注样本(x, y)，将输入的文本x转换成一个包含mask token的完形填空问题。比如，情感分类任务可以表述为：”{SENTENCE}. It’s really [MASK]”。输出label y也同样会被映射到完形填空的答案中。“positive” 和 “negative” 对应的标签就是“good” 和 “bad。</p><p>其实，预训练时，对较长的文本片段进行mask，以确保GLM的文本生成能力。但是在微调的时候，相当于将NLU任务也转换成了生成任务，这样其实是为了适应预训练的目标。但难免有一些牵强。</p><p><img src="image/image_QDjRgdpoc_.png" alt=""></p><div class="table-container"><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>BERT</td><td>XLNet</td><td>T5</td><td>UniLM</td></tr><tr><td>1、无法捕捉mask tokens的相互依赖性。2、不能准确填充多个连续的tokens。为了推断长度为l的答案概率，BERT需要执行l次连续预测。</td><td>与GLM相同，使用自回归目标预训练。1、使用文本mask之前的原始位置编码，推理过程中，需要事先知晓或枚举答案长度，与BERT的问题相同。2、双流自注意力机制，使预训练时间成本增加了一倍。</td><td>使用类似的空格填充目标预训练encoder-decoder Transformer。在编码和解码阶段使用独立的位置编码，使用多个哨兵token来区分mask片段。而在下游任务中，仅使用一个哨兵token，造成模型能力的浪费以及预训练-微调的不一致。</td><td>通过改变双向、单向以及交叉注意力之间的注意力mask，统一不同的预训练目标。1、总是使用[mask] token替代mask片段，限制了它对mask片段及其上下文的依赖关系进行建模的能力。2、在下游任务微调时，自编码比自回归更加低效。</td></tr></tbody></table></div><h1 id="2-ChatGLM-2"><a href="#2-ChatGLM-2" class="headerlink" title="2.ChatGLM-2"></a>2.ChatGLM-2</h1><h2 id="2-1-主要创新"><a href="#2-1-主要创新" class="headerlink" title="2.1 主要创新"></a>2.1 主要创新</h2><ol><li><strong>更长的上下文</strong>：<strong>基于 </strong><a href="https://github.com/HazyResearch/flash-attention" title="FlashAttention"><strong>FlashAttention</strong></a><strong> 技术</strong>，将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的 <strong>2K 扩展到了 32K</strong>，并在对话阶段使用 8K 的上下文长度训练。对于更长的上下文，发布了 <a href="https://huggingface.co/THUDM/chatglm2-6b-32k" title="ChatGLM2-6B-32K">ChatGLM2-6B-32K</a> 模型。<a href="https://github.com/THUDM/LongBench" title="LongBench">LongBench</a> 的测评结果表明，在等量级的开源模型中，ChatGLM2-6B-32K 有着较为明显的竞争优势。</li><li><strong>更强大的性能</strong>：基于 ChatGLM 初代模型的开发经验，全面升级了 ChatGLM2-6B 的基座模型。ChatGLM2-6B <strong>使用了 </strong><a href="https://github.com/THUDM/GLM" title="GLM"><strong>GLM</strong></a><strong> 的混合目标函数</strong>，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，<a href="https://github.com/THUDM/ChatGLM2-6B#评测结果" title="评测结果">评测结果</a>显示，相比于初代模型，ChatGLM2-6B 在 MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。&#x20;</li><li><strong>更高效的推理</strong>：基于 <a href="http://arxiv.org/abs/1911.02150" title="Multi-Query Attention">Multi-Query Attention</a> 技术，ChatGLM2-6B 有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4 量化下，6G 显存支持的对话长度由 1K 提升到了 8K。&#x20;</li><li><strong>更开放的协议</strong>：ChatGLM2-6B 权重对学术研究<strong>完全开放</strong>，在填写<a href="https://open.bigmodel.cn/mla/form" title="问卷">问卷</a>进行登记后<strong>亦允许免费商业使用</strong>。</li></ol><h2 id="2-2-与ChatGLM的变化"><a href="#2-2-与ChatGLM的变化" class="headerlink" title="2.2 与ChatGLM的变化"></a>2.2 与ChatGLM的变化</h2><ol><li><strong>使用了RoPE替换二维位置编码</strong>。这也是GLM中提出的亮点设计之一。但是目前大部分主流的LLMs都在使用RoPE，所以大势所趋。当前版本仍然采用了最初的RoPE设计，事实上现在的RoPE经过了xPOS→线性内插→NTK-Aware Scaled RoPE→…若干次进化。</li><li><strong>Multi-Query Attention</strong>：这是一种共享机制的Attention，相比Multi-Head Attention，其Query部分没有区别，Key和Value可以只用一个Head。计算时，对Key和Value进行expand或者repeat操作，使它们填充到与Query一样的维度，后续计算就与Multi-Head Attention没区别。</li><li><strong>Attention Mask</strong>: V1的attention mask分了2部分，Part A和Part B，Part A部分是双向Attention（代码中的<a href="https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py#L963" title="prefix_attention_mask">prefix_attention_mask</a>），Part B部分是Causal Attention(原代码文件中的get_masks函数)。在V2版本，全部换成了Causal Attention，不再区分是Part A还是Part B，<strong>完全变成了decoder-only的架构</strong>。</li><li><strong>多目标任务</strong>：Chat版本主要还是用的gMask生成式任务，但是在V1版本的代码还能看到mask、gMask等字样，V2已经摒弃了这些特殊token，原因与Attention Mask一致，均因为变成了decoder-only的架构，不再需要区分Part A和Part B。</li></ol><h1 id="3-ChatGLM-3"><a href="#3-ChatGLM-3" class="headerlink" title="3.ChatGLM-3"></a>3.ChatGLM-3</h1><p>省流：<strong>ChatGLM2与ChatGLM3模型架构是完全一致的</strong>，ChatGLM与后继者结构不同。可见ChatGLM3相对于ChatGLM2没有模型架构上的改进。</p><p>相对于ChatGLM，ChatGLM2、ChatGLM3模型上的变化：</p><ol><li>词表的大小从ChatGLM的150528缩小为65024 （一个直观的体验是ChatGLM2、3加载比ChatGLM快不少）</li><li><strong>位置编码从每个GLMBlock一份提升为全局一份</strong></li><li><strong>SelfAttention之后的前馈网络有不同</strong>。ChatGLM用GELU（Gaussian Error Linear Unit）做激活；ChatGLM用Swish-1做激活。而且ChatGLM2、3应该是修正了之前的一个bug，因为GLU（Gated Linear Unit）本质上一半的入参是用来做门控制的，不需要输出到下层，所以ChatGLM2、3看起来前后维度不一致（27392-&gt;13696)反而是正确的。</li></ol><h1 id="4-模型架构比较"><a href="#4-模型架构比较" class="headerlink" title="4.模型架构比较"></a>4.模型架构比较</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(model_path, trust_remote_code=<span class="literal">True</span>).<span class="built_in">float</span>().to(<span class="string">&#x27;mps&#x27;</span>)</span><br><span class="line"><span class="comment"># 多显卡支持，使用下面两行代替上面一行，将num_gpus改为你实际的显卡数量</span></span><br><span class="line"><span class="comment"># from utils import load_model_on_gpus</span></span><br><span class="line"><span class="comment"># model = load_model_on_gpus(&quot;THUDM/chatglm3-6b&quot;, num_gpus=2)</span></span><br><span class="line">model = model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><p>ChatGLM的模型结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">ChatGLMForConditionalGeneration(</span><br><span class="line">  (transformer): ChatGLMModel(</span><br><span class="line">    (word_embeddings): Embedding(<span class="number">150528</span>, <span class="number">4096</span>)</span><br><span class="line">    (layers): ModuleList(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">27</span>): <span class="number">28</span> x GLMBlock(</span><br><span class="line">        (input_layernorm): LayerNorm((<span class="number">4096</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attention): SelfAttention(</span><br><span class="line">          (rotary_emb): RotaryEmbedding()</span><br><span class="line">          (query_key_value): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">12288</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (dense): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (post_attention_layernorm): LayerNorm((<span class="number">4096</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): GLU(</span><br><span class="line">          (dense_h_to_4h): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">16384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (dense_4h_to_h): Linear(in_features=<span class="number">16384</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (final_layernorm): LayerNorm((<span class="number">4096</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (lm_head): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">150528</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>ChatGLM2的模型结构：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">ChatGLMForConditionalGeneration(</span><br><span class="line">  (transformer): ChatGLMModel(</span><br><span class="line">    (embedding): Embedding(</span><br><span class="line">      (word_embeddings): Embedding(65024, 4096)</span><br><span class="line">    )</span><br><span class="line">    (rotary_pos_emb): RotaryEmbedding()</span><br><span class="line">    (encoder): GLMTransformer(</span><br><span class="line">      (layers): ModuleList(</span><br><span class="line">        (0-27): 28 x GLMBlock(</span><br><span class="line">          (input_layernorm): RMSNorm()</span><br><span class="line">          (self_attention): SelfAttention(</span><br><span class="line">            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)</span><br><span class="line">            (core_attention): CoreAttention(</span><br><span class="line">              (attention_dropout): Dropout(p=0.0, inplace=False)</span><br><span class="line">            )</span><br><span class="line">            (dense): Linear(in_features=4096, out_features=4096, bias=False)</span><br><span class="line">          )</span><br><span class="line">          (post_attention_layernorm): RMSNorm()</span><br><span class="line">          (mlp): MLP(</span><br><span class="line">            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)</span><br><span class="line">            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (final_layernorm): RMSNorm()</span><br><span class="line">    )</span><br><span class="line">    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>ChatGLM3的模型结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">ChatGLMForConditionalGeneration(</span><br><span class="line">  (transformer): ChatGLMModel(</span><br><span class="line">    (embedding): Embedding(</span><br><span class="line">      (word_embeddings): Embedding(<span class="number">65024</span>, <span class="number">4096</span>)</span><br><span class="line">    )</span><br><span class="line">    (rotary_pos_emb): RotaryEmbedding()</span><br><span class="line">    (encoder): GLMTransformer(</span><br><span class="line">      (layers): ModuleList(</span><br><span class="line">        (<span class="number">0</span>-<span class="number">27</span>): <span class="number">28</span> x GLMBlock(</span><br><span class="line">          (input_layernorm): RMSNorm()</span><br><span class="line">          (self_attention): SelfAttention(</span><br><span class="line">            (query_key_value): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4608</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (core_attention): CoreAttention(</span><br><span class="line">              (attention_dropout): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">            )</span><br><span class="line">            (dense): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          )</span><br><span class="line">          (post_attention_layernorm): RMSNorm()</span><br><span class="line">          (mlp): MLP(</span><br><span class="line">            (dense_h_to_4h): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">27392</span>, bias=<span class="literal">False</span>)</span><br><span class="line">            (dense_4h_to_h): Linear(in_features=<span class="number">13696</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (final_layernorm): RMSNorm()</span><br><span class="line">    )</span><br><span class="line">    (output_layer): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">65024</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLMs 推理优化技术</title>
      <link href="/llms/llms_article/1.llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/"/>
      <url>/llms/llms_article/1.llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文链接：<a href="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/" title="Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog">Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog</a></p></blockquote><p><img src="image/image_YZh-nrxyKn.png" alt=""></p><p>堆叠Transformer层以创建大型模型可以获得更好的准确性、few-shot学习能力，甚至在各种语言任务中具有接近人类的涌现能力。这些基础模型的训练成本很高，而且在推理过程中可能需要大量的内存和计算（经常性成本）。当今最流行的大型语言模型（LLM）的大小可以达到数百亿到数千亿个参数，并且根据用例的不同，可能需要摄入长输入（或上下文），这也会增加开销。</p><p>这篇文章讨论了LLM推理中最紧迫的挑战，以及一些实用的解决方案。读者应该对transformer架构和注意力机制有一个基本的了解。</p><h1 id="1-理解LLM推理"><a href="#1-理解LLM推理" class="headerlink" title="1.理解LLM推理"></a>1.理解LLM推理</h1><p>大多数流行的only-decode LLM（例如 GPT-3）都是针对因果建模目标进行预训练的，本质上是作为下一个词预测器。这些 LLM 将一系列tokens作为输入，并自回归生成后续tokens，直到满足停止条件（例如，生成tokens数量的限制或遇到停止词）或直到生成特殊的 <code>&lt;end&gt;</code> 标记生成结束的tokens。该过程涉及两个阶段：预填充阶段和解码阶段。</p><p>请注意，tokens是模型处理的语言的原子部分。一个tokens大约是四个英文字符。所有自然语言在输入模型之前都会转换为toikens。</p><h2 id="1-1-预填充阶段或处理输入"><a href="#1-1-预填充阶段或处理输入" class="headerlink" title="1.1 预填充阶段或处理输入"></a>1.1 预填充阶段或处理输入</h2><p>在预填充阶段，LLM处理输入token以计算中间状态（keys和value），用于生成“第一个”token。每个新的token都依赖于所有先前的token，但由于输入的全部已知，因此在运算上，都是高度并行化矩阵运算，可以有效地使用GPU。</p><h2 id="1-2-解码阶段或生成输出"><a href="#1-2-解码阶段或生成输出" class="headerlink" title="1.2 解码阶段或生成输出"></a>1.2 解码阶段或生成输出</h2><p>在解码阶段，LLM一次自回归生成一个输出token，直到满足停止条件。每个输出tokens都需要直到之前迭代的所有输出状态（keys和values）。这与预填充输入处理相比，就像矩阵向量运算未充分利用GPU计算能力。数据（weights, keys, values, activations） 从内存传输到GPU的速度决定了延迟，而不是计算实际时间消耗。即，这是一个内存限制操作。</p><p>本文中的许多推理挑战和相应的解决方案都涉及此解码阶段的优化：高效的注意力模块、有效管理键和值等。</p><p>不同的LLMs可能使用不同的tokenizers，因此比较它们之间的输出tokens可能并不简单。在比较推理吞吐量时，即使两个 LLMs每秒输出的tokens相似，如果它们使用不同的tokenizers，也可能不相等。这是因为相应的tokens可能代表不同数量的字符。</p><h2 id="1-3-批处理（Batching）"><a href="#1-3-批处理（Batching）" class="headerlink" title="1.3 批处理（Batching）"></a>1.3 批处理（Batching）</h2><p>提高 GPU 利用率和有效吞吐量的最简单方法是通过<strong>批处理</strong>。由于多个请求使用相同的模型，因此权重的内存成本被分散。大批量数据传输到 GPU 一次处理，将提高GPU资源的利用率。</p><p>然而，批量大小只能增加到一定限制，此时可能会导致内存溢出。为了防止这种情况发生，需要查看键值 (KV) 缓存和 LLM 内存要求。</p><p>传统批处理（也称为静态批处理， static batching）不是最佳的。这是因为<strong>对于批次中的每个请求，LLM 可能会生成不同数量的tokens，并且不同tokens有不同的执行时间</strong>。因此，批次中的所有请求都必须等待最长token的处理完成，而生成长度的巨大差异可能会加剧这种情况。有一些方法可以缓解这种情况，例如稍动态批处理。</p><h2 id="1-4-KV缓存"><a href="#1-4-KV缓存" class="headerlink" title="1.4 KV缓存"></a>1.4 KV缓存</h2><p>解码阶段的一种常见优化是 KV 缓存。解码阶段在每个时间步生成单个token，但每个token依赖于之前token的键和值张量（包括预填充时计算的输入tokens的 KV 张量，以及当前时间步之前计算的任何新 KV 张量） 。</p><p>为了避免在每个时间步重新计算所有tokens的这些张量，<strong>可以将它们缓存在 GPU 内存中</strong>。每次迭代，当需要计算新token时，它们都会被添加到正在运行的缓存中，以便在下一次迭代中使用。在一些实现中，模型的每一层都有一个KV缓存。</p><p><img src="image/image_LmKhNv__Og.png" alt=""></p><blockquote><p>图1 KV缓存机制</p></blockquote><h2 id="1-5-LLM内存需求"><a href="#1-5-LLM内存需求" class="headerlink" title="1.5 LLM内存需求"></a>1.5 LLM内存需求</h2><p>实际上，LLM对GPU显存的需求主要是模型权重和KV缓存：</p><ul><li><strong>模型权重</strong>：模型参数占用内存。例如，具有 70 亿个参数的模型（例如 Llama2-7B），以 16 位精度（FP16 或 BF16）加载，将占用大约 <code>7B * sizeof(FP16) ~= 14 GB</code> 的内存。</li><li><strong>KV缓存</strong>：自注意力张量的缓存占用内存，避免冗余计算。</li></ul><p>使用批处理时，批处理中每个请求的 KV 缓存仍然必须单独分配，并且可能会占用大量内存。下面的公式描述了 KV 缓存的大小，适用于当今最常见的 LLM 架构。</p><script type="math/tex; mode=display">每个token的KV缓存大小(字节) = 2 * (num\_layers) * (num\_heads * dim\_head) *  precision\_in\_bytes</script><p>第一个因子 2 代表 K 和 V 矩阵。通常，<code>(num_heads * dim_head)</code>的值与Transformer的<code>hidden_​​size</code>（或模型的维度，<code>d_model</code>）相同。这些模型属性通常可以在配置文件中找到。</p><p>输入批次中输入序列中的每个tokens都需要此内存大小。假设半精度，KV缓存的总大小由以下公式给出:</p><script type="math/tex; mode=display">总KV缓存大小(字节)=(batch\_size) * (sequence\_length) * 2 * (num\_layers) * (hidden\_size) *  sizeof(FP16)</script><p>例如，对于 16 位精度的 Llama 2 7B 模型，批量大小为 <code>1</code>，KV 缓存的大小将为 <code>1 * 4096 * 2 * 32 * 4096 * 2</code> 字节，即约 <code>2 GB</code>。</p><p>高效的管理 KV 缓存是一项具有挑战性的工作。内存需求随着批量大小和序列长度线性增长，可以快速扩展。因此，它限制了可服务的吞吐量，并对长上下文输入提出了挑战。这就是本文中介绍的多项优化背后的动机。</p><h1 id="2-模型并行化扩展LLM"><a href="#2-模型并行化扩展LLM" class="headerlink" title="2.模型并行化扩展LLM"></a>2.模型并行化扩展LLM</h1><p>减少模型权重在每设备的显存占用的一种方法是<strong>将模型分布在多个 GPU 上</strong>。分散内存和计算可以运行更大的模型或更大批量的输入。模型并行化是训练或推理模型所必需的，模型并行化需要比单个设备更多的内存，用来训练和推理（延迟或吞吐量）。根据模型权重的划分方式，有多种方法可以并行化模型。</p><p>请注意，数据并行性也是一种经常在与下面列出的其他技术相同的的技术。在这种情况下，模型的权重被复制到多个设备上，并且输入的（全局）批量大小在每个设备上被分成微批次。它通过处理较大的批次来减少总体执行时间。然而，这是一种训练时间优化，在推理过程中不太相关。</p><h2 id="2-1-Pipeline并行"><a href="#2-1-Pipeline并行" class="headerlink" title="2.1 Pipeline并行"></a>2.1 Pipeline并行</h2><p>Pipeline并行化<strong>将模型（垂直）分片为块，其中每个块包含在单独设备上执行的层的子集</strong>。图 2a 说明了四路Pipeline，其中模型按顺序分区，并且所有层的四分之一子集在每个设备上执行。一个设备上的一组操作的输出被传递到下一个设备，后者继续执行后续块。$F_n$和 $B_n$分别表示设备 $n$ 上的前向传播和后向传播。每个设备上存储模型权重的内存需求被分成四份。</p><p>该方法的缺点是，由于处理的顺序性质，<strong>某些设备或层在等待前一层的输出（激活、梯度）时可能保持空闲状态</strong>。这会导致前向和后向传递效率低下或出现“Pipeline bubbles”。在图 2b 中，白色空白区域是Pipeline并行性产生的Pipeline bubbles，其中设备闲置且未得到充分利用。</p><p><strong>微批处理可以在一定程度上缓解这种情况</strong>，如图 2c 所示。输入的全局批次大小被分成子批次，这些子批次被一一处理，最后累积梯度。请注意，$F_{n,m}$ 和 $B_{n,m}$ 分别表示设备<code>n</code>上<code>m</code>批次的前向和后向传递。<strong>这种方法缩小了管道气泡的尺寸，但并没有完全消除它们</strong>。</p><p><img src="image/image_PkhYDpFHjZ.png" alt=""></p><blockquote><p>图2 Pipeline并行，</p></blockquote><h2 id="2-2-Tensor并行"><a href="#2-2-Tensor并行" class="headerlink" title="2.2 Tensor并行"></a>2.2 Tensor并行</h2><p>Tensor并行化<strong>将模型的各个层（水平）分片为更小的、独立的计算块，这些计算块可以在不同的设备上执行</strong>。Transformer的主要组成部分，注意力块和多层感知器（MLP）层是可以利用Tensor并行化的。在多头注意力块中，每个头或一组头可以分配给不同的设备，以便它们可以独立且并行地计算。</p><p><img src="image/image_Z1kiW5PtoV.png" alt=""></p><blockquote><p>图3 Tensor并行化MLP和自注意力</p></blockquote><p>图 3a 显示了两层 MLP Tensor并行的示例，每一层都由一个圆角框表示。在第一层中，权重矩阵$A$分为$A_1$和$A_2$ 。对于输入X，可以在同一批次不同设备上计算$XA_1$ 和$ XA_2  $，其中，f是identity 操作。这将每个设备上存储权重的内存需求减半。归约操作$g$组合了第二层的输出。</p><p>图 3b 是自注意力层中Tensor并行的示例。多个注意力头本质上是并行的，并且可以跨设备分割。</p><h2 id="2-3-Sequence并行"><a href="#2-3-Sequence并行" class="headerlink" title="2.3 Sequence并行"></a>2.3 Sequence并行</h2><p>Tensor并行化是有局限性，它需要将层划分为独立的、可管理的块，不适用于 <code>LayerNorm</code>和 <code>Dropout</code>等操作，而是在tensor并行中复制。虽然 <code>LayerNorm</code>和 <code>Dropout</code>的计算成本较低，但它们确实需要大量内存来存储（冗余）激活。</p><p>如<a href="https://arxiv.org/pdf/2205.05198.pdf" title="Reducing Activation Recomputation in Large Transformer Models">Reducing Activation Recomputation in Large Transformer Models</a>所示，这些操作在输入序列中是独立的，并且这些操作<strong>可以沿着“序列维度”进行分区</strong>，从而提高内存效率。这称为序列并行性。</p><p><img src="image/image_RE2vXpBHJ5.png" alt=""></p><blockquote><p>图4，transformer层的tensor并行化和sequence并行化</p></blockquote><p>模型并行技术不是唯一的，可以结合使用。它们可以帮助扩展和减少 LLM 的每 GPU 内存占用量，但也有专门针对注意力模块的优化技术。</p><h1 id="3-注意力机制优化"><a href="#3-注意力机制优化" class="headerlink" title="3.注意力机制优化"></a>3.注意力机制优化</h1><p>缩放点积注意力 (SDPA， scaled dot-product attention) 操作将<code>query</code>和<code>key</code>对映射到输出，如论文<a href="https://arxiv.org/pdf/1706.03762.pdf" title="Attention Is All You Need">Attention Is All You Need</a>所述。</p><h2 id="3-1-多头注意力（MHA）"><a href="#3-1-多头注意力（MHA）" class="headerlink" title="3.1 多头注意力（MHA）"></a>3.1 多头注意力（MHA）</h2><p>作为 SDPA 的增强，三个变换张量对Q，K，V分别进行线性变换，<strong>这些变换不会改变原有张量的尺寸</strong>，使模型能够共同关注来自不同位置的不同表示子空间的信息。这些子空间是独立学习的，使模型能够更丰富地理解输入中的不同位置。</p><p>如图 5 所示，多个并行注意力操作的输出被拼接后线性投影以组合起来。每个并行注意力层称为“头”，这种方法称为多头注意力（MHA）。</p><p>当使用八个并行注意力头时，每个注意力头的维度都会减少（例如 $d_model/8$）。这使得计算成本与单头注意力相似。</p><p><img src="image/image_RFewX-Wp-I.png" alt=""></p><blockquote><p>图5 缩放点积注意力（左）和多头注意力（右）的图示，并行的多个 SDPA 头</p></blockquote><h2 id="3-2-多查询注意力（MQA）"><a href="#3-2-多查询注意力（MQA）" class="headerlink" title="3.2 多查询注意力（MQA）"></a>3.2 多查询注意力（MQA）</h2><p>MHA 的推理优化之一称为多查询注意力 (MQA)，如 Fast Transformer Decoding 中提出的，<strong>在多个注意力头之间共享键和值</strong>。与以前一样，查询向量仍然被投影多次。</p><p>虽然 MQA 中完成的计算量与 MHA 相同，但从内存读取的数据量（键、值）只是以前的一小部分。当受内存带宽限制时，这可以实现更好的计算利用率。它还减少了内存中 KV 缓存的大小，为更大的批量大小留出了空间。</p><p>key头的减少会带来潜在的准确性下降。此外，需要在推理时利用这种优化的模型需要在启用 MQA 的情况下进行训练（或至少使用大约 5% 的训练量进行微调）。</p><h2 id="3-3-分组注意力（GQA）"><a href="#3-3-分组注意力（GQA）" class="headerlink" title="3.3 分组注意力（GQA）"></a>3.3 分组注意力（GQA）</h2><p>分组查询注意力 (GQA) 通过将键和值投影到几组查询头，在 MHA 和 MQA 之间取得平衡（图 6）。在每个组中，它的行为类似于多查询注意力。</p><p>图 6 显示多头注意力有多个键值头（左）。分组查询注意力（中心）的键值头多于一个，但少于查询头的数量，这是内存需求和模型质量之间的平衡。多查询注意力（右）具有单个键值头，有助于节省内存。</p><p><img src="image/image_R47Naw43sH.png" alt=""></p><p>最初使用 MHA 训练的模型可以使用原始训练计算的一小部分通过 GQA 进行“升级训练”。它们获得接近 MHA 的质量，同时保持接近 MQA 的计算效率。 Llama 2 70B 是利用 GQA 的模型示例。</p><p><strong>MQA 和 GQA 等优化通过减少存储的key头和value头的数量来帮助减少 KV 缓存所需的内存</strong>。 KV 缓存的管理方式可能仍然效率低下。与优化注意力模块本身不同，下一节将介绍一种更高效的 KV 缓存管理技术。</p><h2 id="3-4-Flash-attention"><a href="#3-4-Flash-attention" class="headerlink" title="3.4 Flash attention"></a>3.4 Flash attention</h2><p>优化注意力机制的另一种方法是<strong>修改某些计算的顺序，以更好地利用 GPU 的内存层次结构</strong>。神经网络通常用层来描述，大多数实现也以这种方式布局，每次按顺序对输入数据进行一种计算。这并不总是能带来最佳性能，因为对已经进入内存层次结构的更高、性能更高级别的值进行更多计算可能是有益的。</p><p>在实际计算过程中将多个层融合在一起可以最大限度地减少 GPU 需要读取和写入内存的次数，并将需要相同数据的计算分组在一起，即使它们是神经网络中不同层的一部分。</p><p>一种非常流行的融合是 FlashAttention，这是一种 I/O 感知精确注意算法，详细信息请参阅 <a href="https://arxiv.org/abs/2205.14135" title="FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a>。精确注意力意味着它在数学上与标准多头注意力相同（具有可用于多查询和分组查询注意力的变体），因此可以无需修改即可交换到现有的模型架构，甚至是已经训练的模型。</p><p>I/O 感知意味着在将操作融合在一起时，它会考虑前面讨论的一些内存移动成本。特别是，FlashAttention 使用“平铺”一次性完全计算并写出最终矩阵的一小部分，而不是分步对整个矩阵进行部分计算，写出中间的中间值。</p><p>图 7 显示了 40 GB GPU 上的平铺 FlashAttention 计算模式和内存层次结构。右图显示了对注意力机制的不同组件进行融合和重新排序所带来的相对加速。</p><p><img src="image/image_0qq38on2gP.png" alt=""></p><blockquote><p>图7 40 GB GPU 上的平铺 FlashAttention 计算模式和内存层次结构</p></blockquote><h1 id="4-KV缓存的分页高效管理"><a href="#4-KV缓存的分页高效管理" class="headerlink" title="4.KV缓存的分页高效管理"></a>4.KV缓存的分页高效管理</h1><p>有时，KV 缓存会静态地“过度配置”(over-provisioned)，以考虑最大可能的输入（支持的序列长度），因为输入的大小是不可预测的。例如，如果模型支持的最大序列长度为 2,048，则<strong>无论请求中输入和生成的输出的大小如何，都将在内存中保留大小为 2,048 的数据。该空间可以是连续分配的，并且通常其中大部分未被使用，从而导致内存浪费或碎片</strong>。该保留空间在请求的生命周期内被占用。</p><p><img src="image/image_0EJDxQn6Es.png" alt=""></p><blockquote><p>图8 由于过度配置和低效的 KV 缓存管理而导致的内存浪费和碎片</p></blockquote><p>受操作系统分页的启发，PagedAttention 算法能够<strong>将连续的键和值存储在内存中的不连续空间中</strong>。它将每个请求的 KV 缓存划分为代表固定数量token的块，这些块可以不连续存储。</p><p>在注意力计算期间，使用根据记录索引获取这些块。当新的token产生时，就会进行新的区块分配。这些块的大小是固定的，消除了因不同请求需要不同分配等挑战而产生的低效率。这极大地限制了内存浪费，从而实现了更大的批量大小（从而提高了吞吐量）。</p><h1 id="5-模型优化技术"><a href="#5-模型优化技术" class="headerlink" title="5.模型优化技术"></a>5.模型优化技术</h1><p>到目前为止，我们已经讨论了 LLM 消耗内存的不同方式、跨多个不同 GPU 分配内存的一些方式，以及优化注意力机制和 KV 缓存。还有多种模型优化技术可以通过修改模型权重本身来减少每个 GPU 上的内存使用。 GPU 还具有专用硬件来加速这些修改值的运算，从而为模型提供更多加速。</p><h2 id="5-1-量化（Quantization）"><a href="#5-1-量化（Quantization）" class="headerlink" title="5.1 量化（Quantization）"></a>5.1 量化（Quantization）</h2><p><strong>量化是降低模型权重和激活精度的过程</strong>。大多数模型都以 32 或 16 位精度进行训练，其中每个参数和激活元素占用 32 或 16 位内存（单精度浮点）。然而，大多数深度学习模型可以用每个值八个甚至更少的位来有效表示。</p><p>图 9 显示了一种可能的量化方法之前和之后的值分布。在这种情况下，舍入会丢失一些精度，并且剪裁会丢失一些动态范围，从而允许以更小的格式表示值。</p><p><img src="image/image_M1RfRhceOy.png" alt=""></p><blockquote><p>图9 一种可能的量化方法之前和之后的值分布</p></blockquote><p>降低模型的精度可以带来多种好处。如果模型占用的内存空间较少，则可以在相同数量的硬件上安运行更大的模型。量化还意味着可以在相同的带宽上传输更多参数，这有助于加速带宽有限的模型。</p><p>LLM 有许多不同的量化技术，涉及降低激活、权重或两者的精度。量化权重要简单得多，因为它们在训练后是固定的。然而，这可能会留下一些性能问题，因为激活仍然保持在更高的精度。 GPU 没有用于乘以 INT8 和 FP16 数字的专用硬件，因此必须将权重转换回更高精度以进行实际运算。</p><p>还可以量化激活、Transformer块和网络层的输入，但这也有其自身的挑战。激活向量通常包含异常值，有效地增加了它们的动态范围，并使以比权重更低的精度表示这些值变得更具挑战性。</p><p>一种选择是通过模型传递代表性数据集并选择以比其他激活更高的精度表示某些激活来找出这些异常值可能出现的位置 (<code>LLM.int8()</code>)。另一种选择是借用易于量化的权重的动态范围，并在激活中重用该范围。</p><h2 id="5-2-稀疏（Sparsity）"><a href="#5-2-稀疏（Sparsity）" class="headerlink" title="5.2 稀疏（Sparsity）"></a>5.2 稀疏（Sparsity）</h2><p>与量化类似，事实证明，许多深度学习模型对于修剪或用 <code>0</code> 本身替换某些接近 <code>0</code> 的值具有鲁棒性。稀疏矩阵是许多元素为 0 的矩阵。这些矩阵可以用压缩形式表示，比完整的稠密矩阵占用的空间更少。</p><p><img src="image/image_UKiDga-iHn.png" alt=""></p><blockquote><p>图10，以压缩格式表示的稀疏矩阵，由非零数据值及其相应的两位索引组成</p></blockquote><p>GPU 尤其具有针对某种结构化稀疏性的硬件加速，其中每四个值中有两个由零表示。稀疏表示还可以与量化相结合，以实现更大的执行速度。寻找以稀疏格式表示大型语言模型的最佳方法仍然是一个活跃的研究领域，并为未来提高推理速度提供了一个有希望的方向。</p><h2 id="5-3-蒸馏（Distillation）"><a href="#5-3-蒸馏（Distillation）" class="headerlink" title="5.3 蒸馏（Distillation）"></a>5.3 蒸馏（Distillation）</h2><p>缩小模型大小的另一种方法是通过称为蒸馏的过程<strong>将其知识转移到较小的模型</strong>。此过程涉及训练较小的模型（称为学生）来模仿较大模型（教师）的行为。</p><p>蒸馏模型的成功例子包括 <a href="https://arxiv.org/abs/1910.01108" title="DistilBERT">DistilBERT</a>，它将 BERT 模型压缩了 40%，同时保留了 97% 的语言理解能力，速度提高了 60%。</p><p>虽然LLMs中的蒸馏是一个活跃的研究领域，但神经网络的一般方法首次在<a href="https://arxiv.org/abs/1503.02531" title="Distilling the Knowledge in a Neural Network">Distilling the Knowledge in a Neural Network</a>中提出：</p><ul><li>学生网络经过训练，可以反映较大教师网络的性能，使用损失函数来测量其输出之间的差异。该目标还可能包括将学生的输出与真实标签进行匹配的原始损失函数。</li><li>匹配的教师输出可以是最后一层（称为 <code>logits</code>）或中间层激活。</li></ul><p>图 11 显示了知识蒸馏的总体框架。教师的 <code>logits</code>是学生使用蒸馏损失进行优化的软目标。其他蒸馏方法可能会使用其他损失措施来从老师那里“蒸馏”知识。</p><p><img src="image/image_N8rcSMB0Rs.png" alt=""></p><blockquote><p>图11，知识蒸馏的通用框架</p></blockquote><p>蒸馏的另一种方法是使用教师合成的数据对LLMs学生进行监督培训，这在人工注释稀缺或不可用时特别有用。一步一步蒸馏！更进一步，除了作为基本事实的标签之外，还从LLMs教师那里提取基本原理。这些基本原理作为中间推理步骤，以数据有效的方式培训规模较小的LLMs。</p><p>值得注意的是，当今许多最先进的LLMs都拥有限制性许可证，禁止使用他们的成果来训练其他LLMs，这使得找到合适的教师模型具有挑战性。</p><h1 id="6-模型服务技术"><a href="#6-模型服务技术" class="headerlink" title="6.模型服务技术"></a>6.模型服务技术</h1><p>模型执行通常受内存带宽限制，特别是权重中的带宽限制。即使在应用了前面描述的所有模型优化之后，它仍然很可能受到内存限制。因此，在加载模型权重时尽可能多地处理它们。换句话说，尝试并行。可以采取两种方法：</p><ul><li>动态批处理(<strong>In-flight batching</strong>) ：同时执行多个不同的请求。</li><li>预测推理(<strong>Speculative inference</strong>) ：并行执行序列的多个不同步骤以尝试节省时间。</li></ul><h2 id="6-1-动态批处理（In-flight-batching）"><a href="#6-1-动态批处理（In-flight-batching）" class="headerlink" title="6.1 动态批处理（In-flight batching）"></a>6.1 动态批处理（<strong>In-flight batching</strong>）</h2><p>LLMs 具有一些独特的执行特征，这些特征可能导致在实践中难以有效地处理批量请求。一个模型可以同时用于多种不同的任务。从聊天机器人中的简单问答响应到文档摘要或代码块的生成，工作负载是高度动态的，输出大小变化几个数量级。</p><p>这种多功能性使得批处理请求并有效地并行执行它们变得具有挑战性，这是服务神经网络的常见优化。这可能会导致某些请求比其他请求更早完成。</p><p>为了管理这些动态负载，许多LLMs 服务解决方案包括一种称<strong>为连续或动态批处理的优化调度技术</strong>。这利用了这样一个事实：<strong>LLMs的整个文本生成过程可以分解为模型上的多次执行迭代</strong>。</p><p>通过动态批处理，服务器运行时会<strong>立即从批处理中剔除已完成的序列，而不是等待整个批处理完成后再继续处理下一组请求</strong>。然后，它开始执行新请求，而其他请求仍在进行中。因此，动态批处理可以极大地提高实际用例中 GPU 的整体利用率。</p><h2 id="6-2-预测推理（Speculative-inference）"><a href="#6-2-预测推理（Speculative-inference）" class="headerlink" title="6.2 预测推理（Speculative inference）"></a>6.2 预测推理（<strong>Speculative inference</strong>）</h2><p>预测推理也称为推测采样、辅助生成或分块并行解码，是并行执行 LLM 的另一种方式。通常，GPT 风格的大语言模型是自回归模型，逐个生成文本标记。</p><p>生成的每个标记都依赖于它之前的所有标记来提供上下文。这意味着在常规执行中，<strong>不可能从同一个序列并行生成多个token，必须等待第 n 个token生成后才能生成 n+1 个token</strong>。</p><p>图 12 显示了预测推理的示例，其中临时模型临时预测并行验证或拒绝的多个未来步骤。在这种情况下，临时模型中的前两个预测token被接受，而最后一个在继续生成之前被拒绝并删除。</p><p><img src="image/image_yJilh0txbj.png" alt=""></p><blockquote><p>图12， 预测推理示例</p></blockquote><p>预测性抽样提供了一种解决方法。这种方法的基本思想是使用一些“更便宜”的过程来生成几个token长的临时序列。然后，并行执行多个步骤的主要“验证”模型，使用廉价临时序列作为需要的执行步骤的“预测”上下文。</p><p>如果验证模型生成与临时序列相同的token，那么就知道接受这些token作为输出。否则，可以丢弃第一个不匹配标记之后的所有内容，并使用新的临时序列重复该过程。</p><p>如何生成临时token有许多不同的选项，每个选项都有不同的权衡。可以训练多个模型，或在单个预训练模型上微调多个头，以预测未来多个步骤的标记。或者，可以使用小型模型作为临时模型，使用更大、功能更强大的模型作为验证器。</p><h1 id="7-结论"><a href="#7-结论" class="headerlink" title="7.结论"></a>7.结论</h1><p>这篇文章概述了许多最流行的解决方案，以帮助高效地优化和服务LLMs，无论是在数据中心还是在 PC 边缘。其中许多技术都经过优化并通过 NVIDIA TensorRT-LLM 提供，这是一个开源库，由 TensorRT 深度学习编译器以及优化的内核、预处理和后处理步骤以及多 GPU/多节点通信原语组成，可在 NVIDIA 上实现突破性的性能GPU。</p>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>主流大语言模型的技术原理细节</title>
      <link href="/llms/llms_article/2.%E4%B8%BB%E6%B5%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E7%BB%86%E8%8A%82/"/>
      <url>/llms/llms_article/2.%E4%B8%BB%E6%B5%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E7%BB%86%E8%8A%82/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文链接：<a href="https://mp.weixin.qq.com/s?__biz=MjM5ODYwMjI2MA==\&amp;mid=2649780482\&amp;idx=1\&amp;sn=c3fa07e79ba4336e897825d326b78fe8\&amp;chksm=becce27989bb6b6fbdcfd0929202179c7bd397b0004774f41c38c004a95e2c2806f888bfba05\&amp;mpshare=1\&amp;scene=1\&amp;srcid=12032Oax0bYsEC5VYpWhiYvN\&amp;sharer_shareinfo=94bac8cb7ddbd97d657c0d29daca150a\&amp;sharer_shareinfo_first=94bac8cb7ddbd97d657c0d29daca150a#rd" title="主流大语言模型的技术原理细节 (qq.com)">主流大语言模型的技术原理细节 (qq.com)</a><br>作者：spring</p><p>1.比较 LLaMA、ChatGLM、Falcon 等大语言模型的细节：tokenizer、位置编码、Layer Normalization、激活函数等。<br>2. 大语言模型的分布式训练技术：数据并行、张量模型并行、流水线并行、3D 并行、零冗余优化器 ZeRO、CPU 卸载技术 ZeRo-offload、混合精度训练、激活重计算技术、Flash Attention、Paged Attention。<br>3. 大语言模型的参数高效微调技术：prompt tuning、prefix tuning、adapter、LLaMA-adapter、 LoRA。</p></blockquote><h3 id="0-大纲"><a href="#0-大纲" class="headerlink" title="0. 大纲"></a>0. 大纲</h3><p><img src="image/image_vG712dLCGh.png" alt=""></p><p><img src="image/image_fDhS6e_Q05.png" alt=""></p><p><img src="image/image_h-PwxL33hn.png" alt=""></p><h3 id="1-大语言模型的细节"><a href="#1-大语言模型的细节" class="headerlink" title="1. 大语言模型的细节"></a>1. 大语言模型的细节</h3><h4 id="1-0-transformer-与-LLM"><a href="#1-0-transformer-与-LLM" class="headerlink" title="1.0 transformer 与 LLM"></a>1.0 transformer 与 LLM</h4><p><img src="image/image_zfBv0vDfR3.png" alt=""></p><p><img src="image/image_7AUOzZkcZh.png" alt=""></p><h4 id="1-1-模型结构"><a href="#1-1-模型结构" class="headerlink" title="1.1 模型结构"></a>1.1 模型结构</h4><p><img src="image/image_hJO9LNcB2O.png" alt=""></p><p><img src="image/image_pZ5s7kZYTq.png" alt=""></p><h4 id="1-2-训练目标"><a href="#1-2-训练目标" class="headerlink" title="1.2 训练目标"></a>1.2 训练目标</h4><p><img src="image/image_uYNSWQD8US.png" alt=""></p><h4 id="1-3-tokenizer"><a href="#1-3-tokenizer" class="headerlink" title="1.3 tokenizer"></a>1.3 tokenizer</h4><p><img src="image/image_qX0cWe-UaL.png" alt=""></p><p><img src="image/image_ndc5N3EwSH.png" alt=""></p><p><img src="image/image_OWCP1eEpLW.png" alt=""></p><h4 id="1-4-位置编码"><a href="#1-4-位置编码" class="headerlink" title="1.4 位置编码"></a>1.4 位置编码</h4><p><img src="image/image_UjFBGO1T35.png" alt=""></p><p><img src="image/image_769YzSe6Q0.png" alt=""></p><h4 id="1-5-层归一化"><a href="#1-5-层归一化" class="headerlink" title="1.5 层归一化"></a>1.5 层归一化</h4><p><img src="image/image_SxR2rKRxlk.png" alt=""></p><p><img src="image/image_bUKeFleTjZ.png" alt=""></p><p><img src="image/image__bNqnWpDv4.png" alt=""></p><p><img src="image/image_U4ZlSbYTVt.png" alt=""></p><h4 id="1-6-激活函数"><a href="#1-6-激活函数" class="headerlink" title="1.6 激活函数"></a>1.6 激活函数</h4><p><img src="image/image_pp3NHI3beG.png" alt=""></p><p><img src="image/image_QHVCBhgG1n.png" alt=""></p><h4 id="1-7-Multi-query-Attention-与-Grouped-query-Attention"><a href="#1-7-Multi-query-Attention-与-Grouped-query-Attention" class="headerlink" title="1.7 Multi-query Attention 与 Grouped-query Attention"></a>1.7 Multi-query Attention 与 Grouped-query Attention</h4><p><img src="image/image_vP-RO6IwA8.png" alt=""></p><p><img src="image/image_0diEso76i0.png" alt=""></p><p><img src="image/image_7a3Bg1BPi5.png" alt=""></p><p><img src="image/image_CECcN4JfEf.png" alt=""></p><h4 id="1-8-并行-transformer-block"><a href="#1-8-并行-transformer-block" class="headerlink" title="1.8 并行 transformer block"></a>1.8 并行 transformer block</h4><p><img src="image/image__SS2VjLE2Z.png" alt=""></p><h4 id="1-9-总结-训练稳定性"><a href="#1-9-总结-训练稳定性" class="headerlink" title="1.9 总结-训练稳定性"></a>1.9 总结-训练稳定性</h4><p><img src="image/image_T1MV1A5l3x.png" alt=""></p><h3 id="2-LLM-的分布式预训练"><a href="#2-LLM-的分布式预训练" class="headerlink" title="2. LLM 的分布式预训练"></a>2. LLM 的分布式预训练</h3><p><img src="image/image_g6L5saTdk3.png" alt=""></p><h4 id="2-0-点对点通信与集体通信"><a href="#2-0-点对点通信与集体通信" class="headerlink" title="2.0 点对点通信与集体通信"></a>2.0 点对点通信与集体通信</h4><p><img src="image/image_ihXV9dK0fN.png" alt=""></p><p><img src="image/image_ncVO0b0SUs.png" alt=""></p><h4 id="2-1-数据并行"><a href="#2-1-数据并行" class="headerlink" title="2.1 数据并行"></a>2.1 数据并行</h4><p><img src="image/image_ESYis9e2xf.png" alt=""></p><p><img src="image/image_NcIBEKSkSZ.png" alt=""></p><p><img src="image/image_MWKeJ5U-w2.png" alt=""></p><h4 id="2-2-张量并行"><a href="#2-2-张量并行" class="headerlink" title="2.2 张量并行"></a>2.2 张量并行</h4><p><img src="image/image_uHbQmMndQz.png" alt=""></p><p><img src="image/image_tYzZu8USqj.png" alt=""></p><p><img src="image/image_QPV7tUDWyB.png" alt=""></p><p><img src="image/image_40RjmWBLsE.png" alt=""></p><p><img src="image/image_7peCAn_G43.png" alt=""></p><p><img src="image/image_f6eaZHSgRD.png" alt=""></p><p><img src="image/image_FJ9JFMLxaM.png" alt=""></p><p><img src="image/image_mm-4C0SSZb.png" alt=""></p><p><img src="image/image_8Ij4nFTVE9.png" alt=""></p><p><img src="image/image_FhqE9cEjpM.png" alt=""></p><p><img src="image/image_9hIR96GsZ6.png" alt=""></p><p><img src="image/image_nT1MkxKfTo.png" alt=""></p><p><img src="image/image_jYhnRBsS3p.png" alt=""></p><p><img src="image/image_SnoonSUVKE.png" alt=""></p><h3 id="2-3-流水线并行"><a href="#2-3-流水线并行" class="headerlink" title="2.3 流水线并行"></a>2.3 流水线并行</h3><p><img src="image/image_Qi6n07S4Yn.png" alt=""></p><h4 id="2-4-3D-并行"><a href="#2-4-3D-并行" class="headerlink" title="2.4 3D 并行"></a>2.4 3D 并行</h4><p><img src="image/image_C8X34PTSig.png" alt=""></p><p><img src="image/image_2-q2wgbbNY.png" alt=""></p><h4 id="2-5-混合精度训练"><a href="#2-5-混合精度训练" class="headerlink" title="2.5 混合精度训练"></a>2.5 混合精度训练</h4><p><img src="image/image_HS1P0aBOMX.png" alt=""></p><p><img src="image/image_SpJ8rBd02E.png" alt=""></p><p><img src="image/image_Q49FURFp5-.png" alt=""></p><h4 id="2-6-激活重计算"><a href="#2-6-激活重计算" class="headerlink" title="2.6 激活重计算"></a>2.6 激活重计算</h4><p><img src="image/image_ZN1Srx08dF.png" alt=""></p><p><img src="image/image_NXgve1nebv.png" alt=""></p><h4 id="2-7-ZeRO，零冗余优化器"><a href="#2-7-ZeRO，零冗余优化器" class="headerlink" title="2.7 ZeRO，零冗余优化器"></a>2.7 ZeRO，零冗余优化器</h4><p><img src="image/image_5gdEHzItzl.png" alt=""></p><p><img src="image/image_Ert94Du_Cb.png" alt=""></p><p><img src="image/image_srSrKJ1y4j.png" alt=""></p><p><img src="image/image_2pyK_Xai5w.png" alt=""></p><p><img src="image/image_OizWBLJp1_.png" alt=""></p><p><img src="image/image_63SDqJNGsE.png" alt=""></p><h4 id="2-8-CPU-offload，ZeRO-offload"><a href="#2-8-CPU-offload，ZeRO-offload" class="headerlink" title="2.8 CPU-offload，ZeRO-offload"></a>2.8 CPU-offload，ZeRO-offload</h4><p><img src="image/image_FbeAAmphM2.png" alt=""></p><h4 id="2-9-Flash-Attention"><a href="#2-9-Flash-Attention" class="headerlink" title="2.9 Flash Attention"></a>2.9 Flash Attention</h4><p><img src="image/image_6xewPnPX6o.png" alt=""></p><p><img src="image/image_gPT-SFQ5Cr.png" alt=""></p><p><img src="image/image_V3gPmQIMWh.png" alt=""></p><p><img src="image/image_bB0UcSswFu.png" alt=""></p><h4 id="2-10-vLLM-Paged-Attention"><a href="#2-10-vLLM-Paged-Attention" class="headerlink" title="2.10 vLLM: Paged Attention"></a>2.10 vLLM: Paged Attention</h4><p><img src="image/image_gHry-_rZKE.png" alt=""></p><h3 id="3-LLM-的参数高效微调"><a href="#3-LLM-的参数高效微调" class="headerlink" title="3. LLM 的参数高效微调"></a>3. LLM 的参数高效微调</h3><h4 id="3-0-为什么进行参数高效微调？"><a href="#3-0-为什么进行参数高效微调？" class="headerlink" title="3.0 为什么进行参数高效微调？"></a>3.0 为什么进行参数高效微调？</h4><p><img src="image/image_ZT8aB_yEFo.png" alt=""></p><p><img src="image/image_FNBimj8Sdt.png" alt=""></p><p><img src="image/image_3SPDcb80gZ.png" alt=""></p><h4 id="3-1-prompt-tuning"><a href="#3-1-prompt-tuning" class="headerlink" title="3.1 prompt tuning"></a>3.1 prompt tuning</h4><p><img src="image/image_wzK48Wc6HS.png" alt=""></p><h4 id="3-2-prefix-tuning"><a href="#3-2-prefix-tuning" class="headerlink" title="3.2 prefix tuning"></a>3.2 prefix tuning</h4><p><img src="image/image_UlmhfIonZx.png" alt=""></p><h4 id="3-3-adapter"><a href="#3-3-adapter" class="headerlink" title="3.3 adapter"></a>3.3 adapter</h4><p><img src="image/image_UZWP8kpLG0.png" alt=""></p><h4 id="3-4-LLaMA-adapter"><a href="#3-4-LLaMA-adapter" class="headerlink" title="3.4 LLaMA adapter"></a>3.4 LLaMA adapter</h4><p><img src="image/image_X81mYHpxSG.png" alt=""></p><h4 id="3-5-LoRA"><a href="#3-5-LoRA" class="headerlink" title="3.5 LoRA"></a>3.5 LoRA</h4><p><img src="image/image_KHZlIc4d2-.png" alt=""></p><h4 id="3-6-实验比较"><a href="#3-6-实验比较" class="headerlink" title="3.6 实验比较"></a>3.6 实验比较</h4><p><img src="image/image_B_-weA4duH.png" alt=""></p><p><strong>4. 参考文献</strong></p><p><img src="image/image_DXJ_D6d71c.png" alt=""></p><p><img src="image/image_42rbLF6cY-.png" alt=""></p><ol><li><a href="https://zhuanlan.zhihu.com/p/624740065" title="分析 transformer 模型的参数量、计算量、中间激活、KV cache">分析 transformer 模型的参数量、计算量、中间激活、KV cache</a></li><li><a href="https://zhuanlan.zhihu.com/p/635710004" title="【万字长文】LLaMA, ChatGLM, BLOOM 的高效参数微调实践">【万字长文】LLaMA, ChatGLM, BLOOM 的高效参数微调实践</a></li><li><a href="https://zhuanlan.zhihu.com/p/639228219" title="FlashAttention:加速计算,节省显存, IO 感知的精确注意力">FlashAttention:加速计算,节省显存, IO 感知的精确注意力</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 MoE经典论文简牍</title>
      <link href="/paper_reading/1.7.MoE%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E7%AE%80%E7%89%8D/"/>
      <url>/paper_reading/1.7.MoE%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E7%AE%80%E7%89%8D/</url>
      
        <content type="html"><![CDATA[<p>参考资料：</p><ul><li><a href="https://beyondguo.github.io/paper_notes/notes/MoE——Mixture of Experts经典论文一览.html#开创工作" title="MoE (Mixture-of-Experts) 经典文章简读">MoE (Mixture-of-Experts) 经典文章简读</a></li></ul><h2 id="1-开创工作"><a href="#1-开创工作" class="headerlink" title="1.开创工作"></a>1.开创工作</h2><h3 id="1-1-MoE"><a href="#1-1-MoE" class="headerlink" title="1.1  MoE"></a>1.1  MoE</h3><ul><li>论文名称：Adaptive mixtures of local experts, Neural Computation’1991</li><li>期刊/会议：Neural Computation (1991)</li><li>论文链接：<a href="https://readpaper.com/paper/2150884987" title="https://readpaper.com/paper/2150884987">https://readpaper.com/paper/2150884987</a></li><li>代表性作者：Michael Jordan, Geoffrey Hinton</li></ul><p>这是大多数MoE论文都引用的最早的一篇文章，发表于1991年，作者中有两个大家熟知的大佬：Michael Jordan 和 Geoffrey Hinton。</p><p>提出了一种新的监督学习过程，<strong>一个系统中包含多个分开的网络，每个网络去处理全部训练样本的一个子集</strong>。这种方式可以看做是把多层网络进行了<strong>模块化的转换</strong>。</p><p>假设我们已经知道数据集中存在一些天然的子集（比如来自不同的domain，不同的topic），那么用单个模型去学习，就会受到很多干扰（interference），导致学习很慢、泛化困难。这时，我们可以使用多个模型（即专家，expert）去学习，使用一个门网络（gating network）来决定每个数据应该被哪个模型去训练，这样就可以减轻不同类型样本之间的干扰。</p><p>其实这种做法，也不是该论文第一次提出的，更早就有人提出过类似的方法。对于一个样本 c，第 i 个 expert 的输出为 $\mathbf{o}_i^c$，理想的输出是 $\mathbf{d}^c$，那么损失函数就这么计算：</p><script type="math/tex; mode=display">\mathrm{E}^{\mathrm{c}}=\left\|\mathbf{d}^{\mathrm{c}}-\sum_{\mathrm{i}} \mathrm{p}_{\mathrm{i}}^{\mathrm{c}} \mathbf{o}_{\mathrm{i}}^{\mathrm{c}}\right\|^{2}</script><p>其中 $p_i^c$ 是 gating network 分配给每个 expert 的权重，相当于多个 expert 齐心协力来得到当前样本 c 的输出。</p><p>这是一个很自然的设计方式，但是存在一个问题——<strong>不同的 expert 之间的互相影响会非常大</strong>，一个expert的参数改变了，其他的都会跟着改变，即所谓牵一发而动全身。这样的设计，最终的结果就是一个样本会使用很多的expert来处理。于是，这篇文章设计了一种新的方式，<strong>调整了一下loss的设计，来鼓励不同的expert之间进行竞争</strong>：</p><script type="math/tex; mode=display">E^{\mathrm{c}}=\sum_{i} p_{i}^{c}\left\|\mathbf{d}^{c}-\mathbf{o}_{i}^{\mathrm{c}}\right\|^{2}</script><p>就是<strong>让不同的 expert 单独计算 loss，然后在加权求和得到总体的 loss</strong>。这样的话，每个专家，都有独立判断的能力，而不用依靠其他的 expert 来一起得到预测结果。下面是一个示意图：</p><p><img src="image/image_Rdw4cbbvnS.png" alt=""></p><p>在这种设计下，我们将 experts 和 gating network 一起进行训练，最终的系统就会倾向于让一个 expert 去处理一个样本。</p><p>上面的<strong>两个 loss function，其实长得非常像，但是一个是鼓励合作，一个是鼓励竞争</strong>。这一点还是挺启发人的。</p><p>论文还提到另外一个很启发人的 trick，就是上面那个损失函数，作者在实际做实验的时候，用了一个变体，使得效果更好：</p><script type="math/tex; mode=display">Original : \mathrm{E}^{\mathrm{c}}=\sum_{i} \mathrm{p}_{\mathrm{i}}^{\mathrm{c}}\left\|\mathbf{d}^{\mathrm{c}}-\mathbf{o}_{\mathrm{i}}^{\mathrm{c}}\right\|^{2}</script><script type="math/tex; mode=display">Modified : \mathrm{E}^{\mathrm{c}}=-\log \sum_{\mathrm{i}} \mathrm{p}_{\mathrm{i}}^{\mathrm{C}} \mathrm{e}^{-\frac{1}{2}\left\|\mathrm{~d}^{\mathrm{c}}-\mathbf{o}_{\mathrm{i}}^{\mathrm{c}}\right\|^{2}}</script><p>对比一下可以看出，在计算每个 expert 的损失之后，<strong>先把它给指数化了再进行加权求和，最后取了log</strong>。这也是一个我们在论文中经常见到的技巧。这样做有什么好处呢，我们可以对比一下二者在反向传播的时候有什么样的效果，使用$  E^c  $对 第 i 个 expert 的输出求导，分别得到：</p><script type="math/tex; mode=display">original ~derivative: \frac{\partial E^{c}}{\partial \mathbf{o}_{i}^{c}}=-2 p_{i}^{c}\left(\mathbf{d}^{c}-\mathbf{o}_{i}^{c}\right)</script><script type="math/tex; mode=display">new~derivative: \frac{\partial E^{c}}{\partial \mathbf{o}_{i}^{c}}=-\left[\frac{p_{i}^{c} e^{-\frac{1}{2}\left\|\mathbf{d}^{c}-\mathbf{o}_{i}^{c}\right\|^{2}}}{\sum_{j} p_{j}^{c} e^{-\frac{1}{2}\left\|\mathbf{d}^{c}-\mathbf{o}_{j}^{c}\right\|^{2}}}\right]\left(\mathbf{d}^{c}-\mathbf{o}_{i}^{c}\right)</script><p>可以看到，<strong>前者的导数，只会跟当前 expert 有关，但后者则还考虑其他 experts 跟当前 sample c 的匹配程度</strong>。换句话说，如果当前 sample 跟其他的 experts 也比较匹配，那么 $ E^c  $对 第 i 个 expert 的输出的导数也会相对更小一下。（其实看这个公式，跟我们现在遍地的对比学习loss真的很像！很多道理都是相通的）</p><p>以上就是这篇文章的理论部分，其实很简单，但它提到的MoE的设计，启发了后续无数的工作。</p><p>接下来一篇则是时隔20多年后的另一篇经典论文，可能也是大家更熟悉的MoE工作。</p><h3 id="1-2-Sparsely-Gated-MoE"><a href="#1-2-Sparsely-Gated-MoE" class="headerlink" title="1.2  Sparsely-Gated MoE"></a>1.2  Sparsely-Gated MoE</h3><ul><li>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, ICLR’17</li><li>期刊/会议：ICLR’17</li><li>论文链接：<a href="https://readpaper.com/paper/2952339051" title="https://readpaper.com/paper/2952339051">https://readpaper.com/paper/2952339051</a></li><li>代表性作者：Quoc Le, Geoffrey Hinton, Jeff Dean</li></ul><p>这篇文章，从title上就可以看出来它的背景和目的——希望做出极大的神经网络。在此之前，有很多 <strong>conditional computational</strong> 的工作，在理论上可以在有限的计算成本内把模型做的非常大，但是那些方法在具体实现的时候，有各种各样的问题。这篇文章提出了 Sparsely-Gated Mixture-of-Experts layer ，声称终于解决了传统 conditional computational 的问题，在牺牲极少的计算效率的情况下，把模型规模提升1000多倍。</p><h4 id="（1）Sparsely-Gated-Mixture-of-Experts-layer"><a href="#（1）Sparsely-Gated-Mixture-of-Experts-layer" class="headerlink" title="（1）Sparsely-Gated Mixture-of-Experts layer"></a>（1）Sparsely-Gated Mixture-of-Experts layer</h4><p>跟1991年那个工作对比，这里的MoE主要有两个区别：</p><ul><li><strong>Sparsely-Gated</strong>：不是所有expert都会起作用，而是极少数的expert会被使用来进行推理。这种稀疏性，也使得我们可以使用海量的experts来把模型容量做的超级大。</li><li><strong>token-level</strong>：前面那个文章，是 sample-level 的，即不同的样本，使用不同的experts，但是这篇则是 token-level 的，一个句子中不同的token使用不同的experts。</li></ul><p>这篇文章是在RNN的结构上加入了MoE layer：</p><p><img src="image/image_AzoU_S8TUl.png" alt=""></p><p>如图所示，每个token对应的position，都会有一个MoE Layer，每个MoE layer中包含了一堆的experts，每个expert都是一个小型的FFN，还有一个gating network会根据当前position的输入，选择少数几个expert来进行计算。</p><h4 id="（2）Gating-Network"><a href="#（2）Gating-Network" class="headerlink" title="（2）Gating Network"></a>（2）Gating Network</h4><p>设 $G(x)$ 和 $ E_i(x)  $分别是 gating network 和第 i 个 expert 的输出，那么对于在当前position的输入x，输出就是所有 experts 的加权和：</p><script type="math/tex; mode=display">\mathrm{y}=\sum_{\mathrm{i}=1}^{\mathrm{n}} \mathrm{G}(\mathrm{x})_{\mathrm{i}} \mathrm{E}_{\mathrm{i}}(\mathrm{x})</script><p>(跟第一篇论文的第一个公式类似)</p><p>但是这里我们可能有上千个 experts，如果每个都算的话，计算量会非常大，所以这里的一个关键就是希望 G(x) 的输出是稀疏的，只有部分的 experts 的权重是大于 0 的，其余等于 0 的 expert 直接不参与计算。</p><p>首先看传统的 gating network 如何设计：</p><script type="math/tex; mode=display">\mathrm{G}_{\sigma}(\mathrm{x})=\operatorname{Softmax}\left(\mathrm{x} \cdot \mathrm{W}_{\mathrm{g}}\right)</script><p>然后，作者<strong>加入了 sparsity 和 noise</strong>：</p><script type="math/tex; mode=display">\mathrm{G}(\mathrm{x})=\operatorname{Softmax}(\operatorname{KeepTopK}(\mathrm{H}(\mathrm{x}), \mathrm{k}))</script><script type="math/tex; mode=display">\mathrm{H}(\mathrm{x})_{\mathrm{i}}=\left(\mathrm{x} \cdot \mathrm{W}_{\mathrm{g}}\right)_{\mathrm{i}}+\operatorname{StandardNormal}() \cdot \operatorname{Softplus}\left(\left(\mathrm{x} \cdot \mathrm{W}_{\text {noise }}\right)_{\mathrm{i}}\right)</script><script type="math/tex; mode=display">\operatorname{KeepTopK}(\mathrm{v}, \mathrm{k})_{\mathrm{i}}=\left\{\begin{array}{ll}\mathrm{v}_{\mathrm{i}}, & \text { if } \mathrm{v}_{\mathrm{i}} \text { intopKelements. } \\ -\infty, & \text { otherwise. }\end{array}\right.</script><p>总而言之，<strong>sparsity 是通过 TopK sampling 的方式实现的，对于非 TopK 的部分，由于值是负无穷，这样在经过 softmax 之后就会变成 0，就相当于关门了</strong>。noise 项则可以使得不同 expert 的负载更加均衡。在具体实验中，作者使用的K=2~4.</p><h4 id="（3）Expert-Balancing"><a href="#（3）Expert-Balancing" class="headerlink" title="（3）Expert Balancing"></a>（3）Expert Balancing</h4><p>作者在实验中发现，不同 experts 在竞争的过程中，会出现“<strong>赢者通吃</strong>”的现象：前期变现好的 expert 会更容易被 gating network 选择，导致最终只有少数的几个 experts 真正起作用。因此作者<strong>额外增加了一个 loss，来缓解这种不平衡现象</strong>，公式如下：</p><script type="math/tex; mode=display">\operatorname{Importance}(\mathrm{X})=\sum_{\mathrm{x} \in \mathrm{X}} \mathrm{G}(\mathrm{x})</script><script type="math/tex; mode=display">\mathrm{L}(\mathrm{X})=\lambda \cdot \mathrm{CV}(\text { Importance }(\mathrm{X}))^{2}</script><p>其中 X 代表的是一个batch的样本，把一个batch所有样本的gating weights加起来，然后计算变异系数（ coefficient of variation）。总之，<strong>这个反映了不同 experts 之间不平衡的程度</strong>。最后这个 loss 会加到总体 loss 中，鼓励不同的 experts 都发挥各自的作用。</p><p>上面就是 Sparsely-Gated MoE的主要理论，作者主要在 language modeling 和 machine translation 两个任务上做了实验，因为这两个任务，都是特别受益于大数据和大模型的，而本文的MoE的作用主要就在于极大地扩大了模型容量——通过MoE，把RNN-based网络做到了137B（1.3千亿）参数的规模，还是挺震撼的。效果自然也是极好的。</p><p>经过训练呢，作者发现不同的 experts 确实分化出了不同的“专业”：</p><p><img src="image/image_7Y10AK-9SU.png" alt=""></p><p>上面的两篇，是MoE系列工作的基础，接下来介绍的工作，都是近几年的比较出名的工作：</p><h2 id="2-使用-MoE-开发超大模型"><a href="#2-使用-MoE-开发超大模型" class="headerlink" title="2.使用 MoE 开发超大模型"></a>2.使用 MoE 开发超大模型</h2><h3 id="2-1-GShard"><a href="#2-1-GShard" class="headerlink" title="2.1 GShard"></a>2.1 GShard</h3><ul><li>论文名称：GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding, ICLR’21</li><li>期刊/会议：ICLR’21</li><li>论文链接：<a href="https://readpaper.com/paper/3040573126" title="https://readpaper.com/paper/3040573126">https://readpaper.com/paper/3040573126</a></li></ul><p>GShard，按照文章的说法，是第一个将MoE的思想拓展到Transformer上的工作。具体的做法是，把Transformer的encoder和decoder中，<strong>每隔一个</strong>（every other）的FFN层，替换成position-wise 的 MoE 层，使用的都是 Top-2 gating network。</p><p><img src="image/image_bAlILycLOq.png" alt=""></p><p>文中还提到了很多其他设计：</p><ul><li><strong>Expert capacity balancing</strong>：强制每个expert处理的tokens数量在一定范围内</li><li><strong>Local group dispatching</strong>：通过把一个batch内所有的tokens分组，来实现并行化计算</li><li><strong>Auxiliary loss</strong>：也是为了缓解“赢者通吃”问题</li><li><strong>Random routing</strong>：在Top-2 gating的设计下，两个expert如何更高效地进行routing</li></ul><h3 id="2-2-Switch-Transformers"><a href="#2-2-Switch-Transformers" class="headerlink" title="2.2 Switch Transformers"></a>2.2 Switch Transformers</h3><ul><li>论文名称：Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity, JMLR’22</li><li>期刊/会议：JMLR’22</li><li>论文链接：<a href="https://readpaper.com/paper/4568736324836663297" title="https://readpaper.com/paper/4568736324836663297">https://readpaper.com/paper/4568736324836663297</a></li></ul><p>虽然发表是2022年才在发表在JMLR上，Swith Transformer实际上在21年就提出了。它是在<strong>T5模型的基础上加入了MoE设计</strong>，并在C4数据集上预训练，得到了一个“又快又好”的预训练大模型。</p><p>Swith Transformer 的主要亮点在于——<strong>简化了MoE的routing算法，从而大大提高了计算效率。</strong></p><p>结构如下：</p><p><img src="image/image_AVxM-QvgGS.png" alt=""></p><p>Swith Transformer 在论文中提到其设计的指导原则是——<strong>尽可能地把Transformer模型的参数量做大！</strong>（同时以一种简单高效的实现方式）</p><p>跟其他MoE模型的一个显著不同就是，<strong>Switch Transformer 的 gating network 每次只 route 到 1 个 expert</strong>，而其他的模型都是至少2个。这样就是最稀疏的MoE了，因此单单从MoE layer的计算效率上讲是最高的了。</p><p>下图展示了在同样的计算开销下，增大 experts 个数带来的性能提升，反正就是全面吊打T5，而且效率还一样：</p><p><img src="image/image_MBw7-fLRot.png" alt=""></p><h3 id="2-3-GLaM"><a href="#2-3-GLaM" class="headerlink" title="2.3 GLaM"></a>2.3 GLaM</h3><ul><li>论文名称：GLaM: Efficient Scaling of Language Models with Mixture-of-Experts, 2021</li><li>年份：2021</li><li>论文链接：<a href="https://readpaper.com/paper/4568736324836663297" title="https://readpaper.com/paper/4568736324836663297">https://readpaper.com/paper/4568736324836663297</a></li><li>Google Blog：<a href="https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html" title="https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html">https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html</a></li></ul><p>这是Google在2021年推出的一个超大模型，比GPT-3大三倍，但是由于使用了Sparse MoE的设计，训练成本却只有GPT-3的1/3，而且在29个NLP任务上超越了GPT-3。</p><p>下面这个来自Google Blog的动图很形象地展示了GLaM的结构：</p><p><img src="image/202207161754344_2lH2Rw84vu.gif" alt=""></p><p>其实我们可以发现，跟GShard几乎一模一样。</p><p><img src="image/image_QGXL7c6bPn.png" alt=""></p><p>上表展示了GLaM跟其他大模型的对比。可以看到，虽然GLaM的总参数量有1.2T，但是在计算式实际激活的参数量只有96B，所以在inference的时候，比GPT-3等dense model要快得多。</p><p>GLaM使用的数据量也比Switch-Transformer等要大得多：</p><p><img src="image/image_0ed01JRayq.png" alt=""></p><p>反正最终的结果，是一个比GPT-3更快更强大的通用LM。</p><h3 id="2-4-小结"><a href="#2-4-小结" class="headerlink" title="2.4 小结"></a>2.4 小结</h3><p>上面的三篇文章（GShard，Switch-Transformer，GLaM）都是希望通过MoE的方式把模型做得尽可能的大，大到普通人玩不起（动辄使用几百个experts），下面介绍的两篇文章，则更加亲民一点，是关于如何利用MoE去压缩模型、提高效率：</p><h2 id="3-使用-MoE-来使模型轻量化"><a href="#3-使用-MoE-来使模型轻量化" class="headerlink" title="3.使用 MoE 来使模型轻量化"></a>3.使用 MoE 来使模型轻量化</h2><h3 id="3-1-WideNet"><a href="#3-1-WideNet" class="headerlink" title="3.1 WideNet"></a>3.1 WideNet</h3><ul><li>论文名称：Go Wider Instead of Deeper, AAAI’22</li><li>期刊/会议：AAAI’22</li><li>论文链接：<a href="https://readpaper.com/paper/3184020733" title="https://readpaper.com/paper/3184020733">https://readpaper.com/paper/3184020733</a></li></ul><p>这个文章名字比较唬人，思路也比较新颖，所以介绍一下。</p><p>它提出了名为 WideNet 的结构，想解决的主要问题是，如何<strong>在压缩模型参数量的情况下取得更好的效果</strong>。比如Albert通过参数共享机制降低了BERT的参数量，像tiny-bert之类的则是减少了Transformer的层数，但他们的性能都有了显著的下降。这篇文章提出，<strong>首先通过层之间的参数共享，来压缩模型大小，然后我们使用MoE的设计，扩大模型容量</strong>（但是模型在feed forward的时候计算量并没怎么提升），这样就可以达到“既要模型参数少，还要模型效果好”的效果。示意图如下：</p><p><img src="image/image_vQz8F5XTXs.png" alt=""></p><p>咋一看，似乎跟前面几个文章一模一样，但这里有一个重要区别：<strong>使用了recurrence机制</strong>，即层之间的参数共享（MoE layer也共享）。另外，为了增加学习的多样性，<strong>normalization layer 并不共享</strong>。</p><p>具体实现时，这里使用总共4个experts，每次选择Top2.</p><p>这样做的结果也挺不错：</p><p><img src="image/image_nDHXqSUrEr.png" alt=""></p><h3 id="3-2-MoEBERT"><a href="#3-2-MoEBERT" class="headerlink" title="3.2 MoEBERT"></a>3.2 MoEBERT</h3><ul><li>论文名称：MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation, NAACL’22</li><li>期刊/会议：NAACL’22</li><li>论文链接：<a href="https://readpaper.com/paper/4614341372211634177" title="https://readpaper.com/paper/4614341372211634177">https://readpaper.com/paper/4614341372211634177</a></li></ul><p>这一篇文章，则是结合了 MoE 和 knowledge distillation，在提升 inference 速度的情况下，还能提高效果。主要想解决传统的distillation方法掉点的问题。具体做法是把一个<strong>预训练好</strong>的模型（比如BERT）的FFN层分解成多个experts，这样在计算的时候速度可以大幅提高（相当于只激活原始FFN网络的一部分）。然后再通过模型蒸馏把原始模型的知识蒸馏到MoE版本的模型中。</p><p>注意这个文章其实跟上面介绍的WideNet类似，也是为了减少参数量。但有一个区别在于，WideNet是自己从头开始pre-train的，但是本文的MoEBERT则是想尽可能地把已经pre-train好的模型迁移过来，通过distillation的方式在downstream task上直接学。</p><p>因此，如果按照传统的方式让模型自由的去学习不同的experts，效果可能不好，因为你没有大量的数据来预训练。所以这里涉及到一个关键步骤—— <strong>Importance-Guided Adaptation</strong>：</p><p>在把 Transformer 中的FFN layer 改造成 MoE layer 时，我们先去计算 FFN layer 各个 neuron 的 importance，计算公式如下：</p><script type="math/tex; mode=display">I_{j}=\sum_{(x, y) \in \mathcal{D}}\left|\left(\mathbf{w}_{j}^{1}\right)^{\top} \nabla_{\mathbf{w}_{j}^{1}} \mathcal{L}(x, y)+\left(\mathbf{w}_{j}^{2}\right)^{\top} \nabla_{\mathbf{w}_{j}^{2}} \mathcal{L}(x, y)\right|</script><p>这里的 $w^1$ 和 $w^2$ 分别是 FFN layer 的某个 neuron 的输出和输出 weights vector，这个 importance score 也被应用于很多 model pruning 的工作中来衡量网络的某个 unit 的重要性。然后，在把 FFN 分解的时候，我们<strong>取最重要的一部分 neurons 在每个expert 中共享</strong>，剩下的部分平均分配到每个 expert。由于共享机制的存在，一定会多出一些 neurons，这部分就直接丢弃。（注意，这里我们并没有增加模型的参数量，而只是把一个全连接的FFN层，分解成多个sub-networks，加起来的参数量实际上是一样的）</p><p>这个示意图很形象：</p><p><img src="image/image_c8n4gjb_5G.png" alt=""></p><p>另外一个值得注意的点在于 expert routing 的方式，这里没有使用一个 gating network，而是<strong>在训练前直接给每个 token 都随机分配了一个 expert</strong> （具体是通过一个 hash function）。</p><p>在distillation部分，这里使用的逐层的distillation MSE loss，以及最后预测概率的 KL loss，二者加起来就是distillation 所使用的 loss。然后，再和原本自己的 CE loss 加起来，就是总体模型训练的loss。这里是直接在downstream dataset上面进行训练，属于 task-specific distillation。</p><p><img src="image/image_WStjImtAzp.png" alt=""></p><p>实验的结果也验证了 MoEBERT可以在同样参数量（effective parameters，MoE layer中只考虑被激活的experts）的情况下超越其他 distillation baselines。</p><p>值得注意的时，这里的baselines中，task-agnostic的方法都使用了预训练，而task-specific都没有预训练。总体上看，使用了预训练的模型，效果都会更好一些，但是MoEBERT打破了这个规律，在只使用task dataset的情况下，取得了SOTA的结果。</p><p><img src="image/image_l1uK1DUUBT.png" alt=""></p><p>图a验证了前面提到的 Importance-Guided Adaptation 的有效性；图b则是验证了通过hash function的方式，而不是 trainable gating的方式来进行routing 的有效性。</p><h2 id="4-结语："><a href="#4-结语：" class="headerlink" title="4.结语："></a>4.结语：</h2><p>以上总结了一下笔者在阅读 MoE 相关文献时印象较深的几篇文章，上述所阅读的文献主要与NLP相关的，其实 MoE 在各个领域中的应用已经十分广泛。比如Google提出的多模态MoE模型——LIMoE：</p><p><img src="https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/202207162019899.gif" alt=""></p><p>另外，跟 MoE 的理念相关的还有很多有趣的工作，比如：</p><p><strong>Diverse Ensemble Evolution: Curriculum Data-Model Marriage</strong>, NeurIPS’18</p><p><strong>Diversity and Depth in Per-Example Routing Models</strong>, ICLR’21</p><p>MoE 的思想，其实十分符合 Google 提出的 Pathways 愿景，也更加符合通用人工智能的设计理念。虽然目前 MoE 的工作，多数都是开发“超级模型”，但是上面列举的一些工作也表明 MoE 的用途还有很多，可以启发很多方向上方法的改进。</p>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PaperReading </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 MoE</title>
      <link href="/paper_reading/1.6.MoE/"/>
      <url>/paper_reading/1.6.MoE/</url>
      
        <content type="html"><![CDATA[<p>参考文章：</p><ul><li><a href="https://abdulkaderhelwan.medium.com/mixture-of-experts-introduction-39f244a4ff05" title="Mixture of Experts-Introduction">Mixture of Experts-Introduction</a></li><li><a href="https://medium.com/@jain.sm/understanding-the-mixture-of-experts-model-in-deep-learning-71d2e20650ac" title="Understanding the Mixture-of-Experts Model in Deep Learning">Understanding the Mixture-of-Experts Model in Deep Learning</a></li></ul><p>论文相关：</p><ul><li>论文名称：Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</li><li>论文地址：<a href="https://arxiv.org/abs/1701.06538" title="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a></li></ul><p>混合专家（Mixture of Experts，MoE）就像是神经网络世界中的一种团队合作技术。想象一下，把一项大任务分解成更小的部分，让不同的专家来处理每个部分。然后，有一个聪明的法官，他根据情况决定遵循哪位专家的建议，所有这些建议都融合在一起。</p><p>尽管它最初是用神经网络来解释的，但你可以将这个想法用于任何类型的专家或模型。这有点像你把不同的味道结合在一起做一道美味的菜，这属于一组很酷的综合学习方法，称为元学习。</p><p>因此，在本文中，您将了解专家组合模型的技巧。</p><h2 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1.摘要"></a>1.摘要</h2><ul><li>神经网络的<strong>吸收信息的容量（capacity）受限于</strong>参数数目。</li><li><strong>条件计算（conditional computation）**</strong>针对于每个样本，<strong> ​</strong>激活网络的部分子网络进行计算**，它在理论上已证明，可以作为一种显著增加模型容量的方法。</li><li>在实际中，我们在牺牲少量计算效率的情况下，实现了 <strong>1000 倍</strong>的<strong>模型容量（model capacity）</strong> 的提升。</li><li>我们引入了<strong>稀疏门控专家混合层（Sparsely-Gated Mixture-of-Experts Layer）</strong>，包括数以千计的前馈子网络。对于每一个样本，有一个<strong>可训练的门控网络（gating network）会计算这些</strong>专家（指前馈子网络）<strong>的</strong>稀疏组合。</li><li>我们把<strong>专家混合（MoE）应用于</strong>语言建模和<strong>机器翻译</strong>任务中，对于这些任务，从训练语料库中吸收的巨量知识，是十分关键的。</li><li>在我们提出的模型架构里，MoE 包含 1370 亿个参数，以卷积的方式放在<strong>堆叠 LSTM 层</strong>之间。</li><li>在大型语言建模和及其翻译的基准测试中，该模型以更少的计算成本，实现了比最先进方法更好的结果。</li></ul><h2 id="2-介绍和相关工作"><a href="#2-介绍和相关工作" class="headerlink" title="2.介绍和相关工作"></a>2.介绍和相关工作</h2><h3 id="2-1-条件计算"><a href="#2-1-条件计算" class="headerlink" title="2.1 条件计算"></a>2.1 条件计算</h3><p><strong>充分利用训练数据和模型大小的规模</strong>，一直以来都是深度学习成功的关键。</p><ul><li>当训练集足够大，增加神经网络的容量（即参数数目），可以得到更高的预测准确度。</li><li>对于传统的深度学习模型，<strong>对每一个样本都会激活整个模型</strong>，这会导致在训练成本上，以<strong>大约二次方的速度增长</strong>，因为<strong>模型大小和训练样本数目都增加了</strong>。</li><li>当前计算能力和分布式计算的进展，并不能满足这样的需求。</li></ul><p>因此有很多工作提出了各种形式的条件计算，<strong>它们在不显著增加计算成本的情况下</strong>**，尽量增加模型的容量**。</p><ul><li>在这些算法里，<strong>以每个样本为基础（on a per-example basis）</strong>，会<strong>激活或冻结</strong>网络中的大部分。</li><li>这种<strong>门控决策机制</strong>，可以是<strong>二进制的</strong>，也可以是<strong>稀疏而连续的</strong>；可以是<strong>随机性的</strong>，也可以是<strong>确定性的</strong>。</li><li>门控决策通过有各种形式的强化学习和反向传播来训练。</li></ul><p><img src="image/image_yB43QT3_Fz.png" alt=""></p><blockquote><p>Figure 1：MoE 层嵌入到循环语言模型中。在本例中，稀疏的门控函数选择<strong>两个专家</strong>来执行计算。门控网络会调整专家的输出。</p></blockquote><p>尽管这种思想在理论上很有前景，但是目前为止，还没有工作展现在模型容量、训练时间或模型质量上有足够的提升。我们把原因归结为这些挑战：</p><ul><li>现代计算设备（特别是 GPU），相比<strong>分支（branching）而言，在</strong>数值计算上更快。</li><li>大的批量大小对于性能很关键。而条件计算减少了批量大小。</li><li>网络带宽会成为性能瓶颈。</li><li>损失项可能对于实现好的效果是必需的，因此损失项可能会影响模型质量和负载平衡。</li><li>对于大型数据集，模型容量是最关键的。目前条件计算的文献处理的图像识别数据集都相对太小了，难以为大模型提供足够多的信号。</li></ul><p>本文首先解决了上述挑战，并且最后看到了条件计算的前景。</p><ul><li>我们得到了 1000 倍的模型容量提升，只花费了少量计算开销</li><li>得到的结果也优于最顶尖的结果</li></ul><h3 id="2-2-本文方法：稀疏门控专家混合层"><a href="#2-2-本文方法：稀疏门控专家混合层" class="headerlink" title="2.2 本文方法：稀疏门控专家混合层"></a>2.2 本文方法：稀疏门控专家混合层</h3><p>我们的条件计算方法，就是引入了一个新的通用神经网络组件类型：<strong>稀疏门控专家混合层</strong>。</p><p>MoE 包含：</p><ul><li>一些专家，每个专家都是一个<strong>简单的前馈神经网络</strong>。</li><li>一个<strong>可训练的门控网络</strong>，它会挑选专家的一个稀疏组合，用来处理每个输入。</li><li>所有网络都是<strong>使用反向传播联合训练</strong>的。</li></ul><p>尽管该技术是通用的，但是本文聚焦在语言建模和机器翻译任务中（这些任务都受益于非常大的模型）。</p><ul><li>具体说来，如图一所示，我们把 MoE <strong>以卷积的方式（convolutionally）放在</strong>多层 LSTM 层之间。</li><li>在文本的每个位置上，就会调用 MoE 一次，进而<strong>可能选择不同的专家组合</strong>。</li><li>不同的专家<strong>会倾向于变得高度专业化（基于语法和语义）</strong>。</li></ul><h2 id="3-混合专家层的结构"><a href="#3-混合专家层的结构" class="headerlink" title="3.混合专家层的结构"></a>3.混合专家层的结构</h2><h3 id="3-1-MoE层"><a href="#3-1-MoE层" class="headerlink" title="3.1 MoE层"></a>3.1 MoE层</h3><p>MoE 层包括 ：</p><ul><li>n 个“<strong>专家网络</strong>”：$E1,⋯,En$。</li><li>一个<strong>门控网络</strong> $G$，其输出是一个稀疏的 $n$ 维向量。</li></ul><p>尽管从理论上讲，每个专家网络只要保持一致的输入大小和输出大小就可以了；但是，在本文的研究里，我们限制了专家网络具有相同的网络结构，而网络参数保持独立。</p><p>给定输入 $x$，定义 $G(x)$是门控网络的输出；$Ei(x)$ 是第 $i$ 个专家网络的输出。于是 MoE 模块的输出为：</p><script type="math/tex; mode=display">y=\sum_{i=1}^{n} G(x)_{i} E_{i}(x)</script><p>基于 $G(x)$ 输出的稀疏性，可以节省计算量。</p><ul><li>当 $G(x)i=0$时，我们无需计算 $Ei(x)$。</li><li>在我们的实验中，我们<strong>有数以千计的专家</strong>，但是针对每个样本，只需要用到<strong>少量的专家</strong>。</li><li>如果专家数目非常大，我们可能要采用<strong>层次化的 MoE</strong>；本文我们不会使用层次化的 MoE，相关细节感兴趣可以见附录 B。</li></ul><p><img src="image/image_T8RSJA__tA.png" alt=""></p><h3 id="3-2-层次化MoE"><a href="#3-2-层次化MoE" class="headerlink" title="3.2 层次化MoE"></a>3.2 层次化MoE</h3><p>如果专家数量很大，<strong>可以通过使用两级层次MoE来降低分支因子</strong>。在分层MoE中，主选通网络选择“专家”的稀疏加权组合，每个专家本身就是具有自己选通网络的专家的二次混合。&#x20;</p><p>主选通网络是$Gprimary$，次选通网络为$（G1，G2，…，Ga）$，专家网络为$（E0,0，E0,1，…，Ea，b）$。MoE的输出由以下公式给出：</p><script type="math/tex; mode=display">y_{H}=\sum_{i=1}^{a} \sum_{j=1}^{b} G_{p r i m a r y}(x)_{i} \cdot G_{i}(x)_{j} \cdot E_{i, j}(x)</script><h3 id="3-3-门控网络"><a href="#3-3-门控网络" class="headerlink" title="3.3 门控网络"></a>3.3 门控网络</h3><h4 id="（1）Softmax-Gating"><a href="#（1）Softmax-Gating" class="headerlink" title="（1）Softmax Gating"></a>（1）Softmax Gating</h4><p>一种朴素的想法是，用一个矩阵乘上输入，然后经过一个 Softmax 函数，这种方法实际上是一种非稀疏的门控函数：</p><script type="math/tex; mode=display">G_{\sigma}(x)=\operatorname{Softmax}\left(x \cdot W_{g}\right)</script><h4 id="（2）Noise-Top-K-Gating"><a href="#（2）Noise-Top-K-Gating" class="headerlink" title="（2）Noise Top-K Gating"></a>（2）Noise Top-K Gating</h4><p>在 Softmax 门控网络基础上，**加入两个元素：**<strong>稀疏性和噪声</strong>。在执行 Softmax 函数之前：</p><p>我们加入了<strong>可调的高斯噪声</strong>，噪声项是为了帮助<strong>负载均衡（load balancing）</strong>，我们在附录 A 有详细讨论。</p><p>并且<strong>保留前 k 个值</strong>，其他设置为 $-\infty$。这种稀疏性是为了节省计算资源，<strong>尽管这种形式的稀疏性，从理论上会造成一些可怕的输出间断性</strong>，<strong>但在实际使用中，我们并没有观察到这种问题</strong>。</p><p>每个分量的噪音量，通过另一个可训练的权重矩阵 $W_{noise}$  来控制。</p><script type="math/tex; mode=display">G(x)=\operatorname{Softmax}(\operatorname{KeepTopK}(H(x), k))</script><script type="math/tex; mode=display">H(x)_{i}=\left(x \cdot W_{g}\right)_{i}+ StandardNormal ()\cdot \operatorname{Softplus}\left(\left(x \cdot W_{\text {noise }}\right)_{i}\right)</script><script type="math/tex; mode=display">KeepTopK (v, k)_{i}=\left\{\begin{array}{ll}v_{i} & \text { if } v_{i} \text { is in the top } k \text { elements of } v \\ -\infty & \text { otherwise. }\end{array}\right.</script><h3 id="3-4训练门控网络"><a href="#3-4训练门控网络" class="headerlink" title="3.4训练门控网络"></a>3.4训练门控网络</h3><p>我们使用<strong>简单的反向传播</strong>来训练门控网络以及接下来的模型。</p><h2 id="4-解决性能挑战"><a href="#4-解决性能挑战" class="headerlink" title="4.解决性能挑战"></a>4.解决性能挑战</h2><h3 id="4-1-批量减小问题（The-Shrinking-Batch-Problem）"><a href="#4-1-批量减小问题（The-Shrinking-Batch-Problem）" class="headerlink" title="4.1 批量减小问题（The Shrinking Batch Problem）"></a>4.1 批量减小问题（The Shrinking Batch Problem）</h3><p>由于门控网络对每个样本，在 $n$ 个专家中，选择 $k$ 个。那么对于 $b$个样本的批次，每个转接都会收到更加更加小的批次（大概 $\frac{kb}{n} &lt;&lt; b$）。这会导致朴素的 MoE 实现<strong>在专家数量增加时，非常低效</strong>。解决批量减小问题，就是需要让<strong>原始的批量大小尽可能的大</strong>。然而，批量大小会收到内存的限制。我们提出如下技术来提高批量大小：</p><ul><li><strong>混合数据并行和模型并行（Mixing Data Parallelism and Model Parallelism）</strong>:相当于变相的扩大b，假设有d个device，每个device上一次处理b个样本，那么在这次训练中，batch=bd，从而每个expert会接收kbd/n个样本。</li><li>充分利用卷积</li><li>增加循环 MoE 的批量大小</li></ul><h3 id="4-2-网络带宽"><a href="#4-2-网络带宽" class="headerlink" title="4.2 网络带宽"></a>4.2 网络带宽</h3><h2 id="5-平衡专家的利用率"><a href="#5-平衡专家的利用率" class="headerlink" title="5.平衡专家的利用率"></a>5.平衡专家的利用率</h2><p>我们观察到，门控网络倾向于收敛到<strong>一种不好的状态，即对相同的少量专家，总是会得到较大的权重</strong>。<strong>这种不平衡是不断自我强化的</strong>，随着更好的专家不断训练学习，它们更有可能被门控网络选中。面对这种问题，过去文献有的用<strong>硬性约束</strong>，有的用<strong>软性约束</strong>。</p><p>而我们采用<strong>软性约束方法</strong>。我们定义对<strong>于一个批次训练样本</strong>的<strong>专家重要度（the importance of an expert）</strong>，即该专家<strong>在一个批次上的门控输出值的和</strong>。并且定义损失项 $L_{importance}$ ，加入到模型的总损失上。该损失项等于<strong>所有专家重要度的方差的平方</strong>，再加上一个手工调节的比例因子 $w_{important}$。这个损失项<strong>会鼓励所有专家有相同的重要度</strong>。</p><script type="math/tex; mode=display">Importance (X)=\sum_{x \in X} G(x)</script><script type="math/tex; mode=display">L_{\text {importance }}(X)=w_{\text {importance }} \cdot C V(\text { Importance }(X))^{2}</script><p>尽管现在的损失函数可以保证相同的重要度，<strong>专家仍然可能接收到差异很大的样本数目</strong>。例如，某些专家可能接收到少量的大权重的样本；而某些专家可能接收到更多的小权重的样本。为了解决这个问题，我们引入了第二个损失函数：$ L_{load}  $，它可以保证负载均衡。附录 A 会包含该函数的定义。&#x20;</p><h2 id="6-实验"><a href="#6-实验" class="headerlink" title="6.实验"></a>6.实验</h2><h3 id="6-1-10-亿词汇的语言建模基准"><a href="#6-1-10-亿词汇的语言建模基准" class="headerlink" title="6.1 10 亿词汇的语言建模基准"></a>6.1 10 亿词汇的语言建模基准</h3><p>MoE模型：所提出的模型由两个堆叠的LSTM层组成，它们之间有一个MoE层。</p><p>使用包含4、32和256名专家的平面MoE以及包含256、1024和4096名专家的分层MoE来训练模型。</p><p>每个专家都有大约100万个参数。</p><p>对于所有MoE层，每次输入都有4名专家活跃。</p><p><img src="image/image_lGlKf9jWJ6.png" alt=""></p><p>左图：有4名始终活跃的专家的模型与计算匹配的基线模型表现相似（不足为奇），而最大的模型（4096名专家）在测试集上的困惑度降低了24%，令人印象深刻。&#x20;</p><p>右图：与LSTM模型相比，MoE模型在相似的计算预算下实现了更低的困惑。</p><p><img src="image/image_dcBv61RWEj.png" alt=""></p><p>对于没有MoE的基线模型，观察到的计算效率在1.07–1.29 TFLOPS/GPU之间。</p><p>对于所提出的低计算MoE模型，计算效率在0.74-0.90 TFLOPS/GPU之间，但4专家模型没有充分利用可用的并行性。</p><p>计算量最高的MoE模型在1.56 TFLOPS/GPU时效率更高，这可能是由于矩阵更大。</p><h3 id="6-2-1000-亿词汇的谷歌新闻语料库"><a href="#6-2-1000-亿词汇的谷歌新闻语料库" class="headerlink" title="6.2 1000 亿词汇的谷歌新闻语料库"></a>6.2 1000 亿词汇的谷歌新闻语料库</h3><p><img src="image/image_t1jcKFT5RA.png" alt=""></p><p>当训练超过1000亿个单词时，测试困惑度显著提高，达到65536个专家（680亿个参数），比计算匹配的基线低39%，但在131072个专家时会下降，这可能是稀疏性过大的结果。</p><h3 id="6-3-机器翻译"><a href="#6-3-机器翻译" class="headerlink" title="6.3 机器翻译"></a>6.3 机器翻译</h3><p>这里使用的MoE模型是<a href="https://sh-tsang.medium.com/review-googles-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-518595d87226" title="GNMT">GNMT</a>的修改版本。</p><p>为了减少计算，编码器和解码器中的LSTM层的数量分别从9和8减少到3和2。</p><p>MoE层被插入编码器（在层2和3之间）和解码器（在层1和2之间）中。每个MoE层包含多达2048名专家，每个专家都有大约200万个参数，总共为模型增加了大约80亿个参数。</p><p><img src="image/image_6jwKtLAljL.png" alt=""></p><blockquote><p><strong>Results on WMT’14 En&gt;Fr newstest2014</strong></p></blockquote><p><img src="image/image_m_KbM1Dnld.png" alt=""></p><blockquote><p><strong>Results on WMT’14 En&gt;De newstest2014</strong></p></blockquote><p>所提出的方法在WMT’14 En&gt;Fr和En&gt;De基准上获得了40.56和26.03的BLEU分数，优于GNMT和Deep-Att。</p><p><img src="image/image_uhbo2VvSZ_.png" alt=""></p><p>在Google Production数据集上，MoE模型在训练了六分之一的时间后，测试BLEU得分也提高了1.01。</p><p><img src="image/image_cnqmpG977Q.png" alt=""></p><h2 id="7-结论"><a href="#7-结论" class="headerlink" title="7.结论"></a>7.结论</h2><ul><li>该工作是第一个展现<strong>基于深度网络的条件计算</strong>的重大胜利。</li><li>我们探讨了设计考虑、条件计算的挑战、从算法和工程上的解决方案。</li><li>虽然我们聚焦在文本领域上，条件计算仍然可以在其他领域发挥作用。我们期望有更多条件计算的实现和应用。</li></ul>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PaperReading </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型时代下做科研的四个思路</title>
      <link href="/paper_reading/255.2.%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E4%B8%8B%E5%81%9A%E7%A7%91%E7%A0%94%E7%9A%84%E5%9B%9B%E4%B8%AA%E6%80%9D%E8%B7%AF/"/>
      <url>/paper_reading/255.2.%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E4%B8%8B%E5%81%9A%E7%A7%91%E7%A0%94%E7%9A%84%E5%9B%9B%E4%B8%AA%E6%80%9D%E8%B7%AF/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.bilibili.com/video/BV1oX4y1d7X6/?vd_source=6bc8f793c75740c7bcfb8e281f986a8e">https://www.bilibili.com/video/BV1oX4y1d7X6/?vd_source=6bc8f793c75740c7bcfb8e281f986a8e</a></p><h1 id="1-总览"><a href="#1-总览" class="headerlink" title="1.总览"></a>1.总览</h1><ol><li><strong>Efficient(PEFT)</strong>：提升训练效率，这里以PEFT(parameter efficient fine tuning)为例</li><li><strong>Existing stuff(pretrained model)、New directions</strong>：使用别人的预训练模型，新的研究方向</li><li><strong>plug-and-play</strong>：做一些即插即用的模块，例如模型的模块、目标函数、新损失函数、数据增强方法等等。</li><li><strong>Dataset,evaluation and survey</strong>：构建数据集、发表分析为主的文章或者综述论文</li></ol><h1 id="2-Efficient（PEFT）"><a href="#2-Efficient（PEFT）" class="headerlink" title="2.Efficient（PEFT）"></a><strong>2.Efficient（PEFT</strong>）</h1><p>通过论文AIM为例讲述如何进行PEFT，即在硬件资源有限时对大模型进行高效微调</p><ul><li>论文地址：<a href="https://arxiv.org/abs/2302.03024" title="https://arxiv.org/abs/2302.03024">https://arxiv.org/abs/2302.03024</a></li><li>论文标题：AIM: Adapting Image Models for Efficient Video Action Recognition</li><li>标题翻译：调整图像模型以实现高效的视频动作识别</li></ul><h2 id="2-1-训练好的图像模型是否需要微调"><a href="#2-1-训练好的图像模型是否需要微调" class="headerlink" title="2.1 训练好的图像模型是否需要微调"></a>2.1 训练好的图像模型是否需要微调</h2><p><strong>思考：已经训练好的图像模型是否需要继续微调？</strong></p><ol><li>clip已经证明了即使ZeroShot(模型不变，直接在各个数据集上进行推理)，它的效果也很好。即一个训练很好的图片模型从中提取视觉特征是有泛化性、有效的。</li><li>继续微调会导致灾难性遗忘。如果使用少量数据在大模型上微调，可能会直接过拟合，或者大模型的很多特征丢失。</li></ol><p><strong>结论：</strong> ​<strong>预训练的图像模型不需要继续微调</strong>。</p><p>传统模型和论文改进的微调方法对比图：</p><p><img src="https://i0.hdslb.com/bfs/note/4a4dc4583692acea73ff7c3b4eccf137a88ec637.png@1632w_!web-note.webp" alt=""></p><p>因此，论文的做法是，<strong>尝试将模型参数锁住，在上面加一些时序处理模块、目标函数等修改周边的方式(即PEFT)让图片模型能够做视频理解的任务，不需要重新训练视频模型，省时省力。</strong></p><h2 id="2-2-两种PEFT方法"><a href="#2-2-两种PEFT方法" class="headerlink" title="2.2 两种PEFT方法"></a>2.2 两种PEFT方法</h2><h3 id="（1）adapter"><a href="#（1）adapter" class="headerlink" title="（1）adapter"></a><strong>（1）adapter</strong></h3><p>最早来自于这篇论文：</p><ul><li>论文地址：<a href="https://arxiv.org/abs/1902.00751" title="https://arxiv.org/abs/1902.00751">https://arxiv.org/abs/1902.00751</a></li><li>论文标题：Parameter-Efficient Transfer Learning for NLP</li><li>标题翻译：用于NLP的参数高效转移学习</li></ul><p>Adapter层的结构，如下图右边所示：下采样FC层+非线性激活层+上采样FC层，加上残差连接。</p><p>这里PEFT的方法是指，如下图左边所示，在Transformer中加入了两个adapter，进行微调时，原来的Transformer的参数都是锁住的，只有adapter层的参数在学习。</p><p><img src="https://i0.hdslb.com/bfs/note/35f69fed3832cdd968369902b04733771782a672.png@1272w_!web-note.webp" alt=""></p><p>adapter层参数量和大模型相比非常少，例如在175B的GPT3中使用LoRa，需要训练的参数只要万分之一。因此训练成本大幅降低。</p><h3 id="（2）prompt-tuning"><a href="#（2）prompt-tuning" class="headerlink" title="（2）prompt tuning"></a><strong>（2）prompt tuning</strong></h3><ul><li>论文地址：<a href="https://arxiv.org/abs/2109.01134" title="https://arxiv.org/abs/2109.01134">https://arxiv.org/abs/2109.01134</a></li><li>论文标题：Learning to Prompt for Vision-Language Models</li></ul><p>prompt tuning是指可以任意调整提示词，这样的调整对最后的性能会有很大的影响，能否得到想要的结果，取决于有没有选择一个好的提示词。例如下图所示，不同的提示词对准确率的影响很大。</p><p><img src="https://i0.hdslb.com/bfs/note/b9dd2f1c47264f751aed00d3a2e085551f9819ab.png@1680w_!web-note.webp" alt=""></p><p>上图是如何通过提示给图片分类的？将类别名称CLASS给模型，看哪个文字和图片的相似度最高。</p><p>Prompt分为两种：</p><p><strong>Hard Prompt</strong>：人工设置的提示词，不能修改也无法学习。设置这些需要一定的先验知识，但我们并不会总有这样的先验知识。</p><p><strong>Soft Prompt</strong>：将提示词设置为一个可学习的向量。如下图所示 ，将文本端(text encoder)的输入CLASS设置为learnable context，模型优化的是这个context部分。这样既可以节省很多计算量 ，也可以避免在下游任务时手动设置提示词。</p><p><img src="https://i0.hdslb.com/bfs/note/32946b7127bf45be7fe6f6557a75c894c6bdc9a7.png@1680w_!web-note.webp" alt=""></p><h2 id="2-3-VPT（Visual-Prompt-Tuning）"><a href="#2-3-VPT（Visual-Prompt-Tuning）" class="headerlink" title="2.3 VPT（Visual-Prompt Tuning）"></a>2.3 VPT（Visual-Prompt Tuning）</h2><p>将可学习的Prompt方法用到纯视觉任务中，做法如下图所示。</p><ul><li>论文地址：<a href="https://arxiv.org/abs/2203.12119" title="https://arxiv.org/abs/2203.12119">https://arxiv.org/abs/2203.12119</a></li><li>论文标题：Visual Prompt Tuning</li></ul><p><img src="https://i0.hdslb.com/bfs/note/a8171a277e3e99febe34fda3de8b8b3bb3cd42e0.png@1680w_!web-note.webp" alt=""></p><p>图中蓝色部分是原来训练好的模型，红色是需要微调的prompt，加入Prompt tuning有两种方式：</p><p>1、<strong>VPT: Deep</strong>，在每一层的输入输出都加入prompt。</p><p>2、<strong>VPT: Shallow</strong>，在输入端加入prompt。</p><p>近期PEFT方法总结，从统一的观点进行归纳：</p><ul><li>论文地址：<a href="https://arxiv.org/abs/2110.04366" title="https://arxiv.org/abs/2110.04366">https://arxiv.org/abs/2110.04366</a></li></ul><h2 id="2-4-AIM模型设计"><a href="#2-4-AIM模型设计" class="headerlink" title="2.4 AIM模型设计"></a>2.4 AIM模型设计</h2><p><img src="https://i0.hdslb.com/bfs/note/32cb0bc9888a57759755cf1595346355921ba512.png@1680w_!web-note.webp" alt=""></p><p>如上图所示，AIM模型就是在图b的ViT模型中加入图a的Adapter，共有图c、d、e三种方式：</p><p>1、<strong>Spatial Adaptation</strong>，只在S-MSA层后面加入Adapter，即不增加视频理解能力，只加一些学习的参数。</p><p>2、<strong>Temporal Adaptation</strong>，复用一个MSA层，在两个MSA层后面都加入Adapter，即让模型从Spatial和Temporal两个方向上进行学习，从而有时序建模的能力。</p><p>3、<strong>Joint Adaptation</strong>，在Temporal Adaptation的基础上，在MLP边上也加入Adapter，即让三个Adapter各司其职，使得优化问题更简单一些。</p><p>注：MSA是多头自注意力（MultiHead Self-Attention），S-MSA和T-MSA共享权重，但维度不同。</p><p>效果如下图所示，只用14M参数的AIM模型效果已经高过之前121M的模型。</p><p><img src="https://i0.hdslb.com/bfs/note/467403ae1dfa74b7740b28a72a7255398f1efa93.png@1680w_!web-note.webp" alt=""></p><h1 id="3-Existing-stuff（pretrained-model）"><a href="#3-Existing-stuff（pretrained-model）" class="headerlink" title="3.Existing stuff（pretrained model）"></a><strong>3.Existing stuff（pretrained model</strong>）</h1><p>有两点：</p><ol><li><strong>巧妙使用别人的预训练模型，从而达到去做FewShot，ZeroShot，或者最多Fine Tuning的实验**</strong>。** ​</li><li><strong>新的研究方向。</strong></li></ol><p>通过这篇论文讲述这两点是如何运用的：</p><ul><li>论文地址：<a href="https://arxiv.org/abs/2207.05027" title="https://arxiv.org/abs/2207.05027">https://arxiv.org/abs/2207.05027</a></li><li>论文标题：Unsupervised Semantic Segmentation with Self-supervised Object-centric Representations</li></ul><p>从标题就可以看出这两点技巧：</p><ol><li>这里的Self-supervised是指使用了预训练好的DINO、DeepUSPS、BASNet等网络</li><li>这里做的方向是Object-centric Learning，属于蓬勃发展的题目，玩家不多、数据集不大</li></ol><p><img src="https://i0.hdslb.com/bfs/note/144f28ea191005278c636c7a2a77a8248cf2132d.png@1680w_!web-note.webp" alt=""></p><p>上图展示了如何使用几个预训练好的模型，在无监督的情况下找到新的物体，步骤如下：</p><p>1、通过预训练模型<strong>DeepUSPS</strong>找到一些显著性物体的Mask。</p><p>例如，图片中的篮球可以得到一个圆形的Mask</p><p>2、根据Mask将图片中的对应物体抠出来，并调整大小为224*224。</p><p>例如，将图片中的篮球抠出来并放大</p><p>3、然后将步骤2得到的图片通过预训练模型<strong>DINO</strong>返回一个1024*1024的特征(global representation)。</p><p>4、将所有的特征进行聚类<strong>Clustering</strong>，这样就可以通过无监督学习得到这些物体的分类ID。</p><p>注：聚类只能将相同的物体分类到一起，但并不知道具体是什么物体。</p><p>5、将图片和对应的分类ID去训练一个语义分割网络(<strong>Semantic segmentation network</strong>)。</p><p>注：这里相当于一个有监督的学习，标签来自于步骤4</p><p>6、一张图片可能有多个物体，所以加一个<strong>Self-training</strong>，多做几个轮回。</p><p>这样就可以从图片中找到物体了。</p><h1 id="4-plug-and-play"><a href="#4-plug-and-play" class="headerlink" title="4.plug-and-play"></a><strong>4.plug-and-play</strong></h1><p><strong>做一些通用的、即插即用的模块</strong>，在一个设定的范围内，加入了这样的模块后，能够有一个统一的涨点，并且能给出合适的分析，就非常有说服力了。</p><p>通过MixGen论文讲述如何加入模块：</p><ul><li>论文地址：<a href="https://arxiv.org/abs/2206.08358" title="https://arxiv.org/abs/2206.08358">https://arxiv.org/abs/2206.08358</a></li><li>论文标题：MixGen: A New Multi-Modal Data Augmentation</li></ul><p>文本的模型都很大，图片的模型相对来说小一些，但是自注意力的参数是可以共享的，所以尝试<strong>用文本大模型来蒸馏图片小模型</strong></p><p>注：模型蒸馏：使用训练集训练出来一个完整复杂的teacher模型，然后设计一个小规模的student模型，再固定teacher模型的权重参数，然后使用训练集和teacher模型的输出同时对student模型进行训练，此时就需要设计一系列loss，让student模型在蒸馏学习的过程中逐渐向teacher模型的表现特性靠拢，使得student模型的预测精度逐渐逼近teacher模型。</p><p><strong>为什么之前图片模型不做数据增强？</strong></p><ol><li>图片模型训练时已经用了很多图片了，不需要再做数据增强。</li><li>或者做了数据增强，但是将其中的Color Jittering和Random Filp去掉了，因为这两个对图片的变化会导致图片和文本不匹配。</li></ol><p>例如：图片有白色的狗和绿色的树，只对图片做Color Jittering会导致颜色变化，图片中不再是白色的狗，但是文本依然是白色的狗，这样文本和图片就不匹配了。</p><p><strong>论文的做法</strong>：既然目标是尽可能保留更多信息，这里的做法很简单粗暴，就是直接<strong>将两个句子拼接在一起</strong>，这样就可以做到不丢失信息的情况下得到新的训练样本。</p><p>例如下图，将两个图片通过数据增强得到第三个图片，同时将两个图片的文本进行拼接得到第三个图片的文本。</p><p><img src="https://i0.hdslb.com/bfs/note/a7f63986fa9e091e77b4c7618f265b247f45261d.png@1020w_!web-note.webp" alt=""></p><p>审稿人的建设性提议：在下游任务只有少量数据时进行数据增强。</p><h1 id="5-Dataset-evaluation-and-survey"><a href="#5-Dataset-evaluation-and-survey" class="headerlink" title="5.Dataset,evaluation and survey"></a><strong>5.Dataset,evaluation and survey</strong></h1><p>构建数据集、发表分析为主的文章或者综述论文，这里举了两篇论文为例。</p><p>以数据集为主的big detection，将三个数据集整合到一起：</p><ul><li>论文地址：<a href="https://arxiv.org/abs/2203.13249" title="https://arxiv.org/abs/2203.13249">https://arxiv.org/abs/2203.13249</a></li></ul><p>视频动作检测的综述论文：</p><ul><li>论文地址：<a href="https://arxiv.org/abs/2012.06567" title="https://arxiv.org/abs/2012.06567">https://arxiv.org/abs/2012.06567</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PaperReading </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 ELMo</title>
      <link href="/paper_reading/1.5.ELMo/"/>
      <url>/paper_reading/1.5.ELMo/</url>
      
        <content type="html"><![CDATA[<ul><li>论文名称：Embeddings from Language Models</li><li>论文地址：<a href="https://arxiv.org/pdf/1802.05365.pdf" title="1802.05365.pdf (arxiv.org)">1802.05365.pdf (arxiv.org)</a></li></ul><h1 id="1-ELMo简介"><a href="#1-ELMo简介" class="headerlink" title="1.ELMo简介"></a>1.ELMo简介</h1><p>ELMo是2018年3月由华盛顿大学提出的一种预训练模型。</p><p>ELMo模型提出的动机源于研究人员认为一个好的预训练语言模型应该能够包含丰富的句法和语义信息， 并且能够对多义词进行建模。 而<strong>传统的词向量(2013年的word2vec, 2014年的GloVe)都是上下文无关的</strong>，也就是固定的词向量。最典型的例子就是”apple”在不同的语境下, 应该可以表示水果或公司，但是固定的词向量显然无法做到这一点。 因为<strong>研究团队利用新的语言模型训练一个上下文相关的预训练模型， 成为ELMo</strong>， 并在6个NLP任务上获得提升。</p><h1 id="2-ELMo架构"><a href="#2-ELMo架构" class="headerlink" title="2.ELMo架构"></a>2.ELMo架构</h1><h2 id="2-1-总体架构"><a href="#2-1-总体架构" class="headerlink" title="2.1 总体架构"></a>2.1 总体架构</h2><p><img src="image/image_06zXTLpwnj.png" alt=""></p><p>从上面的架构图中可以看到， 宏观上ELMo分三个主要模块。</p><ul><li>最底层黄色标记的<strong>Embedding模块</strong>。</li><li>中间层蓝色标记的两部分<strong>双层LSTM模块</strong>。</li><li>最上层绿色标记的<strong>词向量表征模块</strong>。</li></ul><h2 id="2-2-Embedding模块"><a href="#2-2-Embedding模块" class="headerlink" title="2.2 Embedding模块"></a>2.2 Embedding模块</h2><p>ELMo最底层的词嵌入采用<strong>CNN对字符级进行编码</strong>，本质就是获得一个静态的词嵌入向量作为网络的底层输入。</p><h2 id="2-3-两部分的双层LSTM模块"><a href="#2-3-两部分的双层LSTM模块" class="headerlink" title="2.3 两部分的双层LSTM模块"></a>2.3 两部分的双层LSTM模块</h2><p>这是整个ELMo中最重要的部分, 架构中分成<strong>左侧的前向LSTM网络</strong>， 和<strong>右侧的反向LSTM网络</strong>。</p><p>ELMo的做法是我们只预训练一个Language Model， 而<strong>word embedding是通过输入的句子实时给出的</strong>，这样<strong>单词的嵌入向量就包含了上下文的信息</strong>，也就彻底改变了Word2Vec和GloVe的静态词向量的做法.</p><p>ELMo的这一模块分为左右两部分, 本质上就是一个双向LM, 对于左半部分, 给定了N个tokens(t1, t2, …, tN), Language Model通过<strong>前面k-1个位置的token序列来计算第k个token出现的概率</strong>, 构成<strong>前向双层LSTM模型</strong>.</p><script type="math/tex; mode=display">p\left(t_{1}, t_{2}, \cdots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} \mid t_{1}, t_{2}, \cdots, t_{k-1}\right)</script><p>同理, 对于架构中的右半部分, 给定了N个tokens(t(k+1), t(k+2), …, t(N)), Language Model<strong>通过后面N-k个位置的token序列来计算第k个token出现的概率,</strong> 构成<strong>后向双层LSTM模型</strong>.</p><script type="math/tex; mode=display">p\left(t_{1}, t_{2}, \cdots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} \mid t_{k+1}, t_{k+2}, \cdots, t_{N}\right)</script><p>ELMo在训练过程中的目标函数就是最大化下面的公式:</p><script type="math/tex; mode=display">\sum_{k=1}^{N}\left(\log p\left(t_{k} \mid t_{1}, \cdots, t_{k-1} ; \Theta_{x}, \vec{\Theta}_{L S T M}, \Theta_{s}\right)+\log p\left(t_{k} \mid t_{k+1}, \cdots, t_{N} ; \Theta_{x}, \overleftarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right)</script><h2 id="2-4-词向量表征模块"><a href="#2-4-词向量表征模块" class="headerlink" title="2.4 词向量表征模块"></a>2.4 词向量表征模块</h2><p>因为ELMo是个语言模型, 对于每个token, 通过一个L层的双向LSTM网络可以计算出<code>2L+1</code>个表示向量如下:</p><script type="math/tex; mode=display">R_{k}=\left\{x_{k}^{L M}, \stackrel{\rightarrow}{h_{k, j}}, \stackrel{\leftarrow}{h} h_{k, j} \mid j=1, \cdots, L\right\}=\left\{h_{k, j}^{L M} \mid j=0, \cdots, L\right\}</script><ul><li>从上面的公式可以清楚的看到, 有3个不同的组成部分, 第一个就是对token直接进行CNN编码的结果, 也是ELMo最底层模块的输出；第二个就是前向LSTM的输出结果, 每一层都会有一个输出， 总共L层就会有L个输出； 第三个就是后向LSTM的输出结果, 每一层都会有一个输出, 总共L层就会有L个输出；综合三部分的输出加在一起, 就是<code>2L+1</code>个输出向量.</li></ul><p>通过整个网络, 每一个token得到了<code>2L+1</code>个表示向量，但是我们希望每一个token能对应一个向量。最简单的做法就是取最上层的输出结果作为token的表示向量，更通用的做法是<strong>加入若干参数来融合所有层的信息</strong>， 如下所示:</p><script type="math/tex; mode=display">E L M o_{k}^{\text {task }}=E\left(R_{k} ; \Theta^{\text {task }}\right)=\gamma^{\text {task }} \sum_{j=0}^{L} s_{j}^{\text {task }} h_{k, j}^{L M}</script><ul><li>上式的意思是对于2L+1个向量，每一个前面都加上一个权重稀疏，然后直接融合成一个向量，最后再乘一个系数作为最终该token的词向量。</li><li>原始论文中提到最前面的那个系数，在不同任务中取不同的值效果会有较大的差异， 需要注意在SQuAD中设置为0.01取得的效果要好于设置为1。</li><li>原始论文中在进行底层token编码时，用CNN形成了一个512维的列向量，也就是初始嵌入维度等于512。 中间层使用了双层的LSTM分别进行前向编码和后向编码，每层的单个LSTM输入维度是512，输出维度也是512，保持一致。因为是双向编码并且分左右两部分，所以每层的输出维度是512*2=1024，最后进行权重融合后的向量维度就是1024。</li></ul><h1 id="3-ELMo预训练任务"><a href="#3-ELMo预训练任务" class="headerlink" title="3.ELMo预训练任务"></a>3.ELMo预训练任务</h1><h2 id="3-1-ELMo的本质思想"><a href="#3-1-ELMo的本质思想" class="headerlink" title="3.1 ELMo的本质思想"></a>3.1 ELMo的本质思想</h2><p>首先用一个语言模型学好一个单词的word embedding， 此时是无法区分多义词的，但没关系。<strong>当实际使用word embedding的时候，该单词已经具备了特定的上下文信息，这个时候可以根据上下文单词的语义去调整单词的word embedding表示</strong>，这样经过调整后得到的word embedding向量就可以准确的表达单词在当前上下文中的真实含义了，也就自然的解决了多义词问题.</p><p>结论就是<strong>ELMo模型是个根据当前上下文对word embedding动态调整的语言模型</strong>。</p><h2 id="3-2-ELMo的预训练"><a href="#3-2-ELMo的预训练" class="headerlink" title="3.2 ELMo的预训练"></a>3.2 ELMo的预训练</h2><p>ELMo的预训练采用了典型的两阶段过程。</p><ul><li>第一阶段：利用语言模型进行预训练。</li><li>第二阶段：在做下游任务时, <strong>从预训练网络中提取对应单词的网络各层的word embedding作为新特征补充到下游任务中</strong>。</li></ul><h3 id="（1）第一阶段-语言模型预训练"><a href="#（1）第一阶段-语言模型预训练" class="headerlink" title="（1）第一阶段: 语言模型预训练"></a>（1）第一阶段: 语言模型预训练</h3><p>再次回到ELMo的总体架构图, 网络结构采用了双层双向LSTM.</p><p>目前语言模型训练的任务目标是根据单词Wi的上下文去正确预测单词Wi，Wi之前的单词序列context-before称为<strong>上文</strong>，Wi之后的单词序列context-after称为<strong>下文</strong>。</p><p>架构图上左侧的前向双层LSTM代表正方向编码器, 输入的是从左向右顺序的除了预测单词Wi之外的上文context-before；右侧的反向双层LSTM代表反方向编码器, 输入的是从右向左的逆序的下文context-after；</p><p>每个编码器的深度都是L=2, 即双层LSTM叠加.</p><p>使用上述的网络结构利用大量语料做语言模型任务就能预训练好这个网络。当输入一个新句子<code>S_new</code>时, 句子中每个单词都能得到对应的3个embedding向量：</p><ul><li>1-最底层的单词的word embedding。</li><li>2-中间第一层双向LSTM中对应单词位置的embedding，这层编码对应单词的句法信息更多一些。</li><li>3-中间第二层双向LSTM中对应单词位置的embedding，这层编码对应单词的语义信息更多一些。</li></ul><p>ELMo的预训练过程不仅仅学会了单词的<strong>word embedding</strong>，还学习了一个<strong>双层双向的LSTM网络</strong>， 这两者后续都会用到，是整个ELMo预训练的两大产出结果。</p><h3 id="（2）第二阶段-下游任务的调整"><a href="#（2）第二阶段-下游任务的调整" class="headerlink" title="（2）第二阶段: 下游任务的调整"></a>（2）第二阶段: 下游任务的调整</h3><p>比如我们的下游任务是QA问题.</p><p>对于问句X, 可以先将句子X作为预训练好的ELMo网络的输入, 这样X中每个单词在ELMo中都能获得3个对应的embedding向量。之后赋给这3个向量各自一个权重a，这个权重a既可以是学习得来的也可以是最简单的平均分布赋值，然后把3个向量加权求和，整个成一个词向量。最后将整合后的词向量作为X在自己任务的那个网络结构中对应单词的输入, 以此作为新特征补充进下游任务中。对于回答Y可以同样处理。</p><p>因为ELMo给下游提供的是每个单词的特征形式，所以这一类预训练方法被称为”<strong>Feature-based Pre-Training</strong>“。</p><h1 id="4-ELMo模型的效果"><a href="#4-ELMo模型的效果" class="headerlink" title="4.ELMo模型的效果"></a>4.ELMo模型的效果</h1><p>ELMo对于多义词问题的解决结果:</p><p><img src="image/image_7OflOnDruG.png" alt=""></p><ul><li>前面提到静态的word embedding无法解决多义词的问题, 那么ELMo引入上下文动态语义调整后的embedding word可以解决多义词问题吗? 答案正如上图所示，而且比我们期待的解决效果要更好。</li><li>上图中的例子，对于GloVe训练出来的word embedding来说，多义词比如play，根据它的embedding找出最接近其语义的单词，发现结果集合几乎全部都在体育领域，这很明显是因为训练数据中包含play的语句中体育领域的数量明显占多数导致的。</li><li>再来看使用ELMo后的效果，根据上下文动态调整后的embedding word不仅仅能找出对应于”play”:”演出”的相同语义的句子， 而且还可以保证找出的句子中的play对应的词性也是相同的，这真的是超出期待之外的惊喜！</li><li>原始论文中提到ELMo的试验效果, 在6个NLP主流任务中性能都有不同幅度的提升, 最高的提升达到25%, 任务的覆盖范围很广, 包含句子语义关系判断, 分类任务, 阅读理解等等.</li></ul><h1 id="5-ELMo待改进点"><a href="#5-ELMo待改进点" class="headerlink" title="5.ELMo待改进点"></a>5.ELMo待改进点</h1><p>ELMo在传统静态word embedding方法（Word2Vec, GloVe）的基础上提升了很多, 但是依然存在缺陷, 有很大的改进余地.</p><ul><li>第一点：一个很明显的缺点<strong>在于特征提取器的选择上</strong>， ELMo使用了双向双层LSTM，而不是现在横扫千军的Transformer，在特征提取能力上肯定是要弱一些的。设想如果ELMo的提升提取器选用Transformer，那么后来的BERT的反响将远不如当时那么火爆了。</li><li>第二点：<strong>ELMo选用双向拼接的方式进行特征融合</strong>，这种方法肯定不如BERT一体化的双向提取特征好.</li></ul><h1 id="6-小结"><a href="#6-小结" class="headerlink" title="6.小结"></a>6.小结</h1><p>学习了什么是ELMo.</p><ul><li>ELMo是2018年3月由华盛顿大学提出的一种预训练语言模型.</li><li>ELMo在6种NLP测试任务中有很大的提升表现.</li></ul><p>学习了ELMo的结构.</p><ul><li>ELMo架构总体上采用了双向双层LSTM的结构.</li><li>最底层的Embedding模块.</li><li>中间层的双向双层LSTM模块.</li><li>最上层的特征融合模块.</li></ul><p>学习了ELMo的预训练任务.</p><ul><li>ELMo的本质思想就是根据当前上下文对word embedding进行动态调整的语言模型.</li><li>ELMo的预训练是一个明显的两阶段过程.<ul><li>第一阶段: 利用语言模型进行预训练, 得到基础静态词向量和双向双层LSTM网络.</li><li>第二阶段: 在拥有上下文的环境中, 将上下文输入双向双层LSTM中, 得到动态调整后的word embedding, 等于将单词融合进了上下文的语义, 可以更准确的表达单词的真实含义.</li></ul></li></ul><p>学习了ELMo的效果.</p><ul><li>经过与GloVe静态词向量的对比, 明显可以看出ELMo的词向量可以更好的表达真实语义, 更好的解决多义词的问题.</li></ul><p>学习了ELMo的待改进点.</p><ul><li>ELMo的特征提取器没有选用更强大的Transformer, 在提取特征上肯定弱于现在的最优结果.</li></ul>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PaperReading </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>20 字符串算法</title>
      <link href="/dsa/leetcode/20_string/README/"/>
      <url>/dsa/leetcode/20_string/README/</url>
      
        <content type="html"><![CDATA[<h1 id="20-字符串算法"><a href="#20-字符串算法" class="headerlink" title="20.字符串算法"></a>20.字符串算法</h1><h1 id="1-字符串基础知识"><a href="#1-字符串基础知识" class="headerlink" title="1.字符串基础知识"></a>1.字符串基础知识</h1><h2 id="1-1-字符串定义"><a href="#1-1-字符串定义" class="headerlink" title="1.1 字符串定义"></a>1.1 字符串定义</h2><p><a href="https://lemire.me/blog/2017/07/07/are-your-strings-immutable/" title="Are your strings immutable? – Daniel Lemire&#39;s blog">Are your strings immutable? – Daniel Lemire’s blog</a></p><p><a href="https://web.mit.edu/6.005/www/fa16/classes/09-immutability/#summary" title="Reading 9: Mutability &amp; Immutability (mit.edu)">Reading 9: Mutability &amp; Immutability (mit.edu)</a></p><p>Python和Java的String是immutable的；immutable指不可变的，即定义了String后，就是不可变的，当增加字符和删减字母时，是新生成了一个字符串。</p><p>C++的String是可变的。</p><p>immutable优点：线程安全的，可以在多线程环境中使用</p><p>Python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="string">&#x27;abbc&#x27;</span></span><br><span class="line">x = <span class="string">&quot;abbc&quot;</span></span><br></pre></td></tr></table></figure><p>Java</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">String x = <span class="string">&quot;abbc&quot;</span>;</span><br></pre></td></tr></table></figure><p>C++</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::string <span class="title">x</span><span class="params">(<span class="string">&quot;abbc&quot;</span>)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="1-2-遍历字符串"><a href="#1-2-遍历字符串" class="headerlink" title="1.2 遍历字符串"></a>1.2 遍历字符串</h2><p>python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> ch <span class="keyword">in</span> <span class="string">&quot;abbc&quot;</span>:</span><br><span class="line">  <span class="built_in">print</span>(ch)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">x</span> <span class="operator">=</span> <span class="string">&quot;abbc&quot;</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; x.size(); i++) &#123;</span><br><span class="line">  <span class="type">char</span> <span class="variable">ch</span> <span class="operator">=</span> x.charAt(i);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ch in x.toCharArray() &#123;</span><br><span class="line">  System.out.println(ch);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>C++</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::string <span class="title">x</span><span class="params">(<span class="string">&quot;abbc&quot;</span>)</span></span></span><br><span class="line"><span class="function"><span class="title">for</span> <span class="params">(<span class="type">int</span> i = <span class="number">0</span>; i &lt; s1.length(); i++)</span> </span>&#123;</span><br><span class="line">  cout &lt;&lt; x[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span>&amp; ch : x) &#123;</span><br><span class="line">  std::cout &lt;&lt; ch;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (std::string::iterator it = x.<span class="built_in">begin</span>(); it != str.<span class="built_in">end</span>(); it++) &#123;</span><br><span class="line">  std::cout &lt;&lt; *it;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-3-基础题目"><a href="#1-3-基础题目" class="headerlink" title="1.3 基础题目"></a>1.3 基础题目</h2><h3 id="（1）字符串中的第一个唯一字符"><a href="#（1）字符串中的第一个唯一字符" class="headerlink" title="（1）字符串中的第一个唯一字符"></a>（1）字符串中的第一个唯一字符</h3><p><a href="https://leetcode.cn/problems/first-unique-character-in-a-string/description/" title="387. 字符串中的第一个唯一字符 - 力扣（LeetCode）">387. 字符串中的第一个唯一字符 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">给定一个字符串 s ，找到 它的第一个不重复的字符，并返回它的索引 。如果不存在，则返回 -1 。</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol><li>暴力方法：for i 0→len  for j 0→len  $O(n^2)$</li><li>map（hashmap, treemap）</li><li>hash array，使用一个简单数组</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">firstUniqChar</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        std::unordered_map&lt;<span class="type">int</span>, <span class="type">int</span>&gt; str_map;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; ch : s) &#123;</span><br><span class="line">            str_map[ch]++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; s.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="type">char</span> ch = s[i];</span><br><span class="line">            <span class="keyword">if</span> (str_map[ch] == <span class="number">1</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（2）字符串转换整数"><a href="#（2）字符串转换整数" class="headerlink" title="（2）字符串转换整数"></a>（2）字符串转换整数</h3><p><a href="https://leetcode.cn/problems/string-to-integer-atoi/description/" title="8. 字符串转换整数 (atoi) - 力扣（LeetCode）">8. 字符串转换整数 (atoi) - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">请你来实现一个 myAtoi(string s) 函数，使其能将字符串转换成一个 32 位有符号整数（类似 C/C++ 中的 atoi 函数）。</span><br><span class="line"></span><br><span class="line">函数 myAtoi(string s) 的算法如下：</span><br><span class="line"></span><br><span class="line">1.读入字符串并丢弃无用的前导空格</span><br><span class="line">2.检查下一个字符（假设还未到字符末尾）为正还是负号，读取该字符（如果有）。 确定最终结果是负数还是正数。 如果两者都不存在，则假定结果为正。</span><br><span class="line">3.读入下一个字符，直到到达下一个非数字字符或到达输入的结尾。字符串的其余部分将被忽略。</span><br><span class="line">4.将前面步骤读入的这些数字转换为整数（即，<span class="string">&quot;123&quot;</span> -&gt; 123， <span class="string">&quot;0032&quot;</span> -&gt; 32）。如果没有读入数字，则整数为 0 。必要时更改符号（从步骤 2 开始）。</span><br><span class="line">5.如果整数数超过 32 位有符号整数范围 [−231,  231 − 1] ，需要截断这个整数，使其保持在这个范围内。具体来说，小于 −231 的整数应该被固定为 −231 ，大于 231 − 1 的整数应该被固定为 231 − 1 。</span><br><span class="line">6.返回整数作为最终结果。</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">- 本题中的空白字符只包括空格字符 <span class="string">&#x27; &#x27;</span> 。</span><br><span class="line">- 除前导空格或数字后的其余字符串外，请勿忽略 任何其他字符。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">myAtoi</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> len = s.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (len == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> idx = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> is_negative = <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> num_abs = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 删除空格</span></span><br><span class="line">        <span class="keyword">while</span> (idx &lt;= len &amp;&amp; s[idx] == <span class="string">&#x27; &#x27;</span>) &#123;</span><br><span class="line">            idx++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理符号</span></span><br><span class="line">        <span class="keyword">if</span> (idx &lt;= len &amp;&amp; (s[idx] == <span class="string">&#x27;-&#x27;</span> || s[idx] == <span class="string">&#x27;+&#x27;</span>)) &#123;</span><br><span class="line">            is_negative = s[idx] == <span class="string">&#x27;-&#x27;</span> ? <span class="literal">true</span> : <span class="literal">false</span>;</span><br><span class="line">            idx++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 转换数字</span></span><br><span class="line">        <span class="keyword">while</span> (idx &lt; len) &#123;</span><br><span class="line">            <span class="type">int</span> tmp = s[idx] - <span class="string">&#x27;0&#x27;</span>;</span><br><span class="line">            <span class="keyword">if</span> (tmp &lt; <span class="number">0</span> || tmp &gt; <span class="number">9</span>) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// overflow</span></span><br><span class="line">            <span class="keyword">if</span> (INT_MAX / <span class="number">10</span> &lt; num_abs || INT_MAX / <span class="number">10</span> == num_abs &amp;&amp; INT_MAX % <span class="number">10</span> &lt; tmp) &#123;</span><br><span class="line">                <span class="keyword">return</span> is_negative ? INT_MIN : INT_MAX;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            num_abs = num_abs * <span class="number">10</span> + tmp;</span><br><span class="line">            idx++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> is_negative ? -num_abs : num_abs;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（3）最长公共前缀"><a href="#（3）最长公共前缀" class="headerlink" title="（3）最长公共前缀"></a>（3）最长公共前缀</h3><p><a href="https://leetcode.cn/problems/longest-common-prefix/description/" title="14. 最长公共前缀 - 力扣（LeetCode）">14. 最长公共前缀 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">编写一个函数来查找字符串数组中的最长公共前缀。</span><br><span class="line"></span><br><span class="line">如果不存在公共前缀，返回空字符串 <span class="string">&quot;&quot;</span>。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">string <span class="title">longestCommonPrefix</span><span class="params">(vector&lt;string&gt;&amp; strs)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> word_num = strs.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (word_num == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 扫描第一个单词的字符</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; strs[<span class="number">0</span>].<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="comment">// 取出第一个单词字符</span></span><br><span class="line">            <span class="type">char</span> ch = strs[<span class="number">0</span>][i];</span><br><span class="line">            <span class="comment">// 依次扫面后续单词的字符</span></span><br><span class="line">            <span class="comment">// 依次和第一个单词的字符匹配</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt; word_num; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (i == strs[j].<span class="built_in">size</span>() || ch != strs[j][i]) &#123;</span><br><span class="line">                    <span class="keyword">return</span> strs[<span class="number">0</span>].<span class="built_in">substr</span>(<span class="number">0</span>, i);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> strs[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（4）字符串翻转"><a href="#（4）字符串翻转" class="headerlink" title="（4）字符串翻转"></a>（4）字符串翻转</h3><p><a href="https://leetcode.cn/problems/reverse-string/description/" title="344. 反转字符串 - 力扣（LeetCode）">344. 反转字符串 - 力扣（LeetCode）</a></p><p><a href="https://leetcode.cn/problems/reverse-string-ii/solutions/946553/fan-zhuan-zi-fu-chuan-ii-by-leetcode-sol-ua7s/" title="541. 反转字符串 II - 力扣（LeetCode）">541. 反转字符串 II - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">给定一个字符串 s 和一个整数 k，从字符串开头算起，每计数至 2k 个字符，就反转这 2k 字符中的前 k 个字符。</span><br><span class="line"></span><br><span class="line">如果剩余字符少于 k 个，则将剩余字符全部反转。</span><br><span class="line">如果剩余字符小于 2k 但大于或等于 k 个，则反转前 k 个字符，其余字符保持原样。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">string <span class="title">reverseStr</span><span class="params">(string s, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = s.<span class="built_in">length</span>();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i += <span class="number">2</span> * k) &#123;</span><br><span class="line">            <span class="built_in">reverse</span>(s.<span class="built_in">begin</span>() + i, s.<span class="built_in">begin</span>() + <span class="built_in">min</span>(i + k, n));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> s;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="（5）翻转字符串中的单词"><a href="#（5）翻转字符串中的单词" class="headerlink" title="（5）翻转字符串中的单词"></a>（5）翻转字符串中的单词</h3><p><a href="https://leetcode.cn/problems/reverse-words-in-a-string/description/" title="151. 反转字符串中的单词 - 力扣（LeetCode）">151. 反转字符串中的单词 - 力扣（LeetCode）</a></p><p><a href="https://leetcode.cn/problems/reverse-words-in-a-string-iii/description/" title="557. 反转字符串中的单词 III - 力扣（LeetCode）">557. 反转字符串中的单词 III - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">给你一个字符串 s ，请你反转字符串中 单词 的顺序。</span><br><span class="line"></span><br><span class="line">单词 是由非空格字符组成的字符串。s 中使用至少一个空格将字符串中的 单词 分隔开。</span><br><span class="line"></span><br><span class="line">返回 单词 顺序颠倒且 单词 之间用单个空格连接的结果字符串。</span><br><span class="line"></span><br><span class="line">注意：输入字符串 s中可能会存在前导空格、尾随空格或者单词间的多个空格。返回的结果字符串中，单词间应当仅用单个空格分隔，且不包含任何额外的空格。</span><br><span class="line"></span><br><span class="line">示例 1：</span><br><span class="line"></span><br><span class="line">输入：s = <span class="string">&quot;the sky is blue&quot;</span></span><br><span class="line">输出：<span class="string">&quot;blue is sky the&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">给定一个字符串 s ，你需要反转字符串中每个单词的字符顺序，同时仍保留空格和单词的初始顺序。</span><br><span class="line">示例 1：</span><br><span class="line"></span><br><span class="line">输入：s = <span class="string">&quot;Let&#x27;s take LeetCode contest&quot;</span></span><br><span class="line">输出：<span class="string">&quot;s&#x27;teL ekat edoCteeL tsetnoc&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><ol><li>split， reverse，join</li><li>reverse整个string，然后在单独reverse每个单词</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">string <span class="title">reverseWords</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> str_len = s.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (str_len == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = s.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        std::string result;</span><br><span class="line">        std::string word;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 删除前面的空格</span></span><br><span class="line">        <span class="keyword">while</span> (left &lt; str_len &amp;&amp; s[left] == <span class="string">&#x27; &#x27;</span>) &#123;</span><br><span class="line">            left++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 删除后面的空格</span></span><br><span class="line">        <span class="keyword">while</span> (right &lt; str_len &amp;&amp; s[right] == <span class="string">&#x27; &#x27;</span>) &#123;</span><br><span class="line">            right--;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 切分单词</span></span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">            <span class="type">char</span> ch = s[left];</span><br><span class="line">            <span class="comment">// 如果单词不为空，且ch字符为空，则到下一个单词了，开始处理</span></span><br><span class="line">            <span class="keyword">if</span> (word.<span class="built_in">size</span>() != <span class="number">0</span> &amp;&amp; ch == <span class="string">&#x27; &#x27;</span>) &#123;</span><br><span class="line">                <span class="comment">// reslut为空的时候，特殊处理</span></span><br><span class="line">                <span class="keyword">if</span> (result.<span class="built_in">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">                    result = word;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    result = word +  <span class="string">&quot; &quot;</span> + result;;</span><br><span class="line">                &#125;</span><br><span class="line">                </span><br><span class="line">                word = <span class="string">&quot;&quot;</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ch != <span class="string">&#x27; &#x27;</span>) &#123;</span><br><span class="line">                word += ch;</span><br><span class="line">            &#125;</span><br><span class="line">            left++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// reslut为空的时候，特殊处理</span></span><br><span class="line">        <span class="keyword">if</span> (result.<span class="built_in">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">            result = word;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            result = word +  <span class="string">&quot; &quot;</span> + result;;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">string <span class="title">reverseWords</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> str_len = s.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (str_len == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = s.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        std::string result;</span><br><span class="line">        std::string word;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 删除前面的空格</span></span><br><span class="line">        <span class="keyword">while</span> (left &lt; str_len &amp;&amp; s[left] == <span class="string">&#x27; &#x27;</span>) &#123;</span><br><span class="line">            left++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 删除后面的空格</span></span><br><span class="line">        <span class="keyword">while</span> (right &lt; str_len &amp;&amp; s[right] == <span class="string">&#x27; &#x27;</span>) &#123;</span><br><span class="line">            right--;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 切分单词</span></span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">            <span class="type">char</span> ch = s[left];</span><br><span class="line">            <span class="comment">// 如果单词不为空，且ch字符为空，则到下一个单词了，开始处理</span></span><br><span class="line">            <span class="keyword">if</span> (word.<span class="built_in">size</span>() != <span class="number">0</span> &amp;&amp; ch == <span class="string">&#x27; &#x27;</span>) &#123;</span><br><span class="line">                <span class="comment">// 翻转单词</span></span><br><span class="line">                <span class="keyword">this</span>-&gt;<span class="built_in">resver_str</span>(word);</span><br><span class="line">                result = result + word +  <span class="string">&quot; &quot;</span>;</span><br><span class="line">                word = <span class="string">&quot;&quot;</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ch != <span class="string">&#x27; &#x27;</span>) &#123;</span><br><span class="line">                word += ch;</span><br><span class="line">            &#125;</span><br><span class="line">            left++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 处理最后一个单词</span></span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">resver_str</span>(word);</span><br><span class="line">        result = result + word;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">resver_str</span><span class="params">(std::string&amp; word)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> low = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> high = word.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (low &lt;= high) &#123;</span><br><span class="line">            <span class="type">char</span> tmp = word[low];</span><br><span class="line">            word[low] = word[high];</span><br><span class="line">            word[high] = tmp;</span><br><span class="line">            low++;</span><br><span class="line">            high--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（6）找到字符串中所有字母异位词"><a href="#（6）找到字符串中所有字母异位词" class="headerlink" title="（6）找到字符串中所有字母异位词"></a>（6）找到字符串中所有字母异位词</h3><p><a href="https://leetcode.cn/problems/find-all-anagrams-in-a-string/description/" title="438. 找到字符串中所有字母异位词 - 力扣（LeetCode）">438. 找到字符串中所有字母异位词 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">给定两个字符串 s 和 p，找到 s 中所有 p 的 异位词 的子串，返回这些子串的起始索引。不考虑答案输出的顺序。</span><br><span class="line"></span><br><span class="line">异位词 指由相同字母重排列形成的字符串（包括相同的字符串）。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">输入: s = <span class="string">&quot;cbaebabacd&quot;</span>, p = <span class="string">&quot;abc&quot;</span></span><br><span class="line">输出: [0,6]</span><br><span class="line">解释:</span><br><span class="line">起始索引等于 0 的子串是 <span class="string">&quot;cba&quot;</span>, 它是 <span class="string">&quot;abc&quot;</span> 的异位词。</span><br><span class="line">起始索引等于 6 的子串是 <span class="string">&quot;bac&quot;</span>, 它是 <span class="string">&quot;abc&quot;</span> 的异位词。</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>滑动窗口，看窗口中的单词是不是某个单词的异位词</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 构造两个相同的滑动窗口，在滑动窗口中维护每个字母的数量</span></span><br><span class="line">    <span class="comment">// 如果字母数量相同，则说明是 异位词</span></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">findAnagrams</span><span class="params">(string s, string p)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> s_len = s.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> p_len = p.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果s的长度小于p的，则直接返回</span></span><br><span class="line">        <span class="keyword">if</span> (s_len &lt; p_len) &#123;</span><br><span class="line">            <span class="keyword">return</span> std::<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 结果</span></span><br><span class="line">        std::vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line">        <span class="comment">// s字符串字母统计</span></span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">s_count</span><span class="params">(<span class="number">26</span>)</span></span>;</span><br><span class="line">        <span class="comment">// p字符串字母统计</span></span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">p_count</span><span class="params">(<span class="number">26</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 首先统计前p_len个字符串中的字符数量</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; p_len; i++) &#123;</span><br><span class="line">            s_count[s[i] - <span class="string">&#x27;a&#x27;</span>]++;</span><br><span class="line">            p_count[p[i] - <span class="string">&#x27;a&#x27;</span>]++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 最开始如果相等，加入0</span></span><br><span class="line">        <span class="keyword">if</span> (s_count == p_count) &#123;</span><br><span class="line">            ans.<span class="built_in">emplace_back</span>(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 再遍历s中剩余的字符串</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; s_len - p_len; i++) &#123;</span><br><span class="line">            s_count[s[i] - <span class="string">&#x27;a&#x27;</span>]--;</span><br><span class="line">            s_count[s[i + p_len] - <span class="string">&#x27;a&#x27;</span>]++;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (s_count == p_count) &#123;</span><br><span class="line">                ans.<span class="built_in">emplace_back</span>(i + <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（7）回文词判断"><a href="#（7）回文词判断" class="headerlink" title="（7）回文词判断"></a>（7）回文词判断</h3><p><a href="https://leetcode.cn/problems/valid-palindrome/" title="125. 验证回文串 - 力扣（LeetCode）">125. 验证回文串 - 力扣（LeetCode）</a></p><p><a href="https://leetcode.cn/problems/valid-palindrome-ii/description/" title="680. 验证回文串 II - 力扣（LeetCode）">680. 验证回文串 II - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">如果在将所有大写字符转换为小写字符、并移除所有非字母数字字符之后，短语正着读和反着读都一样。则可以认为该短语是一个 回文串 。</span><br><span class="line"></span><br><span class="line">字母和数字都属于字母数字字符。</span><br><span class="line"></span><br><span class="line">给你一个字符串 s，如果它是 回文串 ，返回 <span class="literal">true</span> ；否则，返回 <span class="literal">false</span> 。</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给你一个字符串 s，最多 可以从中删除一个字符。</span><br><span class="line"></span><br><span class="line">请你判断 s 是否能成为回文字符串：如果能，返回 <span class="literal">true</span> ；否则，返回 <span class="literal">false</span> 。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isPalindrome</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = s.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="comment">// 左侧移除非字母数字</span></span><br><span class="line">            <span class="keyword">while</span> (left &lt; right &amp;&amp; !(std::<span class="built_in">isdigit</span>(s[left]) || std::<span class="built_in">isalpha</span>(s[left]))) &#123;</span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 右侧移除非字母数字</span></span><br><span class="line">            <span class="keyword">while</span> (left &lt; right &amp;&amp; !(std::<span class="built_in">isdigit</span>(s[right]) || std::<span class="built_in">isalpha</span>(s[right]))) &#123;</span><br><span class="line">                right--;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 判断左右两侧字母是否相同</span></span><br><span class="line">            <span class="keyword">if</span> (std::<span class="built_in">tolower</span>(s[left]) == std::<span class="built_in">tolower</span>(s[right])) &#123;</span><br><span class="line">                left++;</span><br><span class="line">                right--;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>在允许最多删除一个字符的情况下，同样可以使用双指针，通过贪心实现。初始化两个指针 left 和 right 分别指向字符串的第一个字符和最后一个字符。每次判断两个指针指向的字符是否相同，如果相同，则更新指针，将left + 1，right - 1，然后判断更新后的指针范围内的子串是否是回文字符串。</p><p>如果两个指针指向的字符不同，则两个字符中必须有一个被删除，此时就分成两种情况：</p><ul><li>删除左指针对应的字符，留下子串 s[left+1 : right]</li><li>删除右指针对应的字符，留下子串 s[left : right−1]</li></ul><p>当这两个子串中至少有一个是回文串时，就说明原始字符串删除一个字符之后就以成为回文串。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">validPalindrome</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s.<span class="built_in">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = s.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">if</span> (s[left] == s[right]) &#123;</span><br><span class="line">                left++;</span><br><span class="line">                right--;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">check_palindrome</span>(s, left + <span class="number">1</span>, right)  || <span class="keyword">this</span>-&gt;<span class="built_in">check_palindrome</span>(s, left, right - <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 检查字符串子串是不是回文串</span></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">check_palindrome</span><span class="params">(std::string s, <span class="type">int</span> left, <span class="type">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = left, j = right; i &lt; j; i++, j--) &#123;</span><br><span class="line">            <span class="keyword">if</span> (s[i] != s[j]) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（8）最长回文子串"><a href="#（8）最长回文子串" class="headerlink" title="（8）最长回文子串"></a>（8）最长回文子串</h3><p><a href="https://leetcode.cn/problems/longest-palindromic-substring/description/" title="5. 最长回文子串 - 力扣（LeetCode）">5. 最长回文子串 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">给你一个字符串 s，找到 s 中最长的回文子串。</span><br><span class="line"></span><br><span class="line">如果字符串的反序与原始字符串相同，则该字符串称为回文字符串。</span><br><span class="line"></span><br><span class="line">输入：s = <span class="string">&quot;babad&quot;</span></span><br><span class="line">输出：<span class="string">&quot;bab&quot;</span></span><br><span class="line">解释：<span class="string">&quot;aba&quot;</span> 同样是符合题意的答案。</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="暴力求解"><a href="#暴力求解" class="headerlink" title="暴力求解"></a>暴力求解</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.暴力求解，列举所有的子串，判断是否为回文串</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">string <span class="title">longestPalindrome</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s.<span class="built_in">size</span>() &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> s;</span><br><span class="line">        &#125;</span><br><span class="line">        std::string ans;</span><br><span class="line">        <span class="type">int</span> max_len = <span class="number">0</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; s.<span class="built_in">size</span>() - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = i + <span class="number">1</span>; j &lt; s.<span class="built_in">size</span>(); j++) &#123;</span><br><span class="line">                std::string tmp_str = s.<span class="built_in">substr</span>(i, j - i + <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;<span class="built_in">check_palindrome</span>(tmp_str) &amp;&amp; tmp_str.<span class="built_in">size</span>() &gt; max_len) &#123;</span><br><span class="line">                    ans = tmp_str;</span><br><span class="line">                    max_len = tmp_str.<span class="built_in">size</span>();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 检查字符串子串是不是回文串</span></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">check_palindrome</span><span class="params">(std::string&amp; s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>, j = s.<span class="built_in">size</span>() - <span class="number">1</span>; i &lt; j; i++, j--) &#123;</span><br><span class="line">            <span class="keyword">if</span> (s[i] != s[j]) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="暴力-动态规划"><a href="#暴力-动态规划" class="headerlink" title="暴力 + 动态规划"></a>暴力 + 动态规划</h4><p>去掉一些暴力解法中重复的判断。可以基于下边的发现，进行改进。</p><p>状态定义：<code>P(i, j) = true,</code> <code>s[i,j]</code>是回文串；<code>P(i, j) = false</code>, <code>s[i,j]</code>不是是回文串；</p><p>接下来$P(i,j)=(P(i+1,j−1) ~ \&amp;\&amp; ~ S[i]==S[j])$</p><p>所以如果想知道$P（i,j）$的情况，不需要调用判断回文串的函数了，只需要知道$P（i + 1，j - 1）$的情况就可以了，这样时间复杂度就少了 O(n)。因此可以用动态规划的方法，空间换时间，把已经求出的$  P（i，j） $存储起来。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2.暴力 + 动态规划</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">string <span class="title">longestPalindrome</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> str_len = s.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (str_len &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> s;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">bool</span>&gt;&gt; <span class="built_in">dp</span>(str_len, std::<span class="built_in">vector</span>&lt;<span class="type">bool</span>&gt;(str_len));</span><br><span class="line">        <span class="comment">// 最长子串的开始位置和最大长度</span></span><br><span class="line">        <span class="type">int</span> max_len = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> begin = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历所有长度</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> len = <span class="number">1</span>; len &lt;= str_len; len++) &#123;</span><br><span class="line">            <span class="comment">// 枚举左边界</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> start = <span class="number">0</span>; start &lt; str_len; start++) &#123;</span><br><span class="line">                <span class="comment">// 根据左边界和长度，确定结束位置</span></span><br><span class="line">                <span class="type">int</span> end = start + len - <span class="number">1</span>;</span><br><span class="line">                <span class="comment">// 下标越界</span></span><br><span class="line">                <span class="keyword">if</span> (end &gt;= str_len) &#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// dp[i][j] = dp[i + 1][j - 1] &amp;&amp; s[i] == s[j]</span></span><br><span class="line">                dp[start][end] = s[start] == s[end] &amp;&amp; (len == <span class="number">1</span> || len == <span class="number">2</span> || dp[start + <span class="number">1</span>][end - <span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (dp[start][end] &amp;&amp; len &gt; max_len)  &#123;</span><br><span class="line">                    max_len = len;</span><br><span class="line">                    begin = start;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> s.<span class="built_in">substr</span>(begin, max_len);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="中心扩散"><a href="#中心扩散" class="headerlink" title="中心扩散"></a>中心扩散</h4><p>回文串一定是对称的，所以可以每次循环选择一个中心，进行左右扩展，判断左右字符是否相等即可。</p><p><img src="image/image_chQH9WHqhM.png" alt=""></p><p>由于存在奇数的字符串和偶数的字符串，所以需要从一个字符开始扩展，或者从两个字符之间开始扩展，所以总共有 <code>n+n-1</code> 个中心。&#x20;</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">string <span class="title">longestPalindrome</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> len = s.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (len &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> s;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; len - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="comment">// 奇数长度</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;<span class="built_in">extend_palindrome</span>(s, i, i);</span><br><span class="line">            <span class="comment">// 偶数长度</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;<span class="built_in">extend_palindrome</span>(s, i, i + <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> s.<span class="built_in">substr</span>(m_start, m_max_len);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">extend_palindrome</span><span class="params">(std::string&amp; s, <span class="type">int</span> left, <span class="type">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (left &gt;= <span class="number">0</span> &amp;&amp; right &lt; s.<span class="built_in">size</span>() &amp;&amp; s[left] == s[right]) &#123;</span><br><span class="line">            left--;</span><br><span class="line">            right++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (m_max_len &lt; right - left - <span class="number">1</span>) &#123;</span><br><span class="line">            m_start = left + <span class="number">1</span>;</span><br><span class="line">            m_max_len = right - left - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> m_start;</span><br><span class="line">    <span class="type">int</span> m_max_len;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="2-高级字符串算法"><a href="#2-高级字符串算法" class="headerlink" title="2.高级字符串算法"></a>2.高级字符串算法</h1><h2 id="2-1-最长子串、子序列"><a href="#2-1-最长子串、子序列" class="headerlink" title="2.1 最长子串、子序列"></a>2.1 最长子串、子序列</h2><h3 id="（1）编辑距离"><a href="#（1）编辑距离" class="headerlink" title="（1）编辑距离"></a>（1）编辑距离</h3><p><a href="https://leetcode.cn/problems/edit-distance/description/" title="72. 编辑距离 - 力扣（LeetCode）">72. 编辑距离 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">给你两个单词 word1 和 word2， 请返回将 word1 转换成 word2 所使用的最少操作数  。</span><br><span class="line"></span><br><span class="line">你可以对一个单词进行如下三种操作：</span><br><span class="line"></span><br><span class="line">- 插入一个字符</span><br><span class="line">- 删除一个字符</span><br><span class="line">- 替换一个字符</span><br></pre></td></tr></table></figure><ol><li>BFS + 剪枝（单词的长度范围）</li><li>DP<ol><li>状态定义：<code>dp[0..i][0..j]</code>， i表示第一个字符串匹配到第二个字符串的长度；j表示第二个字符串匹配到第一个字符串的长度；<code>word1.substr(0, i)</code> 与 <code>word2.substr(0, j)</code>之间的编辑距离<br>2.</li></ol></li></ol><p><strong>w1和w2的最后一个字符一样</strong></p><blockquote><p>w1 : …x (i)<br>w2 : …x (j)&#x20;</p></blockquote><p><code>edit_dist(w1, w2) = edit_dist(w1[0 : i -1], w2[0, j - 1])</code></p><p><code>edit_dist(i, j) = edit_dist(i - 1, j - 1)</code></p><p><strong>w1和w2的最后一个字符不一样</strong></p><blockquote><p>w1 : …x (i)<br>w2 : …y (j)&#x20;</p></blockquote><p><code>edit_dist(i, j) = ``min``(edit_dist(i - 1, j - 1) + 1 , edit_dist(i - 1, j ) + 1, edit_dist(i, j - 1) + 1)</code></p><ul><li><code>edit_dist(i - 1, j - 1) + 1</code> :  替换，编辑距离 + 1</li><li><code>edit_dist(i - 1, j) + 1</code> : 删除word1最后一个字符， 编辑距离 + 1</li><li><code>edit_dist(i , j - 1) + 1</code> : 删除 word2最后一个字符，编辑距离 + 1</li></ul><p>注意，针对第一行，第一列要单独考虑，我们引入 <code>&#39;&#39;</code> 下图所示：</p><p><img src="image/image_Ci7yWfpKER.png" alt=""></p><p>第一行，是 <code>word1</code> 为空变成 <code>word2</code> 最少步数，就是插入操作</p><p>第一列，是 <code>word2</code> 为空，需要的最少步数，就是删除操作</p><h3 id="（2）最长公共子序列"><a href="#（2）最长公共子序列" class="headerlink" title="（2）最长公共子序列"></a>（2）最长公共子序列</h3><p><a href="https://leetcode.cn/problems/longest-common-subsequence/description/" title="1143. 最长公共子序列 - 力扣（LeetCode）">1143. 最长公共子序列 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">给定两个字符串 text1 和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在 公共子序列 ，返回 <span class="number">0</span> 。</span><br><span class="line"></span><br><span class="line">一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。</span><br><span class="line"></span><br><span class="line">- 例如，<span class="string">&quot;ace&quot;</span> 是 <span class="string">&quot;abcde&quot;</span> 的子序列，但 <span class="string">&quot;aec&quot;</span> 不是 <span class="string">&quot;abcde&quot;</span> 的子序列。</span><br><span class="line">两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。</span><br></pre></td></tr></table></figure><p>注意分区子序列和子串：子序列可以有间隔，子串没有间隔</p><p><img src="image/image_ZSE2OSmBhb.png" alt=""></p><p>状态定义：<code>dp[0..i][0..j]</code>， i表示第一个字符串匹配到第二个字符串的长度；j表示第二个字符串匹配到第一个字符串的长度；<code>word1.substr(0, i)</code> 与 <code>word2.substr(0, j)</code>之间的最长公共子序列</p><p>状态方程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> s1[i - <span class="number">1</span>] == s2[i - <span class="number">1</span>]:</span><br><span class="line">  dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  dp[i][j] = <span class="built_in">max</span>(dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">longestCommonSubsequence</span><span class="params">(string text1, string text2)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> m = text1.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> n = text2.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (m == <span class="number">0</span> || n == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(m + <span class="number">1</span>, std::<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n + <span class="number">1</span>, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (text1.<span class="built_in">at</span>(i - <span class="number">1</span>) == text2.<span class="built_in">at</span>(j - <span class="number">1</span>)) &#123;</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[i][j] = std::<span class="built_in">max</span>(dp[i][j - <span class="number">1</span>], dp[i - <span class="number">1</span>][j]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[m][n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（3）最长公共子串"><a href="#（3）最长公共子串" class="headerlink" title="（3）最长公共子串"></a>（3）最长公共子串</h3><p>LeetCode没有题目</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">最长公共子串，是指两个字符串中最长连续相同的子串长度。</span><br><span class="line">例如：str1=“1AB2345CD”,str2=”12345EF”,则str1，str2的最长公共子串为2345。</span><br></pre></td></tr></table></figure><p><img src="image/image_ZB2F52QA5b.png" alt=""></p><p>状态定义：<code>dp[0..i][0..j]</code>， i表示第一个字符串匹配到第二个字符串的长度；j表示第二个字符串匹配到第一个字符串的长度；<code>word1.substr(0, i)</code> 与 <code>word2.substr(0, j)</code>之间的最长公共子串</p><p>状态方程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> s1[i - <span class="number">1</span>] == s2[i - <span class="number">1</span>]:</span><br><span class="line">  dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  dp[i][j] = <span class="number">0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">longestCommonSubstring</span><span class="params">(string text1, string text2)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> m = text1.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> n = text2.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (m == <span class="number">0</span> || n == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(m + <span class="number">1</span>, std::<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n + <span class="number">1</span>, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (text1.<span class="built_in">at</span>(i - <span class="number">1</span>) == text2.<span class="built_in">at</span>(j - <span class="number">1</span>)) &#123;</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[i][j] = <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 在dp数组中找到最大值，而不是dp[m][n]</span></span><br><span class="line">        <span class="keyword">return</span> *<span class="built_in">max_elemen</span>(dp.<span class="built_in">begin</span>(),dp.<span class="built_in">end</span>())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-2-字符串-递归-or-DP-x20"><a href="#2-2-字符串-递归-or-DP-x20" class="headerlink" title="2.2 字符串 + 递归 or DP&#x20;"></a>2.2 字符串 + 递归 or DP&#x20;</h2><h3 id="（1）正则表达式匹配"><a href="#（1）正则表达式匹配" class="headerlink" title="（1）正则表达式匹配"></a>（1）正则表达式匹配</h3><p><a href="https://leetcode.cn/problems/regular-expression-matching/description/" title="10. 正则表达式匹配 - 力扣（LeetCode）">10. 正则表达式匹配 - 力扣（LeetCode）</a></p><p><a href="https://leetcode.cn/problems/regular-expression-matching/solutions/6673/ji-yu-guan-fang-ti-jie-gen-xiang-xi-de-jiang-jie-b/" title="动态规划解法，只需关注「匹配」和「不匹配」即可">动态规划解法，只需关注「匹配」和「不匹配」即可</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">给你一个字符串 s 和一个字符规律 p，请你来实现一个支持 <span class="string">&#x27;.&#x27;</span> 和 <span class="string">&#x27;*&#x27;</span> 的正则表达式匹配。</span><br><span class="line"></span><br><span class="line">- <span class="string">&#x27;.&#x27;</span> 匹配任意单个字符</span><br><span class="line">- <span class="string">&#x27;*&#x27;</span> 匹配零个或多个前面的那一个元素</span><br><span class="line">所谓匹配，是要涵盖 整个 字符串 s的，而不是部分字符串。</span><br><span class="line"></span><br><span class="line">示例 2:</span><br><span class="line"></span><br><span class="line">输入：s = <span class="string">&quot;aa&quot;</span>, p = <span class="string">&quot;a*&quot;</span></span><br><span class="line">输出：<span class="literal">true</span></span><br><span class="line">解释：因为 <span class="string">&#x27;*&#x27;</span> 代表可以匹配零个或多个前面的那一个元素, 在这里前面的元素就是 <span class="string">&#x27;a&#x27;</span>。因此，字符串 <span class="string">&quot;aa&quot;</span> 可被视为 <span class="string">&#x27;a&#x27;</span> 重复了一次。</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="1）两个字符串匹配"><a href="#1）两个字符串匹配" class="headerlink" title="1）两个字符串匹配"></a>1）两个字符串匹配</h4><p>如果是两个普通字符串，如何进行匹配？</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">isMatch</span><span class="params">(string text, string pattern)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (text.<span class="built_in">size</span>() != pattern.<span class="built_in">size</span>()) </span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; pattern.<span class="built_in">size</span>(); j++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (pattern[j] != text[j])</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将上述改写为递归的形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">isMatch</span>(<span class="params">text, pattern</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">  <span class="keyword">if</span> pattern <span class="keyword">is</span> empty:</span><br><span class="line">    <span class="keyword">return</span> text <span class="keyword">is</span> empty</span><br><span class="line">  first_match = (text <span class="keyword">not</span> empty) <span class="keyword">and</span> pattern[<span class="number">0</span>] == text[<span class="number">0</span>]</span><br><span class="line">  <span class="keyword">return</span> first_match <span class="keyword">and</span> isMatch(text[<span class="number">1</span>:], pattern[<span class="number">1</span>:])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="2）处理点号-·通配符"><a href="#2）处理点号-·通配符" class="headerlink" title="2）处理点号 ·通配符"></a>2）处理点号 <code>·</code>通配符</h4><p>点号可以匹配任意一个字符，修改上面伪代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">isMatch</span>(<span class="params">text, pattern</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> pattern:</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">not</span> <span class="built_in">next</span></span><br><span class="line">  first_match = <span class="built_in">bool</span>(text) <span class="keyword">and</span> pattern[<span class="number">0</span>] <span class="keyword">in</span> &#123;text[<span class="number">0</span>], <span class="string">&#x27;.&#x27;</span>&#125;</span><br><span class="line">  <span class="keyword">return</span> first_match <span class="keyword">and</span> isMatch(text[<span class="number">1</span>:], pattern[<span class="number">1</span>:])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="3）处理-通配符"><a href="#3）处理-通配符" class="headerlink" title="3）处理 * 通配符"></a>3）处理 <code>*</code> 通配符</h4><p>星号通配符可以让前一个字符重复任意次数，那到底重复几次？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">isMatch</span>(<span class="params">text, pattern</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> pattern:</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">not</span> <span class="built_in">next</span></span><br><span class="line">  first_match = <span class="built_in">bool</span>(text) <span class="keyword">and</span> pattern[<span class="number">0</span>] <span class="keyword">in</span> &#123;text[<span class="number">0</span>], <span class="string">&#x27;.&#x27;</span>&#125;</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">len</span>(pattern) &gt;= <span class="number">2</span> <span class="keyword">and</span> pattern[<span class="number">1</span>] == <span class="string">&#x27;*&#x27;</span>:</span><br><span class="line">    <span class="comment"># 发现 * 通配符</span></span><br><span class="line">  <span class="keyword">return</span> first_match <span class="keyword">and</span> isMatch(text[<span class="number">1</span>:], pattern[<span class="number">1</span>:])</span><br></pre></td></tr></table></figure><p>星号前面的字符到底要重复几次呢？不管重复几次，当前的选择只有两个：匹配0次、匹配1次，所以可以这样处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(pattern) &gt;= <span class="number">2</span> <span class="keyword">and</span> pattern[<span class="number">1</span>] == <span class="string">&#x27;*&#x27;</span>:</span><br><span class="line">  <span class="keyword">return</span> isMatch(test, pattern[<span class="number">2</span>:]) <span class="keyword">or</span> \</span><br><span class="line">         first_mactch <span class="keyword">and</span> isMatch(text[<span class="number">1</span>:], pattern)</span><br><span class="line"><span class="comment"># 解释：如果发现有字符和 ‘*’ 结合</span></span><br><span class="line"><span class="comment"># 1.匹配该字符0次，然后跳过该字符</span></span><br><span class="line"><span class="comment"># 2.当pattern[0]和text[0]匹配后，移动text </span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到，通过保留pattern中的<code>*</code>，同时向后推移text，来实现<code>*</code>将字符重复多次的功能。</p><p><img src="image/image_KMzIZXooUN.png" alt=""></p><h4 id="4）动态规划"><a href="#4）动态规划" class="headerlink" title="4）动态规划"></a>4）动态规划</h4><p>选择使用备忘录的递归方法来降低复杂度</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isMatch</span><span class="params">(string s, string p)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 大小+1的目的是因为memo有边界限制</span></span><br><span class="line">        m_memo = std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt;(s.<span class="built_in">size</span>() + <span class="number">1</span>, std::<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(p.<span class="built_in">size</span>() + <span class="number">1</span>, <span class="number">-1</span>));</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">recur</span>(s, p, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">recur</span><span class="params">(std::string&amp; s, std::string&amp; p, <span class="type">int</span> s_idx, <span class="type">int</span> p_idx)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 递归出口</span></span><br><span class="line">        <span class="comment">// 当 s_idx == s.size()，且 p_idx &lt; p.size()</span></span><br><span class="line">        <span class="comment">// 可能p中还有*通配符</span></span><br><span class="line">        <span class="keyword">if</span> (p_idx == p.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> s_idx == s.<span class="built_in">size</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果在memo中有存储，直接返回结果</span></span><br><span class="line">        <span class="keyword">if</span> (m_memo[s_idx][p_idx] != <span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> m_memo[s_idx][p_idx];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 整个结果是否匹配</span></span><br><span class="line">        <span class="type">bool</span> res = <span class="literal">false</span>;</span><br><span class="line">        <span class="comment">// 当前第一个字符是否匹配</span></span><br><span class="line">        <span class="type">bool</span> first_match = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理 . 通配符</span></span><br><span class="line">        <span class="keyword">if</span> (s_idx &lt; s.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            <span class="keyword">if</span>(s[s_idx] == p[p_idx] || p[p_idx] == <span class="string">&#x27;.&#x27;</span>) &#123;</span><br><span class="line">                first_match = <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理 * 通配符</span></span><br><span class="line">        <span class="keyword">if</span> ((p_idx + <span class="number">1</span>) &lt; p.<span class="built_in">size</span>() &amp;&amp; p[p_idx + <span class="number">1</span>] == <span class="string">&#x27;*&#x27;</span>) &#123;</span><br><span class="line">            <span class="comment">// 考虑只需两种情况：</span></span><br><span class="line">            <span class="comment">// 情况1：当前字符出现0次：跳过pattern中的当前字符和下一个&quot;*&quot;==&gt;helper(s, p, si, pi + 2)</span></span><br><span class="line">            <span class="comment">// 情况2：当前字符出现1次：当前是否匹配 &amp;&amp; 将字符s向后移动一位是否匹配==&gt;cur_match &amp;&amp; helper(s, p, si + 1, pi)</span></span><br><span class="line"></span><br><span class="line">            res = <span class="keyword">this</span>-&gt;<span class="built_in">recur</span>(s, p, s_idx, p_idx + <span class="number">2</span>) || (first_match &amp;&amp; <span class="keyword">this</span>-&gt;<span class="built_in">recur</span>(s, p, s_idx + <span class="number">1</span>, p_idx));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// //下一个不是&quot;*&quot;正常向后匹配就好</span></span><br><span class="line">            res = first_match &amp;&amp; <span class="keyword">this</span>-&gt;<span class="built_in">recur</span>(s, p, s_idx + <span class="number">1</span>, p_idx + <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        m_memo[s_idx][p_idx] = res;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; m_memo;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（2）不同的子序列"><a href="#（2）不同的子序列" class="headerlink" title="（2）不同的子序列"></a>（2）不同的子序列</h3><p><a href="https://leetcode.cn/problems/distinct-subsequences/description/" title="115. 不同的子序列 - 力扣（LeetCode）">115. 不同的子序列 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">给你两个字符串 s 和 t ，统计并返回在 s 的 子序列 中 t 出现的个数，结果需要对 10^9 + 7 取模。</span><br><span class="line"></span><br><span class="line">输入：s = <span class="string">&quot;rabbbit&quot;</span>, t = <span class="string">&quot;rabbit&quot;</span></span><br><span class="line">输出：3</span><br><span class="line">解释：</span><br><span class="line">如下所示, 有 3 种可以从 s 中得到 <span class="string">&quot;rabbit&quot;</span> 的方案。</span><br><span class="line">rabbbit</span><br><span class="line">rabbbit</span><br><span class="line">rabbbit</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>状态定义：<code>dp[i][j]</code>代表<code>T</code>的前i字符可以由<code>S</code>前j字符串组成的最多个数</p><p>动态方程：</p><ul><li>当<code>s[j] == t[i]</code>，<code>dp[i][j] = dp[i-1][j-1] + dp[i][j-1]</code></li><li>当<code>s[j] != t[i]</code>， <code>dp[i][j] = dp[i][j-1]</code></li></ul><p><img src="image/image_bW0gNJeNlO.png" alt=""></p><p>对于第一行, <code>T</code> 为空,因为空集是所有字符串子集, 所以我们第一行都是 <code>1</code></p><p>对于第一列, <code>S</code> 为空,这样组成 <code>T</code> 个数当然为 0` 了</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">numDistinct</span><span class="params">(string s, string t)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> m = t.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> n = s.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(m + <span class="number">1</span>, std::<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n + <span class="number">1</span>, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt;= n; j++) &#123;</span><br><span class="line">            dp[<span class="number">0</span>][j] = <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (t[i - <span class="number">1</span>] == s[j - <span class="number">1</span>]) &#123;</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + dp[i][j - <span class="number">1</span>];</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[i][j] = dp[i][j - <span class="number">1</span>];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[m][n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="3-字符串匹配算法"><a href="#3-字符串匹配算法" class="headerlink" title="3.字符串匹配算法"></a>3.字符串匹配算法</h1><h2 id="3-1-暴力法"><a href="#3-1-暴力法" class="headerlink" title="3.1 暴力法"></a>3.1 暴力法</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">forceSearch</span><span class="params">(std::string txt, std::string pat)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> M = txt.<span class="built_in">size</span>();</span><br><span class="line">    <span class="type">int</span> N = pat.<span class="built_in">size</span>();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;= M - N; i++) &#123;</span><br><span class="line">        <span class="type">int</span> j;</span><br><span class="line">        <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; N; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (txt[i + j] != pat[j])</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (j == N) &#123;</span><br><span class="line">            <span class="keyword">return</span> i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>更加聪明？</p><ol><li>Rabin-Karp：预先判断 <code>hash(txt.substr(i, M)) == hash(pat)</code></li><li>KMP：已经匹配的片段中，它的最大前缀和最大后缀</li></ol><h2 id="3-2-Rabin-Karp算法"><a href="#3-2-Rabin-Karp算法" class="headerlink" title="3.2 Rabin-Karp算法"></a>3.2 Rabin-Karp算法</h2><p>在朴素算法中，我们需要挨个比较所有字符，才知道目标字符串中是否包含子串。那么，是否有别的方法可以用来判断目标字符串是否包含子串呢?</p><p>答案是肯定的，确实存在一种更快的方法。为了避免挨个字符对目标字符串和子串进行比较，我们可以<strong>尝试一次性判断两者是否相等</strong>。因此，我们需要一个好的<strong>哈希函数 (hash function)</strong>。 通过哈希函数，我们可以算出子串的哈希值，然后将<strong>它和目标字符串中的子串的哈希值进行比较</strong>。 这个新方法在速度上比暴力法有显著提升。</p><p>算法思想：</p><ol><li>假设子串的长度为 <code>M(pat)</code>，目标字符串的长度为 <code>N(txt)</code></li><li>计算子串的hash值<code>hash_pat</code></li><li>计算目标字符串txt中每个长度为M的子串的hash值（共需要计算 <code>N-M+1</code>次）</li><li>比较hash值：如果hash值不同，字符串必然不匹配；如果hash值相同，还需要使用朴素算法再次判断</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">D</span> <span class="operator">=</span> <span class="number">256</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">Q</span> <span class="operator">=</span> <span class="number">9997</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="type">int</span> <span class="title function_">RabinKarpSerach</span><span class="params">(String txt, String pat)</span> &#123;</span><br><span class="line">  <span class="type">int</span> <span class="variable">M</span> <span class="operator">=</span> pat.length();</span><br><span class="line">  <span class="type">int</span> <span class="variable">N</span> <span class="operator">=</span> txt.length();</span><br><span class="line">  <span class="type">int</span> i, j;</span><br><span class="line">  <span class="type">int</span> <span class="variable">patHash</span> <span class="operator">=</span> <span class="number">0</span>, txtHash = <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; M; i++) &#123;</span><br><span class="line">    pathHash = (D * patHash + pat.charAt(i)) % Q;</span><br><span class="line">    txtHash = (D * txtHash + txt.charAt(i)) % Q;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="type">int</span> <span class="variable">highestPow</span> <span class="operator">=</span> <span class="number">1</span>;  <span class="comment">// pow(256, M-1)</span></span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; M - <span class="number">1</span>; i++)</span><br><span class="line">    highestPow = (highestPow * D) % Q;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt;= N - M; i++) &#123;  <span class="comment">// 枚举起点</span></span><br><span class="line">    <span class="keyword">if</span> (pathHash == txtHash) &#123;</span><br><span class="line">      <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; M; j++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (txt.charAt(i + j) != pat.charAt(j))</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (j == M)</span><br><span class="line">        <span class="keyword">return</span> i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (i &lt; N - M) &#123;</span><br><span class="line">      txtHash = (D * (txtHash - txt.charAt(i) * highestPow) + txt.charAt(i + m)) % Q;</span><br><span class="line">      <span class="keyword">if</span> (txtHash &lt; <span class="number">0</span>)</span><br><span class="line">        txtHash += Q;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-3-KMP算法"><a href="#3-3-KMP算法" class="headerlink" title="3.3 KMP算法"></a>3.3 KMP算法</h2><ul><li><a href="https://www.bilibili.com/video/av11866460?from=search\&amp;seid=17425875345653862171" title="KMP 字符串匹配算法视频">KMP 字符串匹配算法视频</a></li><li><a href="http://www.ruanyifeng.com/blog/2013/05/Knuth–Morris–Pratt_algorithm.html" title="字符串匹配的 KMP 算法">字符串匹配的 KMP 算法</a></li></ul><p>KMP算法 (Knuth-Morris-Pratt) 的思想就是，当子串与目标字符串不匹配时，其实你已经知道了前面已经匹配成功那 一部分的字符 (包括子串与目标字符串)。以阮一峰的文章为例，当空格与 D 不匹配时，你其实 知道前面六个字符是“ABCDAB”。</p><p>KMP 算法的想法是，<strong>设法利用这个已知信息，不要把“搜索位置”移回已经比较过的位置，继续把它向后移，这样就提高了效率</strong>。</p><h2 id="3-4-其他"><a href="#3-4-其他" class="headerlink" title="3.4 其他"></a>3.4 其他</h2><h3 id="（1）Boyer-Moore算法"><a href="#（1）Boyer-Moore算法" class="headerlink" title="（1）Boyer-Moore算法"></a>（1）Boyer-Moore算法</h3><p>BM算法核心思想是，<strong>利用模式串本身的特点，在模式串中某个字符与主串不能匹配的时候，将模式串往后多滑动几位，以此来减少不必要的字符比较，提高匹配的效率。</strong>&#x20;</p><h3 id="（2）Sunday算法"><a href="#（2）Sunday算法" class="headerlink" title="（2）Sunday算法"></a>（2）Sunday算法</h3><ul><li><a href="https://blog.csdn.net/u012505432/article/details/52210975" title="Sunday 算法">Sunday 算法</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法 (Data Structures and Algorithms)</title>
      <link href="/dsa/dsa_idx/"/>
      <url>/dsa/dsa_idx/</url>
      
        <content type="html"><![CDATA[<blockquote><p>建立时间复杂度、空间复杂度意识，写出高质量的代码，能够设计基础架构，提升编程技能，训练逻辑思维。</p></blockquote><h2 id="1-易混淆专题"><a href="#1-易混淆专题" class="headerlink" title="1.易混淆专题"></a>1.易混淆专题</h2><ol><li><a href="/dsa/confusing_topics/1.Binary_Search">二分查找</a></li></ol><h2 id="2-LeetCode刷题专栏"><a href="#2-LeetCode刷题专栏" class="headerlink" title="2.LeetCode刷题专栏"></a>2.LeetCode刷题专栏</h2><ol><li><a href="/dsa/leetcode/01_overview/README">数据结构与算法总览</a></li><li><a href="/dsa/leetcode/02_complexity_analysis/README">复杂度分析</a></li><li><a href="/dsa/leetcode/03_array_list_skiplist/README">数组、链表、跳表</a></li><li><a href="/dsa/leetcode/04_stack_queue/README">栈、队列</a></li><li><a href="/dsa/leetcode/05_hash_map_set/README">哈希表、映射、集合</a></li><li><a href="/dsa/leetcode/06_tree/README">树</a></li><li><a href="/dsa/leetcode/07_recursion/README">递归</a></li><li><a href="/dsa/leetcode/08_divide/README">分治、回溯</a></li><li><a href="/dsa/leetcode/09_dfs_bfs/README">深度优先、广度优先</a></li><li><a href="/dsa/leetcode/10_greedy/README">贪心算法</a></li><li><a href="/dsa/leetcode/11_binary_search/README">二分查找</a></li><li><a href="/dsa/leetcode/12_dp/README">动态规划</a></li><li><a href="/dsa/leetcode/13_trie_disjoint_set/README">字典树和并查集</a></li><li><a href="/dsa/leetcode/14_advanced_search/README">高级搜索</a></li><li><a href="/dsa/leetcode/15_avl_tree/README">红黑树和AVL树</a></li><li><a href="/dsa/leetcode/16_bitwise/README">位运算</a></li><li><a href="/dsa/leetcode/17_bloom_filter_lru/README">布隆过滤器和LRU缓存</a></li><li><a href="/dsa/leetcode/18_sort/README">排序算法</a></li><li><a href="/dsa/leetcode/19_advice_dp/README">高级动态规划</a></li><li><a href="/dsa/leetcode/20_string/README">字符串算法</a></li></ol><h2 id="3-LeetCode专题系列文章"><a href="#3-LeetCode专题系列文章" class="headerlink" title="3.LeetCode专题系列文章"></a>3.LeetCode专题系列文章</h2>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编程语言 (Programming Language)</title>
      <link href="/program_language/pl_idx/"/>
      <url>/program_language/pl_idx/</url>
      
        <content type="html"><![CDATA[<blockquote><p>建立时间复杂度、空间复杂度意识，写出高质量的代码，能够设计基础架构，提升编程技能，训练逻辑思维。</p></blockquote><h2 id="1-C"><a href="#1-C" class="headerlink" title="1.C++"></a>1.C++</h2><h2 id="2-Cython"><a href="#2-Cython" class="headerlink" title="2.Cython"></a>2.Cython</h2><ol><li><a href="/program_language/cython/1.Cython概述">Cython概述</a></li><li><a href="/program_language/cython/2.Cython编译运行">Cython编译运行</a></li><li><a href="/program_language/cython/3.Cython语法介绍">Cython语法介绍</a></li><li><a href="/program_language/cython/4.Cython中扩展类">Cython中扩展类</a></li><li><a href="/program_language/cython/5.Cython模块导入">Cython模块导入</a></li><li><a href="/program_language/cython/6.Cython使用C_C++外部库">Cython使用C/C++外部库</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> PL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01 数据结构与算法总览</title>
      <link href="/dsa/leetcode/01_overview/README/"/>
      <url>/dsa/leetcode/01_overview/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-数据结构"><a href="#1-数据结构" class="headerlink" title="1.数据结构"></a>1.数据结构</h1><p><a href="https://naotu.baidu.com/file/b832f043e2ead159d584cca4efb19703?token=7a6a56eb2630548c" title="百度脑图－便捷的思维工具 (baidu.com)">百度脑图－便捷的思维工具 (baidu.com)</a></p><p>一维</p><ul><li>基础：数组 array (string)，链表 linked list</li><li>高级：栈 stack, 队列 queue，双端队列 deque，集合 set，映射 map (hash or map)，etc</li></ul><p>二维</p><ul><li>基础：树tree，图 graph</li><li>高级：二叉搜索树 binary search tree (red-black tree, AVL)，堆 heap，并查集 disjoint set，字典树 Trie</li></ul><p>特殊</p><ul><li>位运算 Bitwise，布隆过滤器 BloomFilter</li><li>LRU Cache</li></ul><p><img src="image/数据结构_EDEgq_OfBy.png" alt=""></p><h1 id="2-算法"><a href="#2-算法" class="headerlink" title="2.算法"></a>2.算法</h1><p><a href="https://naotu.baidu.com/file/0a53d3a5343bd86375f348b2831d3610?token=5ab1de1c90d5f3ec" title="百度脑图－便捷的思维工具 (baidu.com)">百度脑图－便捷的思维工具 (baidu.com)</a></p><ul><li>if-else, switch → branch</li><li>for, while loop → iteration</li><li>递归 Recursion（Divide &amp; Conquer, Backtrace）</li><li>搜索 Search：DFS， BFS， A*</li><li>动态规划 Dynamic Programming</li><li>二分查找 Binary Search</li><li>贪心 Greedy</li><li>数学 Math， 集合 Geometry</li></ul><p><img src="image/算法脑图_8pig1Yg08P.png" alt=""></p><h1 id="3-如何做题"><a href="#3-如何做题" class="headerlink" title="3.如何做题"></a>3.如何做题</h1><h2 id="3-1-切题四件套"><a href="#3-1-切题四件套" class="headerlink" title="3.1 切题四件套"></a>3.1 切题四件套</h2><ol><li><strong>Clarification</strong>：看清题目</li><li><strong>Possible solutions</strong>：想到所有可能的想法，都过一遍<ol><li>compare (time / space) ：对比事项复杂度和空间复杂度</li><li>optimal</li></ol></li><li><strong>Coding</strong>：写代码</li><li>**Test cases **： 多写写测试样例</li></ol><h2 id="3-2-刷题（五毒神掌）"><a href="#3-2-刷题（五毒神掌）" class="headerlink" title="3.2 刷题（五毒神掌）"></a>3.2 刷题（五毒神掌）</h2><ol><li>第一遍刷题<ol><li>5分钟：读题 + 思考</li><li>直接看解法：注意！多解法，比较解法优劣</li><li>背诵、默写好的解法</li></ol></li><li>第二遍刷题：<ol><li>立即自己写 → LeetCode上提交</li><li>多种解法比较、体会 → 优化！</li></ol></li><li>第三遍刷题<ol><li>过了一天后，再重复做题</li><li>不同解法的熟练程度 → 专项练习</li></ol></li><li>第四遍刷题<ol><li>过了一周：反复回来练习相同题目</li></ol></li><li>第五遍刷题<ol><li>面试前一周，恢复性训练</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>02 复杂度分析</title>
      <link href="/dsa/leetcode/02_complexity_analysis/README/"/>
      <url>/dsa/leetcode/02_complexity_analysis/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-常用工具配置"><a href="#1-常用工具配置" class="headerlink" title="1.常用工具配置"></a>1.常用工具配置</h1><h2 id="1-1-电脑设置"><a href="#1-1-电脑设置" class="headerlink" title="1.1 电脑设置"></a>1.1 电脑设置</h2><ul><li>Google</li><li>Mac：iTerm2 + zsh (or my zsh)</li><li>Windows :&#x20;</li><li><p>VsCode :&#x20;</p><p><a href="https://vscodethemes.com/" title="VS Code Themes">VS Code Themes</a></p><p><a href="https://juejin.cn/post/6844903846871842823" title="炫酷的VS Code毛玻璃效果 - 掘金 (juejin.cn)">炫酷的VS Code毛玻璃效果 - 掘金 (juejin.cn)</a></p></li><li>LeetCode plugin (VsCode)</li></ul><h2 id="1-2-Code-Style"><a href="#1-2-Code-Style" class="headerlink" title="1.2 Code Style"></a>1.2 Code Style</h2><h2 id="1-3-LeetCode"><a href="#1-3-LeetCode" class="headerlink" title="1.3 LeetCode"></a>1.3 LeetCode</h2><ul><li>国内版：刷题联系</li><li>国际版：最高票的题解，</li></ul><h2 id="1-4-指法操作"><a href="#1-4-指法操作" class="headerlink" title="1.4 指法操作"></a>1.4 指法操作</h2><ul><li><code>ctrl+left/right</code>：选择单词</li><li>IDE自动补全</li><li>Top tips for \<IDE-name></li></ul><h2 id="1-5自定向下的编程方式"><a href="#1-5自定向下的编程方式" class="headerlink" title="1.5自定向下的编程方式"></a>1.5自定向下的编程方式</h2><p><a href="https://www.markhneedham.com/blog/2008/09/15/clean-code-book-review/" title="Clean Code: Book Review | Mark Needham (markhneedham.com)">Clean Code: Book Review | Mark Needham (markhneedham.com)</a></p><blockquote><p>The best idea in this book for me was the <strong>newspaper metaphor</strong> that is mentioned with regards to formatting your code. This describes the idea of making code read like a newspaper article. We should be able to get a general idea of how it works near the top of the class before reading more and more details further down. This can be achieved by breaking the code out into lots of small methods. It was strange how involved I got with the newspaper metaphor. Having read about it early on I started looking at all code after that to be in that format and when it wasn’t (when showing examples of not such clean code) I became disappointed.</p></blockquote><p>最开始，思考大的逻辑，而不是细节；以高层次主干逻辑为主</p><p>例题：<a href="https://leetcode.cn/problems/valid-palindrome/description/" title="125. 验证回文串 - 力扣（LeetCode）">125. 验证回文串 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isPalindrome</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 高层次（主干）逻辑</span></span><br><span class="line">        <span class="comment">// 1. filter out number &amp; char  2. reverse and compare</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 过滤</span></span><br><span class="line">        std::string filtered_str = <span class="keyword">this</span>-&gt;<span class="built_in">filter_number_char</span>(s);</span><br><span class="line"></span><br><span class="line">        std::string reverse_str = <span class="keyword">this</span>-&gt;<span class="built_in">reverse_str</span>(filtered_str);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> filtered_str == reverse_str;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function">std::string <span class="title">filter_number_char</span><span class="params">(std::string s)</span> </span>&#123;</span><br><span class="line">        std::string filter_str;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> ch : s) &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">isalnum</span>(ch)) &#123;</span><br><span class="line">                filter_str += <span class="built_in">tolower</span>(ch);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> filter_str;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">std::string <span class="title">reverse_str</span><span class="params">(std::string s)</span> </span>&#123;</span><br><span class="line">        <span class="function">std::string <span class="title">rever_str</span><span class="params">(s.rbegin(), s.rend())</span></span>;</span><br><span class="line">        <span class="keyword">return</span> rever_str;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="2-时间复杂度和空间复杂度"><a href="#2-时间复杂度和空间复杂度" class="headerlink" title="2.时间复杂度和空间复杂度"></a>2.时间复杂度和空间复杂度</h1><p><a href="https://www.zhihu.com/question/21387264" title="如何理解算法时间复杂度的表示法，例如 O(n²)、O(n)、O(1)、O(nlogn) 等？ - 知乎 (zhihu.com)">如何理解算法时间复杂度的表示法，例如 O(n²)、O(n)、O(1)、O(nlogn) 等？ - 知乎 (zhihu.com)</a></p><h2 id="2-1-Big-O-notation"><a href="#2-1-Big-O-notation" class="headerlink" title="2.1 Big O notation"></a>2.1 Big O notation</h2><ul><li>$O(1)$ : Constant Complexity 常数复杂度</li><li>$O(log n)$ : Logarithmic Complexity 对数复杂度</li><li>$O(n)$ : Linear Complexity 线性时间复杂度</li><li>$O(n^2)$ : N square Complexity 平方</li><li>$O(n^3)$ : N square Complexity 立方</li><li>$O(2^n)$ : Exponential Growth 指数</li><li>$O(n!)$ : Factorial 阶乘</li></ul><p>注意 : 只看最高复杂度的运算</p><h4 id="O-1"><a href="#O-1" class="headerlink" title="$O(1)$"></a>$O(1)$</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> n = <span class="number">1000</span>;</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;Hey - you input is: &quot;</span> + n &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> n = <span class="number">1000</span>;</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;Hey - you input is: &quot;</span> + n &lt;&lt; std::endl;</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;Hmm... I&#x27;m doing more stuff with: &quot;</span> + n &lt;&lt; std::endl;</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;And more: &quot;</span> + n &lt;&lt; std::endl;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="O-n"><a href="#O-n" class="headerlink" title="$O(n)$"></a>$O(n)$</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;Hey - I&#x27;m busy looking at: &quot;</span> + i &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="O-n-2"><a href="#O-n-2" class="headerlink" title="$O(n^2)$"></a>$O(n^2)$</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n; j++) &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Hey - I&#x27;m busy looking at: &quot;</span> + i + <span class="string">&quot;and &quot;</span> + j &lt;&lt; std::endl;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="O-log-n"><a href="#O-log-n" class="headerlink" title="$O(log ~ n)$"></a>$O(log ~ n)$</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; n; i = i * <span class="number">2</span>) &#123;</span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;Hey - I&#x27;m busy looking at: &quot;</span> + i &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="O-k-n"><a href="#O-k-n" class="headerlink" title="$O(k^n)$"></a>$O(k^n)$</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (n &gt; <span class="number">2</span>) <span class="keyword">return</span> n;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">fib</span>(n - <span class="number">1</span>) + <span class="built_in">fib</span>(n - <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-2-时间复杂度曲线"><a href="#2-2-时间复杂度曲线" class="headerlink" title="2.2 时间复杂度曲线"></a>2.2 时间复杂度曲线</h2><p><img src="image/image_uPqP1tcFmX.png" alt=""></p><ul><li>对自己写的程序的时间复杂度和空间复杂度有所了解；</li><li>用最简洁的时间和空间复杂度完成是顶尖选手的职业素养。</li></ul><h2 id="2-2-计算-1-2-3-…-n"><a href="#2-2-计算-1-2-3-…-n" class="headerlink" title="2.2 计算 1+2+3+… + n"></a>2.2 计算 1+2+3+… + n</h2><h4 id="方法一：从1到n循环累加-O-n"><a href="#方法一：从1到n循环累加-O-n" class="headerlink" title="方法一：从1到n循环累加 O(n)"></a>方法一：从1到n循环累加 <code>O(n)</code></h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to n:</span><br><span class="line">    y += i</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="方法二：求和公式-sum-n-n-1-2-O-1"><a href="#方法二：求和公式-sum-n-n-1-2-O-1" class="headerlink" title="方法二：求和公式 $sum=n(n+1)/2$ O(1)"></a>方法二：求和公式 $sum=n(n+1)/2$ <code>O(1)</code></h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = n * (n + <span class="number">1</span>) / <span class="number">2</span></span><br></pre></td></tr></table></figure><h2 id="2-3-递归"><a href="#2-3-递归" class="headerlink" title="2.3 递归"></a>2.3 递归</h2><p>画出递归树</p><p>题目：Fib : 0, 1, 1, 2, 3, 5, 8, 13, 21, …, $F(n)=F(n-1) + F(n-2)$</p><p>直接用递归，时间复杂度$O(2^n)$</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (n &lt;= <span class="number">2</span>) <span class="keyword">return</span> n;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">fib</span>(n - <span class="number">1</span>) + <span class="built_in">fib</span>(n - <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="image/image_qRKmZve2_6.png" alt=""></p><h3 id="主定理"><a href="#主定理" class="headerlink" title="主定理"></a>主定理</h3><p><a href="https://en.wikipedia.org/wiki/Master_theorem_\(analysis_of_algorithms\">Master theorem (analysis of algorithms) - Wikipedia</a> “Master theorem (analysis of algorithms) - Wikipedia”)</p><p><a href="https://zh.wikipedia.org/wiki/主定理" title="主定理 - 维基百科，自由的百科全书 (wikipedia.org)">主定理 - 维基百科，自由的百科全书 (wikipedia.org)</a></p><p>常用算法中的应用：</p><p><img src="image/image_Rp5mP3dErq.png" alt=""></p><ol><li>二分查找：有序数列中，查找目标树；时间复杂度 $O(log n)$</li><li>二叉树遍历：时间复杂度 $O(n)$，每次一分为二，以相等的时间复杂度下去；二叉树遍历，每个节点仅访问一次，所以时间复杂度为$O(n)$</li><li>排序后的二维矩阵中查找：$O(n)$</li><li>归并排序：$O(nlog~n)$</li></ol><h2 id="2-4-思考题"><a href="#2-4-思考题" class="headerlink" title="2.4 思考题"></a>2.4 思考题</h2><p>1、二叉树遍历：前序、中序、后序遍历的时间复杂度？</p><blockquote><p>每个节点只访问一次，线性正比于节点数，所以为$O(n)$</p></blockquote><p>2、图的遍历：时间复杂度？</p><blockquote><p>每个节点只访问一次，线性正比于节点数，所以为$O(n)$</p></blockquote><p>3、搜索算法：DFS、BFS时间复杂度？</p><blockquote><p>每个节点只访问一次，线性正比于节点数，所以为$O(n)$</p></blockquote><p>4、二分查找：时间复杂度？</p><blockquote><p>时间复杂度 $O(log n)$</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>03 数组、链表、跳表</title>
      <link href="/dsa/leetcode/03_array_list_skiplist/README/"/>
      <url>/dsa/leetcode/03_array_list_skiplist/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-基本实现和特性"><a href="#1-基本实现和特性" class="headerlink" title="1.基本实现和特性"></a>1.基本实现和特性</h1><h2 id="1-1-数组"><a href="#1-1-数组" class="headerlink" title="1.1 数组"></a>1.1 数组</h2><ul><li><a href="http://developer.classpath.org/doc/java/util/ArrayList-source.html" title="Java 源码分析（ArrayList）">Java 源码分析（ArrayList）</a></li></ul><p>内存中，一段连续的地址，可以通过内存管理器直接访问，时间复杂度为$O(1)$，访问时间比较快；</p><p>增加删除元素比较麻烦，时间复杂度为 $O(n)$</p><h2 id="1-2-链表"><a href="#1-2-链表" class="headerlink" title="1.2 链表"></a>1.2 链表</h2><ul><li><a href="http://www.geeksforgeeks.org/implementing-a-linked-list-in-java-using-class/" title="Linked List 的标准实现代码">Linked List 的标准实现代码</a></li><li><a href="http://www.cs.cmu.edu/~adamchik/15-121/lectures/Linked Lists/code/LinkedList.java" title="Linked List 示例代码">Linked List 示例代码</a></li><li><a href="http://developer.classpath.org/doc/java/util/LinkedList-source.html" title="Java 源码分析（LinkedList）">Java 源码分析（LinkedList）</a></li><li>LRU Cache - Linked list： <a href="http://leetcode-cn.com/problems/lru-cache" title="LRU 缓存机制">LRU 缓存机制</a></li></ul><p><img src="image/image_O59ESUlR11.png" alt=""></p><ul><li>增加删除结点时间复杂度：$O(1)$</li><li>访问结点时间复杂度：$O(n)$</li></ul><h2 id="1-3-跳表-SkipList"><a href="#1-3-跳表-SkipList" class="headerlink" title="1.3 跳表 SkipList"></a>1.3 跳表 SkipList</h2><ul><li>Redis - Skip List：<a href="http://redisbook.readthedocs.io/en/latest/internal-datastruct/skiplist.html" title="跳跃表">跳跃表</a></li><li><a href="http://www.zhihu.com/question/20202931" title="为啥 Redis 使用跳表（Skip List）而不是使用 Red-Black？">为啥 Redis 使用跳表（Skip List）而不是使用 Red-Black？</a></li></ul><p>主要在Redis中使用</p><p>链表的缺陷：访问时间复杂度比较高 $O(n)$</p><p>给链表进行加速中心思想：<strong>升维（空间换时间）</strong></p><p>跳表：<strong>索引</strong>，增加索引，链表next速度为1，一级索引速度为2，二级索引速度为4，</p><p><img src="image/image_Eoc0JMAh5v.png" alt=""></p><p>实际使用中，可以增加多级索引，实际增加 $log~2n$级索引</p><p><img src="image/image_cU-88Rgcbi.png" alt=""></p><h4 id="跳表查询的时间复杂度-O-logn"><a href="#跳表查询的时间复杂度-O-logn" class="headerlink" title="跳表查询的时间复杂度 $O(logn)$"></a>跳表查询的时间复杂度 $O(logn)$</h4><p>n/2, n/4, n/8, 第k级索引结点的个数就是 $n/(2^k)$</p><p>假设索引有h级，最高级的索引有2个结点。$n(2^h)=2$，从而求得$h=log2(n)-1$</p><p><img src="image/image_9gpkqo0fLA.png" alt=""></p><h4 id="现实中跳表的形态"><a href="#现实中跳表的形态" class="headerlink" title="现实中跳表的形态"></a>现实中跳表的形态</h4><ul><li>维护成本比较高，</li><li>增加和删除的时间复杂度 $O(logn)$</li></ul><p><img src="image/image_lYZcchH4YB.png" alt=""></p><h4 id="跳表空间复杂度-O-n"><a href="#跳表空间复杂度-O-n" class="headerlink" title="跳表空间复杂度$O(n)$"></a>跳表空间复杂度$O(n)$</h4><h1 id="2-例题"><a href="#2-例题" class="headerlink" title="2.例题"></a>2.例题</h1><h2 id="2-1零移动"><a href="#2-1零移动" class="headerlink" title="2.1零移动"></a>2.1零移动</h2><p><a href="https://leetcode.cn/problems/move-zeroes/description/" title="https://leetcode.cn/problems/move-zeroes/description/">https://leetcode.cn/problems/move-zeroes/description/</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给定一个数组 nums，编写一个函数将所有 0 移动到数组的末尾，同时保持非零元素的相对顺序。</span><br><span class="line"></span><br><span class="line">请注意 ，必须在不复制数组的情况下原地对数组进行操作。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 用一个指针，记录0元素的位置，每次循环到非零元素，</span></span><br><span class="line">    <span class="comment">// 将非零元素放置到记录0元素的位置，并移动记录0元素位置指针</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">moveZeroes</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// j 记录每次循环中的非零元素下标</span></span><br><span class="line">        <span class="type">int</span> j = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nums.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[i] != <span class="number">0</span>) &#123;</span><br><span class="line">                nums[j] = nums[i];</span><br><span class="line">                <span class="comment">// 处理最后一个数</span></span><br><span class="line">                <span class="keyword">if</span> (i != j) &#123;</span><br><span class="line">                    nums[i] = <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                j++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">moveZeroes_swap</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// j 记录每次循环中的非零元素下标</span></span><br><span class="line">        <span class="type">int</span> j = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nums.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[i] != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="type">int</span> tmp = nums[j];</span><br><span class="line">                nums[j] = nums[i];</span><br><span class="line">                nums[i] = tmp;</span><br><span class="line">                j++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-2-盛水最多的容器"><a href="#2-2-盛水最多的容器" class="headerlink" title="2.2 盛水最多的容器"></a>2.2 盛水最多的容器</h2><p><a href="https://leetcode.cn/problems/container-with-most-water/description/" title="11. 盛最多水的容器 - 力扣（LeetCode）">11. 盛最多水的容器 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。</span><br><span class="line"></span><br><span class="line">找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。</span><br><span class="line"></span><br><span class="line">返回容器可以储存的最大水量。</span><br><span class="line"></span><br><span class="line">说明：你不能倾斜容器。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1.枚举： left bar x, right bar y, (x-y)*height_diff</span></span><br><span class="line">    <span class="comment">// 超出时间限制</span></span><br><span class="line">    <span class="comment">// O(n^2)</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">maxArea1</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; height)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> max_water = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; height.<span class="built_in">size</span>() - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = i + <span class="number">1</span>; j &lt; height.<span class="built_in">size</span>(); j++) &#123;</span><br><span class="line">                <span class="type">int</span> tmp = std::<span class="built_in">min</span>(height[i], height[j]) * (j - i);</span><br><span class="line">                max_water = std::<span class="built_in">max</span>(max_water, tmp);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_water;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.双指针</span></span><br><span class="line">    <span class="comment">// left和right两个指针，那么bar的高度小，那个往里面移动</span></span><br><span class="line">    <span class="comment">// O(n)</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">maxArea2</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; height)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = height.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> max_water = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right) &#123;</span><br><span class="line">            <span class="type">int</span> tmp = std::<span class="built_in">min</span>(height[left], height[right]) * (right - left);</span><br><span class="line">            max_water = tmp &gt; max_water ? tmp : max_water;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (height[left] &lt; height[right]) &#123;</span><br><span class="line">                left++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                right--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_water;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 另一种双指针简单的写法</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">maxArea</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; height)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> max_water = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>, j = height.<span class="built_in">size</span>() - <span class="number">1</span>; i &lt; j; ) &#123;</span><br><span class="line">            <span class="type">int</span> min_height = height[i] &lt; height[j] ? height[i ++] : height[j --];</span><br><span class="line">            max_water = std::<span class="built_in">max</span>(max_water, (j - i + <span class="number">1</span>) * min_height);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_water;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-3-爬楼梯"><a href="#2-3-爬楼梯" class="headerlink" title="2.3 爬楼梯"></a>2.3 爬楼梯</h2><p><a href="https://leetcode.cn/problems/climbing-stairs/description/?utm_source=LCUS\&amp;utm_medium=ip_redirect\&amp;utm_campaign=transfer2china" title="70. 爬楼梯 - 力扣（LeetCode）">70. 爬楼梯 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">假设你正在爬楼梯。需要 n 阶你才能到达楼顶。</span><br><span class="line"></span><br><span class="line">每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 1 : 1</span></span><br><span class="line"><span class="comment">// 2 : 2</span></span><br><span class="line"><span class="comment">// 3 : f(1) + f(2)</span></span><br><span class="line"><span class="comment">// 4 : f(2) + f(3)</span></span><br><span class="line"><span class="comment">// n : f(n) = f(n-1) + f(n-2)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">climbStairs</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (n &lt;= <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> n;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> f1 = <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> f2 = <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> f3 = <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">3</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            f3 = f1 + f2;</span><br><span class="line">            f1 = f2;</span><br><span class="line">            f2 = f3;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> f3;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-4-三数之和"><a href="#2-4-三数之和" class="headerlink" title="2.4 三数之和"></a>2.4 三数之和</h2><p><a href="https://leetcode.cn/problems/3sum/description/" title="15. 三数之和 - 力扣（LeetCode）">15. 三数之和 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数数组 nums ，判断是否存在三元组 [nums[i], nums[j], nums[k]] 满足 i != j、i != k 且 j != k ，同时还满足 nums[i] + nums[j] + nums[k] == 0 。请</span><br><span class="line"></span><br><span class="line">你返回所有和为 0 且不重复的三元组。</span><br><span class="line"></span><br><span class="line">注意：答案中不可以包含重复的三元组。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.暴力求解，需要三重循环 O(n^3)</span></span><br><span class="line"><span class="comment">// 2.hash表来记录， a+b到hash表中去查，是否存在-c</span></span><br><span class="line"><span class="comment">// 3.左右下标推进</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1.暴力求解方法</span></span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">threeSum1</span>(vector&lt;<span class="type">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; ans;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nums.<span class="built_in">size</span>() - <span class="number">2</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = i + <span class="number">1</span>; j &lt; nums.<span class="built_in">size</span>() - <span class="number">1</span>; j++) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> k = j + <span class="number">1</span>; k &lt; nums.<span class="built_in">size</span>(); k++) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (nums[i] + nums[j] + nums[k] == <span class="number">0</span>) &#123;</span><br><span class="line">                        ans.<span class="built_in">push_back</span>(&#123;nums[i], nums[j], nums[k]&#125;);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.左右下标推进</span></span><br><span class="line">    <span class="comment">// 固定3个指针中最左数字的指针k，双指针i，j分设在数据两端</span></span><br><span class="line">    <span class="comment">// 通过双指针交替向中间移动，记录对于每个固定指针k所有满足</span></span><br><span class="line">    <span class="comment">// nums[k] + nums[i] + nums[j] == 0 的 i, j 组合</span></span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">threeSum</span>(vector&lt;<span class="type">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        <span class="type">int</span> size = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (size &lt; <span class="number">3</span>)</span><br><span class="line">            <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; res;</span><br><span class="line">        <span class="comment">// 排序</span></span><br><span class="line">        std::<span class="built_in">sort</span>(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>());</span><br><span class="line">        <span class="comment">// 固定第一个数，转化为求两数之和</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果第一个数为正数，因为是递增的，后面你的数不可能为0了</span></span><br><span class="line">            <span class="keyword">if</span> (nums[i] &gt; <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">return</span> res;</span><br><span class="line">            <span class="comment">// 去重，如果被选过了，跳过</span></span><br><span class="line">            <span class="keyword">if</span> (i &gt; <span class="number">0</span> &amp;&amp; nums[i] == nums[i<span class="number">-1</span>])</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="comment">// 双指针在nums[i]后面的区间中寻找和为0-nums[i]的另外两个数</span></span><br><span class="line">            <span class="type">int</span> left = i + <span class="number">1</span>;</span><br><span class="line">            <span class="type">int</span> right = size - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">while</span>(left &lt; right)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 两数之和太大，右指针左移</span></span><br><span class="line">                <span class="keyword">if</span> (nums[left] + nums[right] &gt; -nums[i])</span><br><span class="line">                    right--;</span><br><span class="line">                <span class="comment">// 两数之和太小，左指针右移</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(nums[left] + nums[right] &lt; -nums[i])</span><br><span class="line">                    left++;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 找到一个和为零的三元组，添加到结果中，左右指针内缩，继续寻找</span></span><br><span class="line">                    res.<span class="built_in">push_back</span>(std::vector&lt;<span class="type">int</span>&gt;&#123;nums[i], nums[left], nums[right]&#125;);</span><br><span class="line">                    left++;</span><br><span class="line">                    right--;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 去重：第二个数和第三个数也不重复选取</span></span><br><span class="line">                    <span class="comment">// 例如：[-4,1,1,1,2,3,3,3], i=0, left=1, right=5</span></span><br><span class="line">                    <span class="keyword">while</span> (left &lt; right &amp;&amp; nums[left] == nums[left<span class="number">-1</span>])  left++;</span><br><span class="line">                    <span class="keyword">while</span> (left &lt; right &amp;&amp; nums[right] == nums[right+<span class="number">1</span>])    right--;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>04 栈、队列</title>
      <link href="/dsa/leetcode/04_stack_queue/README/"/>
      <url>/dsa/leetcode/04_stack_queue/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-栈和队列的基本实现和特性"><a href="#1-栈和队列的基本实现和特性" class="headerlink" title="1.栈和队列的基本实现和特性"></a>1.栈和队列的基本实现和特性</h1><h2 id="1-1-基本性质"><a href="#1-1-基本性质" class="headerlink" title="1.1 基本性质"></a>1.1 基本性质</h2><p>Stack：<strong>先入后出</strong>；添加、删除为$O(1)$，查询为$O(n)$</p><p>Queue ：先入先出；添加、查询为$O(1)$</p><p>栈</p><p><img src="image/image_9Q9ELxv3ZN.png" alt=""></p><p>队列</p><p><img src="image/image_5UOoGSf7Da.png" alt=""></p><h2 id="1-2-双端队列-x20"><a href="#1-2-双端队列-x20" class="headerlink" title="1.2 双端队列&#x20;"></a>1.2 双端队列&#x20;</h2><p>插入和删除是 $O(1)$，查询为 $O(n)$</p><p><img src="image/image_TBLR5D7gbu.png" alt=""></p><h2 id="1-3-工程实现"><a href="#1-3-工程实现" class="headerlink" title="1.3 工程实现"></a>1.3 工程实现</h2><ul><li><a href="http://developer.classpath.org/doc/java/util/Stack-source.html" title="Java 的 Stack 源码">Java 的 Stack 源码</a></li><li><a href="http://fuseyism.com/classpath/doc/java/util/Queue-source.html" title="Java 的 Queue 源码">Java 的 Queue 源码</a></li><li><a href="http://docs.python.org/2/library/heapq.html" title="Python 的 heapq">Python 的 heapq</a></li><li><a href="http://docs.python.org/2/library/collections.html" title="高性能的 container 库">高性能的 container 库</a></li></ul><h2 id="1-4-优先队列"><a href="#1-4-优先队列" class="headerlink" title="1.4 优先队列"></a>1.4 优先队列</h2><p><a href="http://docs.oracle.com/javase/10/docs/api/java/util/PriorityQueue.html" title="Java 的 PriorityQueue 文档">Java 的 PriorityQueue 文档</a></p><ul><li>插入操作： $O(1)$</li><li>取出操作： $O(log~n)$，按照元素的优先级取出</li><li>底层具体实现的数据结构较为多样和复杂：heap、bst、treap</li></ul><h2 id="1-5-复杂度分析"><a href="#1-5-复杂度分析" class="headerlink" title="1.5 复杂度分析"></a>1.5 复杂度分析</h2><p><a href="https://www.bigocheatsheet.com/" title="Big-O Algorithm Complexity Cheat Sheet (Know Thy Complexities!) @ericdrowell (bigocheatsheet.com)">Big-O Algorithm Complexity Cheat Sheet (Know Thy Complexities!) @ericdrowell (bigocheatsheet.com)</a></p><p><img src="image/image_Rp-74X_Zev.png" alt=""></p><h1 id="2-题目"><a href="#2-题目" class="headerlink" title="2.题目"></a>2.题目</h1><h2 id="2-1-有效括号"><a href="#2-1-有效括号" class="headerlink" title="2.1 有效括号"></a>2.1 有效括号</h2><p><a href="https://leetcode.cn/problems/valid-parentheses/description/" title="20. 有效的括号 - 力扣（LeetCode）">20. 有效的括号 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">给定一个只包括 <span class="string">&#x27;(&#x27;</span>，<span class="string">&#x27;)&#x27;</span>，<span class="string">&#x27;&#123;&#x27;</span>，<span class="string">&#x27;&#125;&#x27;</span>，<span class="string">&#x27;[&#x27;</span>，<span class="string">&#x27;]&#x27;</span> 的字符串 s ，判断字符串是否有效。</span><br><span class="line"></span><br><span class="line">有效字符串需满足：</span><br><span class="line">- 左括号必须用相同类型的右括号闭合。</span><br><span class="line">- 左括号必须以正确的顺序闭合。</span><br><span class="line">- 每个右括号都有一个对应的相同类型的左括号。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.暴力求解 ： 不断replace匹配的括号 -&gt; “ ”，直到所有字符串替换为空</span></span><br><span class="line"><span class="comment">//   a.()[]&#123;&#125;</span></span><br><span class="line"><span class="comment">//   b.(((&#123;[]&#125;)))</span></span><br><span class="line"><span class="comment">//   c. O(n^2)</span></span><br><span class="line"><span class="comment">// 2.栈，左括号压入栈，右括号和栈顶匹配</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isValid</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s.<span class="built_in">size</span>() % <span class="number">2</span> == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::stack&lt;<span class="type">char</span>&gt; stack;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> ch : s) &#123;</span><br><span class="line">            <span class="keyword">if</span> (ch == <span class="string">&#x27;(&#x27;</span> || ch == <span class="string">&#x27;[&#x27;</span> || ch == <span class="string">&#x27;&#123;&#x27;</span>) &#123;</span><br><span class="line">                stack.<span class="built_in">push</span>(ch);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ch == <span class="string">&#x27;)&#x27;</span> || ch == <span class="string">&#x27;]&#x27;</span> || ch == <span class="string">&#x27;&#125;&#x27;</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (!stack.<span class="built_in">empty</span>() &amp;&amp; <span class="keyword">this</span>-&gt;<span class="built_in">judge_vaild</span>(stack.<span class="built_in">top</span>(), ch)) &#123;</span><br><span class="line">                    stack.<span class="built_in">pop</span>();</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> stack.<span class="built_in">empty</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">judge_vaild</span><span class="params">(<span class="type">char</span> s, <span class="type">char</span> d)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> ((s == <span class="string">&#x27;(&#x27;</span> &amp;&amp; d == <span class="string">&#x27;)&#x27;</span>)</span><br><span class="line">            ||  (s == <span class="string">&#x27;[&#x27;</span> &amp;&amp; d == <span class="string">&#x27;]&#x27;</span>)</span><br><span class="line">            ||  (s == <span class="string">&#x27;&#123;&#x27;</span> &amp;&amp; d == <span class="string">&#x27;&#125;&#x27;</span>)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-2-最小栈"><a href="#2-2-最小栈" class="headerlink" title="2.2 最小栈"></a>2.2 最小栈</h2><p><a href="https://leetcode.cn/problems/min-stack/" title="155. 最小栈 - 力扣（LeetCode）">155. 最小栈 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">设计一个支持 push ，pop ，top 操作，并能在常数时间内检索到最小元素的栈。</span><br><span class="line"></span><br><span class="line">实现 MinStack 类:</span><br><span class="line"></span><br><span class="line">MinStack() 初始化堆栈对象。</span><br><span class="line">void push(int val) 将元素val推入堆栈。</span><br><span class="line">void pop() 删除堆栈顶部的元素。</span><br><span class="line">int top() 获取堆栈顶部的元素。</span><br><span class="line">int getMin() 获取堆栈中的最小元素。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MinStack</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">MinStack</span>() &#123;</span><br><span class="line">        m_min_stack.<span class="built_in">push</span>(m_min);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">push</span><span class="params">(<span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        m_stack.<span class="built_in">push</span>(val);</span><br><span class="line">        <span class="keyword">if</span> (m_stack.<span class="built_in">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">            m_min = val;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            m_min = std::<span class="built_in">min</span>(m_min, val);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        m_min_stack.<span class="built_in">push</span>(m_min);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        m_stack.<span class="built_in">pop</span>();</span><br><span class="line">        m_min_stack.<span class="built_in">pop</span>();</span><br><span class="line">        m_min = m_min_stack.<span class="built_in">top</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">top</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> m_stack.<span class="built_in">top</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">getMin</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> m_min_stack.<span class="built_in">top</span>();</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    std::stack&lt;<span class="type">int</span>&gt; m_stack;</span><br><span class="line">    std::stack&lt;<span class="type">int</span>&gt; m_min_stack;</span><br><span class="line">    <span class="type">int</span> m_min = INT_MAX;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-3-柱状图"><a href="#2-3-柱状图" class="headerlink" title="2.3 柱状图"></a>2.3 柱状图</h2><p><a href="https://leetcode.cn/problems/largest-rectangle-in-histogram/description/" title="84. 柱状图中最大的矩形 - 力扣（LeetCode）">84. 柱状图中最大的矩形 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">给定 n 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。</span><br><span class="line"></span><br><span class="line">求在该柱状图中，能够勾勒出来的矩形的最大面积。</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1.固定高度，枚举宽度， 超时</span></span><br><span class="line">    <span class="comment">// 固定每个数组元素为每个矩形的高，然后遍历数组，寻找每个矩形高能构成的最大面积，</span></span><br><span class="line">    <span class="comment">// 当左右两边第一次出现比当前高小的元素值，即为当前高能构成的最大值，每次保存最大值</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">largestRectangleArea1</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; heights)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> max_area = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 遍历高度</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> mid = <span class="number">0</span>; mid &lt; heights.<span class="built_in">size</span>(); mid++) &#123;</span><br><span class="line">            <span class="type">int</span> h = heights[mid];</span><br><span class="line">            <span class="type">int</span> left = mid;</span><br><span class="line">            <span class="type">int</span> right = mid;</span><br><span class="line">            <span class="comment">// 左侧寻找最大宽度 </span></span><br><span class="line">            <span class="keyword">while</span> (left - <span class="number">1</span> &gt;= <span class="number">0</span> &amp;&amp; heights[left - <span class="number">1</span>] &gt;= h) &#123;</span><br><span class="line">                left--;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 右侧寻找最大宽度</span></span><br><span class="line">            <span class="keyword">while</span> (right + <span class="number">1</span> &lt; heights.<span class="built_in">size</span>() &amp;&amp; heights[right + <span class="number">1</span>] &gt;= h) &#123;</span><br><span class="line">                right++;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            max_area = std::<span class="built_in">max</span>(max_area, h * (right - left + <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> max_area;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.固定宽度，枚举高度, 超时</span></span><br><span class="line">    <span class="comment">// 固定左右两边的长度即固定宽的长度，然后遍历数组，寻找当前长度中高最短的元素，</span></span><br><span class="line">    <span class="comment">// 即当前宽能构成的最大矩形，每次保存最大值</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">largestRectangleArea2</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; heights)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> max_area = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> n = heights.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> heights[<span class="number">0</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> left = <span class="number">0</span>; left &lt; n; left++) &#123;</span><br><span class="line">            <span class="type">int</span> min_height = heights[left];</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> right = left; right &lt; n; right++) &#123;</span><br><span class="line">                min_height = std::<span class="built_in">min</span>(min_height, heights[right]);</span><br><span class="line">                max_area = std::<span class="built_in">max</span>(max_area, min_height * (right - left + <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_area;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.单调栈</span></span><br><span class="line">    <span class="comment">// 在枚举宽的同时需要寻找高，在枚举高的时候又要寻找宽，时间消耗非常大</span></span><br><span class="line">    <span class="comment">// 那么可以利用递增栈优化暴力暴力求解的过程</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 当元素大于栈顶元素时，入栈</span></span><br><span class="line">    <span class="comment">// 当元素小于栈顶元素时，维护栈的递增性，将小于当前元素的栈顶元素弹出，并计算面积</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">largestRectangleArea</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; heights)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = heights.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> heights[<span class="number">0</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="type">int</span> max_area = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        std::stack&lt;<span class="type">int</span>&gt; stack;</span><br><span class="line">        <span class="comment">// 遍历数组</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="keyword">while</span> (!stack.<span class="built_in">empty</span>() &amp;&amp; heights[stack.<span class="built_in">top</span>()] &gt;= heights[i]) &#123;</span><br><span class="line">                <span class="comment">// 出栈，并计算面积，维护递增性，需要对小于的元素全部出栈</span></span><br><span class="line">                <span class="type">int</span> length = heights[stack.<span class="built_in">top</span>()];</span><br><span class="line">                stack.<span class="built_in">pop</span>();</span><br><span class="line"></span><br><span class="line">                <span class="type">int</span> weight = i;</span><br><span class="line">                <span class="comment">// 最后一个栈顶元素，出栈计算面积需要包含一下前面和后面，</span></span><br><span class="line">                <span class="comment">// 因为矩形可以延伸，这里需要好好想一想</span></span><br><span class="line">                <span class="keyword">if</span> (!stack.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">                    weight = i - stack.<span class="built_in">top</span>() - <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                max_area = std::<span class="built_in">max</span>(max_area, length * weight);</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 入栈</span></span><br><span class="line">            stack.<span class="built_in">push</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 数组元素全部遍历完了，但是栈还有元素，进行清空栈</span></span><br><span class="line">        <span class="keyword">while</span> (!stack.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            <span class="type">int</span> length = heights[stack.<span class="built_in">top</span>()];</span><br><span class="line">            stack.<span class="built_in">pop</span>();</span><br><span class="line">            <span class="type">int</span> weight = n;</span><br><span class="line">            <span class="keyword">if</span> (!stack.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">                weight = n - stack.<span class="built_in">top</span>() - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            max_area = std::<span class="built_in">max</span>(max_area, length * weight);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_area;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-4-滑动窗口的最大值"><a href="#2-4-滑动窗口的最大值" class="headerlink" title="2.4 滑动窗口的最大值"></a>2.4 滑动窗口的最大值</h2><p><a href="https://leetcode.cn/problems/sliding-window-maximum/description/" title="239. 滑动窗口最大值 - 力扣（LeetCode）">239. 滑动窗口最大值 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。</span><br><span class="line"></span><br><span class="line">返回 滑动窗口中的最大值 。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">maxSlidingWindow</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">    std::deque&lt;<span class="type">int</span>&gt; que;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将前k个元素的下标加入队列中，其中队列头为最大值，队列尾为最小值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;k; i++) &#123;</span><br><span class="line">        <span class="comment">// 将小于队列低的元素加在后面</span></span><br><span class="line">        <span class="keyword">while</span> (!que.<span class="built_in">empty</span>() &amp;&amp; nums[i] &gt;= nums[que.<span class="built_in">back</span>()]) &#123;</span><br><span class="line">            que.<span class="built_in">pop_back</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        que.<span class="built_in">push_back</span>(i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将前k个元素的最大值加进去</span></span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; ans = &#123;nums[que.<span class="built_in">front</span>()]&#125;;</span><br><span class="line">    <span class="comment">// 开始遍历</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = k; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">while</span> (!que.<span class="built_in">empty</span>() &amp;&amp; nums[i] &gt;= nums[que.<span class="built_in">back</span>()]) &#123;</span><br><span class="line">            que.<span class="built_in">pop_back</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        que.<span class="built_in">push_back</span>(i);</span><br><span class="line">        <span class="comment">//</span></span><br><span class="line">        <span class="keyword">while</span> (que.<span class="built_in">front</span>() &lt;= i - k) &#123;</span><br><span class="line">            que.<span class="built_in">pop_front</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        ans.<span class="built_in">push_back</span>(nums[que.<span class="built_in">front</span>()]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ans;        </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>05 哈希表、映射、集合</title>
      <link href="/dsa/leetcode/05_hash_map_set/README/"/>
      <url>/dsa/leetcode/05_hash_map_set/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-基本特性"><a href="#1-基本特性" class="headerlink" title="1.基本特性"></a>1.基本特性</h1><h2 id="1-1-Hash-Table"><a href="#1-1-Hash-Table" class="headerlink" title="1.1 Hash Table"></a>1.1 Hash Table</h2><p>哈希表（Hash table），也叫散列表，是根据关键码值（Key value）而直接进行访问的数据结构。它通过把关键码值映射到一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数（Hash Function），存放记录的数组叫做哈希表（或散列表）。</p><h4 id="hash函数"><a href="#hash函数" class="headerlink" title="hash函数"></a>hash函数</h4><p><img src="image/image_1SgVjbhNJ_.png" alt=""></p><h4 id="hash冲突"><a href="#hash冲突" class="headerlink" title="hash冲突"></a>hash冲突</h4><p><img src="image/image_evMP8Kzdro.png" alt=""></p><h1 id="2-示例"><a href="#2-示例" class="headerlink" title="2.示例"></a>2.示例</h1><h2 id="2-1-有效字母异位词"><a href="#2-1-有效字母异位词" class="headerlink" title="2.1 有效字母异位词"></a>2.1 有效字母异位词</h2><p><a href="https://leetcode.cn/problems/valid-anagram/solutions/" title="242. 有效的字母异位词 - 力扣（LeetCode）">242. 有效的字母异位词 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。</span><br><span class="line"></span><br><span class="line">注意：若 s 和 t 中每个字符出现的次数都相同，则称 s 和 t 互为字母异位词。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.暴力 sort， sorted_str是否相等？ O(NlogN)</span></span><br><span class="line"><span class="comment">// 2.hash, map --&gt; 统计每个字符的频次</span></span><br><span class="line"><span class="comment">//  第一个字符串，遇到一个字符加一，第二个字符串，遇到字符减一</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1.暴力，排序</span></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isAnagram1</span><span class="params">(string s, string t)</span> </span>&#123;</span><br><span class="line">        std::<span class="built_in">sort</span>(s.<span class="built_in">begin</span>(), s.<span class="built_in">end</span>());</span><br><span class="line">        std::<span class="built_in">sort</span>(t.<span class="built_in">begin</span>(), t.<span class="built_in">end</span>());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> s == t;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.hash </span></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isAnagram</span><span class="params">(string s, string t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s.<span class="built_in">size</span>() != t.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">counter</span><span class="params">(<span class="number">26</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; s.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            counter[s[i] - <span class="string">&#x27;a&#x27;</span>]++;</span><br><span class="line">            counter[t[i] - <span class="string">&#x27;a&#x27;</span>]--;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> count : counter) &#123;</span><br><span class="line">            <span class="keyword">if</span> (count != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-2-字母异位词分组"><a href="#2-2-字母异位词分组" class="headerlink" title="2.2 字母异位词分组"></a>2.2 字母异位词分组</h2><p><a href="https://leetcode.cn/problems/group-anagrams/description/" title="49. 字母异位词分组 - 力扣（LeetCode）">49. 字母异位词分组 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给你一个字符串数组，请你将 字母异位词 组合在一起。可以按任意顺序返回结果列表。</span><br><span class="line"></span><br><span class="line">字母异位词 是由重新排列源单词的所有字母得到的一个新单词。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.排序，匹配</span></span><br><span class="line"><span class="comment">// 2.hash 计数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1.排序</span></span><br><span class="line">    <span class="comment">// 对两个字符串，排序之后得到的字符串一定是相同的，可以将排序后的字符串作为哈希表的键</span></span><br><span class="line">    vector&lt;vector&lt;string&gt;&gt; <span class="built_in">groupAnagrams</span>(vector&lt;string&gt;&amp; strs) &#123;</span><br><span class="line">        std::unordered_map&lt;std::string, std::vector&lt;std::string&gt;&gt; map;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; str : strs) &#123;</span><br><span class="line">            std::string key = str;</span><br><span class="line">            std::<span class="built_in">sort</span>(key.<span class="built_in">begin</span>(), key.<span class="built_in">end</span>());</span><br><span class="line">            map[key].<span class="built_in">emplace_back</span>(str);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        std::vector&lt;std::vector&lt;std::string&gt;&gt; ans;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> it = map.<span class="built_in">begin</span>(); it != map.<span class="built_in">end</span>(); it++) &#123;</span><br><span class="line">            ans.<span class="built_in">emplace_back</span>(it-&gt;second);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.hash 计数</span></span><br><span class="line">    <span class="comment">// 两个字符串中相同字母出现的次数一样，可以将每个字母出现的次数使用字符串表示，作为hash的键</span></span><br><span class="line">    <span class="comment">// 由于字符串只包含小写，使用长度为26的数组记录每个字母的出现次数。</span></span><br><span class="line">    vector&lt;vector&lt;string&gt;&gt; <span class="built_in">groupAnagrams2</span>(vector&lt;string&gt;&amp; strs) &#123;</span><br><span class="line">        <span class="comment">// 自定义 array&lt;int, 26&gt; 类型的哈希函数</span></span><br><span class="line">        <span class="keyword">auto</span> array_hash = [fn = hash&lt;<span class="type">int</span>&gt;&#123;&#125;](<span class="type">const</span> std::array&lt;<span class="type">int</span>, <span class="number">26</span>&gt;&amp; arr) -&gt; <span class="type">size_t</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> std::<span class="built_in">accumulate</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), <span class="number">0u</span>, [&amp;](<span class="type">size_t</span> acc, <span class="type">int</span> num) &#123;</span><br><span class="line">                <span class="built_in">return</span> (acc &lt;&lt; <span class="number">1</span>) ^ <span class="built_in">fn</span>(num);</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        std::unordered_map&lt;std::array&lt;<span class="type">int</span> ,26&gt;, std::vector&lt;std::string&gt;, <span class="keyword">decltype</span>(array_hash)&gt; <span class="built_in">map</span>(<span class="number">0</span>, array_hash);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (std::string&amp; str : strs) &#123;</span><br><span class="line">            std::array&lt;<span class="type">int</span> ,26&gt; counts&#123;&#125;;</span><br><span class="line">            <span class="type">int</span> length = str.<span class="built_in">length</span>();</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; length; i++) &#123;</span><br><span class="line">                counts[str[i] - <span class="string">&#x27;a&#x27;</span>]++;</span><br><span class="line">            &#125;</span><br><span class="line">            map[counts].<span class="built_in">emplace_back</span>(str);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        std::vector&lt;std::vector&lt;std::string&gt;&gt; ans;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> it = map.<span class="built_in">begin</span>(); it != map.<span class="built_in">end</span>(); it++) &#123;</span><br><span class="line">            ans.<span class="built_in">emplace_back</span>(it-&gt;second);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>06 树</title>
      <link href="/dsa/leetcode/06_tree/README/"/>
      <url>/dsa/leetcode/06_tree/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-树"><a href="#1-树" class="headerlink" title="1.树"></a>1.树</h1><h2 id="1-1-树的概念"><a href="#1-1-树的概念" class="headerlink" title="1.1 树的概念"></a>1.1 树的概念</h2><p>树和图的区别：有没有环；</p><p>Linked List是特殊化的Tree；Tree是特殊化的Graph</p><p><img src="image/image_Z2EZxsV9YQ.png" alt=""></p><h2 id="1-2-二叉树"><a href="#1-2-二叉树" class="headerlink" title="1.2 二叉树"></a>1.2 二叉树</h2><p>结点定义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TreeNode</span>:</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val</span>):</span><br><span class="line">    self.val = val</span><br><span class="line">    self.left, self.right = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">TreeNode</span> &#123;</span><br><span class="line">  <span class="type">int</span> val;</span><br><span class="line">  TreeNode* left;</span><br><span class="line">  TreeNode* right;</span><br><span class="line">  <span class="built_in">TreeNode</span>(<span class="type">int</span> x) : <span class="built_in">val</span>(x), <span class="built_in">left</span>(<span class="literal">NULL</span>), <span class="built_in">right</span>(<span class="literal">NULL</span>) &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TreeNode</span> &#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> val;</span><br><span class="line">  <span class="keyword">public</span> TreeNode left, right;</span><br><span class="line">  <span class="keyword">public</span> <span class="title function_">TreeNode</span><span class="params">(<span class="type">int</span> val)</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.val = val;</span><br><span class="line">    <span class="built_in">this</span>.left = <span class="literal">null</span>;</span><br><span class="line">    <span class="built_in">this</span>.right = <span class="literal">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-3-二叉树的遍历"><a href="#1-3-二叉树的遍历" class="headerlink" title="1.3 二叉树的遍历"></a>1.3 二叉树的遍历</h2><ul><li>前序（pre-order）：根 → 左 → 右</li><li>中序（in-order）：左 → 根 → 右</li><li>后序（post-order）：左 → 右 → 根</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preorder</span>(<span class="params">self, root</span>):</span><br><span class="line">  <span class="keyword">if</span> root:</span><br><span class="line">    self.traverse_path.append(root, val)</span><br><span class="line">    self.preorder(root.left)</span><br><span class="line">    self.preorder(root.right)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inorder</span>(<span class="params">self, root</span>):</span><br><span class="line">  <span class="keyword">if</span> root:</span><br><span class="line">    self.inorder(root.left)</span><br><span class="line">    self.traverse_path.append(root, val)</span><br><span class="line">    self.inorder(root.right)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">postorder</span>(<span class="params">self, root</span>):</span><br><span class="line">  <span class="keyword">if</span> root:</span><br><span class="line">    self.postorder(root.left)</span><br><span class="line">    self.postorder(root.right)</span><br><span class="line">    self.traverse_path.append(root, val)</span><br></pre></td></tr></table></figure><h2 id="1-4-二叉搜索树-Binary-Search-Tree"><a href="#1-4-二叉搜索树-Binary-Search-Tree" class="headerlink" title="1.4 二叉搜索树 Binary Search Tree"></a>1.4 二叉搜索树 Binary Search Tree</h2><ul><li><a href="https://visualgo.net/zh/bst" title="二叉搜索树 Demo">二叉搜索树 Demo</a></li></ul><p>二叉搜索树，也称二叉搜索树、有序二叉树 (Ordered Binary Tree) 、排序二叉树 (Sorted Binary Tree) ，是指一棵空树或者具有下列性质的二又树:</p><ol><li>左子树上<strong>所有结点</strong>的值均小于它的根结点的值</li><li>右子树上<strong>所有结点</strong>的值均大于它的根结点的值</li><li>以此类推: 左、右子树也分别为二又查找树。(这就是 重复性!)</li></ol><p>中序遍历：升序排列</p><ul><li>查询、删除、插入：$O(log~n)$</li></ul><h1 id="2-示例"><a href="#2-示例" class="headerlink" title="2.示例"></a>2.示例</h1><h2 id="2-1-前序遍历"><a href="#2-1-前序遍历" class="headerlink" title="2.1 前序遍历"></a>2.1 前序遍历</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">preorderTraversal1</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        std::vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">preorder</span>(root, ans);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">preorderTraversal</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        std::vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        std::stack&lt;TreeNode*&gt; stack;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (!stack.<span class="built_in">empty</span>() || root != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">while</span> (root != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                ans.<span class="built_in">emplace_back</span>(root-&gt;val);</span><br><span class="line">                stack.<span class="built_in">push</span>(root);</span><br><span class="line">                root = root-&gt;left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            root = stack.<span class="built_in">top</span>();</span><br><span class="line">            stack.<span class="built_in">pop</span>();</span><br><span class="line"></span><br><span class="line">            root = root-&gt;right;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">preorder</span><span class="params">(TreeNode* root, std::vector&lt;<span class="type">int</span>&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!root) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ans.<span class="built_in">push_back</span>(root-&gt;val);</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">preorder</span>(root-&gt;left, ans);</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">preorder</span>(root-&gt;right, ans);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-2-中序遍历"><a href="#2-2-中序遍历" class="headerlink" title="2.2 中序遍历"></a>2.2 中序遍历</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1.递归</span></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">inorderTraversal1</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        std::vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">inorder</span>(root, ans);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.手动维护栈</span></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">inorderTraversal</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        std::vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        std::stack&lt;TreeNode*&gt; stack;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (root != <span class="literal">nullptr</span> || !stack.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            <span class="comment">// 左子树加入栈</span></span><br><span class="line">            <span class="keyword">while</span> (root != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                stack.<span class="built_in">push</span>(root);</span><br><span class="line">                root = root-&gt;left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 栈中取出</span></span><br><span class="line">            root = stack.<span class="built_in">top</span>();</span><br><span class="line">            stack.<span class="built_in">pop</span>();</span><br><span class="line">            ans.<span class="built_in">push_back</span>(root-&gt;val);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 遍历右子树</span></span><br><span class="line">            root = root-&gt;right;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">inorder</span><span class="params">(TreeNode* root, std::vector&lt;<span class="type">int</span>&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!root) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">inorder</span>(root-&gt;left, ans);</span><br><span class="line">        ans.<span class="built_in">push_back</span>(root-&gt;val);</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">inorder</span>(root-&gt;right, ans);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-3-后序遍历"><a href="#2-3-后序遍历" class="headerlink" title="2.3 后序遍历"></a>2.3 后序遍历</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">postorderTraversal1</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        std::vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">postorder</span>(root, ans);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">postorderTraversal</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        std::vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        std::stack&lt;TreeNode*&gt; stack;</span><br><span class="line">        TreeNode* prev = <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (!stack.<span class="built_in">empty</span>() || root != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">while</span> (root != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                stack.<span class="built_in">push</span>(root);</span><br><span class="line">                root = root-&gt;left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            root = stack.<span class="built_in">top</span>();</span><br><span class="line">            stack.<span class="built_in">pop</span>();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (root-&gt;right == <span class="literal">nullptr</span> || root-&gt;right == prev) &#123;</span><br><span class="line">                ans.<span class="built_in">emplace_back</span>(root-&gt;val);</span><br><span class="line">                prev = root;</span><br><span class="line">                root = <span class="literal">nullptr</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                stack.<span class="built_in">push</span>(root);</span><br><span class="line">                root = root-&gt;right;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">postorder</span><span class="params">(TreeNode* root, std::vector&lt;<span class="type">int</span>&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!root) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">postorder</span>(root-&gt;left, ans);</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">postorder</span>(root-&gt;right, ans);</span><br><span class="line">        ans.<span class="built_in">push_back</span>(root-&gt;val);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>07 递归</title>
      <link href="/dsa/leetcode/07_recursion/README/"/>
      <url>/dsa/leetcode/07_recursion/README/</url>
      
        <content type="html"><![CDATA[<p>递归 - 循环：通过函数体来进行循环</p><p><strong>重复性</strong></p><h1 id="1-递归：盗梦空间"><a href="#1-递归：盗梦空间" class="headerlink" title="1.递归：盗梦空间"></a>1.递归：盗梦空间</h1><h2 id="1-1-特点"><a href="#1-1-特点" class="headerlink" title="1.1 特点"></a>1.1 特点</h2><ul><li>向下进入到不同梦境；向上又回到原来的一层；</li><li>通过声音同步回到上一层</li><li>每一层的环境和周围的人都是一份拷贝、主角等几人穿越不同层级的梦境（发生和携带变化）</li></ul><h2 id="1-2-计算-n"><a href="#1-2-计算-n" class="headerlink" title="1.2 计算 n!"></a>1.2 计算 n!</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Factorial</span>(<span class="params">n</span>):</span><br><span class="line">  <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> n * Factorial(n - <span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>递归栈</p><p><img src="image/image_LqrElF5P6B.png" alt=""></p><h2 id="1-3-递归代码模板"><a href="#1-3-递归代码模板" class="headerlink" title="1.3 递归代码模板"></a>1.3 递归代码模板</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">recursion</span>(<span class="params">level, param1, param2, ...</span>):</span><br><span class="line">  <span class="comment"># 1.recursion terminator (递归终止条件)</span></span><br><span class="line">  <span class="keyword">if</span> level &gt; MAX_LeVEL:</span><br><span class="line">    process_result</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2.process logic in current level (处理当前层逻辑)</span></span><br><span class="line">  process(level, data, ...)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3.drill down (下探到下一层)</span></span><br><span class="line">  self.recursion(level + <span class="number">1</span>, p1, ...)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 4.reverse the current level status if needed (清理当前层)</span></span><br></pre></td></tr></table></figure><h2 id="1-4-思维要点"><a href="#1-4-思维要点" class="headerlink" title="1.4 思维要点"></a>1.4 思维要点</h2><ol><li>不要人肉递归（最大误区）</li><li>找到最近最简方法，将其拆解成课重复解决的问题（重复子问题）</li><li>数学归纳法思维 (n=1, n=2成立，若n成立，能推到出n+1成立)</li></ol><h1 id="2-习题"><a href="#2-习题" class="headerlink" title="2.习题"></a>2.习题</h1><h2 id="2-1-爬楼梯"><a href="#2-1-爬楼梯" class="headerlink" title="2.1 爬楼梯"></a>2.1 爬楼梯</h2><p><a href="https://leetcode.cn/problems/climbing-stairs/" title="https://leetcode.cn/problems/climbing-stairs/">https://leetcode.cn/problems/climbing-stairs/</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1 : 1</span></span><br><span class="line">    <span class="comment">// 2 : 2</span></span><br><span class="line">    <span class="comment">// f(1) + f(2)</span></span><br><span class="line">    <span class="comment">// f(2) + f(3)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// f(n) = f(n-1) + f(n-2)</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">climbStairs</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (n &lt;= <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> n;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> f1 = <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> f2 = <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> f3 = <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">3</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            f3 = f1 + f2;</span><br><span class="line">            f1 = f2;</span><br><span class="line">            f2 = f3;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> f3;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-2-生成括号"><a href="#2-2-生成括号" class="headerlink" title="2.2 生成括号"></a>2.2 生成括号</h2><p><a href="https://leetcode.cn/problems/generate-parentheses" title="https://leetcode.cn/problems/generate-parentheses">https://leetcode.cn/problems/generate-parentheses</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 递归</span></span><br><span class="line">    <span class="comment">// 左括号: 随时加，只要不超标</span></span><br><span class="line">    <span class="comment">// 右括号 : 必须之前有左括号，且左括号个数 &gt; 右括号个数</span></span><br><span class="line">    <span class="function">vector&lt;string&gt; <span class="title">generateParenthesis</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        result.<span class="built_in">clear</span>();</span><br><span class="line">        rhis-&gt;_generate(<span class="number">0</span>, <span class="number">0</span>, n, <span class="string">&quot;&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">void</span> _generate(<span class="type">int</span> left, <span class="type">int</span> right, <span class="type">int</span> num, std::string s) &#123;</span><br><span class="line">        <span class="comment">// 1.terminator</span></span><br><span class="line">        <span class="keyword">if</span> (left == num &amp;&amp; right == num) &#123;</span><br><span class="line">            result.<span class="built_in">emplace_back</span>(s);</span><br><span class="line">            <span class="comment">// std::cout &lt;&lt; s &lt;&lt; std::endl;</span></span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.process current logic</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.drill down</span></span><br><span class="line">        <span class="keyword">if</span> (left &lt; num)</span><br><span class="line">            <span class="keyword">this</span>-&gt;_generate(left + <span class="number">1</span>, right, num, s + <span class="string">&quot;(&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (left &gt; right)</span><br><span class="line">            <span class="keyword">this</span>-&gt;_generate(left, right + <span class="number">1</span>, num, s + <span class="string">&quot;)&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.reverse states</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    std::vector&lt;std::string&gt; result;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-3-验证二叉搜索树"><a href="#2-3-验证二叉搜索树" class="headerlink" title="2.3 验证二叉搜索树"></a>2.3 验证二叉搜索树</h2><p><a href="https://leetcode.cn/problems/validate-binary-search-tree/" title="98. 验证二叉搜索树 - 力扣（LeetCode）">98. 验证二叉搜索树 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 给你一个二叉树的根节点 root ，判断其是否是一个有效的二叉搜索树。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 有效 二叉搜索树定义如下：</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 节点的左子树只包含 小于 当前节点的数。</span></span><br><span class="line"><span class="comment">// 节点的右子树只包含 大于 当前节点的数。</span></span><br><span class="line"><span class="comment">// 所有左子树和右子树自身必须也是二叉搜索树。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// BST --&gt; 中序遍历是递增的</span></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isValidBST</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历左子树</span></span><br><span class="line">        <span class="keyword">if</span> (!<span class="keyword">this</span>-&gt;<span class="built_in">isValidBST</span>(root-&gt;left)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 当前结点不大于父节点，不是排序二叉树</span></span><br><span class="line">        <span class="keyword">if</span> (root-&gt;val &lt;= m_last) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 记录父节点值</span></span><br><span class="line">            m_last = root-&gt;val;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 遍历右子树</span></span><br><span class="line">        <span class="keyword">if</span> (!<span class="keyword">this</span>-&gt;<span class="built_in">isValidBST</span>(root-&gt;right)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 子树遍历完成，或者不是二叉排序树，退出</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 用于存储最新遍历的父亲结点值</span></span><br><span class="line">    <span class="type">long</span> m_last = LONG_MIN;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-4-二叉树最大深度"><a href="#2-4-二叉树最大深度" class="headerlink" title="2.4 二叉树最大深度"></a>2.4 二叉树最大深度</h2><p><a href="https://leetcode.cn/problems/maximum-depth-of-binary-tree/description/" title="104. 二叉树的最大深度 - 力扣（LeetCode）">104. 二叉树的最大深度 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 左子树和左子树最大深度 + 1</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">maxDepth</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!root) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> std::<span class="built_in">max</span>(<span class="built_in">maxDepth</span>(root-&gt;left), <span class="built_in">maxDepth</span>(root-&gt;right)) + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-5-二叉树的最小深度"><a href="#2-5-二叉树的最小深度" class="headerlink" title="2.5 二叉树的最小深度"></a>2.5 二叉树的最小深度</h2><p><a href="https://leetcode.cn/problems/minimum-depth-of-binary-tree/description/" title="111. 二叉树的最小深度 - 力扣（LeetCode）">111. 二叉树的最小深度 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给定一个二叉树，找出其最小深度。</span><br><span class="line">最小深度是从根节点到最近叶子节点的最短路径上的节点数量。</span><br><span class="line">说明：叶子节点是指没有子节点的节点。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minDepth</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 当 root 为空时，返回 0</span></span><br><span class="line">        <span class="keyword">if</span> (!root) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> deep = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> left_deep = <span class="built_in">minDepth</span>(root-&gt;left);</span><br><span class="line">        <span class="type">int</span> right_deep = <span class="built_in">minDepth</span>(root-&gt;right);</span><br><span class="line">        <span class="comment">// 当 root 节点左右孩子有一个为空时，返回不为空的孩子节点的深度</span></span><br><span class="line">        <span class="comment">// 当 root 节点左右孩子都不为空时，返回左右孩子较小深度的节点值</span></span><br><span class="line">        <span class="keyword">if</span> (!root-&gt;left || !root-&gt;right) &#123;</span><br><span class="line">            deep = left_deep + right_deep + <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            deep = std::<span class="built_in">min</span>(left_deep, right_deep) + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> deep;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-6-二叉树的序列化和反序列化"><a href="#2-6-二叉树的序列化和反序列化" class="headerlink" title="2.6 二叉树的序列化和反序列化"></a>2.6 二叉树的序列化和反序列化</h2><p><a href="https://leetcode.cn/problems/serialize-and-deserialize-binary-tree/description/" title="297. 二叉树的序列化与反序列化 - 力扣（LeetCode）">297. 二叉树的序列化与反序列化 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">序列化是将一个数据结构或者对象转换为连续的比特位的操作，进而可以将转换后的数据存储在一个文件或者内存中，同时也可以通过网络传输到另一个计算机环境，采取相反方式重构得到原数据。</span><br><span class="line"></span><br><span class="line">请设计一个算法来实现二叉树的序列化与反序列化。这里不限定你的序列 / 反序列化算法执行逻辑，你只需要保证一个二叉树可以被序列化为一个字符串并且将这个字符串反序列化为原始的树结构。</span><br><span class="line"></span><br><span class="line">提示: 输入输出格式与 LeetCode 目前使用的方式一致，详情请参阅 LeetCode 序列化二叉树的格式。你并非必须采取这种方式，你也可以采用其他的方法解决这个问题。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Codec</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Encodes a tree to a single string.</span></span><br><span class="line">    <span class="function">string <span class="title">serialize</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        std::string ans;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">pre_order</span>(root, ans);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先序遍历输出</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">pre_order</span><span class="params">(TreeNode* root, std::string&amp; str)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            str += <span class="string">&quot;None,&quot;</span>;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        str += std::<span class="built_in">to_string</span>(root-&gt;val) + <span class="string">&quot;,&quot;</span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">pre_order</span>(root-&gt;left, str);</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">pre_order</span>(root-&gt;right, str);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Decodes your encoded data to tree.</span></span><br><span class="line">    <span class="function">TreeNode* <span class="title">deserialize</span><span class="params">(string data)</span> </span>&#123;</span><br><span class="line">        std::list&lt;std::string&gt; data_array;</span><br><span class="line">        std::string str;</span><br><span class="line">        <span class="comment">// 将字符串分割，存在list中</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; ch : data) &#123;</span><br><span class="line">            <span class="keyword">if</span> (ch == <span class="string">&#x27;,&#x27;</span>) &#123;</span><br><span class="line">                data_array.<span class="built_in">push_back</span>(str);</span><br><span class="line">                str.<span class="built_in">clear</span>();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                str.<span class="built_in">push_back</span>(ch);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理最后一个</span></span><br><span class="line">        <span class="keyword">if</span> (!str.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            data_array.<span class="built_in">push_back</span>(str);</span><br><span class="line">            str.<span class="built_in">clear</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 先序遍历创建树</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">pre_create_tree</span>(data_array);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先序遍历创建树，创建一棵树</span></span><br><span class="line">    <span class="function">TreeNode* <span class="title">pre_create_tree</span><span class="params">(std::list&lt;std::string&gt;&amp; data_array)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 如果碰到 None 返回</span></span><br><span class="line">        <span class="keyword">if</span> (data_array.<span class="built_in">front</span>() == <span class="string">&quot;None&quot;</span>) &#123;</span><br><span class="line">            data_array.<span class="built_in">erase</span>(data_array.<span class="built_in">begin</span>());</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 新建结点</span></span><br><span class="line">        TreeNode* root = <span class="keyword">new</span> <span class="built_in">TreeNode</span>(std::<span class="built_in">stoi</span>(data_array.<span class="built_in">front</span>().<span class="built_in">c_str</span>()));</span><br><span class="line">        data_array.<span class="built_in">erase</span>(data_array.<span class="built_in">begin</span>());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建左子树</span></span><br><span class="line">        root-&gt;left = <span class="keyword">this</span>-&gt;<span class="built_in">pre_create_tree</span>(data_array);</span><br><span class="line">        <span class="comment">// 创建右子树</span></span><br><span class="line">        root-&gt;right = <span class="keyword">this</span>-&gt;<span class="built_in">pre_create_tree</span>(data_array);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2-7-公共祖先"><a href="#2-7-公共祖先" class="headerlink" title="2.7 公共祖先"></a>2.7 公共祖先</h2><p><a href="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/description/" title="236. 二叉树的最近公共祖先 - 力扣（LeetCode）">236. 二叉树的最近公共祖先 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。</span><br><span class="line"></span><br><span class="line">百度百科中最近公共祖先的定义为：“对于有根树 T 的两个节点 p、q，最近公共祖先表示为一个节点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">lowestCommonAncestor</span><span class="params">(TreeNode* root, TreeNode* p, TreeNode* q)</span> </span>&#123;</span><br><span class="line">        TreeNode* father = <span class="literal">nullptr</span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(root, p, q, father);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> father;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">dfs</span><span class="params">(TreeNode* root, TreeNode* p, TreeNode* q, TreeNode*&amp; father)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 到达叶子结点，返回false</span></span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 搜索左右子树是否是公共根节点</span></span><br><span class="line">        <span class="type">bool</span> left = <span class="built_in">dfs</span>(root-&gt;left, p, q, father);</span><br><span class="line">        <span class="type">bool</span> right = <span class="built_in">dfs</span>(root-&gt;right, p, q, father);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断条件</span></span><br><span class="line">        <span class="comment">// 1.左子树存在 p 或 q，右子树存在p或q，则此结点是公共祖先</span></span><br><span class="line">        <span class="comment">// 2.左子树或右子树其中一方存在p和q，另一方没有，那这个结点就是公共祖先</span></span><br><span class="line">        <span class="keyword">if</span> ((left &amp;&amp; right) || ((root-&gt;val == p-&gt;val || root-&gt;val == q-&gt;val) &amp;&amp; (left || right))) &#123;</span><br><span class="line">            father = root;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 公共祖先条件</span></span><br><span class="line">        <span class="keyword">return</span> left || right || (root-&gt;val == p-&gt;val || root-&gt;val == q-&gt;val);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-8-先序中序遍历构造树"><a href="#2-8-先序中序遍历构造树" class="headerlink" title="2.8 先序中序遍历构造树"></a>2.8 先序中序遍历构造树</h2><p><a href="https://leetcode.cn/problems/construct-binary-tree-from-preorder-and-inorder-traversal/description/" title="105. 从前序与中序遍历序列构造二叉树 - 力扣（LeetCode）">105. 从前序与中序遍历序列构造二叉树 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">给定两个整数数组 preorder 和 inorder ，其中 preorder 是二叉树的先序遍历， inorder 是同一棵树的中序遍历，请构造二叉树并返回其根节点。</span><br></pre></td></tr></table></figure><p>一颗树的先序序列和中序序列能确定这棵树。</p><ul><li>先序序列：根，左，右</li><li>中序序列：左，根，右</li><li>后序序列：左，右，根</li></ul><p><strong>表示子树不存在：</strong></p><ul><li>序列长度为零，代表子树不存在</li><li>给定起始位置索引和终止位置索引。起始位置和终止位置不合法，也就是终止位置大于起始位置来代表序列不存在，即代表字数不存在</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">buildTree</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; preorder, vector&lt;<span class="type">int</span>&gt;&amp; inorder)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">pre_inorder_build_tree</span>(preorder, <span class="number">0</span>, preorder.<span class="built_in">size</span>() - <span class="number">1</span>, inorder, <span class="number">0</span>, inorder.<span class="built_in">size</span>() - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">pre_inorder_build_tree</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; preorder, <span class="type">int</span> pre_start_idx, <span class="type">int</span> pre_end_idx,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    std::vector&lt;<span class="type">int</span>&gt;&amp; inorder, <span class="type">int</span> in_start_idx, <span class="type">int</span> in_end_idx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (pre_start_idx &gt; pre_end_idx) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建根节点，根节点的值使用前序遍历的第一个</span></span><br><span class="line">        TreeNode* root = <span class="keyword">new</span> <span class="built_in">TreeNode</span>(preorder[pre_start_idx]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 在中序遍历中找到根节点，划分为两个数组，分别是左右子树的，</span></span><br><span class="line">        <span class="type">int</span> root_idx = in_start_idx;</span><br><span class="line">        <span class="keyword">for</span> (; root_idx &lt;= in_end_idx; root_idx++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (root-&gt;val == inorder[root_idx]) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 左子树的长度</span></span><br><span class="line">        <span class="type">int</span> left_lens = root_idx - in_start_idx;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建左子树</span></span><br><span class="line">        root-&gt;left = <span class="keyword">this</span>-&gt;<span class="built_in">pre_inorder_build_tree</span>(preorder, pre_start_idx + <span class="number">1</span>, pre_start_idx + left_lens, </span><br><span class="line">                                                  inorder, in_start_idx, root_idx - <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 创建右子树</span></span><br><span class="line">        root-&gt;right = <span class="keyword">this</span>-&gt;<span class="built_in">pre_inorder_build_tree</span>(preorder, pre_start_idx + left_lens + <span class="number">1</span>, pre_end_idx, </span><br><span class="line">                                                  inorder, root_idx + <span class="number">1</span>, in_end_idx);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-9-组合"><a href="#2-9-组合" class="headerlink" title="2.9 组合"></a>2.9 组合</h2><p><a href="https://leetcode.cn/problems/combinations/description/" title="77. 组合 - 力扣（LeetCode）">77. 组合 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">给定两个整数 n 和 k，返回范围 [1, n] 中所有可能的 k 个数的组合。</span><br><span class="line">你可以按 任何顺序 返回答案。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">combine</span>(<span class="type">int</span> n, <span class="type">int</span> k) &#123;</span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; ans;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(<span class="number">1</span>, n, k, ans);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> curr, <span class="type">int</span> n, <span class="type">int</span> k, std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 终止条件</span></span><br><span class="line">        <span class="comment">// tmp长度加上区间[cur, n]的长度小于k，不可能构造出长度k的tmp</span></span><br><span class="line">        <span class="keyword">if</span> (m_tmp.<span class="built_in">size</span>() + (n - curr + <span class="number">1</span>) &lt; k) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 记录合法的答案</span></span><br><span class="line">        <span class="keyword">if</span> (m_tmp.<span class="built_in">size</span>() == k) &#123;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(m_tmp);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理当前层</span></span><br><span class="line">        m_tmp.<span class="built_in">push_back</span>(curr);</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(curr + <span class="number">1</span>, n, k, ans);</span><br><span class="line">        m_tmp.<span class="built_in">pop_back</span>();</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(curr + <span class="number">1</span>, n, k, ans);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; m_tmp;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>08 分治、回溯</title>
      <link href="/dsa/leetcode/08_divide/README/"/>
      <url>/dsa/leetcode/08_divide/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-分治"><a href="#1-分治" class="headerlink" title="1.分治"></a>1.分治</h1><p>本质上就是一种递归</p><p>找问题<strong>重复性</strong>，分解问题，组合每个子问题结果</p><p>递归状态树</p><p><img src="image/image_R7QOFHqn6z.png" alt=""></p><h2 id="1-1-分治-Divide-amp-Conquer"><a href="#1-1-分治-Divide-amp-Conquer" class="headerlink" title="1.1 分治 Divide &amp; Conquer"></a>1.1 分治 Divide &amp; Conquer</h2><p><img src="image/image_71BFjehgOf.png" alt=""></p><h2 id="1-2-分治代码模板"><a href="#1-2-分治代码模板" class="headerlink" title="1.2 分治代码模板"></a>1.2 分治代码模板</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">divide_conquer</span>(<span class="params">problem, param1, param2, ...</span>):</span><br><span class="line">  <span class="comment"># 1.recursion terminator (递归终止条件)</span></span><br><span class="line">  <span class="keyword">if</span> problem <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    print_result</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 2.prepare data (拆分问题)</span></span><br><span class="line">  data = prepare_data(problem)</span><br><span class="line">  subproblems = split_problem(problem, data)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3.conquer subproblems (调字问题的递归函数)</span></span><br><span class="line">  subresult1 = self.divide_conquer(subproblems[<span class="number">0</span>], p1, ...)</span><br><span class="line">  subresult1 = self.divide_conquer(subproblems[<span class="number">0</span>], p1, ...)</span><br><span class="line">  subresult1 = self.divide_conquer(subproblems[<span class="number">0</span>], p1, ...)</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 4.process and generate the final result (合并结果)</span></span><br><span class="line">  result = process_result(subresult1, subresult2, subresult3, ...)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 5.revert the current level status (回复当前层状态)</span></span><br></pre></td></tr></table></figure><p>泛型递归代码模板</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">recursion</span>(<span class="params">level, param1, param2, ...</span>):</span><br><span class="line">  <span class="comment"># 1.recursion terminator (递归终止条件)</span></span><br><span class="line">  <span class="keyword">if</span> level &gt; MAX_LeVEL:</span><br><span class="line">    process_result</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2.process logic in current level (处理当前层逻辑)</span></span><br><span class="line">  process(level, data, ...)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3.drill down (下探到下一层)</span></span><br><span class="line">  self.recursion(level + <span class="number">1</span>, p1, ...)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 4.reverse the current level status if needed (清理当前层)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="2-回溯"><a href="#2-回溯" class="headerlink" title="2.回溯"></a>2.回溯</h1><p>回溯法采用试错的思想，它尝试分步的去解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确的解答的时候，它将取消上一步甚至是上几步的计算，再通过其它的可能的分步解答再次尝试寻找问题的答案。</p><p>回溯法通常用最简单的递归方法来实现，在反复重复上述的步骤后可能出现两种情况：</p><ul><li>找到一个可能存在的正确的答案</li><li>在尝试了所有可能的分步方法后宣告该问题没有答案</li></ul><p>在最坏的情况下，回溯法会导致一次复杂度为指数时间的计算。</p><p>最典型应用：八皇后、数独</p><h1 id="3-实战题目"><a href="#3-实战题目" class="headerlink" title="3.实战题目"></a>3.实战题目</h1><h2 id="3-1-pow-x-n"><a href="#3-1-pow-x-n" class="headerlink" title="3.1 pow(x, n)"></a>3.1 pow(x, n)</h2><p><a href="https://leetcode.cn/problems/powx-n/" title="https://leetcode.cn/problems/powx-n/">https://leetcode.cn/problems/powx-n/</a></p><h3 id="（1）暴力-O-n"><a href="#（1）暴力-O-n" class="headerlink" title="（1）暴力 O(n)"></a>（1）暴力 O(n)</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">  result *= x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）分治"><a href="#（2）分治" class="headerlink" title="（2）分治"></a>（2）分治</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>:</span><br><span class="line">  <span class="number">1.</span> terminator</span><br><span class="line">  <span class="number">2.</span> <span class="built_in">process</span> (split your big problem)</span><br><span class="line">  <span class="number">3.</span> <span class="function">drill <span class="title">down</span> <span class="params">(subproblems)</span>, <span class="title">merge</span><span class="params">(subresult)</span></span></span><br><span class="line"><span class="function">  4. reverse states</span></span><br><span class="line"><span class="function"></span></span><br></pre></td></tr></table></figure><p>x^n  → 2^10 → 2^5 → (2^2)*2</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">pow</span>(x, n):</span><br><span class="line">  subproblem : subresult = <span class="built_in">power</span>(x, n/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">merge:</span><br><span class="line">  <span class="keyword">if</span> (n % <span class="number">2</span> == <span class="number">1</span>) &#123;</span><br><span class="line">    result = subresult * subresult * x;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    result = subresult * subresult;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>实现代码</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 分治，递归</span></span><br><span class="line">    <span class="comment">// 2^10 --&gt; 2^5  -&gt; (2^2) * 2</span></span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">fast_pow</span><span class="params">(<span class="type">double</span> x, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1.0</span>;</span><br><span class="line">        <span class="type">double</span> sub_result = <span class="built_in">fast_pow</span>(x, n/<span class="number">2</span>);</span><br><span class="line">        <span class="keyword">return</span> n % <span class="number">2</span> == <span class="number">0</span> ? sub_result * sub_result : sub_result * sub_result * x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 分治递归处理</span></span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">myPow</span><span class="params">(<span class="type">double</span> x, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> N = n;</span><br><span class="line">        <span class="comment">// 如果n是负数，特殊处理</span></span><br><span class="line">        <span class="keyword">return</span> N &gt;= <span class="number">0</span> ? <span class="built_in">fast_pow</span>(x, N) : <span class="built_in">fast_pow</span>(<span class="number">1</span>/x, -N);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（3）牛顿迭代法"><a href="#（3）牛顿迭代法" class="headerlink" title="（3）牛顿迭代法"></a>（3）牛顿迭代法</h3><ul><li><a href="http://www.matrix67.com/blog/archives/361" title="牛顿迭代法原理">牛顿迭代法原理</a></li><li><a href="http://www.voidcn.com/article/p-eudisdmk-zm.html" title="牛顿迭代法代码">牛顿迭代法代码</a></li></ul><h2 id="3-2-子集"><a href="#3-2-子集" class="headerlink" title="3.2 子集"></a>3.2 子集</h2><p><a href="https://leetcode.cn/problems/subsets/" title="78. 子集 - 力扣（LeetCode）">78. 子集 - 力扣（LeetCode）</a></p><h3 id="（1）递归"><a href="#（1）递归" class="headerlink" title="（1）递归"></a>（1）递归</h3><p>类似爬楼梯，可选可不选</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt;&amp; ans, std::vector&lt;<span class="type">int</span>&gt;&amp; nums, vector&lt;<span class="type">int</span>&gt; list, <span class="type">int</span> idx)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// terminator</span></span><br><span class="line">        <span class="keyword">if</span> (idx == nums.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(list);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// not pick the number at this idx</span></span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(ans, nums, list, idx + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// pick the number at this idx</span></span><br><span class="line">        list.<span class="built_in">push_back</span>(nums[idx]);</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(ans, nums, list, idx + <span class="number">1</span>);</span><br><span class="line">        list.<span class="built_in">pop_back</span>();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">subsets</span>(vector&lt;<span class="type">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; ans;</span><br><span class="line">        std::vector&lt;<span class="type">int</span>&gt; list;</span><br><span class="line">        <span class="keyword">if</span> (nums.<span class="built_in">empty</span>())</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(ans, nums, list, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（2）迭代"><a href="#（2）迭代" class="headerlink" title="（2）迭代"></a>（2）迭代</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># []</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># [1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># [2]</span></span><br><span class="line">    <span class="comment"># [1, 2]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># [3]</span></span><br><span class="line">    <span class="comment"># [1, 3]</span></span><br><span class="line">    <span class="comment"># [2, 3]</span></span><br><span class="line">    <span class="comment"># [1, 2, 3]</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">subsets</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        results = [[]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            new_sets = []</span><br><span class="line">            <span class="keyword">for</span> subset <span class="keyword">in</span> results:</span><br><span class="line">                new_subset = subset + [num]</span><br><span class="line">                new_sets.append(new_subset)</span><br><span class="line">            results.extend(new_sets)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><h2 id="3-3-众数"><a href="#3-3-众数" class="headerlink" title="3.3 众数"></a>3.3 众数</h2><p><a href="https://leetcode-cn.com/problems/majority-element/description/" title="https://leetcode-cn.com/problems/majority-element/description/">https://leetcode-cn.com/problems/majority-element/description/</a> （简单、但是高频）</p><h3 id="（1）哈希表"><a href="#（1）哈希表" class="headerlink" title="（1）哈希表"></a>（1）哈希表</h3><p>我们使用哈希映射（HashMap）来存储每个元素以及出现的次数。对于哈希映射中的每个键值对，键表示一个元素，值表示该元素出现的次数。</p><p>我们用一个循环遍历数组 nums 并将数组中的每个元素加入哈希映射中。在这之后，我们遍历哈希映射中的所有键值对，返回值最大的键。我们同样也可以在遍历数组 nums 时候使用打擂台的方法，维护最大的值，这样省去了最后对哈希映射的遍历。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 哈希表映射法</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">majorityElement</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        std::unordered_map&lt;<span class="type">int</span>, <span class="type">int</span>&gt; counts;</span><br><span class="line">        <span class="type">int</span> majority = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> max_cnt = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> num : nums) &#123;</span><br><span class="line">            counts[num]++;</span><br><span class="line">            <span class="keyword">if</span> (counts[num] &gt; max_cnt) &#123;</span><br><span class="line">                majority = num;</span><br><span class="line">                max_cnt = counts[num];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> majority;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="3-3-电话号码的字母组合"><a href="#3-3-电话号码的字母组合" class="headerlink" title="3.3 电话号码的字母组合"></a>3.3 电话号码的字母组合</h2><p><a href="https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/" title="https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/">https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number/</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。答案可以按 任意顺序 返回。</span><br><span class="line"></span><br><span class="line">给出数字到字母的映射如下（与电话按键相同）。注意 1 不对应任何字母。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;string&gt; <span class="title">letterCombinations</span><span class="params">(string digits)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (digits.<span class="built_in">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        std::vector&lt;std::string&gt; ans;</span><br><span class="line"></span><br><span class="line">        std::unordered_map&lt;<span class="type">char</span>, std::string&gt; map&#123;</span><br><span class="line">            &#123;<span class="string">&#x27;2&#x27;</span>, <span class="string">&quot;abc&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;3&#x27;</span>, <span class="string">&quot;def&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;4&#x27;</span>, <span class="string">&quot;ghi&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;5&#x27;</span>, <span class="string">&quot;jkl&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;6&#x27;</span>, <span class="string">&quot;mno&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;7&#x27;</span>, <span class="string">&quot;pqrs&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;8&#x27;</span>, <span class="string">&quot;tuv&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;9&#x27;</span>, <span class="string">&quot;wxyz&quot;</span>&#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">search</span>(<span class="string">&quot;&quot;</span>, digits, <span class="number">0</span>, ans, map);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">search</span><span class="params">(std::string s, std::string&amp; digits,  <span class="type">int</span> i,</span></span></span><br><span class="line"><span class="params"><span class="function">        std::vector&lt;std::string&gt;&amp; ans, </span></span></span><br><span class="line"><span class="params"><span class="function">        std::unordered_map&lt;<span class="type">char</span>, std::string&gt; map)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// terminator</span></span><br><span class="line">        <span class="keyword">if</span> (i == digits.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(s);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// process</span></span><br><span class="line">        std::string letters = map.<span class="built_in">at</span>(digits[i]);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; letters.<span class="built_in">size</span>(); j++) &#123;</span><br><span class="line">            <span class="comment">// drill down</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;<span class="built_in">search</span>(s+letters[j], digits, i+<span class="number">1</span>, ans, map);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="3-4-N皇后"><a href="#3-4-N皇后" class="headerlink" title="3.4 N皇后"></a>3.4 N皇后</h2><p><a href="https://leetcode.cn/problems/n-queens/description/" title="51. N 皇后 - 力扣（LeetCode）">51. N 皇后 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">按照国际象棋的规则，皇后可以攻击与之处在同一行或同一列或同一斜线上的棋子。</span><br><span class="line"></span><br><span class="line">n 皇后问题 研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。</span><br><span class="line"></span><br><span class="line">给你一个整数 n ，返回所有不同的 n 皇后问题 的解决方案。</span><br><span class="line"></span><br><span class="line">每一种解法包含一个不同的 n 皇后问题 的棋子放置方案，该方案中 <span class="string">&#x27;Q&#x27;</span> 和 <span class="string">&#x27;.&#x27;</span> 分别代表了皇后和空位。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">solveNQueens</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]]:</span><br><span class="line">        <span class="keyword">if</span> n &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        </span><br><span class="line">        self.result = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 之前的皇后所占的位置 (列, pie, na)</span></span><br><span class="line">        self.cols = <span class="built_in">set</span>()</span><br><span class="line">        self.pie = <span class="built_in">set</span>()</span><br><span class="line">        self.na = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">        self.dfs(n, <span class="number">0</span>, [])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self._generate_result(n)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">self, n, row, curr_state</span>):</span><br><span class="line">        <span class="comment"># 终止条件</span></span><br><span class="line">        <span class="keyword">if</span> row &gt;= n:</span><br><span class="line">            self.result.append(curr_state)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 当前层处理</span></span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> col <span class="keyword">in</span> self.cols <span class="keyword">or</span> row + col <span class="keyword">in</span> self.pie <span class="keyword">or</span> row - col <span class="keyword">in</span> self.na:</span><br><span class="line">                <span class="comment"># 不能放</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 更新标志位</span></span><br><span class="line">            self.cols.add(col)</span><br><span class="line">            self.pie.add(row + col)</span><br><span class="line">            self.na.add(row - col)</span><br><span class="line"></span><br><span class="line">            self.dfs(n, row + <span class="number">1</span>, curr_state + [col])</span><br><span class="line"></span><br><span class="line">            self.cols.remove(col)</span><br><span class="line">            self.pie.remove(row + col)</span><br><span class="line">            self.na.remove(row - col)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_generate_result</span>(<span class="params">self, n</span>):</span><br><span class="line">        board = []</span><br><span class="line">        <span class="keyword">for</span> res <span class="keyword">in</span> self.result:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> res:</span><br><span class="line">                board.append(<span class="string">&quot;.&quot;</span> * i + <span class="string">&quot;Q&quot;</span> + <span class="string">&quot;.&quot;</span> * (n - i - <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> [board[i : i + n] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(board), n)]</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;string&gt;&gt; <span class="built_in">solveNQueens</span>(<span class="type">int</span> n) &#123;</span><br><span class="line">        <span class="function">std::vector&lt;std::string&gt; <span class="title">curr_state</span><span class="params">(n, std::string(n, <span class="string">&#x27;.&#x27;</span>))</span></span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(curr_state, <span class="number">0</span>, n);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> m_ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    std::vector&lt;std::vector&lt;std::string&gt;&gt; m_ans;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(std::vector&lt;std::string&gt;&amp; curr_state, <span class="type">int</span> row, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 终止条件</span></span><br><span class="line">        <span class="keyword">if</span> (row == n) &#123;</span><br><span class="line">            m_ans.<span class="built_in">push_back</span>(curr_state);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 循环每列，</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> col = <span class="number">0</span>; col &lt; n; col++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;<span class="built_in">is_valid</span>(curr_state, row, col)) &#123;</span><br><span class="line">                <span class="comment">// doing</span></span><br><span class="line">                curr_state[row][col] = <span class="string">&#x27;Q&#x27;</span>;</span><br><span class="line">                <span class="comment">// 下一行</span></span><br><span class="line">                <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(curr_state, row + <span class="number">1</span>, n);</span><br><span class="line">                <span class="comment">// reverse</span></span><br><span class="line">                curr_state[row][col] = <span class="string">&#x27;.&#x27;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">is_valid</span><span class="params">(<span class="type">const</span> std::vector&lt;std::string&gt;&amp; curr_state, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = curr_state.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 上</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (curr_state[i][col] == <span class="string">&#x27;Q&#x27;</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 左上</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = row, j = col; i &gt;= <span class="number">0</span> &amp;&amp; j &gt;= <span class="number">0</span>; i--, j--) &#123;</span><br><span class="line">            <span class="keyword">if</span> (curr_state[i][j] == <span class="string">&#x27;Q&#x27;</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 右上</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = row, j = col; i &gt;= <span class="number">0</span> &amp;&amp; j &lt; n; i--, j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (curr_state[i][j] == <span class="string">&#x27;Q&#x27;</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 其他方向都是未放过皇后的，不可能为false</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>写的很好的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># 写的很好的python代码</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">solveNQueens</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]]:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">queens, xy_dif, xy_sum</span>):</span><br><span class="line">            p = <span class="built_in">len</span>(queens)</span><br><span class="line">            <span class="keyword">if</span> p == n:</span><br><span class="line">                result.append(queens)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                <span class="keyword">if</span> q <span class="keyword">not</span> <span class="keyword">in</span> queens <span class="keyword">and</span> p-q <span class="keyword">not</span> <span class="keyword">in</span> xy_dif <span class="keyword">and</span> p+q <span class="keyword">not</span> <span class="keyword">in</span> xy_sum:</span><br><span class="line">                    dfs(queens + [q], xy_dif+[p-q], xy_sum+[p+q])</span><br><span class="line">        result = []</span><br><span class="line">        dfs([], [], [])</span><br><span class="line">        <span class="keyword">return</span> [[<span class="string">&quot;.&quot;</span>*i + <span class="string">&quot;Q&quot;</span> + <span class="string">&quot;.&quot;</span>*(n-i-<span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> sol] <span class="keyword">for</span> sol <span class="keyword">in</span> result]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> algo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>09 深度优先、广度优先</title>
      <link href="/dsa/leetcode/09_dfs_bfs/README/"/>
      <url>/dsa/leetcode/09_dfs_bfs/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-搜索"><a href="#1-搜索" class="headerlink" title="1.搜索"></a>1.搜索</h1><h2 id="1-1-遍历搜索"><a href="#1-1-遍历搜索" class="headerlink" title="1.1 遍历搜索"></a>1.1 遍历搜索</h2><p>在树（图/状态集）中虚招特定结点</p><ul><li>每个结点都要访问一次</li><li>每个结点仅仅要访问一次</li><li>对于结点的访问顺序不同，分为DFS，BFS</li></ul><p><img src="image/image_oFwJ_RYgO0.png" alt=""></p><h2 id="1-2-深度优先搜索-Depth-First-Search"><a href="#1-2-深度优先搜索-Depth-First-Search" class="headerlink" title="1.2 深度优先搜索 (Depth First Search)"></a>1.2 深度优先搜索 (Depth First Search)</h2><p><img src="image/image_oRgWt568My.png" alt=""></p><p>示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span> (node):</span><br><span class="line">  <span class="keyword">if</span> node <span class="keyword">in</span> visited:</span><br><span class="line">    <span class="comment"># already visited</span></span><br><span class="line">    <span class="keyword">return</span> </span><br><span class="line">    </span><br><span class="line">  visited.add(node)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># process current node</span></span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line">  dfs(node.left)</span><br><span class="line">  dfs(node.right)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>递归写法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">visited = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">node, visited</span>):</span><br><span class="line">  visited.add(node)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># process currend node here</span></span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> next_node <span class="keyword">in</span> node.children():</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> next_node <span class="keyword">in</span> visited:</span><br><span class="line">      dfs(next_node, visited)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>非递归写法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">self, tree</span>):</span><br><span class="line">  <span class="keyword">if</span> tree.root <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">return</span> []</span><br><span class="line">  visited, stack = [], [tree.root]</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> stack:</span><br><span class="line">    node = stack.pop()</span><br><span class="line">    visited.add(node)</span><br><span class="line">    </span><br><span class="line">    process(node)</span><br><span class="line">    nodes = generate_related_nodes(node)</span><br><span class="line">    stack.push(nodes)</span><br><span class="line">    </span><br><span class="line">  <span class="comment"># other processing work</span></span><br><span class="line">  ...</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>C++代码示例</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 递归写法</span></span><br><span class="line">map&lt;<span class="type">int</span>, <span class="type">int</span>&gt; visited;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(Node* root)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// terminator</span></span><br><span class="line">    <span class="keyword">if</span> (!root) </span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> (visited.<span class="built_in">count</span>(root-&gt;val)) &#123;</span><br><span class="line">        <span class="comment">// already visited</span></span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    visited[root-&gt;val] = <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// process current node here.</span></span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; root-&gt;children.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">        <span class="built_in">dfs</span>(root-&gt;children[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//非递归写法：</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(Node* root)</span> </span>&#123;</span><br><span class="line">    map&lt;<span class="type">int</span>, <span class="type">int</span>&gt; visited;</span><br><span class="line">    <span class="keyword">if</span>(!root) </span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">        </span><br><span class="line">    stack&lt;Node*&gt; stackNode;</span><br><span class="line">    stackNode.<span class="built_in">push</span>(root);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (!stackNode.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">        Node* node = stackNode.<span class="built_in">top</span>();</span><br><span class="line">        stackNode.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">if</span> (visited.<span class="built_in">count</span>(node-&gt;val)) </span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        visited[node-&gt;val] = <span class="number">1</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = node-&gt;children.<span class="built_in">size</span>() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; --i) &#123;</span><br><span class="line">            stackNode.<span class="built_in">push</span>(node-&gt;children[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-3-广度优先搜索-Breadth-First-Search"><a href="#1-3-广度优先搜索-Breadth-First-Search" class="headerlink" title="1.3 广度优先搜索 (Breadth First Search)"></a>1.3 广度优先搜索 (Breadth First Search)</h2><p>BFS与DFS对比</p><p><img src="image/image_zUjQLyHrTx.png" alt=""></p><p>BFS示例代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bfs</span>(<span class="params">graph, start, end</span>):</span><br><span class="line">  </span><br><span class="line">  queue = []</span><br><span class="line">  queue.append([start])</span><br><span class="line">  visited.add(start)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> queue:</span><br><span class="line">    node = queue.pop()</span><br><span class="line">    visited.add(node)</span><br><span class="line">    </span><br><span class="line">    process(node)</span><br><span class="line">    nodes = generate_related_nodes(node)</span><br><span class="line">    queue.push(nodes)</span><br><span class="line">    </span><br><span class="line">  <span class="comment"># other processing work</span></span><br><span class="line">  ...</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>C++代码</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">bfs</span><span class="params">(Node* root)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    map&lt;<span class="type">int</span>, <span class="type">int</span>&gt; visited;</span><br><span class="line">    <span class="keyword">if</span>(!root) </span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">        </span><br><span class="line">    queue&lt;Node*&gt; queueNode;</span><br><span class="line">    queueNode.<span class="built_in">push</span>(root);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (!queueNode.<span class="built_in">empty</span>())  &#123;</span><br><span class="line">        Node* node = queueNode.<span class="built_in">top</span>();</span><br><span class="line">        queueNode.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">if</span> (visited.<span class="built_in">count</span>(node-&gt;val)) </span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        visited[node-&gt;val] = <span class="number">1</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; node-&gt;children.<span class="built_in">size</span>(); ++i)  &#123;</span><br><span class="line">            queueNode.<span class="built_in">push</span>(node-&gt;children[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="2-示例题目"><a href="#2-示例题目" class="headerlink" title="2.示例题目"></a>2.示例题目</h1><h2 id="2-1-二叉树层序遍历"><a href="#2-1-二叉树层序遍历" class="headerlink" title="2.1 二叉树层序遍历"></a>2.1 二叉树层序遍历</h2><p><a href="https://leetcode.cn/problems/binary-tree-level-order-traversal/description/" title="102. 二叉树的层序遍历 - 力扣（LeetCode）">102. 二叉树的层序遍历 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">给你二叉树的根节点 root ，返回其节点值的 层序遍历 。 （即逐层地，从左到右访问所有节点）。</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.BFS</span></span><br><span class="line"><span class="comment">// 2.DFS， 遍历时，增加深度信息，最后输出按照深度来分类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1.BFS</span></span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">levelOrder1</span>(TreeNode* root) &#123;</span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; ans;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        std::queue&lt;TreeNode*&gt; queue;</span><br><span class="line">        queue.<span class="built_in">push</span>(root);</span><br><span class="line">        TreeNode* node = <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (!queue.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            std::vector&lt;<span class="type">int</span>&gt; level_node;</span><br><span class="line">            <span class="type">int</span> node_count = queue.<span class="built_in">size</span>();</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; node_count; i++) &#123;</span><br><span class="line">                node = queue.<span class="built_in">front</span>();</span><br><span class="line">                queue.<span class="built_in">pop</span>();</span><br><span class="line">                level_node.<span class="built_in">push_back</span>(node-&gt;val);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (node-&gt;left) &#123;</span><br><span class="line">                    queue.<span class="built_in">push</span>(node-&gt;left);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (node-&gt;right) &#123;</span><br><span class="line">                    queue.<span class="built_in">push</span>(node-&gt;right);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(std::<span class="built_in">move</span>(level_node));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DFS</span></span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">levelOrder</span>(TreeNode* root) &#123;</span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; ans;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(root, <span class="number">0</span>, ans);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(TreeNode* root, <span class="type">int</span> depth, std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 说明该添加下一层了</span></span><br><span class="line">        <span class="keyword">if</span> (depth &gt;= ans.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(std::vector&lt;<span class="type">int</span>&gt;&#123;&#125;);</span><br><span class="line">        &#125;</span><br><span class="line">        ans[depth].<span class="built_in">push_back</span>(root-&gt;val);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(root-&gt;left, depth + <span class="number">1</span>, ans);</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(root-&gt;right, depth + <span class="number">1</span>, ans);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-2-最小基因变化"><a href="#2-2-最小基因变化" class="headerlink" title="2.2 最小基因变化"></a>2.2 最小基因变化</h2><p><a href="https://leetcode.cn/problems/minimum-genetic-mutation/" title="433. 最小基因变化 - 力扣（LeetCode）">433. 最小基因变化 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">基因序列可以表示为一条由 8 个字符组成的字符串，其中每个字符都是 <span class="string">&#x27;A&#x27;</span>、<span class="string">&#x27;C&#x27;</span>、<span class="string">&#x27;G&#x27;</span> 和 <span class="string">&#x27;T&#x27;</span> 之一。</span><br><span class="line"></span><br><span class="line">假设我们需要调查从基因序列 start 变为 end 所发生的基因变化。一次基因变化就意味着这个基因序列中的一个字符发生了变化。</span><br><span class="line"></span><br><span class="line">例如，<span class="string">&quot;AACCGGTT&quot;</span> --&gt; <span class="string">&quot;AACCGGTA&quot;</span> 就是一次基因变化。</span><br><span class="line">另有一个基因库 bank 记录了所有有效的基因变化，只有基因库中的基因才是有效的基因序列。（变化后的基因必须位于基因库 bank 中）</span><br><span class="line"></span><br><span class="line">给你两个基因序列 start 和 end ，以及一个基因库 bank ，请你找出并返回能够使 start 变化为 end 所需的最少变化次数。如果无法完成此基因变化，返回 -1 。</span><br><span class="line"></span><br><span class="line">注意：起始基因序列 start 默认是有效的，但是它并不一定会出现在基因库中。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// BFS</span></span><br><span class="line">    <span class="comment">// 经过分析可知，题目要求将一个基因序列 A 变化至另一个基因序列 B，需要满足以下条件：</span></span><br><span class="line">    <span class="comment">// 1.序列 A 与 序列 B 之间只有一个字符不同；</span></span><br><span class="line">    <span class="comment">// 2.变化字符只能从 ‘A’, ‘C’, ‘G’, ‘T’中进行选择；</span></span><br><span class="line">    <span class="comment">// 3.变换后的序列 B 一定要在字符串数组 bank中。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 步骤如下：</span></span><br><span class="line">    <span class="comment">// 1.如果 start 与 end相等，此时直接返回 0；如果最终的基因序列不在 bank中，则此时按照题意要求，无法生成，直接返回 −1；</span></span><br><span class="line">    <span class="comment">// 2.首先我们将可能变换的基因 s 从队列中取出，按照上述的变换规则，尝试所有可能的变化后的基因，</span></span><br><span class="line">    <span class="comment">//   比如一个 AACCGGTA，我们依次尝试改变基因 s 的一个字符，并尝试所有可能的基因变化序列 s0,s1,s2,⋯ ,si,⋯ ,s23，</span></span><br><span class="line">    <span class="comment">//   变化一次最多可能会生成 3×8=24 种不同的基因序列。</span></span><br><span class="line">    <span class="comment">// 3.需要检测当前生成的基因序列的合法性 si，首先利用哈希表检测 si 是否在数组 bank 中，</span></span><br><span class="line">    <span class="comment">//   如果是则认为该基因合法，否则改变化非法直接丢弃；其次还需要用哈希表记录已经遍历过的基因序列，</span></span><br><span class="line">    <span class="comment">//   如果该基因序列已经遍历过，则此时直接跳过；如果合法且未遍历过的基因序列，则将其加入到队列中。</span></span><br><span class="line">    <span class="comment">// 4.如果当前变换后的基因序列与 end 相等，则此时我们直接返回最小的变化次数即可；</span></span><br><span class="line">    <span class="comment">//   如果队列中所有的元素都已经遍历完成还无法变成 end，则此时无法实现目标变化，返回 −1。</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minMutation</span><span class="params">(string startGene, string endGene, vector&lt;string&gt;&amp; bank)</span> </span>&#123;</span><br><span class="line">        std::unordered_set&lt;std::string&gt; cnt;</span><br><span class="line">        std::unordered_set&lt;std::string&gt; visited;</span><br><span class="line">        <span class="type">char</span> keys[<span class="number">4</span>] = &#123;<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;G&#x27;</span>, <span class="string">&#x27;T&#x27;</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; b : bank) &#123;</span><br><span class="line">            cnt.<span class="built_in">emplace</span>(b);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (startGene == endGene) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!cnt.<span class="built_in">count</span>(endGene)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        std::queue&lt;std::string&gt; queue;</span><br><span class="line">        queue.<span class="built_in">emplace</span>(startGene);</span><br><span class="line">        <span class="type">int</span> step = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (!queue.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            <span class="type">int</span> size = queue.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">                std::string curr = queue.<span class="built_in">front</span>();</span><br><span class="line">                queue.<span class="built_in">pop</span>();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">8</span>; j++) &#123;</span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; <span class="number">4</span>; k++) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (keys[k] != curr[j]) &#123;</span><br><span class="line">                            std::string next = curr;</span><br><span class="line">                            next[j] = keys[k];</span><br><span class="line"></span><br><span class="line">                            <span class="keyword">if</span> (!visited.<span class="built_in">count</span>(next) &amp;&amp; cnt.<span class="built_in">count</span>(next)) &#123;</span><br><span class="line">                                <span class="keyword">if</span> (next == endGene) &#123;</span><br><span class="line">                                    <span class="keyword">return</span> step;</span><br><span class="line">                                &#125;</span><br><span class="line">                                queue.<span class="built_in">emplace</span>(next);</span><br><span class="line">                                visited.<span class="built_in">emplace</span>(next);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            step++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-3-BFS括号生成"><a href="#2-3-BFS括号生成" class="headerlink" title="2.3 BFS括号生成"></a>2.3 BFS括号生成</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SolutionDFS</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// DFS</span></span><br><span class="line">    <span class="function">vector&lt;string&gt; <span class="title">generateParenthesis</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        std::vector&lt;std::string&gt; ans;</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(<span class="string">&quot;&quot;</span>, n, n, ans);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(std::string curr_str, <span class="type">int</span> left, <span class="type">int</span> right, std::vector&lt;std::string&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 终止条件，</span></span><br><span class="line">        <span class="keyword">if</span> (left == <span class="number">0</span> &amp;&amp; right == <span class="number">0</span>) &#123;</span><br><span class="line">            ans.<span class="built_in">emplace_back</span>(curr_str);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注意剪枝，（左括号个数严格大于右括号个数）</span></span><br><span class="line">        <span class="keyword">if</span> (left &gt; right) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (left &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(curr_str + <span class="string">&quot;(&quot;</span>, left - <span class="number">1</span>, right, ans);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (right &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(curr_str + <span class="string">&quot;(&quot;</span>, left, right - <span class="number">1</span>, ans);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BFS实现，需要手动构造结点类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Node</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Node</span> (std::string str, <span class="type">int</span> left, <span class="type">int</span> right) &#123;</span><br><span class="line">        m_str = str;</span><br><span class="line">        m_left = left;</span><br><span class="line">        m_right = right;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 当前得到的字符串</span></span><br><span class="line">    std::string m_str;</span><br><span class="line">    <span class="comment">// 左括号剩余个数</span></span><br><span class="line">    <span class="type">int</span> m_left;</span><br><span class="line">    <span class="comment">// 右括号剩余个数</span></span><br><span class="line">    <span class="type">int</span> m_right;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// BFS</span></span><br><span class="line">    <span class="function">vector&lt;string&gt; <span class="title">generateParenthesis</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        std::vector&lt;std::string&gt; ans;</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        std::queue&lt;Node&gt; queue;</span><br><span class="line">        queue.<span class="built_in">push</span>(<span class="built_in">Node</span>(<span class="string">&quot;&quot;</span>, n, n));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(!queue.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            Node curr_node = queue.<span class="built_in">front</span>();</span><br><span class="line">            queue.<span class="built_in">pop</span>();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (curr_node.m_left == <span class="number">0</span> &amp;&amp; curr_node.m_right == <span class="number">0</span>) &#123;</span><br><span class="line">                ans.<span class="built_in">emplace_back</span>(curr_node.m_str);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (curr_node.m_left &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                queue.<span class="built_in">push</span>(<span class="built_in">Node</span>(curr_node.m_str + <span class="string">&quot;(&quot;</span>, curr_node.m_left - <span class="number">1</span>, curr_node.m_right));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (curr_node.m_right &gt; <span class="number">0</span> &amp;&amp; curr_node.m_left &lt; curr_node.m_right) &#123;</span><br><span class="line">                queue.<span class="built_in">push</span>(<span class="built_in">Node</span>(curr_node.m_str + <span class="string">&quot;)&quot;</span>, curr_node.m_left, curr_node.m_right - <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-4-岛屿数量（高频）"><a href="#2-4-岛屿数量（高频）" class="headerlink" title="2.4 岛屿数量（高频）"></a>2.4 岛屿数量（高频）</h2><p><a href="https://leetcode.cn/problems/number-of-islands/description/" title="200. 岛屿数量 - 力扣（LeetCode）">200. 岛屿数量 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">给你一个由 <span class="string">&#x27;1&#x27;</span>（陆地）和 <span class="string">&#x27;0&#x27;</span>（水）组成的的二维网格，请你计算网格中岛屿的数量。</span><br><span class="line"></span><br><span class="line">岛屿总是被水包围，并且每座岛屿只能由水平方向和或竖直方向上相邻的陆地连接形成。</span><br><span class="line"></span><br><span class="line">此外，你可以假设该网格的四条边均被水包围。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1.洪水算法(填海造陆)</span></span><br><span class="line">    <span class="comment">//   遍历地图，遇到一个1，将1进行BFS/DFS遍历，将周围的1全部变为0，操作一次记为1</span></span><br><span class="line">    <span class="comment">//   操作次数，就是岛屿的数量</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">numIslands</span><span class="params">(vector&lt;vector&lt;<span class="type">char</span>&gt;&gt;&amp; grid)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> island_num = <span class="number">0</span>;</span><br><span class="line">        m_grid = grid;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m_grid.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; m_grid[i].<span class="built_in">size</span>(); j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (m_grid[i][j] == <span class="string">&#x27;0&#x27;</span>) &#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// if (this-&gt;sink(i, j)) &#123;</span></span><br><span class="line">                <span class="comment">//     island_num++;</span></span><br><span class="line">                <span class="comment">// &#125;</span></span><br><span class="line">                island_num += <span class="keyword">this</span>-&gt;<span class="built_in">sink</span>(i, j);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> island_num;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">sink</span><span class="params">(<span class="type">int</span> i, <span class="type">int</span> j)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (m_grid[i][j] == <span class="string">&#x27;0&#x27;</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// i, j = &quot;1&quot;</span></span><br><span class="line">        m_grid[i][j] = <span class="string">&#x27;0&#x27;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; <span class="number">4</span>; k++) &#123;</span><br><span class="line">            <span class="type">int</span> x = i + m_dx[k];</span><br><span class="line">            <span class="type">int</span> y = j + m_dy[k];</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (x &gt;= <span class="number">0</span> &amp;&amp; x &lt; m_grid.<span class="built_in">size</span>() &amp;&amp; y &gt;= <span class="number">0</span> &amp;&amp; y &lt; m_grid[i].<span class="built_in">size</span>()) &#123;</span><br><span class="line">                <span class="keyword">if</span> (m_grid[x][y] == <span class="string">&#x27;0&#x27;</span>) &#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">this</span>-&gt;<span class="built_in">sink</span>(x, y);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 方向</span></span><br><span class="line">    <span class="type">int</span> m_dx[<span class="number">4</span>] = &#123;<span class="number">-1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">int</span> m_dy[<span class="number">4</span>] = &#123;<span class="number">0</span>, <span class="number">0</span>, <span class="number">-1</span>, <span class="number">1</span>&#125;;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;std::vector&lt;<span class="type">char</span>&gt;&gt; m_grid;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10 贪心算法</title>
      <link href="/dsa/leetcode/10_greedy/README/"/>
      <url>/dsa/leetcode/10_greedy/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-贪心算法"><a href="#1-贪心算法" class="headerlink" title="1.贪心算法"></a>1.贪心算法</h1><h2 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h2><p>贪心算法是一种在每一步选择中都采取在当前状态下最好或最优 (即最有利)的选择，从而希望导致结果是全局最好或最优的算法。</p><p>贪心算法与动态规划的不同在于它对每个子问题的解决方案都做出选择不能回退。动态规划则会保存以前的运算结果，并根据以前的结果对当前进行选择，有回退功能。</p><ul><li>贪心：当下做局部最优判断</li><li>回溯：能够回退</li><li>动态规划：最有判断 + 回退</li></ul><p>贪心法可以解决一些最优化问题，如: 求图中的最小生成树、求哈夫曼编码等。然而对于工程和生活中的问题，贪心法一般不能得到我们所要求的答案</p><p>一旦一个问题可以通过贪心法来解决，那么贪心法一般是解决这个问题的最好办法。由于贪心法的高效性以及其所求得的答案比较接近最优结果，贪心法也可以用作辅助算法或者直接解决一些要求结果不特别精确的问题。</p><h2 id="1-2-举例：coin-change"><a href="#1-2-举例：coin-change" class="headerlink" title="1.2 举例：coin change"></a>1.2 举例：coin change</h2><p><a href="https://leetcode.cn/problems/coin-change/description/" title="322. 零钱兑换 - 力扣（LeetCode）">322. 零钱兑换 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。</span><br><span class="line"></span><br><span class="line">计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。</span><br><span class="line"></span><br><span class="line">你可以认为每种硬币的数量是无限的。</span><br></pre></td></tr></table></figure><p>当硬币可选集合固定 : coins = [20, 10, 5, 1]，求最少可以几个硬币拼出综述，比如 total=36</p><p><img src="image/image_dnw5Oy3JyY.png" alt=""></p><p><strong>贪心法反例</strong>：</p><p>非整除关系的硬币，可选集合: coins=[10, 9, 1]，求评出总数为18最少需要几个硬币？</p><p><img src="image/image_ZbP7ogcp_p.png" alt=""></p><h2 id="1-3-适用贪心算法的场景"><a href="#1-3-适用贪心算法的场景" class="headerlink" title="1.3 适用贪心算法的场景"></a>1.3 适用贪心算法的场景</h2><p>简单地说，<strong>问题能够分解成子问题来解决，子问题的最优解能递推到最终问题的最优解</strong>。这种子问题最优解称为最优子结构。</p><p>贪心算法与动态规划的不同在于它对每个子问题的解决方案<strong>都做出选择不能回退</strong>。<strong>动态规划则会保存以前的运算结果，并根据以前的结果对当前进行选择，有回退功能</strong>。</p><h1 id="2-示例"><a href="#2-示例" class="headerlink" title="2.示例"></a>2.示例</h1><h2 id="2-1-分发饼干"><a href="#2-1-分发饼干" class="headerlink" title="2.1 分发饼干"></a>2.1 分发饼干</h2><p><a href="https://leetcode.cn/problems/assign-cookies/description/" title="455. 分发饼干 - 力扣（LeetCode）">455. 分发饼干 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">假设你是一位很棒的家长，想要给你的孩子们一些小饼干。但是，每个孩子最多只能给一块饼干。</span><br><span class="line"></span><br><span class="line">对每个孩子 i，都有一个胃口值 g[i]，这是能让孩子们满足胃口的饼干的最小尺寸；并且每块饼干 j，都有一个尺寸 s[j] 。如果 s[j] &gt;= g[i]，我们可以将这个饼干 j 分配给孩子 i ，这个孩子会得到满足。你的目标是尽可能满足越多数量的孩子，并输出这个最大数值。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 贪心算法，先排序，然后匹配两个升序的数组</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findContentChildren</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; g, vector&lt;<span class="type">int</span>&gt;&amp; s)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (g.<span class="built_in">size</span>() == <span class="number">0</span> || s.<span class="built_in">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> res;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 排序</span></span><br><span class="line">        std::<span class="built_in">sort</span>(g.<span class="built_in">begin</span>(), g.<span class="built_in">end</span>());</span><br><span class="line">        std::<span class="built_in">sort</span>(s.<span class="built_in">begin</span>(), s.<span class="built_in">end</span>());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历，匹配两个升序数组</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>; i &lt; g.<span class="built_in">size</span>() &amp;&amp; j &lt; s.<span class="built_in">size</span>(); ) &#123;</span><br><span class="line">            <span class="comment">// 可以满足胃口，把小饼干喂给小朋友</span></span><br><span class="line">            <span class="keyword">if</span> (g[i] &lt;= s[j]) &#123;</span><br><span class="line">                res++;</span><br><span class="line"></span><br><span class="line">                i++;</span><br><span class="line">                j++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 不满足胃口，查看下一块小饼干</span></span><br><span class="line">                j++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-2-最佳买卖股票时机"><a href="#2-2-最佳买卖股票时机" class="headerlink" title="2.2 最佳买卖股票时机"></a>2.2 最佳买卖股票时机</h2><p><a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-ii/description/" title="122. 买卖股票的最佳时机 II - 力扣（LeetCode）">122. 买卖股票的最佳时机 II - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数数组 prices ，其中 prices[i] 表示某支股票第 i 天的价格。</span><br><span class="line"></span><br><span class="line">在每一天，你可以决定是否购买和/或出售股票。你在任何时候 最多 只能持有 一股 股票。你也可以先购买，然后在 同一天 出售。</span><br><span class="line"></span><br><span class="line">返回 你能获得的 最大 利润 。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 贪心，如果第二个数字大于第一个数字，拿获得的总利润将是最大的</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">maxProfit</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; prices)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> max_profit = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; prices.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (prices[i] &gt; prices[i - <span class="number">1</span>]) &#123;</span><br><span class="line">                max_profit += prices[i] - prices[i - <span class="number">1</span>];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_profit;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="2-3-跳跃游戏"><a href="#2-3-跳跃游戏" class="headerlink" title="2.3 跳跃游戏"></a>2.3 跳跃游戏</h2><p><a href="https://leetcode.cn/problems/jump-game/description/" title="55. 跳跃游戏 - 力扣（LeetCode）">55. 跳跃游戏 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给你一个非负整数数组 nums ，你最初位于数组的 第一个下标 。数组中的每个元素代表你在该位置可以跳跃的最大长度。</span><br><span class="line"></span><br><span class="line">判断你是否能够到达最后一个下标，如果可以，返回 <span class="literal">true</span> ；否则，返回 <span class="literal">false</span> 。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2.贪心算法</span></span><br><span class="line"><span class="comment">// 从后往前贪心</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">canJump</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (nums.<span class="built_in">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> end_reachalbe = nums.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = nums.<span class="built_in">size</span>() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums[i] + i &gt;= end_reachalbe) &#123;</span><br><span class="line">            end_reachalbe = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> end_reachalbe == <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> algo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11 二分查找</title>
      <link href="/dsa/leetcode/11_binary_search/README/"/>
      <url>/dsa/leetcode/11_binary_search/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-二分查找"><a href="#1-二分查找" class="headerlink" title="1.二分查找"></a>1.二分查找</h1><h2 id="1-1二分查找的前提"><a href="#1-1二分查找的前提" class="headerlink" title="1.1二分查找的前提"></a>1.1二分查找的前提</h2><ol><li>目标函数单调性（单调递增或递减）</li><li>存在上下界（bounded）</li><li>能够通过索引访问（index accessible）</li></ol><h2 id="1-2-代码模板"><a href="#1-2-代码模板" class="headerlink" title="1.2 代码模板"></a>1.2 代码模板</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">left, right = <span class="number">0</span>, <span class="built_in">len</span>(array) - <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> left &lt;= right:</span><br><span class="line">  mid = (left + right) / <span class="number">2</span></span><br><span class="line">  <span class="keyword">if</span> array[mid] == target:</span><br><span class="line">    <span class="comment"># find the target</span></span><br><span class="line">    <span class="keyword">break</span> <span class="keyword">or</span> <span class="keyword">return</span> result</span><br><span class="line">  <span class="keyword">elif</span> array[mid] &lt; target:</span><br><span class="line">    left = mid + <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    right = mid - <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="1-3-示例"><a href="#1-3-示例" class="headerlink" title="1.3 示例"></a>1.3 示例</h2><p>在递增数组里 <code>[10,14,19,26,27,31,33,35,42,44]</code> 中， 查找<code>31</code></p><p><img src="image/image_1jB9EqpM8I.png" alt=""></p><h1 id="2-题目"><a href="#2-题目" class="headerlink" title="2.题目"></a>2.题目</h1><h2 id="2-1-x的平方根"><a href="#2-1-x的平方根" class="headerlink" title="2.1 x的平方根"></a>2.1 x的平方根</h2><p><a href="https://leetcode.cn/problems/sqrtx/description/" title="69. x 的平方根 - 力扣（LeetCode）">69. x 的平方根 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">给你一个非负整数 x ，计算并返回 x 的 算术平方根 。</span><br><span class="line"></span><br><span class="line">由于返回类型是整数，结果只保留 整数部分 ，小数部分将被 舍去 。</span><br><span class="line"></span><br><span class="line">注意：不允许使用任何内置指数函数和算符，例如 pow(x, 0.5) 或者 x ** 0.5 。</span><br></pre></td></tr></table></figure><h3 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.二分查找</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">mySqrt1</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (x == <span class="number">0</span> || x == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> left = <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> right = x;</span><br><span class="line">    <span class="type">int</span> ans = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">        <span class="comment">// 注意此处，主要是为了防止数据溢出</span></span><br><span class="line">        <span class="type">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> ((<span class="type">long</span> <span class="type">long</span>)mid * mid &gt; x) &#123;</span><br><span class="line">            right = mid - <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            left = mid + <span class="number">1</span>;</span><br><span class="line">            ans = mid;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="牛顿迭代法"><a href="#牛顿迭代法" class="headerlink" title="牛顿迭代法"></a>牛顿迭代法</h3><p><a href="https://www.beyond3d.com/content/articles/8/" title="Beyond3D - Origin of Quake3&#39;s Fast InvSqrt()">Beyond3D - Origin of Quake3’s Fast InvSqrt()</a></p><p>牛顿迭代法是一种可以用来快速求解函数零点的方法。</p><p>为了方便，用 $C$ 表示待求出平方根的那个整数。显然，$C$ 的平方根就是函数$y = f(x) = x^2 - C$的零点。</p><p>牛顿迭代法的本质是借助泰勒级数，从初始值开始快速向零点逼近。我们任取一个 $x_0$作为初始值，在每一步的迭代中，我们找到函数图像上的点$(x_i, f(x_i))$，过该点作一条斜率为该点导数 $f’(x_i)$的直线，与横轴的交点记为 $x_{i+1}$。$x_{i+1}$相较于 $x_i$而言距离零点更近。在经过多次迭代后，我们就可以得到一个距离零点非常接近的交点。下图给出了从 $x_0$开始迭代两次，得到 x_1和 $x_2$的过程。</p><p><img src="image/image_qkp1VQVS_0.png" alt=""></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2.牛顿迭代法</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">mySqrt</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (x == <span class="number">0</span> || x == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">long</span> ans = x;</span><br><span class="line">    <span class="keyword">while</span> (ans * ans &gt; x) &#123;</span><br><span class="line">        ans = (ans + x / ans) / <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-2-搜索选择排序数组"><a href="#2-2-搜索选择排序数组" class="headerlink" title="2.2 搜索选择排序数组"></a>2.2 搜索选择排序数组</h2><p><a href="https://leetcode.cn/problems/search-in-rotated-sorted-array/description/" title="33. 搜索旋转排序数组 - 力扣（LeetCode）">33. 搜索旋转排序数组 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">整数数组 nums 按升序排列，数组中的值 互不相同 。</span><br><span class="line"></span><br><span class="line">在传递给函数之前，nums 在预先未知的某个下标 k（0 &lt;= k &lt; nums.length）上进行了 旋转，使数组变为 [nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]]（下标 从 0 开始 计数）。例如， [0,1,2,4,5,6,7] 在下标 3 处经旋转后可能变为 [4,5,6,7,0,1,2] 。</span><br><span class="line"></span><br><span class="line">给你 旋转后 的数组 nums 和一个整数 target ，如果 nums 中存在这个目标值 target ，则返回它的下标，否则返回 -1 。</span><br><span class="line"></span><br><span class="line">你必须设计一个时间复杂度为 O(<span class="built_in">log</span> n) 的算法解决此问题。</span><br></pre></td></tr></table></figure><p>使用二分查找：</p><ul><li>如果<code>target</code>在<code>[mid+1, high]</code>序列中，则<code>low=mid+1</code>，否则，<code>high=mid</code>，关键是如何判断<code>target</code>在<code>[mid+1, high]</code>序列中，具体判断如下：</li><li>当<code>[0, mid]</code>序列是升序：<code>nums[0] ≤ nums[mid]</code>，当t<code>arget&gt;nums[mid] || target &lt;nums[0]</code>，则向后规约；</li><li>当<code>[0, mid]</code>序列存在旋转位：<code>nums[0] &gt; nums[mid]</code>，当<code>target&lt;nums[0] &amp;&amp; target &gt;nums[mid]</code>，则向后规约；</li><li>其他情况就是向前规约了</li></ul><p>循环判断，直到排除到只剩一个元素时，退出循环，如果该元素和target相同，直接返回下标，否则返回-1.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">search</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right = nums.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> ans = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="comment">// 注意此处，主要是为了防止数据溢出</span></span><br><span class="line">            <span class="type">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (nums[mid] == target) &#123;</span><br><span class="line">                ans = mid;</span><br><span class="line">            &#125; </span><br><span class="line">            <span class="comment">// 当[0,mid]有序时，向后规约条件</span></span><br><span class="line">            <span class="keyword">if</span> (nums[<span class="number">0</span>] &lt;= nums[mid] &amp;&amp; (target &gt; nums[mid] || target &lt; nums[<span class="number">0</span>])) &#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 当[0, mid]发生旋转时，向后规约条件</span></span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (target &gt; nums[mid] &amp;&amp; target &lt; nums[<span class="number">0</span>]) &#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> left == right &amp;&amp; nums[left] == target ? left : <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>12 动态规划</title>
      <link href="/dsa/leetcode/12_dp/README/"/>
      <url>/dsa/leetcode/12_dp/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-复习"><a href="#1-复习" class="headerlink" title="1.复习"></a>1.复习</h1><p>分治、回溯、递归、动态规划，没有本质上的区别，主要是一些小细节不一样</p><h2 id="1-1-递归"><a href="#1-1-递归" class="headerlink" title="1.1 递归"></a>1.1 递归</h2><p>递归代码模板：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">recursion</span>(<span class="params">level, param1, param2, ...</span>):</span><br><span class="line">  <span class="comment"># 1.recursion terminator (递归终止条件)</span></span><br><span class="line">  <span class="keyword">if</span> level &gt; MAX_LeVEL:</span><br><span class="line">    process_result</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2.process logic in current level (处理当前层逻辑)</span></span><br><span class="line">  process(level, data, ...)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3.drill down (下探到下一层)</span></span><br><span class="line">  self.recursion(level + <span class="number">1</span>, p1, ...)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 4.reverse the current level status if needed (清理当前层)</span></span><br></pre></td></tr></table></figure><h2 id="1-2-分治"><a href="#1-2-分治" class="headerlink" title="1.2 分治"></a>1.2 分治</h2><p><img src="image/image_gQDfAYjwtM.png" alt=""></p><p>分治代码模板：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">divide_conquer</span>(<span class="params">problem, param1, param2, ...</span>):</span><br><span class="line">  <span class="comment"># 1.recursion terminator (递归终止条件)</span></span><br><span class="line">  <span class="keyword">if</span> problem <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    print_result</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 2.prepare data (拆分问题)</span></span><br><span class="line">  data = prepare_data(problem)</span><br><span class="line">  subproblems = split_problem(problem, data)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3.conquer subproblems (调字问题的递归函数)</span></span><br><span class="line">  subresult1 = self.divide_conquer(subproblems[<span class="number">0</span>], p1, ...)</span><br><span class="line">  subresult1 = self.divide_conquer(subproblems[<span class="number">0</span>], p1, ...)</span><br><span class="line">  subresult1 = self.divide_conquer(subproblems[<span class="number">0</span>], p1, ...)</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 4.process and generate the final result (合并结果)</span></span><br><span class="line">  result = process_result(subresult1, subresult2, subresult3, ...)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 5.revert the current level status (回复当前层状态)</span></span><br></pre></td></tr></table></figure><h2 id="1-3-感触"><a href="#1-3-感触" class="headerlink" title="1.3 感触"></a>1.3 感触</h2><p><strong>本质</strong>：<strong>寻找重复性 → 计算机指令集</strong></p><ol><li>人肉递归低效、很累</li><li>找到最近最简方法，将其拆解成可重复解决的问题</li><li>数学归纳法思维 （抵制人肉递归的诱惑）</li></ol><h1 id="2-动态规划-Dynamic-Programming"><a href="#2-动态规划-Dynamic-Programming" class="headerlink" title="2.动态规划 (Dynamic Programming)"></a>2.动态规划 (Dynamic Programming)</h1><h2 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h2><ol><li>Wiki定义：<a href="https://en.wikipedia.org/wiki/Dynamic_programming" title="Dynamic programming - Wikipedia">Dynamic programming - Wikipedia</a></li><li>In both contexts it refers to** simplifying a complicated problem by breaking it down into simpler sub-problems** in a <a href="https://en.wikipedia.org/wiki/Recursion" title="recursive">recursive</a> manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively.</li><li>Divide &amp; Conquer + Optimal substructure （分治 + 最优子结构）</li></ol><h2 id="2-2-关键点"><a href="#2-2-关键点" class="headerlink" title="2.2 关键点"></a>2.2 关键点</h2><ol><li>动态规划 和 递归或者分治 没有根本上的区别（关键看有无最优子结构）</li><li><strong>共性：找到重复问题</strong></li><li>差异性：最优子结构、中途可以<strong>淘汰</strong>次优结构</li></ol><h2 id="2-3-DP关键点"><a href="#2-3-DP关键点" class="headerlink" title="2.3 DP关键点"></a>2.3 DP关键点</h2><ol><li><strong>化繁为简</strong>：最优子结构，分成各种子问题：$opt[n]= best_{of}(opt [n-1], opt[n-2], \ldots)$</li><li><strong>定义状态空间</strong>：<strong>存储中间过程</strong>：$opt[i]$</li><li><strong>递推公式</strong>（状态转移方程或DP方程）<ol><li>Fib : $opt[i] = opt[n-1] + opt[n-2]$</li><li>二位路径 : $\operatorname{opt}[i, j]=\operatorname{opt}[i+1][j]+\operatorname{opt}[i][j+1]$(且判断$a[i, j]$ 是否空地)</li></ol></li></ol><h2 id="2-4-动态规划小结"><a href="#2-4-动态规划小结" class="headerlink" title="2.4 动态规划小结"></a>2.4 动态规划小结</h2><ol><li>打破自己的思维惯性，形成机器思维</li><li>理解复杂逻辑的关键</li><li>也是职业进阶的要点要领</li></ol><h1 id="3-例题"><a href="#3-例题" class="headerlink" title="3.例题"></a>3.例题</h1><h2 id="3-1-Fibonacci数列"><a href="#3-1-Fibonacci数列" class="headerlink" title="3.1 Fibonacci数列"></a>3.1 Fibonacci数列</h2><p><a href="https://leetcode.cn/problems/fibonacci-number/" title="509. 斐波那契数 - 力扣（LeetCode）">509. 斐波那契数 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">斐波那契数 （通常用 F(n) 表示）形成的序列称为 斐波那契数列 。该数列由 0 和 1 开始，后面的每一项数字都是前面两项数字的和。也就是：</span><br><span class="line"></span><br><span class="line">F(0) = 0，F(1) = 1</span><br><span class="line">F(n) = F(n - 1) + F(n - 2)，其中 n &gt; 1</span><br><span class="line">给定 n ，请计算 F(n) 。</span><br></pre></td></tr></table></figure><h3 id="（1）递归"><a href="#（1）递归" class="headerlink" title="（1）递归"></a>（1）递归</h3><p><img src="image/image_BfgwSby2ep.png" alt=""></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> n &lt;= <span class="number">1</span> ? n : <span class="keyword">this</span>-&gt;<span class="built_in">fib</span>(n - <span class="number">1</span>) + <span class="keyword">this</span>-&gt;<span class="built_in">fib</span>(n - <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）记忆化搜索"><a href="#（2）记忆化搜索" class="headerlink" title="（2）记忆化搜索"></a>（2）记忆化搜索</h3><p><img src="image/image_QmBJq7zjjc.png" alt=""></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> n;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">memo</span><span class="params">(n+<span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">fib2</span>(n, memo);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib2</span><span class="params">(<span class="type">int</span> n, std::vector&lt;<span class="type">int</span>&gt;&amp; memo)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> n;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (memo[n] == <span class="number">0</span>) &#123;</span><br><span class="line">        memo[n] = <span class="keyword">this</span>-&gt;<span class="built_in">fib2</span>(n - <span class="number">1</span>, memo) + <span class="built_in">fib2</span>(n <span class="number">-2</span>, memo);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> memo[n];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）自底向上（Bottom-up）"><a href="#（3）自底向上（Bottom-up）" class="headerlink" title="（3）自底向上（Bottom up）"></a>（3）自底向上（Bottom up）</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> n;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">memo</span><span class="params">(n+<span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    memo[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    memo[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">2</span>; i &lt;= n; i++) &#123;</span><br><span class="line">        memo[i] = memo[i - <span class="number">1</span>] + memo[i - <span class="number">2</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> memo[n];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-2-不同路径"><a href="#3-2-不同路径" class="headerlink" title="3.2 不同路径"></a>3.2 不同路径</h2><p><a href="https://leetcode.cn/problems/unique-paths/description/" title="62. 不同路径 - 力扣（LeetCode）">62. 不同路径 - 力扣（LeetCode）</a></p><p><a href="https://leetcode.cn/problems/unique-paths-ii/description/" title="63. 不同路径 II - 力扣（LeetCode）">63. 不同路径 II - 力扣（LeetCode）</a></p><p><a href="https://leetcode.cn/problems/unique-paths-iii/description/" title="980. 不同路径 III - 力扣（LeetCode）">980. 不同路径 III - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。</span><br><span class="line"></span><br><span class="line">机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish”）。</span><br><span class="line"></span><br><span class="line">现在考虑网格中有障碍物。那么从左上角到右下角将会有多少条不同的路径？</span><br><span class="line"></span><br><span class="line">网格中的障碍物和空位置分别用 1 和 0 来表示。</span><br></pre></td></tr></table></figure><h3 id="（1）递归-1"><a href="#（1）递归-1" class="headerlink" title="（1）递归"></a>（1）递归</h3><p>绿色小人只能走到A或B，所以，可以分解成 $paths(start, end) = paths(A, end) + paths(B, end)$，转为子问题</p><p><img src="image/image_bhNYDh_UFr.png" alt=""></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">coutPaths</span><span class="params">(boolean[][], <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (!<span class="built_in">vaildSquare</span>(grid, row, col))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">isAtEnd</span>(grid, row, col)) </span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">coutPaths</span>(grid, row+<span class="number">1</span>, col) + <span class="built_in">coutPaths</span>(grid, row, col+<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">uniquePathsWithObstacles</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; obstacleGrid)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> m = obstacleGrid.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> n = obstacleGrid[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">cache</span><span class="params">(n * m, <span class="number">-1</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">count_path</span>(cache, obstacleGrid, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">count_path</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; cache, </span></span></span><br><span class="line"><span class="params"><span class="function">            std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt;&amp; grid, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 边界</span></span><br><span class="line">        <span class="keyword">if</span> (row &gt;= grid.<span class="built_in">size</span>() || col &gt;= grid[<span class="number">0</span>].<span class="built_in">size</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 障碍</span></span><br><span class="line">        <span class="keyword">if</span> (grid[row][col] == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 到达终点</span></span><br><span class="line">        <span class="keyword">if</span> (row == grid.<span class="built_in">size</span>() - <span class="number">1</span> &amp;&amp; col == grid[<span class="number">0</span>].<span class="built_in">size</span>() - <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 继续往右、往下递归调用</span></span><br><span class="line">        <span class="keyword">if</span> (cache[col * grid.<span class="built_in">size</span>() + row] != <span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> cache[col * grid.<span class="built_in">size</span>() + row];</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">int</span> res = <span class="keyword">this</span>-&gt;<span class="built_in">count_path</span>(cache, grid, row + <span class="number">1</span>, col) + <span class="keyword">this</span>-&gt;<span class="built_in">count_path</span>(cache, grid, row, col + <span class="number">1</span>);</span><br><span class="line">            cache[col * grid.<span class="built_in">size</span>() + row] = res;</span><br><span class="line">            <span class="keyword">return</span> res;</span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="状态转移方程（DP方程）"><a href="#状态转移方程（DP方程）" class="headerlink" title="状态转移方程（DP方程）"></a>状态转移方程（DP方程）</h3><p>$opt[i, j] = opt[i + 1, j] + opt[i, j+1]$</p><p>完整逻辑：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> a[i, j] = <span class="string">&#x27;空地&#x27;</span>:</span><br><span class="line">  opt[i, j] = opt[i + 1, j] + opt[i, j+1]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  opt[i,j] = 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="image/image_uXntoGDqDz.png" alt=""></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">uniquePathsWithObstacles</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; obstacleGrid)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> row = obstacleGrid.<span class="built_in">size</span>();</span><br><span class="line">    <span class="type">int</span> col = obstacleGrid[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">    std::vector&lt;std::vector&lt;<span class="type">long</span> <span class="type">long</span>&gt;&gt; <span class="built_in">dp</span>(row, std::<span class="built_in">vector</span>&lt;<span class="type">long</span> <span class="type">long</span>&gt;(col, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断终点是否有障碍物</span></span><br><span class="line">    dp[row - <span class="number">1</span>][col - <span class="number">1</span>] = (obstacleGrid[row - <span class="number">1</span>][col - <span class="number">1</span>] == <span class="number">1</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 处理最后一列</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = row - <span class="number">2</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="keyword">if</span> (obstacleGrid[i][col - <span class="number">1</span>] == <span class="number">1</span> || dp[i + <span class="number">1</span>][col - <span class="number">1</span>] == <span class="number">0</span>) &#123;</span><br><span class="line">            dp[i][col - <span class="number">1</span>] = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            dp[i][col - <span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 处理最后一行</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = col - <span class="number">2</span>; j &gt;= <span class="number">0</span>; j--) &#123;</span><br><span class="line">        <span class="keyword">if</span> (obstacleGrid[row - <span class="number">1</span>][j] == <span class="number">1</span> || dp[row - <span class="number">1</span>][j + <span class="number">1</span>] == <span class="number">0</span>) &#123;</span><br><span class="line">            dp[row - <span class="number">1</span>][j] = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            dp[row - <span class="number">1</span>][j] = <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = row - <span class="number">2</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = col - <span class="number">2</span>; j &gt;= <span class="number">0</span>; j--) &#123;</span><br><span class="line">            <span class="comment">// 如果当前格子是障碍物</span></span><br><span class="line">            <span class="keyword">if</span> (obstacleGrid[i][j] == <span class="number">1</span>) &#123;</span><br><span class="line">                dp[i][j] = <span class="number">0</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 路径总和: opt[i, j] = opt[i + 1, j] + opt[i, j+1]</span></span><br><span class="line">                dp[i][j] = dp[i + <span class="number">1</span>][j] + dp[i][j + <span class="number">1</span>];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dp[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-3-最长公共子序列"><a href="#3-3-最长公共子序列" class="headerlink" title="3.3 最长公共子序列"></a>3.3 最长公共子序列</h2><p><a href="https://leetcode.cn/problems/longest-common-subsequence/description/" title="1143. 最长公共子序列 - 力扣（LeetCode）">1143. 最长公共子序列 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">给定两个字符串 text1 和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在 公共子序列 ，返回 0 。</span><br><span class="line"></span><br><span class="line">一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。</span><br><span class="line"></span><br><span class="line">例如，<span class="string">&quot;ace&quot;</span> 是 <span class="string">&quot;abcde&quot;</span> 的子序列，但 <span class="string">&quot;aec&quot;</span> 不是 <span class="string">&quot;abcde&quot;</span> 的子序列。</span><br><span class="line">两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。</span><br></pre></td></tr></table></figure><p>方法1：暴力方法：生成字符串一的所有子序列，在字符串二中验证；生成方式：递归判断每一个字符是取还是不取</p><p>方法2：找重复性：</p><p>两个字符串分别为一个二维数组的行和列，数组中的数组为当前行列之前字符串的公共子串个数；</p><ul><li>初始时，第一行和第一列可以先求出来；</li><li>如3行6列的数值3，表示”ABAZDC” 和 “BAC” 的最长子序列， 发现两个字符串的最后一个字符是一样的，可以转换成求 “ABAZD” 和 “BA” 这两个序列的最长子序列 <code>+1</code>，即 <code>2+1=3</code></li><li>DP方程：</li><li>$if(s_1[n-1] ≠ s_2[n-1]) ~:~ LCS[s_1, s_2] = Max(LSC[s_1 - 1, s_2], LSC[s_1, s_2-1])$</li><li>$if(s_1[n-1] == s_2[n-1]) ~:~ LCS[s_1, s_2] = LSC[s_1 - 1, s_2 - 1] + 1$</li></ul><p><img src="image/image__vfnOHPvP4.png" alt=""></p><p><img src="image/image_C2Kk2ZE6dy.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestCommonSubsequence</span>(<span class="params">self, text1: <span class="built_in">str</span>, text2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> text1 <span class="keyword">or</span> <span class="keyword">not</span> text2:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        m = <span class="built_in">len</span>(text1)</span><br><span class="line">        n = <span class="built_in">len</span>(text2)</span><br><span class="line">        dp = [[<span class="number">0</span>]*(n + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> text1[i - <span class="number">1</span>] == text2[j - <span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i][j - <span class="number">1</span>], dp[i - <span class="number">1</span>][j])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[m][n]</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">longestCommonSubsequence</span><span class="params">(string text1, string text2)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> m = text1.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> n = text2.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (m == <span class="number">0</span> || n == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(m + <span class="number">1</span>, std::<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n + <span class="number">1</span>, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (text1.<span class="built_in">at</span>(i - <span class="number">1</span>) == text2.<span class="built_in">at</span>(j - <span class="number">1</span>)) &#123;</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[i][j] = std::<span class="built_in">max</span>(dp[i][j - <span class="number">1</span>], dp[i - <span class="number">1</span>][j]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[m][n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="4-实战题目"><a href="#4-实战题目" class="headerlink" title="4.实战题目"></a>4.实战题目</h1><h2 id="4-1-爬楼梯问题"><a href="#4-1-爬楼梯问题" class="headerlink" title="4.1 爬楼梯问题"></a>4.1 爬楼梯问题</h2><h3 id="（1）一次可以上1个或2个台阶"><a href="#（1）一次可以上1个或2个台阶" class="headerlink" title="（1）一次可以上1个或2个台阶"></a>（1）一次可以上1个或2个台阶</h3><blockquote><p>有一个楼梯，总共有n个台阶。每一次，可以上一个台阶，也可以上两个台阶。问：爬上这样一个楼梯，一共有多少种不同的方法？</p></blockquote><p>dp方程：$f(n) = f(n-1) + f(n-2)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">climbStairs</span>(<span class="params">n</span>):</span><br><span class="line">  <span class="keyword">if</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">return</span> n</span><br><span class="line">  dp = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)]</span><br><span class="line">  dp[<span class="number">0</span>], dp[<span class="number">1</span>] = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n+<span class="number">1</span>):</span><br><span class="line">    dp[i] = dp[i - <span class="number">1</span>] + dp[i - <span class="number">2</span>]</span><br><span class="line">  <span class="keyword">return</span> dp[-<span class="number">1</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>时间复杂度O(n), 空间复杂度O(n)。  空间复杂度可以优化成O(1)，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">climbStairs</span>(<span class="params">n</span>):</span><br><span class="line">  <span class="keyword">if</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">return</span> n</span><br><span class="line">  a, b, c = <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n+<span class="number">1</span>):</span><br><span class="line">    c = a + b</span><br><span class="line">    a = b</span><br><span class="line">    b = c</span><br><span class="line">  <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><h3 id="（2）一次可以上1个、2个和3个台阶"><a href="#（2）一次可以上1个、2个和3个台阶" class="headerlink" title="（2）一次可以上1个、2个和3个台阶"></a>（2）一次可以上1个、2个和3个台阶</h3><blockquote><p>有一个楼梯，总共有n个台阶。每一次，可以上一个台阶，可以上二个台阶，也可以上三个台阶。问：爬上这样一个楼梯，一共有多少种不同的方法？</p></blockquote><p>dp方程：$f(n) = f(n-1) + f(n-2) + f(n-3)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">climbStairs</span>(<span class="params">n</span>):</span><br><span class="line">  <span class="keyword">if</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">return</span> n</span><br><span class="line">  dp = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)]</span><br><span class="line">  dp[<span class="number">0</span>], dp[<span class="number">1</span>], dp[<span class="number">2</span>] = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span> </span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>, n+<span class="number">1</span>):</span><br><span class="line">    dp[i] = dp[i - <span class="number">1</span>] + dp[i - <span class="number">2</span>] + dp[i - <span class="number">3</span>]</span><br><span class="line">  <span class="keyword">return</span> dp[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>时间复杂度O(n), 空间复杂度O(n)。  空间复杂度可以优化成O(1)，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">climbStairs</span>(<span class="params">n</span>):</span><br><span class="line">  <span class="keyword">if</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">return</span> n</span><br><span class="line">  a, b, c, d, e = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n+<span class="number">1</span>):</span><br><span class="line">    e = b + c + d</span><br><span class="line">    a = b</span><br><span class="line">    b = c</span><br><span class="line">    c = d</span><br><span class="line">    d = e</span><br><span class="line">  <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><h3 id="（3）爬任意台阶"><a href="#（3）爬任意台阶" class="headerlink" title="（3）爬任意台阶"></a>（3）爬任意台阶</h3><blockquote><p>有一个楼梯，总共有n个台阶。每次可以走任意台阶。问：爬上这样一个楼梯，一共有多少种不同的方法？</p></blockquote><p>dp方程：</p><ul><li>$f(n) = f(n-1) + f(n-2) + … + f(3) + f(2) + f(1)$</li><li>$ f(n-1) = f(n-2) + … + f(3) + f(2) + f(1)  $</li><li>则，$ f(n) - f(n-1) = f(n-1)  $→$f(n) = 2 * f(n-1)$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">climbStairs</span>(<span class="params">n</span>):</span><br><span class="line">  <span class="keyword">if</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">return</span> n</span><br><span class="line">  dp = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)]</span><br><span class="line">  dp[<span class="number">0</span>], dp[<span class="number">1</span>] = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n+<span class="number">1</span>):</span><br><span class="line">    dp[i] = <span class="number">2</span> * dp[i - <span class="number">1</span>]</span><br><span class="line">  <span class="keyword">return</span> dp[-<span class="number">1</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>时间复杂度O(n), 空间复杂度O(n)。</p><p>继续推导：$f(n) = 2 <em> f(n-1) = 2 </em> 2 <em> f(n-2) = 2 </em> 2 <em> 2 </em> f(n-3) = 2^{(n-1)} * f(1) = 2^{(n-1)}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">climbStairs</span>(<span class="params">n</span>):</span><br><span class="line">  <span class="keyword">if</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">return</span> n</span><br><span class="line">  <span class="keyword">return</span> <span class="number">2</span>**(n - <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="4-2-三角形最小路径和"><a href="#4-2-三角形最小路径和" class="headerlink" title="4.2 三角形最小路径和"></a>4.2 三角形最小路径和</h2><p><a href="https://leetcode.cn/problems/triangle/description/" title="120. 三角形最小路径和 - 力扣（LeetCode）">120. 三角形最小路径和 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给定一个三角形 triangle ，找出自顶向下的最小路径和。</span><br><span class="line"></span><br><span class="line">每一步只能移动到下一行中相邻的结点上。相邻的结点 在这里指的是 下标 与 上一层结点下标 相同或者等于 上一层结点下标 + 1 的两个结点。也就是说，如果正位于当前行的下标 i ，那么下一步可以移动到下一行的下标 i 或 i + 1 。</span><br></pre></td></tr></table></figure><p>求解方法：</p><ol><li>暴力方法：递归，n层 : left or right，$O(2^n)$</li><li>DP<ol><li>重复性（分治）：$problem(i, j) = min(sub(i+1, j) + sub(i+1, j+1)) + a(i, j)$</li><li>定义状态数组：$f[i, j]$</li><li>DP方程：$f(i, j) = min(f(i+1, j), f(i + 1, j + 1)) + a[i, j]$</li></ol></li></ol><p>DP python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minimumTotal</span>(<span class="params">self, triangle: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 注意初始化，不开新的数组也可以，直接将 triangle 当作 dp</span></span><br><span class="line">        dp = triangle</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(triangle)-<span class="number">2</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(triangle[i])):</span><br><span class="line">                <span class="comment"># 之前依据初始化过了，直接累加</span></span><br><span class="line">                dp[i][j] += <span class="built_in">min</span>(dp[i + <span class="number">1</span>][j], dp[i + <span class="number">1</span>][j + <span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>DP C++</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minimumTotal</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; triangle)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 注意初始化，不开新的数组也可以，直接将 triangle 当作 dp</span></span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(triangle);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = dp.<span class="built_in">size</span>() - <span class="number">2</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = dp[i].<span class="built_in">size</span>() - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j--) &#123;</span><br><span class="line">                <span class="comment">// 之前依据初始化过了，直接累加</span></span><br><span class="line">                dp[i][j] += std::<span class="built_in">min</span>(dp[i + <span class="number">1</span>][j], dp[i + <span class="number">1</span>][j + <span class="number">1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>暴力递归</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 2. 暴力方法：递归，n层 : left or right，2^n</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minimumTotal</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; triangle)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> size = triangle.<span class="built_in">size</span>();</span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">memo</span>(size, std::<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(size, <span class="number">-1</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">bfs</span>(<span class="number">0</span>, <span class="number">0</span>, memo, triangle);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">bfs</span><span class="params">(<span class="type">int</span> level, <span class="type">int</span> c, std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt;&amp; memo, </span></span></span><br><span class="line"><span class="params"><span class="function">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt;&amp; triangle)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (memo[level][c] != <span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> memo[level][c];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (level == triangle.<span class="built_in">size</span>() - <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> memo[level][c] = triangle[level][c];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> left = <span class="keyword">this</span>-&gt;<span class="built_in">bfs</span>(level + <span class="number">1</span>, c, memo, triangle);</span><br><span class="line">        <span class="type">int</span> right = <span class="keyword">this</span>-&gt;<span class="built_in">bfs</span>(level + <span class="number">1</span>, c + <span class="number">1</span>, memo, triangle);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> memo[level][c] = std::<span class="built_in">min</span>(left, right) + triangle[level][c];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="4-3-最大子序列和"><a href="#4-3-最大子序列和" class="headerlink" title="4.3 最大子序列和"></a>4.3 最大子序列和</h2><p><a href="https://leetcode.cn/problems/maximum-subarray/description/" title="53. 最大子数组和 - 力扣（LeetCode）">53. 最大子数组和 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数数组 nums ，请你找出一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。</span><br><span class="line"></span><br><span class="line">子数组 是数组中的一个连续部分。</span><br></pre></td></tr></table></figure><p>求解方法：</p><ol><li>暴力：枚举起点和终点，复杂度为 O(n^2)，其中优化点：起点和终点必须是正数，不可能是负数；</li><li>DP求解：<ol><li>分治（子问题）：如果第i个元素，则子序列和是多少？$max_sum(i) = Max(max_sum(i-1), 0) + a[i]$</li><li>状态数组定义：$f[1]$</li><li>DP方程：$f(i) = Max(f(i-1), 0) + a[i]$</li></ol></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxSubArray</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            1. dp问题，公式为: dp[i] = max(nums[i], nums[i] + dp[i - 1])</span></span><br><span class="line"><span class="string">            2. 最大子序列和 = 当前元素自身最大，或者 包含之前后最大</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="comment"># nums[i - 1]代表dp[i - 1]</span></span><br><span class="line">            nums[i] = <span class="built_in">max</span>(nums[i], nums[i] + nums[i - <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(nums)</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1. DP求解：</span></span><br><span class="line">    <span class="comment">//     1. 分治（子问题）：如果第i个元素，则子序列和是多少？$max_sum(i) = Max(max_sum(i-1), 0) + a[i]$</span></span><br><span class="line">    <span class="comment">//     2. 状态数组定义：$f[1]$</span></span><br><span class="line">    <span class="comment">//     3. DP方程：$f(i) = Max(f(i-1), 0) + a[i]$</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">maxSubArray</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(nums)</span></span>;</span><br><span class="line">        <span class="type">int</span> max_sum = dp[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; nums.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            dp[i] = std::<span class="built_in">max</span>(dp[i], dp[i] + dp[i - <span class="number">1</span>]);</span><br><span class="line">            max_sum = dp[i] &gt;= max_sum ? dp[i] : max_sum;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="扩展：乘积最大数组"><a href="#扩展：乘积最大数组" class="headerlink" title="扩展：乘积最大数组"></a>扩展：乘积最大数组</h4><p><a href="https://leetcode.cn/problems/maximum-product-subarray/description/" title="152. 乘积最大子数组 - 力扣（LeetCode）">152. 乘积最大子数组 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数数组 nums ，请你找出数组中乘积最大的非空连续子数组（该子数组中至少包含一个数字），并返回该子数组所对应的乘积。</span><br><span class="line"></span><br><span class="line">测试用例的答案是一个 32-位 整数。</span><br><span class="line"></span><br><span class="line">子数组 是数组的连续子序列。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProduct</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 记录绝对值最大的两个数</span></span><br><span class="line">        <span class="comment"># mi ： 负数中最小的</span></span><br><span class="line">        <span class="comment"># ma ： 正数中最大的</span></span><br><span class="line">        mi = ma = res = nums[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums[i] &lt; <span class="number">0</span> :</span><br><span class="line">                mi, ma = ma, mi</span><br><span class="line">            ma = <span class="built_in">max</span>(ma * nums[i], nums[i])</span><br><span class="line">            mi = <span class="built_in">min</span>(mi * nums[i], nums[i])</span><br><span class="line">            res = <span class="built_in">max</span>(res, ma)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">maxProduct</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> mi = nums[<span class="number">0</span>];</span><br><span class="line">        <span class="type">int</span> ma = nums[<span class="number">0</span>];</span><br><span class="line">        <span class="type">int</span> res = nums[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; nums.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[i] &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="type">int</span> tmp = ma;</span><br><span class="line">                ma = mi;</span><br><span class="line">                mi = tmp;</span><br><span class="line">            &#125;</span><br><span class="line">            ma = std::<span class="built_in">max</span>(ma * nums[i], nums[i]);</span><br><span class="line">            mi = std::<span class="built_in">min</span>(mi * nums[i], nums[i]);</span><br><span class="line"></span><br><span class="line">            res = std::<span class="built_in">max</span>(res, ma);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="4-4-Coin-change"><a href="#4-4-Coin-change" class="headerlink" title="4.4 Coin change"></a>4.4 Coin change</h2><p><a href="https://leetcode.cn/problems/coin-change/?utm_source=LCUS\&amp;utm_medium=ip_redirect\&amp;utm_campaign=transfer2china" title="322. 零钱兑换 - 力扣（LeetCode）">322. 零钱兑换 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。</span><br><span class="line"></span><br><span class="line">计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。</span><br><span class="line"></span><br><span class="line">你可以认为每种硬币的数量是无限的。</span><br></pre></td></tr></table></figure><ol><li>暴力方法，递归，第几层表示用了几个硬币，层数最小 → 广度优先遍历，数字为0的结点</li></ol><p><img src="image/image_y-BBnRoP2u.png" alt=""></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 2.递归方法</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">coinChange</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; coins, <span class="type">int</span> amount)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (amount &lt; <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">count</span><span class="params">(amount, <span class="number">0</span>)</span></span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">dp</span>(coins, amount, count);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// rem : 剩余的价值</span></span><br><span class="line">    <span class="comment">// count[rem] 最小到达的数量</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">dp</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; coins, <span class="type">int</span> rem, std::vector&lt;<span class="type">int</span>&gt;&amp; count)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 无效数据</span></span><br><span class="line">        <span class="keyword">if</span> (rem &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 终止条件</span></span><br><span class="line">        <span class="keyword">if</span> (rem == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (count[rem - <span class="number">1</span>] != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> count[rem - <span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> min = INT_MAX;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span>&amp; coin : coins) &#123;</span><br><span class="line">            <span class="type">int</span> res = <span class="keyword">this</span>-&gt;<span class="built_in">dp</span>(coins, rem - coin, count);</span><br><span class="line">            <span class="keyword">if</span> (res &gt;= <span class="number">0</span> &amp;&amp; res &lt; min) &#123;</span><br><span class="line">                min = res + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        count[rem - <span class="number">1</span>] = (min == INT_MAX) ? <span class="number">-1</span> : min;</span><br><span class="line">        <span class="keyword">return</span> count[rem - <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ol><li>DP方法<ol><li>分治（子问题）：$f[n] = min {f(n-k), for k in [1, 2, 5]} + 1$</li><li>状态数组定义：$f(n)$</li><li>DP方程：$f[n] = min {f(n-k), for k in [1, 2, 5]} + 1$</li></ol></li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 1.动态规划</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">coinChange</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; coins, <span class="type">int</span> amount)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> max_amount = amount + <span class="number">1</span>;</span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(amount + <span class="number">1</span>, max_amount)</span></span>;</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= amount; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; coins.<span class="built_in">size</span>(); j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (coins[j] &lt;= i) &#123;</span><br><span class="line">                    dp[i] = std::<span class="built_in">min</span>(dp[i], dp[i - coins[j]] + <span class="number">1</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[amount] &gt; amount ? <span class="number">-1</span> : dp[amount];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="4-5-打家劫舍"><a href="#4-5-打家劫舍" class="headerlink" title="4.5 打家劫舍"></a>4.5 打家劫舍</h2><h3 id="（1）-打家劫舍"><a href="#（1）-打家劫舍" class="headerlink" title="（1） 打家劫舍"></a>（1） 打家劫舍</h3><p><a href="https://leetcode.cn/problems/house-robber/description/" title="198. 打家劫舍 - 力扣（LeetCode）">198. 打家劫舍 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。</span><br><span class="line"></span><br><span class="line">给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。</span><br></pre></td></tr></table></figure><p>DP方法：</p><ol><li>分治子问题：</li><li>状态数组定义：</li><li>DP方程：</li></ol><p>分析：</p><ul><li><code>a[i]</code> ： 0~i 能偷盗的最大数量，结果为<code>a[n-1]</code></li><li><code>a[i][0, 1]</code> ： 增加一个维度，其中，0表示i偷，1表示i不偷</li><li>$a[i][0] = max(a[i-1][0], a[i-1][1])$：当前不偷，前一个偷还是不偷的最大值</li><li>$a[i][1] = a[i-1][0] + nums[i]$ ： 当前偷，等于前一个的值 + 当前的值</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 二维动态规划</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">rob</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 增加一个维度，其中，0表示i偷，1表示i不偷</span></span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(n, std::<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(<span class="number">2</span>, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">1</span>] = nums[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="comment">// 当前不偷，前一个偷还是不偷的最大值</span></span><br><span class="line">            dp[i][<span class="number">0</span>] = std::<span class="built_in">max</span>(dp[i - <span class="number">1</span>][<span class="number">0</span>], dp[i - <span class="number">1</span>][<span class="number">1</span>]);</span><br><span class="line">            <span class="comment">//  当前偷，等于前一个的值 + 当前的值</span></span><br><span class="line">            dp[i][<span class="number">1</span>] = dp[i - <span class="number">1</span>][<span class="number">0</span>] + nums[i];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> std::<span class="built_in">max</span>(dp[n<span class="number">-1</span>][<span class="number">0</span>], dp[n<span class="number">-1</span>][<span class="number">1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>进一步简化：</p><ul><li><code>a[i]</code> ： 0~i 能偷盗的最大数量，结果为<code>max(a)</code></li><li><code>a[i]</code> ： 0~i 能偷盗的最大数量，且<code>nums[i]</code>必须偷的最大值</li><li>$a[i] = max(a[i-1] + 0, a[i-2] + nums[i])$：当前的最大值等于，上一次偷的最大值 + 0(今天不偷) 和 上上一次偷的最大值 + 偷今天</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 简化为一维动态规划</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">rob</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> nums[<span class="number">0</span>];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(n,<span class="number">0</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">        dp[<span class="number">0</span>] = nums[<span class="number">0</span>];</span><br><span class="line">        dp[<span class="number">1</span>] = std::<span class="built_in">max</span>(nums[<span class="number">0</span>], nums[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> res = dp[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">2</span>; i &lt; n; i++) &#123;</span><br><span class="line">            dp[i] = std::<span class="built_in">max</span>(dp[i - <span class="number">1</span>] + <span class="number">0</span>, dp[i - <span class="number">2</span>] + nums[i]);</span><br><span class="line"></span><br><span class="line">            res = std::<span class="built_in">max</span>(res, dp[i]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（2）打家劫舍2"><a href="#（2）打家劫舍2" class="headerlink" title="（2）打家劫舍2"></a>（2）打家劫舍2</h3><p><a href="https://leetcode.cn/problems/house-robber-ii/" title="213. 打家劫舍 II - 力扣（LeetCode）">213. 打家劫舍 II - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都 围成一圈 ，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警 。</span><br><span class="line"></span><br><span class="line">给定一个代表每个房屋存放金额的非负整数数组，计算你 在不触动警报装置的情况下 ，今晚能够偷窃到的最高金额。</span><br></pre></td></tr></table></figure><p>如何才能保证第一间房屋和最后一间房屋不同时偷窃呢？如果偷窃了第一间房屋，则不能偷窃最后一间房屋，因此偷窃房屋的范围是第一间房屋到最后第二间房屋；如果偷窃了最后一间房屋，则不能偷窃第一间房屋，因此偷窃房屋的范围是第二间房屋到最后一间房屋。</p><p>假设数组 nums 的长度为 n。</p><ul><li>如果不偷窃最后一间房屋，则偷窃房屋的下标范围是 <code>[0,n−2]</code>；</li><li>如果不偷窃第一间房屋，则偷窃房屋的下标范围是 <code>[1,n−1]</code>。</li></ul><p>在确定偷窃房屋的下标范围之后，即可用第 198 题的方法解决。对于两段下标范围分别计算可以偷窃到的最高总金额，其中的最大值即为在 n 间房屋中可以偷窃到的最高总金额。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">rob</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (n == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> nums[<span class="number">0</span>];</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (n == <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> std::<span class="built_in">max</span>(nums[<span class="number">0</span>], nums[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> std::<span class="built_in">max</span>(<span class="keyword">this</span>-&gt;<span class="built_in">rob_range</span>(nums, <span class="number">0</span>, n - <span class="number">2</span>), <span class="keyword">this</span>-&gt;<span class="built_in">rob_range</span>(nums, <span class="number">1</span>, n - <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">rob_range</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> start, <span class="type">int</span> end)</span> </span>&#123;</span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(nums.size(), <span class="number">0</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">        dp[start] = nums[start];</span><br><span class="line">        dp[start + <span class="number">1</span>] = std::<span class="built_in">max</span>(nums[start], nums[start + <span class="number">1</span>]);</span><br><span class="line">        <span class="type">int</span> res = dp[start + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = start + <span class="number">2</span>; i &lt;= end; i++) &#123;</span><br><span class="line">            dp[i] = std::<span class="built_in">max</span>(dp[i - <span class="number">1</span>] + <span class="number">0</span>, dp[i - <span class="number">2</span>] + nums[i]);</span><br><span class="line"></span><br><span class="line">            res = std::<span class="built_in">max</span>(res, dp[i]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>13 字典树和并查集</title>
      <link href="/dsa/leetcode/13_trie_disjoint_set/README/"/>
      <url>/dsa/leetcode/13_trie_disjoint_set/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-字典树（Trie）"><a href="#1-字典树（Trie）" class="headerlink" title="1.字典树（Trie）"></a>1.字典树（Trie）</h1><h2 id="1-1-字典树数据结构"><a href="#1-1-字典树数据结构" class="headerlink" title="1.1 字典树数据结构"></a>1.1 字典树数据结构</h2><h3 id="（1）树-Tree"><a href="#（1）树-Tree" class="headerlink" title="（1）树 Tree"></a>（1）树 Tree</h3><p><img src="image/image_eUPl1llbkM.png" alt=""></p><h3 id="（2）二叉搜索树"><a href="#（2）二叉搜索树" class="headerlink" title="（2）二叉搜索树"></a>（2）二叉搜索树</h3><p>注意：是根节点大于左子树中的全部结点，小于右子树的全部结点，不是左儿子和右儿子。</p><p><img src="image/image_2Ro3IoYuxd.png" alt=""></p><h3 id="（3）字典树"><a href="#（3）字典树" class="headerlink" title="（3）字典树"></a>（3）字典树</h3><p>字典树，即 Trie 树，又称单词查找树或键树，是一种树形结构。典型应用是用于统计和排序大量的字符串(但不仅限于字符串)，所以经常被搜索引擎系统用于文本词频统计。</p><p>它的优点是：<strong>最大限度地减少无谓的字符串比较，查询效率比哈希表高</strong>。</p><p>结点可以存储额外信息：</p><ul><li>如下图中，数字表示单词出现的频次</li></ul><p><img src="image/image_a9avI8cLoG.png" alt=""></p><h3 id="（4）结点的内部实现"><a href="#（4）结点的内部实现" class="headerlink" title="（4）结点的内部实现"></a>（4）结点的内部实现</h3><p><img src="image/image_4d_3lcNuV1.png" alt=""></p><h2 id="1-2-核心思想"><a href="#1-2-核心思想" class="headerlink" title="1.2 核心思想"></a>1.2 核心思想</h2><p>Trie树的核心思想：空间换时间</p><p>利用字符串的公共前缀来降低查询时间的开销，以达到提高效率的目的</p><h2 id="1-3-基本性质"><a href="#1-3-基本性质" class="headerlink" title="1.3 基本性质"></a>1.3 基本性质</h2><ol><li>结点本身不存完整单词</li><li>从根结点到某一结点，路径上经过的字符连接起来，为该结点对应的字符串</li><li>每个结点的所有子结点路径代表的字符都不相同</li></ol><h2 id="1-4-字典树实现"><a href="#1-4-字典树实现" class="headerlink" title="1.4 字典树实现"></a>1.4 字典树实现</h2><p>python代码模板</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trie</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 字典树</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 指向子节点的指针数组 children。数组长度为 26，即小写英文字母的数量</span></span><br><span class="line">        self.children = [<span class="literal">None</span>] * <span class="number">26</span></span><br><span class="line">        <span class="comment"># 表示该节点是否为字符串的结尾</span></span><br><span class="line">        self.is_end = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">insert</span>(<span class="params">self, word: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 插入字符串，从根开始，判断下面两种情况</span></span><br><span class="line"><span class="string">            1.子节点存在。沿着指针移动到子节点，继续处理下一个字符。</span></span><br><span class="line"><span class="string">            2.子节点不存在。创建一个新的子节点，记录在 children数组的对应位置上，</span></span><br><span class="line"><span class="string">            然后沿着指针移动到子节点，继续搜索下一个字符。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        node = self</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> word:</span><br><span class="line">            ch = <span class="built_in">ord</span>(ch) - <span class="built_in">ord</span>(<span class="string">&quot;a&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> node.children[ch]:</span><br><span class="line">                node.children[ch] = Trie()</span><br><span class="line">            node = node.children[ch]</span><br><span class="line">        node.is_end = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, word: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 搜索字符串是否存在</span></span><br><span class="line"><span class="string">            若搜索到了前缀的末尾，就说明字典树中存在该前缀。</span></span><br><span class="line"><span class="string">            此外，若前缀末尾对应节点的 isEnd为真，则说明字典树中存在该字符串。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        node = self._search_prefix(word)</span><br><span class="line">        <span class="keyword">return</span> node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> node.is_end</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">startsWith</span>(<span class="params">self, prefix: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">return</span> self._search_prefix(prefix) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_search_prefix</span>(<span class="params">self, prefix : <span class="built_in">str</span></span>) -&gt; <span class="string">&quot;Trie&quot;</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 查找前缀，对于当前字符对应的子节点，有两种情况：</span></span><br><span class="line"><span class="string">            1.子节点存在。沿着指针移动到子节点，继续搜索下一个字符。</span></span><br><span class="line"><span class="string">            2.子节点不存在。说明字典树中不包含该前缀，返回空指针。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        node = self</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> prefix:</span><br><span class="line">            ch = <span class="built_in">ord</span>(ch) - <span class="built_in">ord</span>(<span class="string">&quot;a&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> node.children[ch]:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">            node = node.children[ch]</span><br><span class="line">        <span class="keyword">return</span> node</span><br></pre></td></tr></table></figure><p>C++实现</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trie</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Trie</span>() : <span class="built_in">m_children</span>(<span class="number">26</span>), <span class="built_in">m_is_end</span>(<span class="literal">false</span>) &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 插入字符串，从根开始，判断下面两种情况</span></span><br><span class="line">    <span class="comment">// 1.子节点存在。沿着指针移动到子节点，继续处理下一个字符。</span></span><br><span class="line">    <span class="comment">// 2.子节点不存在。创建一个新的子节点，记录在 children数组的对应位置上，</span></span><br><span class="line">    <span class="comment">//   然后沿着指针移动到子节点，继续搜索下一个字符。</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">insert</span><span class="params">(string word)</span> </span>&#123;</span><br><span class="line">        Trie* node = <span class="keyword">this</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> ch : word) &#123;</span><br><span class="line">            ch -= <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">            <span class="keyword">if</span> (node-&gt;m_children[ch] == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                node-&gt;m_children[ch] = <span class="keyword">new</span> <span class="built_in">Trie</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            node = node-&gt;m_children[ch];</span><br><span class="line">        &#125;</span><br><span class="line">        node-&gt;m_is_end = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 搜索字符串是否存在</span></span><br><span class="line">    <span class="comment">// 若搜索到了前缀的末尾，就说明字典树中存在该前缀。</span></span><br><span class="line">    <span class="comment">// 此外，若前缀末尾对应节点的 isEnd为真，则说明字典树中存在该字符串。</span></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">search</span><span class="params">(string word)</span> </span>&#123;</span><br><span class="line">        Trie* node = <span class="keyword">this</span>-&gt;<span class="built_in">search_prefix</span>(word);</span><br><span class="line">        <span class="keyword">return</span> node != <span class="literal">nullptr</span> &amp;&amp; node-&gt;m_is_end;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">startsWith</span><span class="params">(string prefix)</span> </span>&#123;</span><br><span class="line">        Trie* node = <span class="keyword">this</span>-&gt;<span class="built_in">search_prefix</span>(prefix);</span><br><span class="line">        <span class="keyword">return</span> node != <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 指向子节点的指针数组 children。数组长度为 26，即小写英文字母的数量</span></span><br><span class="line">    std::vector&lt;Trie*&gt; m_children;</span><br><span class="line">    <span class="comment">// 表示该节点是否为字符串的结尾</span></span><br><span class="line">    <span class="type">bool</span> m_is_end;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查找前缀，对于当前字符对应的子节点，有两种情况：</span></span><br><span class="line">    <span class="comment">// 1.子节点存在。沿着指针移动到子节点，继续搜索下一个字符。</span></span><br><span class="line">    <span class="comment">// 2.子节点不存在。说明字典树中不包含该前缀，返回空指针。</span></span><br><span class="line">    <span class="function">Trie* <span class="title">search_prefix</span><span class="params">(std::string&amp; prefix)</span> </span>&#123;</span><br><span class="line">        Trie* node = <span class="keyword">this</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> ch : prefix) &#123;</span><br><span class="line">            ch -= <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">            <span class="keyword">if</span> (node-&gt;m_children[ch] == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            node = node-&gt;m_children[ch];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> node;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="2-Tire实战题目"><a href="#2-Tire实战题目" class="headerlink" title="2.Tire实战题目"></a>2.Tire实战题目</h1><h2 id="2-1-单词搜索2"><a href="#2-1-单词搜索2" class="headerlink" title="2.1 单词搜索2"></a>2.1 单词搜索2</h2><p><a href="https://leetcode.cn/problems/word-search-ii/description/" title="212. 单词搜索 II - 力扣（LeetCode）">212. 单词搜索 II - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给定一个 m x n 二维字符网格 board 和一个单词（字符串）列表 words， 返回所有二维网格上的单词 。</span><br><span class="line"></span><br><span class="line">单词必须按照字母顺序，通过 相邻的单元格 内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母在一个单词中不允许被重复使用。</span><br></pre></td></tr></table></figure><ol><li>words中遍历 → board search，$  O(N<em>m</em>n*4^k)  $(4表示4个联通区域，k表示单词的平均长度)</li><li>Trie<ol><li>all words  → Trie 构建起 prefix</li><li>board， DFS，遍历每一个字符</li></ol></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trie</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 指向子节点的指针数组 children。数组长度为 26，即小写英文字母的数量</span></span><br><span class="line">        self.children = [<span class="literal">None</span>] * <span class="number">26</span></span><br><span class="line">        <span class="comment"># 表示该节点是否为字符串的结尾</span></span><br><span class="line">        self.is_end = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">insert</span>(<span class="params">self, word: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 插入字符串，从根开始，判断下面两种情况</span></span><br><span class="line"><span class="string">            1.子节点存在。沿着指针移动到子节点，继续处理下一个字符。</span></span><br><span class="line"><span class="string">            2.子节点不存在。创建一个新的子节点，记录在 children数组的对应位置上，</span></span><br><span class="line"><span class="string">            然后沿着指针移动到子节点，继续搜索下一个字符。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        node = self</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> word:</span><br><span class="line">            ch = <span class="built_in">ord</span>(ch) - <span class="built_in">ord</span>(<span class="string">&quot;a&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> node.children[ch]:</span><br><span class="line">                node.children[ch] = Trie()</span><br><span class="line">            node = node.children[ch]</span><br><span class="line">        node.is_end = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findWords</span>(<span class="params">self, board: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]], words: <span class="type">List</span>[<span class="built_in">str</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">str</span>]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> board <span class="keyword">or</span> <span class="keyword">not</span> board[<span class="number">0</span>]:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> words:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">        self.result = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建trie</span></span><br><span class="line">        trie = Trie()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            trie.insert(word)</span><br><span class="line"></span><br><span class="line">        self.m, self.n = <span class="built_in">len</span>(board), <span class="built_in">len</span>(board[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(self.n):</span><br><span class="line">                <span class="keyword">if</span> trie.children[<span class="built_in">ord</span>(board[i][j]) - <span class="built_in">ord</span>(<span class="string">&quot;a&quot;</span>)] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    self._dfs(board, i, j, <span class="string">&quot;&quot;</span>, trie.children[<span class="built_in">ord</span>(board[i][j]) - <span class="built_in">ord</span>(<span class="string">&quot;a&quot;</span>)] )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(self.result)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_dfs</span>(<span class="params">self, board, i, j, curr_word, trie</span>):</span><br><span class="line">        curr_word += board[i][j]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> trie.is_end:</span><br><span class="line">            self.result.add(curr_word)</span><br><span class="line"></span><br><span class="line">        tmp, board[i][j] = board[i][j], <span class="string">&#x27;@&#x27;</span></span><br><span class="line">        dx = [-<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        dy = [<span class="number">0</span>, <span class="number">0</span>, -<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dx)):</span><br><span class="line">            x, y = i + dx[k], j + dy[k]</span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; self.m <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; self.n <span class="keyword">and</span> board[x][y] != <span class="string">&#x27;@&#x27;</span> \</span><br><span class="line">                <span class="keyword">and</span> trie.children[<span class="built_in">ord</span>(board[x][y]) - <span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>)] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self._dfs(board, x, y, curr_word, trie.children[<span class="built_in">ord</span>(board[x][y]) - <span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>)])</span><br><span class="line">        board[i][j] = tmp</span><br></pre></td></tr></table></figure><h1 id="3-并查集（Disjoint-Set）"><a href="#3-并查集（Disjoint-Set）" class="headerlink" title="3.并查集（Disjoint Set）"></a>3.并查集（Disjoint Set）</h1><h2 id="3-1-使用场景"><a href="#3-1-使用场景" class="headerlink" title="3.1 使用场景"></a>3.1 使用场景</h2><ul><li>组团、配对问题</li><li>Group or not ?</li></ul><h2 id="3-2-基本操作"><a href="#3-2-基本操作" class="headerlink" title="3.2 基本操作"></a>3.2 基本操作</h2><ul><li><code>makeSte(s)</code>: 建立一个新的并查集，其中包含s个单元集合</li><li><code>unionSet(x, y)</code>: 把元素x和y所在的集合合并，要求x和y所在的集合不相交，如果相交则不合并</li><li><code>find(x)</code> : 找到元素x所在的集合的代表，该操作也可以用于判断两个元素是否位于同一个集合，只要将它们各自的代表比较一下就可以了。</li></ul><h3 id="（1）初始化"><a href="#（1）初始化" class="headerlink" title="（1）初始化"></a>（1）初始化</h3><p>每个集合的领头元素 <code>i = parent[i]</code></p><p><img src="image/image_-0F_QT2LQA.png" alt=""></p><h3 id="（2）查询、合并"><a href="#（2）查询、合并" class="headerlink" title="（2）查询、合并"></a>（2）查询、合并</h3><p><img src="image/image_jceTpuPK0N.png" alt=""></p><h3 id="（3）路径压缩"><a href="#（3）路径压缩" class="headerlink" title="（3）路径压缩"></a>（3）路径压缩</h3><p><img src="image/image_RAppUEcBdn.png" alt=""></p><h2 id="3-3-代码模板"><a href="#3-3-代码模板" class="headerlink" title="3.3 代码模板"></a>3.3 代码模板</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UnionFind</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span>[] parent;</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">UnionFind</span><span class="params">(<span class="type">int</span> n)</span> &#123;</span><br><span class="line">        count = n;</span><br><span class="line">        parent = <span class="keyword">new</span> <span class="title class_">int</span>[n];</span><br><span class="line">        <span class="comment">// parent初始化</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            parent[i] = [i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">find</span><span class="params">(<span class="type">int</span> p)</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (p != parent[p]) &#123;</span><br><span class="line">            parent[p] = parent[parent[p]];</span><br><span class="line">            p = parent[p];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> p;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">union</span><span class="params">(<span class="type">int</span> p, <span class="type">int</span> q)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">rootP</span> <span class="operator">=</span> find(p);</span><br><span class="line">        <span class="type">int</span> <span class="variable">rootQ</span> <span class="operator">=</span> find(q);</span><br><span class="line">        <span class="keyword">if</span> (rootP == rootQ) </span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        parent[rootP] = rootQ;</span><br><span class="line">        count--;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>python实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UnionFind</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">n</span>):</span><br><span class="line">        p = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">union</span>(<span class="params">self, p, i, j</span>):</span><br><span class="line">        p1 = self.parent(p, i)</span><br><span class="line">        p2 = self.parent(p, j)</span><br><span class="line">        p[p1] = p2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parent</span>(<span class="params">self, p, i</span>):</span><br><span class="line">        root = i</span><br><span class="line">        <span class="keyword">while</span> p[root] != root:</span><br><span class="line">            root = p[root]</span><br><span class="line">        <span class="comment"># 路径压缩</span></span><br><span class="line">        <span class="keyword">while</span> p[i] != i:</span><br><span class="line">            x = i</span><br><span class="line">            i = p[i]</span><br><span class="line">            p[x] = root</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure><h2 id="3-4例题"><a href="#3-4例题" class="headerlink" title="3.4例题"></a>3.4例题</h2><h3 id="（1）省份数量"><a href="#（1）省份数量" class="headerlink" title="（1）省份数量"></a>（1）省份数量</h3><p><a href="https://leetcode.cn/problems/number-of-provinces/description/" title="547. 省份数量 - 力扣（LeetCode）">547. 省份数量 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">有 n 个城市，其中一些彼此相连，另一些没有相连。如果城市 a 与城市 b 直接相连，且城市 b 与城市 c 直接相连，那么城市 a 与城市 c 间接相连。</span><br><span class="line"></span><br><span class="line">省份 是一组直接或间接相连的城市，组内不含其他没有相连的城市。</span><br><span class="line"></span><br><span class="line">给你一个 n x n 的矩阵 isConnected ，其中 isConnected[i][j] = 1 表示第 i 个城市和第 j 个城市直接相连，而 isConnected[i][j] = 0 表示二者不直接相连。</span><br><span class="line"></span><br><span class="line">返回矩阵中 省份 的数量。</span><br></pre></td></tr></table></figure><p>转化为“岛屿数目”问题</p><ol><li>DFS，从一个点出发，将相邻的1变为0，最后看需要多少次DFS</li><li>BFS，从一个点出发，将相邻的1变为0，最后看需要多少次BFS</li><li>并查集，<ol><li>N个人 → 各自独立集合</li><li>遍历好友关系矩阵 M ：<code>M[i][j] == 1</code> ?  → 并查集集合合并</li><li>最后，看有多少孤立的集合</li></ol></li></ol><h4 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h4><p>遍历所有城市，对于每个城市，如果该城市尚未被访问过，则从该城市开始深度优先搜索，通过矩阵 isConnected得到与该城市直接相连的城市有哪些，这些城市和该城市属于同一个连通分量，然后对这些城市继续深度优先搜索，直到同一个连通分量的所有城市都被访问到，即可得到一个省份。遍历完全部城市以后，即可得到连通分量的总数，即省份的总数。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findCircleNum</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; isConnected)</span> </span>&#123;</span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">visited</span><span class="params">(isConnected.size(), <span class="number">0</span>)</span></span>;</span><br><span class="line">        <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; isConnected.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (visited[i] == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(isConnected, visited, i);</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; isConnected, std::vector&lt;<span class="type">int</span>&gt;&amp; visited, <span class="type">int</span> i)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; isConnected.<span class="built_in">size</span>(); j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (isConnected[i][j] == <span class="number">1</span> &amp;&amp; visited[j] == <span class="number">0</span>) &#123;</span><br><span class="line">                visited[j] = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(isConnected, visited, j);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="BFS"><a href="#BFS" class="headerlink" title="BFS"></a>BFS</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findCircleNum</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; isConnected)</span> </span>&#123;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">visited</span><span class="params">(isConnected.size(), <span class="number">0</span>)</span></span>;</span><br><span class="line">        <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">        std::queue&lt;<span class="type">int</span>&gt; Q;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; isConnected.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!visited[i]) &#123;</span><br><span class="line">                Q.<span class="built_in">push</span>(i);</span><br><span class="line">                <span class="keyword">while</span> (!Q.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">                    <span class="type">int</span> j = Q.<span class="built_in">front</span>(); Q.<span class="built_in">pop</span>();</span><br><span class="line">                    visited[j] = <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; isConnected.<span class="built_in">size</span>(); k++) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (isConnected[j][k] == <span class="number">1</span> &amp;&amp; !visited[k]) &#123;</span><br><span class="line">                            Q.<span class="built_in">push</span>(k);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h4><p>计算连通分量数的另一个方法是使用并查集。初始时，每个城市都属于不同的连通分量。遍历矩阵 isConnected，如果两个城市之间有相连关系，则它们属于同一个连通分量，对它们进行合并。</p><p>遍历矩阵 isConnected 的全部元素之后，计算连通分量的总数，即为省份的总数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findCircleNum</span>(<span class="params">self, isConnected: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isConnected:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        n = <span class="built_in">len</span>(isConnected)</span><br><span class="line">        p = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                <span class="keyword">if</span> isConnected[i][j] == <span class="number">1</span>:</span><br><span class="line">                    self._union(p, i, j)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="built_in">set</span>([self._parent(p, i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_union</span>(<span class="params">self, p, i, j</span>):</span><br><span class="line">        p1 = self._parent(p, i)</span><br><span class="line">        p2 = self._parent(p, j)</span><br><span class="line">        p[p1] = p2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_parent</span>(<span class="params">self, p, i</span>):</span><br><span class="line">        root = i</span><br><span class="line">        <span class="keyword">while</span> p[root] != root:</span><br><span class="line">            root = p[root]</span><br><span class="line">        <span class="comment"># 路径压缩</span></span><br><span class="line">        <span class="keyword">while</span> p[i] != i:</span><br><span class="line">            x = i</span><br><span class="line">            i = p[i]</span><br><span class="line">            p[x] = root</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findCircleNum</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; isConnected)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = isConnected.<span class="built_in">size</span>();</span><br><span class="line">        m_count = n;</span><br><span class="line">        m_parent.<span class="built_in">resize</span>(n);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="keyword">this</span>-&gt;m_parent[i] = i;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (isConnected[i][j] == <span class="number">1</span>) &#123;</span><br><span class="line">                    <span class="keyword">this</span>-&gt;<span class="built_in">union_set</span>(i, j);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> m_count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">union_set</span><span class="params">(<span class="type">int</span> p, <span class="type">int</span> q)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> root_p = <span class="keyword">this</span>-&gt;<span class="built_in">parent_set</span>(p);</span><br><span class="line">        <span class="type">int</span> root_q = <span class="keyword">this</span>-&gt;<span class="built_in">parent_set</span>(q);</span><br><span class="line">        <span class="keyword">if</span> (root_p == root_q) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        m_parent[root_p] = root_q;</span><br><span class="line">        m_count--;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">parent_set</span><span class="params">(<span class="type">int</span> i)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> p = i;</span><br><span class="line">        <span class="keyword">while</span>(p != m_parent[p]) &#123;</span><br><span class="line">            p = m_parent[p];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> p;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> m_count;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; m_parent;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（2）岛屿数量"><a href="#（2）岛屿数量" class="headerlink" title="（2）岛屿数量"></a>（2）岛屿数量</h3><p><a href="https://leetcode.cn/problems/number-of-islands/description/">200. 岛屿数量 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">给你一个由 <span class="string">&#x27;1&#x27;</span>（陆地）和 <span class="string">&#x27;0&#x27;</span>（水）组成的的二维网格，请你计算网格中岛屿的数量。</span><br><span class="line"></span><br><span class="line">岛屿总是被水包围，并且每座岛屿只能由水平方向和/或竖直方向上相邻的陆地连接形成。</span><br><span class="line"></span><br><span class="line">此外，你可以假设该网格的四条边均被水包围。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numIslands</span>(<span class="params">self, grid: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        为了求出岛屿的数量，可以扫描整个二维网格。如果一个位置为 1，</span></span><br><span class="line"><span class="string">        则将其与相邻四个方向上的 111 在并查集中进行合并。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        最终岛屿的数量就是并查集中连通分量的数目。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> grid:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        m = <span class="built_in">len</span>(grid)</span><br><span class="line">        n = <span class="built_in">len</span>(grid[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 初始化并查集</span></span><br><span class="line">        self.parent = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m * n)]</span><br><span class="line">        self.count =<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                <span class="keyword">if</span> grid[i][j] == <span class="string">&quot;1&quot;</span>:</span><br><span class="line">                    self.parent[i * n + j] = i * n + j</span><br><span class="line">                    self.count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 合并</span></span><br><span class="line">        dx = [-<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        dy = [<span class="number">0</span>, <span class="number">0</span>, -<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                <span class="keyword">if</span> grid[i][j] == <span class="string">&quot;1&quot;</span>:</span><br><span class="line">                    grid[i][j] = <span class="string">&quot;0&quot;</span></span><br><span class="line">                    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dx)):</span><br><span class="line">                        x, y = i + dx[k], j + dy[k]</span><br><span class="line">                        <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; m <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; n <span class="keyword">and</span> grid[x][y] == <span class="string">&quot;1&quot;</span>:</span><br><span class="line">                            self._union(i * n + j, x * n + y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.count</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_union</span>(<span class="params">self, i, j</span>):</span><br><span class="line">        p1 = self._parent(i)</span><br><span class="line">        p2 = self._parent(j)</span><br><span class="line">        <span class="keyword">if</span> p1 == p2:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        self.parent[p1] = p2</span><br><span class="line">        self.count -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_parent</span>(<span class="params">self, i</span>):</span><br><span class="line">        root = i</span><br><span class="line">        <span class="keyword">while</span> self.parent[root] != root:</span><br><span class="line">            root = self.parent[root]</span><br><span class="line">        <span class="comment"># 路径压缩</span></span><br><span class="line">        <span class="keyword">while</span> self.parent[i] != i:</span><br><span class="line">            x = i</span><br><span class="line">            i = self.parent[i]</span><br><span class="line">            self.parent[x] = root</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure><h3 id="（3）被围绕的区域"><a href="#（3）被围绕的区域" class="headerlink" title="（3）被围绕的区域"></a>（3）被围绕的区域</h3><p><a href="https://leetcode.cn/problems/surrounded-regions/description/">130. 被围绕的区域 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">给你一个 m x n 的矩阵 board ，由若干字符 <span class="string">&#x27;X&#x27;</span> 和 <span class="string">&#x27;O&#x27;</span> ，找到所有被 <span class="string">&#x27;X&#x27;</span> 围绕的区域，并将这些区域里所有的 <span class="string">&#x27;O&#x27;</span> 用 <span class="string">&#x27;X&#x27;</span> 填充。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    把所有边界上的 O 看做一个连通区域。遇到 O 就执行并查集合并操作，这样所有的 O 就会被分成两类</span></span><br><span class="line"><span class="string">        1.和边界上的 O 在一个连通区域内的。这些 O 保留。</span></span><br><span class="line"><span class="string">        2.不和边界上的 O 在一个连通区域内的。这些 O 就是被包围的，替换。</span></span><br><span class="line"><span class="string">    由于并查集一般用一维数组来记录，方便查找 parants，所以将二维坐标用 node 函数转化为一维坐标。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">solve</span>(<span class="params">self, board: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]]</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Do not return anything, modify board in-place instead.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> board:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        m = <span class="built_in">len</span>(board)</span><br><span class="line">        n = <span class="built_in">len</span>(board[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 初始化并查集, 最后增加一个虚拟结点，用于比较</span></span><br><span class="line">        self.parent = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m * n + <span class="number">1</span>)]</span><br><span class="line">        self.count =<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                self.parent[i * n + j] = i * n + j</span><br><span class="line">                self.count += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 用一个虚拟节点, 边界上的O 的父节点都是这个虚拟节点</span></span><br><span class="line">        dummy_node = m * n</span><br><span class="line">        self.parent[dummy_node] = dummy_node</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 合并</span></span><br><span class="line">        dx = [-<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        dy = [<span class="number">0</span>, <span class="number">0</span>, -<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                <span class="comment"># 遇到O进行并查集操作合并</span></span><br><span class="line">                <span class="keyword">if</span> board[i][j] == <span class="string">&#x27;O&#x27;</span>:</span><br><span class="line">                    <span class="comment"># 边界上的O,把它和dummy_node 合并成一个连通区域.</span></span><br><span class="line">                    <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">or</span> i == m - <span class="number">1</span> <span class="keyword">or</span> j == <span class="number">0</span> <span class="keyword">or</span> j == n - <span class="number">1</span>:</span><br><span class="line">                        self._union(i * n + j, dummy_node)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="comment"># 和上下左右合并成一个连通区域.</span></span><br><span class="line">                        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dx)):</span><br><span class="line">                            x, y = i + dx[k], j + dy[k]</span><br><span class="line">                            <span class="keyword">if</span> board[x][y] == <span class="string">&#x27;O&#x27;</span>:</span><br><span class="line">                                self._union(i * n + j, x * n + y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                <span class="comment"># 和dummy_node 在一个连通区域的,那么就是O</span></span><br><span class="line">                <span class="keyword">if</span> self._parent(i * n + j) == self._parent(dummy_node):</span><br><span class="line">                    board[i][j] = <span class="string">&#x27;O&#x27;</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    board[i][j] = <span class="string">&#x27;X&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_union</span>(<span class="params">self, i, j</span>):</span><br><span class="line">        p1 = self._parent(i)</span><br><span class="line">        p2 = self._parent(j)</span><br><span class="line">        <span class="keyword">if</span> p1 == p2:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        self.parent[p1] = p2</span><br><span class="line">        self.count -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_parent</span>(<span class="params">self, i</span>):</span><br><span class="line">        root = i</span><br><span class="line">        <span class="keyword">while</span> self.parent[root] != root:</span><br><span class="line">            root = self.parent[root]</span><br><span class="line">        <span class="comment"># 路径压缩</span></span><br><span class="line">        <span class="keyword">while</span> self.parent[i] != i:</span><br><span class="line">            x = i</span><br><span class="line">            i = self.parent[i]</span><br><span class="line">            self.parent[x] = root</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>14 高级搜索</title>
      <link href="/dsa/leetcode/14_advanced_search/README/"/>
      <url>/dsa/leetcode/14_advanced_search/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-初级搜索"><a href="#1-初级搜索" class="headerlink" title="1.初级搜索"></a>1.初级搜索</h1><ol><li>朴素搜索</li><li>优化方式：<strong>不重复（fibonacci）</strong>、<strong>剪枝（生成括号问题）</strong></li><li>搜索方向：<ol><li>BFS：深度优先搜索  (depth first search)，栈</li><li>BFS：广度优先搜索 (breadth first search)，队列</li></ol></li><li>高级搜索：双向搜索、启发式搜索（优先队列）</li></ol><p>Coin change（零钱置换）的状态树</p><p><img src="image/image_rxk11_YWk_.png" alt=""></p><p>DFS代码 - 递归写法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">visited = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">node, visited</span>):</span><br><span class="line">  <span class="comment"># terminator</span></span><br><span class="line">  <span class="keyword">if</span> node <span class="keyword">in</span> visited:</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">  visited.add(node)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># process currend node here</span></span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> next_node <span class="keyword">in</span> node.children():</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> next_node <span class="keyword">in</span> visited:</span><br><span class="line">      dfs(next_node, visited)</span><br></pre></td></tr></table></figure><p>DFS代码 - 非递归写法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">self, tree</span>):</span><br><span class="line">  <span class="keyword">if</span> tree.root <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">return</span> []</span><br><span class="line">    </span><br><span class="line">  visited, stack = [], [tree.root]</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> stack:</span><br><span class="line">    node = stack.pop()</span><br><span class="line">    visited.add(node)</span><br><span class="line">    </span><br><span class="line">    process(node)</span><br><span class="line">    nodes = generate_related_nodes(node)</span><br><span class="line">    stack.push(nodes)</span><br><span class="line">    </span><br><span class="line">  <span class="comment"># other processing work</span></span><br><span class="line">  ...</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>BFS代码&#x20;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bfs</span>(<span class="params">graph, start, end</span>):</span><br><span class="line">  </span><br><span class="line">  queue = []</span><br><span class="line">  queue.append([start])</span><br><span class="line">  visited.add(start)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> queue:</span><br><span class="line">    node = queue.pop()</span><br><span class="line">    visited.add(node)</span><br><span class="line">    </span><br><span class="line">    process(node)</span><br><span class="line">    nodes = generate_related_nodes(node)</span><br><span class="line">    queue.push(nodes)</span><br><span class="line">    </span><br><span class="line">  <span class="comment"># other processing work</span></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><h1 id="2-剪枝"><a href="#2-剪枝" class="headerlink" title="2.剪枝"></a>2.剪枝</h1><ul><li><a href="https://nikcheerla.github.io/deeplearningschool/2018/01/01/AlphaZero-Explained/" title="AlphaZero Explained">AlphaZero Explained</a></li><li><a href="https://en.wikipedia.org/wiki/Game_complexity" title="棋类复杂度">棋类复杂度</a></li></ul><h2 id="2-1回溯法"><a href="#2-1回溯法" class="headerlink" title="2.1回溯法"></a>2.1回溯法</h2><p>回溯法采用试错的思想，它尝试分步的去解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确的解答的时候，它将取消上一步甚至是上几步的计算，再通过其它的可能的分步解答再次尝试寻找问题的答案.</p><p>回溯法通常用最简单的递归方法来实现，在反复重复上述的步骤后可能出现两种情况</p><ul><li>找到一个可能存在的正确的答案</li><li>在尝试了所有可能的分步方法后宣告该问题没有答案</li></ul><p>在最坏的情况下，回溯法会导致一次复杂度为指数时间的计算</p><h2 id="2-2-实战题目"><a href="#2-2-实战题目" class="headerlink" title="2.2 实战题目"></a>2.2 实战题目</h2><h3 id="（1）括号生成"><a href="#（1）括号生成" class="headerlink" title="（1）括号生成"></a>（1）括号生成</h3><p><a href="https://leetcode.cn/problems/generate-parentheses/" title="22. 括号生成 - 力扣（LeetCode）">22. 括号生成 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数字 n 代表生成括号的对数，请你设计一个函数，用于能够生成所有可能的并且 有效的 括号组合。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 递归</span></span><br><span class="line">    <span class="comment">// 左括号: 随时加，只要不超标</span></span><br><span class="line">    <span class="comment">// 右括号 : 必须之前有左括号，且左括号个数 &gt; 右括号个数</span></span><br><span class="line">    <span class="function">vector&lt;string&gt; <span class="title">generateParenthesis</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        result.<span class="built_in">clear</span>();</span><br><span class="line">        <span class="keyword">this</span>-&gt;_generate(<span class="number">0</span>, <span class="number">0</span>, n, <span class="string">&quot;&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">void</span> _generate(<span class="type">int</span> left, <span class="type">int</span> right, <span class="type">int</span> num, std::string s) &#123;</span><br><span class="line">        <span class="comment">// 1.terminator</span></span><br><span class="line">        <span class="keyword">if</span> (left == num &amp;&amp; right == num) &#123;</span><br><span class="line">            result.<span class="built_in">emplace_back</span>(s);</span><br><span class="line">            <span class="comment">// std::cout &lt;&lt; s &lt;&lt; std::endl;</span></span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.process current logic</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.drill down</span></span><br><span class="line">        <span class="comment">// 剪枝</span></span><br><span class="line">        <span class="keyword">if</span> (left &lt; num)</span><br><span class="line">            <span class="keyword">this</span>-&gt;_generate(left + <span class="number">1</span>, right, num, s + <span class="string">&quot;(&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (left &gt; right)</span><br><span class="line">            <span class="keyword">this</span>-&gt;_generate(left, right + <span class="number">1</span>, num, s + <span class="string">&quot;)&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.reverse states</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    std::vector&lt;std::string&gt; result;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（2）有效数独"><a href="#（2）有效数独" class="headerlink" title="（2）有效数独"></a>（2）有效数独</h3><p><a href="https://leetcode.cn/problems/valid-sudoku/description/" title="36. 有效的数独 - 力扣（LeetCode）">36. 有效的数独 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">请你判断一个 9 x 9 的数独是否有效。只需要 根据以下规则 ，验证已经填入的数字是否有效即可。</span><br><span class="line"></span><br><span class="line">- 数字 1-9 在每一行只能出现一次。</span><br><span class="line">- 数字 1-9 在每一列只能出现一次。</span><br><span class="line">- 数字 1-9 在每一个以粗实线分隔的 3x3 宫内只能出现一次。（请参考示例图）</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line"></span><br><span class="line">一个有效的数独（部分已被填充）不一定是可解的。</span><br><span class="line">只需要根据以上规则，验证已经填入的数字是否有效即可。</span><br><span class="line">空白格用 <span class="string">&#x27;.&#x27;</span> 表示。</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以使用哈希表记录每一行、每一列和每一个小九宫格中，每个数字出现的次数。只需要遍历数独一次，在遍历的过程中更新哈希表中的计数，并判断是否满足有效的数独的条件即可。</p><p>对于数独的第 i 行第 j 列的单元格，其中 $0≤i,j&lt;9$，该单元格所在的行下标和列下标分别为 i 和 j，该单元格所在的小九宫格的行数和列数分别为 $i/3$和 $j/3$，其中 $0 ≤  i/3, j/3 &lt; 3.$</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isValidSudoku</span><span class="params">(vector&lt;vector&lt;<span class="type">char</span>&gt;&gt;&amp; board)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 使用哈希表记录每一行、每一列和每一个小九宫格中，每个数字出现的次数</span></span><br><span class="line">        <span class="comment">// 只需要遍历数独一次，在遍历的过程中更新哈希表中的计数，并判断是否满足有效的数独条件即可</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 每一行的hash表</span></span><br><span class="line">        <span class="type">int</span> rows[<span class="number">9</span>][<span class="number">9</span>];</span><br><span class="line">        <span class="comment">// 每一列的hash表</span></span><br><span class="line">        <span class="type">int</span> columns[<span class="number">9</span>][<span class="number">9</span>];</span><br><span class="line">        <span class="comment">// 每一个小方格的hash表</span></span><br><span class="line">        <span class="type">int</span> subboxes[<span class="number">3</span>][<span class="number">3</span>][<span class="number">9</span>];</span><br><span class="line"></span><br><span class="line">        <span class="built_in">memset</span>(rows, <span class="number">0</span>, <span class="built_in">sizeof</span>(rows));</span><br><span class="line">        <span class="built_in">memset</span>(columns, <span class="number">0</span>, <span class="built_in">sizeof</span>(columns));</span><br><span class="line">        <span class="built_in">memset</span>(subboxes, <span class="number">0</span>, <span class="built_in">sizeof</span>(subboxes));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">9</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">9</span>; j++) &#123;</span><br><span class="line">                <span class="type">char</span> c = board[i][j];</span><br><span class="line">                <span class="keyword">if</span> (c != <span class="string">&#x27;.&#x27;</span>) &#123;</span><br><span class="line">                    <span class="type">int</span> idx = c - <span class="string">&#x27;0&#x27;</span> - <span class="number">1</span>;</span><br><span class="line">                    rows[i][idx]++;</span><br><span class="line">                    columns[j][idx]++;</span><br><span class="line">                    subboxes[i / <span class="number">3</span>][j / <span class="number">3</span>][idx]++;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (rows[i][idx] &gt; <span class="number">1</span> || columns[j][idx] &gt; <span class="number">1</span> || subboxes[i / <span class="number">3</span>][j / <span class="number">3</span>][idx] &gt; <span class="number">1</span>) &#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="（3）解数独"><a href="#（3）解数独" class="headerlink" title="（3）解数独"></a>（3）解数独</h3><p><a href="https://leetcode.cn/problems/sudoku-solver/description/" title="37. 解数独 - 力扣（LeetCode）">37. 解数独 - 力扣（LeetCode）</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">编写一个程序，通过填充空格来解决数独问题。</span><br><span class="line"></span><br><span class="line">数独的解法需 遵循如下规则：</span><br><span class="line"></span><br><span class="line">数字 <span class="number">1</span><span class="number">-9</span> 在每一行只能出现一次。</span><br><span class="line">数字 <span class="number">1</span><span class="number">-9</span> 在每一列只能出现一次。</span><br><span class="line">数字 <span class="number">1</span><span class="number">-9</span> 在每一个以粗实线分隔的 <span class="number">3</span>x3 宫内只能出现一次。（请参考示例图）</span><br><span class="line">数独部分空格内已填入了数字，空白格用 <span class="string">&#x27;.&#x27;</span> 表示。</span><br></pre></td></tr></table></figure><p>DFS + 回溯</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">solveSudoku</span><span class="params">(vector&lt;vector&lt;<span class="type">char</span>&gt;&gt;&amp; board)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (board.<span class="built_in">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(board);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">dfs</span><span class="params">(vector&lt;vector&lt;<span class="type">char</span>&gt;&gt;&amp; board)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; board.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; board[<span class="number">0</span>].<span class="built_in">size</span>(); j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (board[i][j] == <span class="string">&#x27;.&#x27;</span>) &#123;</span><br><span class="line">                    <span class="comment">// 尝试放入 1~9</span></span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">char</span> c = <span class="string">&#x27;1&#x27;</span>; c &lt;= <span class="string">&#x27;9&#x27;</span>; c++) &#123;</span><br><span class="line">                        <span class="comment">// i, j位置放入c</span></span><br><span class="line">                        board[i][j] = c;</span><br><span class="line">                        <span class="comment">// 判断数独是否有效</span></span><br><span class="line">                        <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;<span class="built_in">isValidSudoku</span>(board) &amp;&amp; <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(board))</span><br><span class="line">                            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">                        <span class="comment">// 回溯</span></span><br><span class="line">                        board[i][j] = <span class="string">&#x27;.&#x27;</span>;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isValidSudoku</span><span class="params">(vector&lt;vector&lt;<span class="type">char</span>&gt;&gt;&amp; board)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 使用哈希表记录每一行、每一列和每一个小九宫格中，每个数字出现的次数</span></span><br><span class="line">        <span class="comment">// 只需要遍历数独一次，在遍历的过程中更新哈希表中的计数，并判断是否满足有效的数独条件即可</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 每一行的hash表</span></span><br><span class="line">        <span class="type">int</span> rows[<span class="number">9</span>][<span class="number">9</span>];</span><br><span class="line">        <span class="comment">// 每一列的hash表</span></span><br><span class="line">        <span class="type">int</span> columns[<span class="number">9</span>][<span class="number">9</span>];</span><br><span class="line">        <span class="comment">// 每一个小方格的hash表</span></span><br><span class="line">        <span class="type">int</span> subboxes[<span class="number">3</span>][<span class="number">3</span>][<span class="number">9</span>];</span><br><span class="line"></span><br><span class="line">        <span class="built_in">memset</span>(rows, <span class="number">0</span>, <span class="built_in">sizeof</span>(rows));</span><br><span class="line">        <span class="built_in">memset</span>(columns, <span class="number">0</span>, <span class="built_in">sizeof</span>(columns));</span><br><span class="line">        <span class="built_in">memset</span>(subboxes, <span class="number">0</span>, <span class="built_in">sizeof</span>(subboxes));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">9</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">9</span>; j++) &#123;</span><br><span class="line">                <span class="type">char</span> c = board[i][j];</span><br><span class="line">                <span class="keyword">if</span> (c != <span class="string">&#x27;.&#x27;</span>) &#123;</span><br><span class="line">                    <span class="type">int</span> idx = c - <span class="string">&#x27;0&#x27;</span> - <span class="number">1</span>;</span><br><span class="line">                    rows[i][idx]++;</span><br><span class="line">                    columns[j][idx]++;</span><br><span class="line">                    subboxes[i / <span class="number">3</span>][j / <span class="number">3</span>][idx]++;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (rows[i][idx] &gt; <span class="number">1</span> || columns[j][idx] &gt; <span class="number">1</span> || subboxes[i / <span class="number">3</span>][j / <span class="number">3</span>][idx] &gt; <span class="number">1</span>) &#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="3-双向BFS"><a href="#3-双向BFS" class="headerlink" title="3.双向BFS"></a>3.双向BFS</h1><p>BFS</p><p><img src="image/image_8BjugZlNkC.png" alt=""></p><p>BFS Levels</p><p><img src="image/image_QGFfJs9vII.png" alt=""></p><p>Two-ended BFS 双向BFS</p><p><img src="image/image_6QfEVvVay6.png" alt=""></p><h1 id="4-启发式搜索-Heuristic-Search（A-）"><a href="#4-启发式搜索-Heuristic-Search（A-）" class="headerlink" title="4.启发式搜索  Heuristic Search（A *）"></a>4.启发式搜索  Heuristic Search（A *）</h1><p>本质：通过优先级不断的找</p><h2 id="4-1-代码模板"><a href="#4-1-代码模板" class="headerlink" title="4.1 代码模板"></a>4.1 代码模板</h2><p>BFS代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bfs</span>(<span class="params">graph, start, end</span>):</span><br><span class="line">  </span><br><span class="line">  queue = []</span><br><span class="line">  queue.append([start])</span><br><span class="line">  visited.add(start)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> queue:</span><br><span class="line">    <span class="comment"># can we add more intelligence here?</span></span><br><span class="line">    node = queue.pop()</span><br><span class="line">    visited.add(node)</span><br><span class="line">    </span><br><span class="line">    process(node)</span><br><span class="line">    nodes = generate_related_nodes(node)</span><br><span class="line">    queue.push(nodes)</span><br><span class="line">    </span><br><span class="line">  <span class="comment"># other processing work</span></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>A* search</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">AstarSearch</span>(<span class="params">graph, start, end</span>):</span><br><span class="line">  </span><br><span class="line">  pq = collections.priority_queue()   <span class="comment"># 优先级 -&gt; 估价函数</span></span><br><span class="line">  queue.append([start])</span><br><span class="line">  visited.add(start)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> queue:</span><br><span class="line">    <span class="comment"># can we add more intelligence here?</span></span><br><span class="line">    node = pq.pop()</span><br><span class="line">    visited.add(node)</span><br><span class="line">    </span><br><span class="line">    process(node)</span><br><span class="line">    nodes = generate_related_nodes(node)</span><br><span class="line">    unvisited = [node <span class="keyword">for</span> node <span class="keyword">in</span> nodes <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> visited]</span><br><span class="line">    pq.push(unvisited)</span><br><span class="line">    </span><br><span class="line">  <span class="comment"># other processing work</span></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><h2 id="4-2-估价函数"><a href="#4-2-估价函数" class="headerlink" title="4.2 估价函数"></a>4.2 估价函数</h2><p>启发式函数 ： <code>h(n)</code>，它用来评价哪些结点最有希望的是一个我们要找的结点，<code>h(n)</code> 会返回一个非负实数,也可以认为是从结点n的目标结点路径的估计成本。</p><p>启发式函数是一种<strong>告知搜索方向</strong>的方法。它提供了一种明智的方法来猜测哪个邻居结点会导向一个目标。</p><h2 id="4-3-例题"><a href="#4-3-例题" class="headerlink" title="4.3 例题"></a>4.3 例题</h2><h3 id="（1）二进制矩阵中的最短路径"><a href="#（1）二进制矩阵中的最短路径" class="headerlink" title="（1）二进制矩阵中的最短路径"></a>（1）二进制矩阵中的最短路径</h3><p><a href="https://leetcode.cn/problems/shortest-path-in-binary-matrix/description/" title="1091. 二进制矩阵中的最短路径 - 力扣（LeetCode）">1091. 二进制矩阵中的最短路径 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">给你一个 n x n 的二进制矩阵 grid 中，返回矩阵中最短 畅通路径 的长度。如果不存在这样的路径，返回 -1 。</span><br><span class="line"></span><br><span class="line">二进制矩阵中的 畅通路径 是一条从 左上角 单元格（即，(0, 0)）到 右下角 单元格（即，(n - 1, n - 1)）的路径，该路径同时满足下述要求：</span><br><span class="line"></span><br><span class="line">- 路径途经的所有单元格的值都是 0 。</span><br><span class="line">- 路径中所有相邻的单元格应当在 8 个方向之一 上连通（即，相邻两单元之间彼此不同且共享一条边或者一个角）。</span><br><span class="line"></span><br><span class="line">畅通路径的长度 是该路径途经的单元格总数。</span><br></pre></td></tr></table></figure><ol><li>DP</li><li>BFS</li><li>A*</li></ol><h4 id="BFS实现"><a href="#BFS实现" class="headerlink" title="BFS实现"></a>BFS实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">shortestPathBinaryMatrix</span>(<span class="params">self, grid: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># q ： row, col, 步数</span></span><br><span class="line">        q, n = [(<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>)], <span class="built_in">len</span>(grid)</span><br><span class="line">        <span class="keyword">if</span> grid[<span class="number">0</span>][<span class="number">0</span>] <span class="keyword">or</span> grid[-<span class="number">1</span>][-<span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> n</span><br><span class="line">        </span><br><span class="line">        dx = [-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        dy = [-<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i, j ,d <span class="keyword">in</span> q:</span><br><span class="line">            <span class="comment"># current node : i, j, distance = d</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dx)):</span><br><span class="line">                x = i + dx[k]</span><br><span class="line">                y = j + dy[k]</span><br><span class="line">                <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; n <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; n <span class="keyword">and</span> <span class="keyword">not</span> grid[x][y]:</span><br><span class="line">                    <span class="keyword">if</span> x == n - <span class="number">1</span> <span class="keyword">and</span> y == n - <span class="number">1</span>:</span><br><span class="line">                        <span class="keyword">return</span> d</span><br><span class="line">                    q += [(x, y, d + <span class="number">1</span>)]</span><br><span class="line">                    <span class="comment"># 点已经被访问过了</span></span><br><span class="line">                    grid[x][y] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure><h4 id="A-search"><a href="#A-search" class="headerlink" title="A* search"></a>A* search</h4><ul><li><a href="https://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/" title="相似度测量方法">相似度测量方法</a></li><li><a href="https://leetcode.com/problems/shortest-path-in-binary-matrix/discuss/313347/A*-search-in-Python" title="二进制矩阵中的最短路径的 A* 解法">二进制矩阵中的最短路径的 A* 解法</a></li><li><a href="https://zxi.mytechroad.com/blog/searching/8-puzzles-bidirectional-astar-vs-bidirectional-bfs/" title="8 puzzles 解法比较">8 puzzles 解法比较</a></li></ul><p>估值函数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h(current_point) = dist(curr_point, desttination_point)</span><br></pre></td></tr></table></figure><h3 id="（2）滑动谜题"><a href="#（2）滑动谜题" class="headerlink" title="（2）滑动谜题"></a>（2）滑动谜题</h3><p><a href="https://zxi.mytechroad.com/blog/searching/8-puzzles-bidirectional-astar-vs-bidirectional-bfs/" title="8 puzzles 解法比较">8 puzzles 解法比较</a></p><p><a href="https://leetcode.cn/problems/sliding-puzzle/description/" title="773. 滑动谜题 - 力扣（LeetCode）">773. 滑动谜题 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在一个 2 x 3 的板上（board）有 5 块砖瓦，用数字 1~5 来表示, 以及一块空缺用 0 来表示。一次 移动 定义为选择 0 与一个相邻的数字（上下左右）进行交换.</span><br><span class="line"></span><br><span class="line">最终当板 board 的结果是 [[1,2,3],[4,5,0]] 谜板被解开。</span><br><span class="line"></span><br><span class="line">给出一个谜板的初始状态 board ，返回最少可以通过多少次移动解开谜板，如果不能解开谜板，则返回 -1 。</span><br></pre></td></tr></table></figure><ol><li>DFS</li><li>BFS - 更快找到最优解</li><li>A*</li></ol><h4 id="BFS代码"><a href="#BFS代码" class="headerlink" title="BFS代码"></a>BFS代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">slidingPuzzle</span>(<span class="params">self, board: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 方向向量</span></span><br><span class="line">        <span class="comment"># 如果0位于下标0位置，则可以向下标1和下标3互换位置</span></span><br><span class="line">        <span class="comment"># 如果0位于下标1位置，则可以向下标0、下标2和下标4互换位置</span></span><br><span class="line">        moves = [[<span class="number">1</span>, <span class="number">3</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">4</span>]]</span><br><span class="line">    </span><br><span class="line">        initial = <span class="string">&quot;&quot;</span>.join(<span class="built_in">str</span>(c) <span class="keyword">for</span> row <span class="keyword">in</span> board <span class="keyword">for</span> c <span class="keyword">in</span> row)</span><br><span class="line">        <span class="keyword">if</span> initial == <span class="string">&quot;123450&quot;</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># 访问过的字符串</span></span><br><span class="line">        used = <span class="built_in">set</span>()</span><br><span class="line">        <span class="comment"># [字符，步数] [s, cnt]</span></span><br><span class="line">        q = deque([(initial, <span class="number">0</span>)])</span><br><span class="line">        used.add(initial)</span><br><span class="line">        <span class="keyword">while</span> q:</span><br><span class="line">            s, cnt = q.popleft()</span><br><span class="line">            used.add(s)</span><br><span class="line">            <span class="keyword">if</span> s == <span class="string">&quot;123450&quot;</span>:</span><br><span class="line">                <span class="keyword">return</span> cnt</span><br><span class="line">            <span class="comment"># 将字符串变为列表，方便交换</span></span><br><span class="line">            arr = [c <span class="keyword">for</span> c <span class="keyword">in</span> s]</span><br><span class="line">            <span class="comment"># 开始移动0</span></span><br><span class="line">            zero_idx = s.index(<span class="string">&#x27;0&#x27;</span>)</span><br><span class="line">            <span class="keyword">for</span> move <span class="keyword">in</span> moves[zero_idx]:</span><br><span class="line">                <span class="comment"># copy一份</span></span><br><span class="line">                new_arr = arr[:]</span><br><span class="line">                <span class="comment"># 交换</span></span><br><span class="line">                new_arr[zero_idx], new_arr[move] = new_arr[move], new_arr[zero_idx]</span><br><span class="line">                new_s = <span class="string">&quot;&quot;</span>.join(new_arr)</span><br><span class="line">                <span class="keyword">if</span> new_s <span class="keyword">not</span> <span class="keyword">in</span> used:</span><br><span class="line">                    q.append((new_s, cnt + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">slidingPuzzle</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; board)</span> </span>&#123;</span><br><span class="line">        std::string init_str;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; b : board) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; c : b) &#123;</span><br><span class="line">                init_str += <span class="built_in">char</span>(c + <span class="string">&#x27;0&#x27;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        std::string end_str = <span class="string">&quot;123450&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// [string, count]</span></span><br><span class="line">        std::queue&lt;std::pair&lt;std::string, <span class="type">int</span>&gt;&gt; queue;</span><br><span class="line">        queue.<span class="built_in">emplace</span>(init_str, <span class="number">0</span>);</span><br><span class="line">        <span class="comment">// 访问过的字符串</span></span><br><span class="line">        std::unordered_set&lt;std::string&gt; used = &#123;init_str&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (!queue.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            <span class="keyword">auto</span> [str, cnt] = queue.<span class="built_in">front</span>();</span><br><span class="line">            queue.<span class="built_in">pop</span>();</span><br><span class="line">            used.<span class="built_in">insert</span>(str);</span><br><span class="line">            <span class="keyword">if</span> (str == end_str) &#123;</span><br><span class="line">                <span class="keyword">return</span> cnt;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 字符串变为列表，方便操作</span></span><br><span class="line">            std::vector&lt;<span class="type">int</span>&gt; arr_str;</span><br><span class="line">            <span class="type">int</span> zero_idx = <span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; str.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">                <span class="type">char</span> c = str[i];</span><br><span class="line">                <span class="keyword">if</span> (c == <span class="string">&#x27;0&#x27;</span>) &#123;</span><br><span class="line">                    zero_idx = i;</span><br><span class="line">                &#125;</span><br><span class="line">                arr_str.<span class="built_in">push_back</span>(<span class="built_in">int</span>(c - <span class="string">&#x27;0&#x27;</span>));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 开始移动0</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; move : m_moves[zero_idx]) &#123;</span><br><span class="line">                std::vector&lt;<span class="type">int</span>&gt; new_arr = arr_str;</span><br><span class="line">                <span class="comment">// 交换</span></span><br><span class="line">                <span class="type">int</span> tmp = new_arr[zero_idx];</span><br><span class="line">                new_arr[zero_idx] = new_arr[move];</span><br><span class="line">                new_arr[move] = tmp;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 移动完成，变为字符串</span></span><br><span class="line">                std::string new_s;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; c : new_arr) &#123;</span><br><span class="line">                    new_s += <span class="built_in">char</span>(c + <span class="string">&#x27;0&#x27;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                </span><br><span class="line">                <span class="comment">// 如果在访问字符串中没有，则加入队列</span></span><br><span class="line">                <span class="keyword">if</span> (!used.<span class="built_in">count</span>(new_s)) &#123;</span><br><span class="line">                    queue.<span class="built_in">emplace</span>(new_s, cnt + <span class="number">1</span>);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 方向向量</span></span><br><span class="line">    <span class="comment">// 如果0位于下标0位置，则可以向下标1和下标3互换位置</span></span><br><span class="line">    <span class="comment">// 如果0位于下标1位置，则可以向下标0、下标2和下标4互换位置</span></span><br><span class="line">    std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; m_moves = &#123;&#123;<span class="number">1</span>, <span class="number">3</span>&#125;, &#123;<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>&#125;, &#123;<span class="number">1</span>, <span class="number">5</span>&#125;, &#123;<span class="number">0</span>, <span class="number">4</span>&#125;, &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>&#125;, &#123;<span class="number">2</span>, <span class="number">4</span>&#125;&#125;;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="A"><a href="#A" class="headerlink" title="A*"></a>A*</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AStar</span>:</span><br><span class="line">    DIST = [</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># 计算启发函数</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_h</span>(<span class="params">s : <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        ret = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">            <span class="keyword">if</span> s[i] != <span class="string">&quot;0&quot;</span>:</span><br><span class="line">                ret += AStar.DIST[i][<span class="built_in">int</span>(s[i]) - <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> ret</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, s : <span class="built_in">str</span>, cnt : <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.s = s</span><br><span class="line">        self.cnt = cnt</span><br><span class="line">        self.h = AStar.get_h(s)</span><br><span class="line">        self.f = self.cnt + self.h</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__lt__</span>(<span class="params">self, other:<span class="string">&quot;AStar&quot;</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">return</span> self.f &lt; other.f</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># BFS</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">slidingPuzzle</span>(<span class="params">self, board: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 方向向量</span></span><br><span class="line">        <span class="comment"># 如果0位于下标0位置，则可以向下标1和下标3互换位置</span></span><br><span class="line">        <span class="comment"># 如果0位于下标1位置，则可以向下标0、下标2和下标4互换位置</span></span><br><span class="line">        moves = [[<span class="number">1</span>, <span class="number">3</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">4</span>]]</span><br><span class="line">    </span><br><span class="line">        initial = <span class="string">&quot;&quot;</span>.join(<span class="built_in">str</span>(c) <span class="keyword">for</span> row <span class="keyword">in</span> board <span class="keyword">for</span> c <span class="keyword">in</span> row)</span><br><span class="line">        end_state = <span class="string">&quot;123450&quot;</span></span><br><span class="line">        <span class="keyword">if</span> initial == end_state:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># 访问过的字符串</span></span><br><span class="line">        used = <span class="built_in">set</span>()</span><br><span class="line">        <span class="comment"># [字符，步数] [s, cnt]</span></span><br><span class="line">        q = [AStar(initial, <span class="number">0</span>)]</span><br><span class="line">        used.add(initial)</span><br><span class="line">        <span class="keyword">while</span> q:</span><br><span class="line">            node = heapq.heappop(q)</span><br><span class="line">            s = node.s</span><br><span class="line">            cnt = node.cnt</span><br><span class="line">            used.add(s)</span><br><span class="line">            <span class="keyword">if</span> s == end_state:</span><br><span class="line">                <span class="keyword">return</span> cnt</span><br><span class="line">            <span class="comment"># 将字符串变为列表，方便交换</span></span><br><span class="line">            arr = [c <span class="keyword">for</span> c <span class="keyword">in</span> s]</span><br><span class="line">            <span class="comment"># 开始移动0</span></span><br><span class="line">            zero_idx = s.index(<span class="string">&#x27;0&#x27;</span>)</span><br><span class="line">            <span class="keyword">for</span> move <span class="keyword">in</span> moves[zero_idx]:</span><br><span class="line">                <span class="comment"># copy一份</span></span><br><span class="line">                new_arr = arr[:]</span><br><span class="line">                <span class="comment"># 交换</span></span><br><span class="line">                new_arr[zero_idx], new_arr[move] = new_arr[move], new_arr[zero_idx]</span><br><span class="line">                new_s = <span class="string">&quot;&quot;</span>.join(new_arr)</span><br><span class="line">                <span class="keyword">if</span> new_s <span class="keyword">not</span> <span class="keyword">in</span> used:</span><br><span class="line">                    <span class="comment"># q.append((new_s, cnt + 1))</span></span><br><span class="line">                    heapq.heappush(q, AStar(new_s, cnt + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>15 红黑树和AVL树</title>
      <link href="/dsa/leetcode/15_avl_tree/README/"/>
      <url>/dsa/leetcode/15_avl_tree/README/</url>
      
        <content type="html"><![CDATA[<h1 id="0-复习"><a href="#0-复习" class="headerlink" title="0.复习"></a>0.复习</h1><h3 id="树"><a href="#树" class="headerlink" title="树"></a>树</h3><p><img src="image/image_-90MARaTbP.png" alt=""></p><h3 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h3><p><img src="image/image_Sp04HM85rB.png" alt=""></p><h3 id="二叉树遍历"><a href="#二叉树遍历" class="headerlink" title="二叉树遍历"></a>二叉树遍历</h3><ul><li>前序（pre-order）：根 → 左 → 右</li><li>中序（in-order）：左 → 根 → 右</li><li>后序（post-order）：左 → 右 → 根</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preorder</span>(<span class="params">self, root</span>):</span><br><span class="line">  <span class="keyword">if</span> root:</span><br><span class="line">    self.traverse_path.append(root, val)</span><br><span class="line">    self.preorder(root.left)</span><br><span class="line">    self.preorder(root.right)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inorder</span>(<span class="params">self, root</span>):</span><br><span class="line">  <span class="keyword">if</span> root:</span><br><span class="line">    self.inorder(root.left)</span><br><span class="line">    self.traverse_path.append(root, val)</span><br><span class="line">    self.inorder(root.right)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">postorder</span>(<span class="params">self, root</span>):</span><br><span class="line">  <span class="keyword">if</span> root:</span><br><span class="line">    self.postorder(root.left)</span><br><span class="line">    self.postorder(root.right)</span><br><span class="line">    self.traverse_path.append(root, val)</span><br></pre></td></tr></table></figure><h3 id="二叉搜索树-Binary-Search-Tree"><a href="#二叉搜索树-Binary-Search-Tree" class="headerlink" title="二叉搜索树 Binary Search Tree"></a>二叉搜索树 Binary Search Tree</h3><p>二叉搜索树，也称二又搜索树、有序二叉树 (Ordered Binary Tree) 、排序二叉树 (Sorted Binary Tree) ，是指一棵空树或者具有下列性质的二叉树:</p><ol><li>左子树上<strong>所有结点</strong>的值均小于它的根结点的值</li><li>右子树上<strong>所有结点</strong>的值均大于它的根结点的值</li><li>以此类推:左、右子树也分别为二叉查找树。(这就是 重复性!)</li></ol><p>中序遍历: 升序排列</p><p><img src="image/image_hK1PgvIUw3.png" alt=""></p><h3 id="保证性能的关键"><a href="#保证性能的关键" class="headerlink" title="保证性能的关键"></a>保证性能的关键</h3><p><a href="https://en.wikipedia.org/wiki/Self-balancing_binary_search_tree" title="Self-balancing binary search tree - Wikipedia">Self-balancing binary search tree - Wikipedia</a></p><ol><li>保证二维维度  → 左右子树结点平衡 （recursively）</li><li>Balanced（平衡二叉树有很多种，面试主要AVL和红黑树，还有treap及伸展树）</li></ol><h1 id="1-AVL树"><a href="#1-AVL树" class="headerlink" title="1.AVL树"></a>1.AVL树</h1><p>发明者 G.M.Adelson-Velsky 和 Evgenii Landis</p><h2 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h2><ol><li>Balance Factor（平衡因子）：是它的左右子树的高度减去它的右子树高度（有时相反）。$balance ~factor = \{-1, 0, 1\}$</li><li>通过旋转操作来进行平衡（四种）</li></ol><p>记录左右子树高度</p><p><img src="image/image_MgJaxOCCah.png" alt=""></p><p>增加14</p><p><img src="image/image_61T7l5ZI54.png" alt=""></p><p>增加3</p><p><img src="image/image_MJ_Ql6BWR3.png" alt=""></p><p><img src="image/image_-pecUh4j5C.png" alt=""></p><h2 id="1-2-旋转操作"><a href="#1-2-旋转操作" class="headerlink" title="1.2 旋转操作"></a>1.2 旋转操作</h2><ol><li>左旋</li><li>右旋</li><li>左右旋</li><li>右左旋</li></ol><p><strong>关键技巧：</strong></p><ul><li>中值为根，然后将子树拆分挂到对应位置，</li><li>最后再根据二叉排序树的左小右大的规则拼凑起来。</li></ul><p><strong>最关键核心的操作就是**</strong>让失衡根结点转换成中间值的孩子<strong>**。拆的时候是根据做小右大的规则进行拆，挂的时候也是根左小右大的规则进行挂。</strong></p><p><strong>LR和RL进行转换，将其利用LL和RR将其转换成LL或者RR。</strong></p><h3 id="（1）-右右子树-→-左旋"><a href="#（1）-右右子树-→-左旋" class="headerlink" title="（1） 右右子树 → 左旋"></a>（1） 右右子树 → 左旋</h3><p>插入位置是失衡根结点的左子树的左子树。</p><p><strong>调整方法</strong>：<strong>最小子树A的左孩子右上旋转</strong>。</p><p><img src="image/image_74XxbBLI4w.png" alt=""></p><p><img src="image/image_p6TGayxHw6.png" alt=""></p><h3 id="（2）左左子树-→-右旋"><a href="#（2）左左子树-→-右旋" class="headerlink" title="（2）左左子树 → 右旋"></a>（2）左左子树 → 右旋</h3><p>插入位置是失衡结点的右子树的右子树。</p><p><strong>调整方法</strong>：<strong>最小子树A的右孩子左上旋转</strong>。</p><p><img src="image/image_zgjjU8k8IU.png" alt=""></p><p><img src="image/image_PG7GiDa_2C.png" alt=""></p><h3 id="（3）左右子树-→-左右旋"><a href="#（3）左右子树-→-左右旋" class="headerlink" title="（3）左右子树 → 左右旋"></a>（3）左右子树 → 左右旋</h3><p>插入位置是失衡结点的左子树的右子树。<strong>（**</strong>关键是将R变成L,LL<strong>**）</strong></p><p><strong>调整方法</strong>：<strong>最小子树A的左孩子的右孩子，先左上旋转再右上旋转</strong>。</p><p><img src="image/image_jQXsWbdQrT.png" alt=""></p><p><img src="image/image_SBxnW3DHYk.png" alt=""></p><h3 id="（4）右左子树-→-右左旋"><a href="#（4）右左子树-→-右左旋" class="headerlink" title="（4）右左子树 → 右左旋"></a>（4）右左子树 → 右左旋</h3><p>插入位置是失衡结点的右子树的左子树。<strong>(**</strong>关键是将L变成R，RR<strong>**)</strong></p><p><strong>调整方法</strong>：<strong>最小子树A的右孩子的左孩子，先**</strong>右上<strong>**旋转再左上旋转</strong>。</p><p><img src="image/image_hMG7eNnwM_.png" alt=""></p><p><img src="image/image_1Pa6DcCBaQ.png" alt=""></p><h2 id="1-3-AVL总结"><a href="#1-3-AVL总结" class="headerlink" title="1.3 AVL总结"></a>1.3 AVL总结</h2><ol><li>平衡二又搜索树</li><li>每个结点存 $balance ~factor =\{-1,0,1\}$</li><li>四种旋转操作</li></ol><p>不足 :  结点需要存储额外信息、且调整次数频繁</p><h1 id="3-红黑树（Red-black-Tree）"><a href="#3-红黑树（Red-black-Tree）" class="headerlink" title="3.红黑树（Red-black Tree）"></a>3.红黑树（Red-black Tree）</h1><p>近似平衡二叉树</p><h2 id="3-1-概念"><a href="#3-1-概念" class="headerlink" title="3.1 概念"></a>3.1 概念</h2><p>红黑树是一种<strong>近似平衡</strong>的二又搜索树( Binary Search Tree)，它能够确保任何一个结点的左右子树的<strong>高度差小于两倍</strong>。具体来说，红黑树是满足如下条件的二叉搜索树：</p><ul><li>每个结点要么是红色，要么是黑色</li><li>根结点是黑色</li><li>每个叶结点 (NIL结点，空结点) 是黑色的。</li><li>不能有相邻接的两个红色结点</li><li>从任一结点到其每个叶子的所有路径都包含相同数目的黑色结点。</li></ul><p><img src="image/image_-Hw3McwOC1.png" alt=""></p><p><img src="image/image_eH7d4SGa62.png" alt=""></p><h2 id="3-2-关键性质"><a href="#3-2-关键性质" class="headerlink" title="3.2 关键性质"></a>3.2 关键性质</h2><p>从根到叶子的最长的可能路径不多于最短的可能路径的两倍长。</p><h1 id="4-对比"><a href="#4-对比" class="headerlink" title="4.对比"></a>4.对比</h1><ul><li>AVL trees provide <strong>faster lookups</strong> than Red Black Trees because they are <strong>more strictl<br>balanced</strong></li><li>Red Black Trees provide <strong>faster insertion and removal</strong> operations than AVL trees as<br>fewer rotations are done due to relatively relaxed balancing.</li><li>AVL trees store balance <strong>factors or heights</strong> with each node, thus requires storage for<br>an integer per node whereas Red Black Tree requires only 1 bit of information per<br>node.</li><li>Red Black Trees are used in most of the <strong>language libraries</strong>. like <strong>map, multimap, multisetin C++</strong> whereas AVL trees are used in <strong>databases</strong> where faster retrievals are required.</li></ul>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>16 位运算</title>
      <link href="/dsa/leetcode/16_bitwise/README/"/>
      <url>/dsa/leetcode/16_bitwise/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-位运算符"><a href="#1-位运算符" class="headerlink" title="1.位运算符"></a>1.位运算符</h1><ul><li><a href="https://zh.wikihow.com/从十进制转换为二进制" title="如何从十进制转换为二进制">如何从十进制转换为二进制</a></li></ul><p>机器里的数字表示方式和存储格式就是二进制</p><h1 id="2-算数移位与逻辑移位"><a href="#2-算数移位与逻辑移位" class="headerlink" title="2.算数移位与逻辑移位"></a>2.算数移位与逻辑移位</h1><div class="table-container"><table><thead><tr><th>含义</th><th>运算符</th><th>示例</th></tr></thead><tbody><tr><td>左移</td><td>&lt;&lt;</td><td>0011 → 0110</td></tr><tr><td>右移</td><td>&gt;&gt;</td><td>0110 → 0011</td></tr><tr><td>按位或</td><td>\</td><td></td><td>0011 \</td><td>1011 → 1011</td></tr><tr><td>按位与</td><td>&amp;</td><td>0011 &amp; 1011→ 0011</td></tr><tr><td>按位取反</td><td>~</td><td>0011  → 1100</td></tr><tr><td>按位异或&#xA;(相同为0不同为1)</td><td>^</td><td>0011 \</td><td>1011 → 1000</td></tr></tbody></table></div><h1 id="3-位运算的应用"><a href="#3-位运算的应用" class="headerlink" title="3.位运算的应用"></a>3.位运算的应用</h1><h2 id="3-1-XOR-异或"><a href="#3-1-XOR-异或" class="headerlink" title="3.1 XOR - 异或"></a>3.1 XOR - 异或</h2><p>异或：相同为0，不同为1。也可用“不进位加法”来理解</p><p>异或的一些特点：</p><ul><li><code>x^0 = x</code></li><li><code>x^1s = ~x</code>  (注意 1s = ~0， “全1”)</li><li><code>x^(~x) = 1s</code></li><li><code>x^x = 0</code></li><li><code>c = a ^ b → a^c = b, b^c = a</code> (交换两个数)</li><li><code>a^b^c = a^(b^c)=(a^b)^c</code> (associative)</li></ul><h2 id="3-2-指定位置的位运算"><a href="#3-2-指定位置的位运算" class="headerlink" title="3.2 指定位置的位运算"></a>3.2 指定位置的位运算</h2><ol><li>将x最右边的 n位清零: <code>x &amp; (~0&lt;&lt; n)</code></li><li>获取x的第 n位值 (0或者 1) : <code>(x&gt;&gt; n) &amp; 1</code></li><li>获取x的第 n位的幂值: <code>x &amp; (1 &lt;&lt;(n-1))</code></li><li>仅将第n位置为 1: <code>x | (1 &lt;&lt; n)</code></li><li>仅将第n位置为 0: <code>x &amp; (~(1 &lt;&lt; n)</code></li><li>将x最高位至第n位 (含) 清零: <code>x &amp; ((1 &lt;&lt; n)-1)</code></li><li>将第n位至第0位 (含) 清零: <code>x &amp; (~((1&lt;&lt;(n +1))-1))</code></li></ol><h2 id="3-3-实战位运算要点"><a href="#3-3-实战位运算要点" class="headerlink" title="3.3 实战位运算要点"></a>3.3 实战位运算要点</h2><ul><li>判断奇偶<ul><li><code>x % 2==1</code>  →  <code>(x &amp; 1)==1</code></li><li><code>x % 2==0</code>  →  <code>(x &amp; 1)==0</code></li></ul></li><li><code>x&gt;&gt;1</code> → <code>x/2</code><br>即: <code>x=x/2;</code>  -&gt; <code>X=X &gt;&gt; 1;``mid =(left +right) / 2;</code> → <code>mid = (left +right) &gt;&gt; 1</code></li><li><code>X = X &amp; (X - 1)</code> : 清零最低位的 1</li><li><code>X &amp; -X</code>  → 得到最低位的1</li><li><code>X&amp;~X → 0</code></li></ul><h1 id="4-实战题目"><a href="#4-实战题目" class="headerlink" title="4.实战题目"></a>4.实战题目</h1><h2 id="4-1-位1的个数"><a href="#4-1-位1的个数" class="headerlink" title="4.1 位1的个数"></a>4.1 位1的个数</h2><p><a href="https://leetcode.cn/problems/number-of-1-bits/description/" title="191. 位1的个数 - 力扣（LeetCode）">191. 位1的个数 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">编写一个函数，输入是一个无符号整数（以二进制串的形式），返回其二进制表达式中数字位数为 <span class="string">&#x27;1&#x27;</span> 的个数（也被称为汉明重量）。</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol><li>for loop : 0 → 32</li><li>%2, /2</li><li>&amp;1, x = x &gt;&gt; 1</li><li>while (x &gt; 0) { count ++; x = x &amp; (x - 1) }</li></ol><p>循环和位移动</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 循环和位移动</span></span><br><span class="line"><span class="comment">// 遍历数字的32位，如果某一位为1，计数器加一</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">hammingWeight1</span><span class="params">(<span class="type">uint32_t</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> mask  = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">32</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> ((n &amp; mask) != <span class="number">0</span>) &#123;</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        mask &lt;&lt;= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>位操作</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2.位操作</span></span><br><span class="line"><span class="comment">// 不断把数字最后一个1翻转，并计数+1.当数字变为0时，此时没有1了</span></span><br><span class="line"><span class="comment">// n 和 n-1 做与运算，会把最后一个1变为0</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">hammingWeight</span><span class="params">(<span class="type">uint32_t</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (n != <span class="number">0</span>) &#123;</span><br><span class="line">        count++;</span><br><span class="line">        n &amp;= (n - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4-2-2的幂"><a href="#4-2-2的幂" class="headerlink" title="4.2 2的幂"></a>4.2 2的幂</h2><p><a href="https://leetcode.cn/problems/power-of-two/" title="231. 2 的幂 - 力扣（LeetCode）">231. 2 的幂 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数 n，请你判断该整数是否是 2 的幂次方。如果是，返回 <span class="literal">true</span> ；否则，返回 <span class="literal">false</span> 。</span><br><span class="line"></span><br><span class="line">如果存在一个整数 x 使得 n == 2^x ，则认为 n 是 2 的幂次方。</span><br></pre></td></tr></table></figure><p>2的幂：<strong>这个数的二进制表示，有且只有一个二进制位是1</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isPowerOfTwo</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1.n &gt; 0</span></span><br><span class="line">        <span class="comment">// 2.二进制表示只有一个1，打掉之后，肯定为0</span></span><br><span class="line">        <span class="keyword">return</span> (n &gt; <span class="number">0</span>) &amp;&amp; (n &amp; (n - <span class="number">1</span>)) == <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="4-3-颠倒二进制位"><a href="#4-3-颠倒二进制位" class="headerlink" title="4.3 颠倒二进制位"></a>4.3 颠倒二进制位</h2><p><a href="https://leetcode.cn/problems/reverse-bits/description/" title="190. 颠倒二进制位 - 力扣（LeetCode）">190. 颠倒二进制位 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">颠倒给定的 32 位无符号整数的二进制位。</span><br><span class="line"></span><br><span class="line">提示：</span><br><span class="line"></span><br><span class="line">- 请注意，在某些语言（如 Java）中，没有无符号整数类型。在这种情况下，输入和输出都将被指定为有符号整数类型，并且不应影响您的实现，因为无论整数是有符号的还是无符号的，其内部的二进制表示形式都是相同的。</span><br><span class="line">- 在 Java 中，编译器使用二进制补码记法来表示有符号整数。因此，在 示例 2 中，输入表示有符号整数 -3，输出表示有符号整数 -1073741825。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 不断把n的最后一位输送到res的最后一位，res再不断的左移</span></span><br><span class="line">    <span class="function"><span class="type">uint32_t</span> <span class="title">reverseBits</span><span class="params">(<span class="type">uint32_t</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="type">uint32_t</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 操作32此移位操作</span></span><br><span class="line">        <span class="type">int</span> idx = <span class="number">32</span>;</span><br><span class="line">        <span class="keyword">while</span> (idx--) &#123;</span><br><span class="line">            <span class="comment">// 结果左移一位，空出位置与n的最后一位相加</span></span><br><span class="line">            res &lt;&lt;= <span class="number">1</span>;</span><br><span class="line">            <span class="comment">// 加上n的最后一位</span></span><br><span class="line">            res += n &amp; <span class="number">1</span>;</span><br><span class="line">            <span class="comment">// n右移一位，供下一轮与结果相加</span></span><br><span class="line">            n &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 多次移位</span></span><br><span class="line">    <span class="function"><span class="type">uint32_t</span> <span class="title">reverseBits2</span><span class="params">(<span class="type">uint32_t</span> n)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">uint32_t</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">32</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (n &amp; (<span class="number">1</span> &lt;&lt; i)) &#123;</span><br><span class="line">                res |= <span class="number">1</span> &lt;&lt; (<span class="number">31</span> - i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="4-4-n皇后问题"><a href="#4-4-n皇后问题" class="headerlink" title="4.4 n皇后问题"></a>4.4 n皇后问题</h2><p><a href="https://leetcode.cn/problems/n-queens/" title="51. N 皇后 - 力扣（LeetCode）">51. N 皇后 - 力扣（LeetCode）</a></p><p><a href="https://leetcode.cn/problems/n-queens-ii/description/" title="52. N 皇后 II - 力扣（LeetCode）">52. N 皇后 II - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n 皇后问题 研究的是如何将 n 个皇后放置在 n × n 的棋盘上，并且使皇后彼此之间不能相互攻击。</span><br><span class="line"></span><br><span class="line">给你一个整数 n ，返回 n 皇后问题 不同的解决方案的数量。</span><br></pre></td></tr></table></figure><p>使用位运算解法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">totalNQueens</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> n &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        self.count = <span class="number">0</span></span><br><span class="line">        self.dfs(n, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> self.count</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">self, n, row, cols, pie, na</span>):</span><br><span class="line">        <span class="comment"># 递归终止条件</span></span><br><span class="line">        <span class="keyword">if</span> row &gt;= n:</span><br><span class="line">            self.count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 得到当前所有的空位</span></span><br><span class="line">        bits = (~(cols | pie | na)) &amp; ((<span class="number">1</span> &lt;&lt; n) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> bits:</span><br><span class="line">            <span class="comment"># 取到最低为的1</span></span><br><span class="line">            p = bits &amp; -bits</span><br><span class="line">            <span class="comment"># 表示在p位置上放入皇后</span></span><br><span class="line">            bits = bits &amp; (bits - <span class="number">1</span>)</span><br><span class="line">            self.dfs(n, row + <span class="number">1</span>, cols | p, (pie | p) &lt;&lt; <span class="number">1</span>, (na | p) &gt;&gt; <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 不需要revert cols, pie, na 的状态</span></span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">totalNQueens</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        m_count = <span class="number">0</span>;</span><br><span class="line">        m_size = (<span class="number">1</span> &lt;&lt; n) - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> m_count;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> m_size;</span><br><span class="line">    <span class="type">int</span> m_count;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> row, <span class="type">int</span> pie, <span class="type">int</span> na)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 递归终止条件</span></span><br><span class="line">        <span class="keyword">if</span> (row == m_size) &#123;</span><br><span class="line">            m_count++;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 得到当前所有空位</span></span><br><span class="line">        <span class="type">int</span> pos = m_size &amp; (~(row | pie | na));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (pos != <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 取到最低位的1</span></span><br><span class="line">            <span class="type">int</span> p = pos &amp; (-pos);</span><br><span class="line">            <span class="comment">// 将p位置放入皇后</span></span><br><span class="line">            pos -= p; <span class="comment">// pos &amp;= pos - 1</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;<span class="built_in">dfs</span>(row | p, (pie | p) &lt;&lt; <span class="number">1</span>, (na | p) &gt;&gt; <span class="number">1</span>);</span><br><span class="line">            <span class="comment">// 不需要revert cols, pie, na 的状态</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="4-5-比特位计数（DP-）"><a href="#4-5-比特位计数（DP-）" class="headerlink" title="4.5 比特位计数（DP+）"></a>4.5 比特位计数（DP+）</h2><p><a href="https://leetcode.cn/problems/counting-bits/description/" title="338. 比特位计数 - 力扣（LeetCode）">338. 比特位计数 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数 n ，对于 0 &lt;= i &lt;= n 中的每个 i ，计算其二进制表示中 1 的个数 ，返回一个长度为 n + 1 的数组 ans 作为答案。</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>对于任意整数 xxx，令 <code>x=x &amp; (x−1)</code>，该运算将 x 的二进制表示的最后一个 1 变成 0。因此，对 x 重复该操作，直到 x 变成 0，则操作次数即为 x 的「一比特数」。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">countOnes</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> ones = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (x &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            x &amp;= (x - <span class="number">1</span>);</span><br><span class="line">            ones++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ones;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">countBits</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">bits</span><span class="params">(n + <span class="number">1</span>)</span></span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            bits[i] = <span class="built_in">countOnes</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> bits;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>17 布隆过滤器和LRU缓存</title>
      <link href="/dsa/leetcode/17_bloom_filter_lru/README/"/>
      <url>/dsa/leetcode/17_bloom_filter_lru/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-布隆过滤器（Bloom-Filter）"><a href="#1-布隆过滤器（Bloom-Filter）" class="headerlink" title="1.布隆过滤器（Bloom Filter）"></a>1.布隆过滤器（Bloom Filter）</h1><h2 id="1-1-HashTable-拉链存储重复元素"><a href="#1-1-HashTable-拉链存储重复元素" class="headerlink" title="1.1 HashTable + 拉链存储重复元素"></a>1.1 HashTable + 拉链存储重复元素</h2><p><img src="image/image_3UUR0wWxlM.png" alt=""></p><p>实际应用时，更多需要存储 <strong>有还是没有</strong>，hashtable中存储有大量信息，比较占内存</p><h2 id="1-2-Bloom-Filter"><a href="#1-2-Bloom-Filter" class="headerlink" title="1.2 Bloom Filter"></a>1.2 Bloom Filter</h2><p>一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以<strong>用于检索一个元素是否在一个集合中。</strong></p><p>优点是<strong>空间效率和查询时间都远远超过一般的算法</strong></p><p>缺点是有一定的误识别率和删除困难</p><p><img src="image/image_Qp6FwcB59Q.png" alt=""></p><p>插入AE元素，当测试B元素时，发现映射的二进制位为1，出现误识别；</p><ul><li>当元素对应的二进制位为1时，此元素可能存在在布隆过滤器中；</li><li>当元素对应的二进制位有一个为0时，此元素不可能在布隆过滤器中；</li></ul><p><img src="image/image_GFQvohkbIQ.png" alt=""></p><p>布隆<strong>过滤器一般放在外面当一个缓存使用，做一个快速的判断</strong>；当B在布隆过滤器中查询到，会到DB中去查找数据。</p><h2 id="1-3-案例"><a href="#1-3-案例" class="headerlink" title="1.3 案例"></a>1.3 案例</h2><ol><li>比特币</li><li>分布式系统（Map-Reduce） - Hadoop、search engin</li><li>Redis缓存</li><li>垃圾邮件、评论过滤</li></ol><ul><li><a href="https://www.cnblogs.com/cpselvis/p/6265825.html" title="布隆过滤器的原理和实现">布隆过滤器的原理和实现</a></li><li><a href="https://blog.csdn.net/tianyaleixiaowu/article/details/74721877" title="使用布隆过滤器解决缓存击穿、垃圾邮件识别、集合判重">使用布隆过滤器解决缓存击穿、垃圾邮件识别、集合判重</a></li></ul><h2 id="1-4-实现"><a href="#1-4-实现" class="headerlink" title="1.4 实现"></a>1.4 实现</h2><ul><li><a href="https://www.geeksforgeeks.org/bloom-filters-introduction-and-python-implementation/" title="布隆过滤器 Python 实现示例">布隆过滤器 Python 实现示例</a></li><li><a href="https://github.com/jhgg/pybloof" title="高性能布隆过滤器 Python 实现示例">高性能布隆过滤器 Python 实现示例</a></li><li><a href="https://github.com/lovasoa/bloomfilter/blob/master/src/main/java/BloomFilter.java" title="布隆过滤器 Java 实现示例 1">布隆过滤器 Java 实现示例 1</a></li><li><a href="https://github.com/Baqend/Orestes-Bloomfilter" title="布隆过滤器 Java 实现示例 2">布隆过滤器 Java 实现示例 2</a></li></ul><p><strong>python实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bitarray <span class="keyword">import</span> bitarray</span><br><span class="line"><span class="keyword">import</span> mmh3</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BloomFilter</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, hash_num</span>):</span><br><span class="line">        self.size = size</span><br><span class="line">        self.hash_num = hash_num</span><br><span class="line">        self.bit_array = bitarray(size)</span><br><span class="line">        self.bit_array.setall(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, s</span>):</span><br><span class="line">        <span class="keyword">for</span> seed <span class="keyword">in</span> <span class="built_in">range</span>(self.hash_num):</span><br><span class="line">            result = mmh3.<span class="built_in">hash</span>(s, seed) % self.size</span><br><span class="line">            self.bit_array[result] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lookup</span>(<span class="params">self, s</span>):</span><br><span class="line">        <span class="keyword">for</span> seed <span class="keyword">in</span> <span class="built_in">range</span>(self.hash_num):</span><br><span class="line">            result = mmh3.<span class="built_in">hash</span>(s, seed) % self.size</span><br><span class="line">            <span class="keyword">if</span> self.bit_array[result] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;None&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Probably&quot;</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">bf = BloomFilter(<span class="number">500000</span>, <span class="number">7</span>)</span><br><span class="line">bf.add(<span class="string">&quot;12345&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(bf.lookup(<span class="string">&quot;12345&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(bf.lookup(<span class="string">&quot;123456&quot;</span>))</span><br></pre></td></tr></table></figure><h1 id="2-LRU-cache"><a href="#2-LRU-cache" class="headerlink" title="2.LRU cache"></a>2.LRU cache</h1><h2 id="2-1-Cache缓存"><a href="#2-1-Cache缓存" class="headerlink" title="2.1 Cache缓存"></a>2.1 Cache缓存</h2><ul><li><a href="https://www.sqlpassion.at/archive/2018/01/06/understanding-the-meltdown-exploit-in-my-own-simple-words/" title="Understanding the Meltdown exploit">Understanding the Meltdown exploit</a></li></ul><p>cpu socket</p><p><img src="image/image_3OhrkCZ00U.png" alt=""></p><h2 id="2-2-LRU-Cache"><a href="#2-2-LRU-Cache" class="headerlink" title="2.2 LRU Cache"></a>2.2 LRU Cache</h2><ul><li>两个要素：大小、替换策略</li><li>Hash Table + Double Linklist</li><li><code>O(1)</code>查询</li><li><code>O(1)</code>修改、更新</li></ul><p><img src="image/image_71l_bqj4Am.png" alt=""></p><h2 id="2-2-替换策略"><a href="#2-2-替换策略" class="headerlink" title="2.2 替换策略"></a>2.2 替换策略</h2><ul><li><a href="https://en.wikipedia.org/wiki/Cache_replacement_policies" title="替换算法总揽">替换算法总揽</a></li><li>LFU - least frequently used</li><li>LRU - least recently used</li></ul><h2 id="2-3-实战题目"><a href="#2-3-实战题目" class="headerlink" title="2.3 实战题目"></a>2.3 实战题目</h2><h3 id="（1）LRU缓存"><a href="#（1）LRU缓存" class="headerlink" title="（1）LRU缓存"></a>（1）LRU缓存</h3><p><a href="https://leetcode.cn/problems/lru-cache/" title="146. LRU 缓存 - 力扣（LeetCode）">146. LRU 缓存 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">请你设计并实现一个满足  LRU (最近最少使用) 缓存 约束的数据结构。</span><br><span class="line">实现 LRUCache 类：</span><br><span class="line">- LRUCache(int capacity) 以 正整数 作为容量 capacity 初始化 LRU 缓存</span><br><span class="line">- int get(int key) 如果关键字 key 存在于缓存中，则返回关键字的值，否则返回 -1 。</span><br><span class="line">- void put(int key, int value) 如果关键字 key 已经存在，则变更其数据值 value ；如果不存在，则向缓存中插入该组 key-value 。如果插入操作导致关键字数量超过 capacity ，则应该 逐出 最久未使用的关键字。</span><br><span class="line">函数 get 和 put 必须以 O(1) 的平均时间复杂度运行。</span><br></pre></td></tr></table></figure><p><img src="image/image_XqNUUDro5E.png" alt=""></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LRUCache</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> Pair = std::pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;;</span><br><span class="line">    <span class="keyword">using</span> List = std::list&lt;Pair&gt;;</span><br><span class="line">    <span class="keyword">using</span> Map = std::unordered_map&lt;<span class="type">int</span>, <span class="keyword">typename</span> List::iterator&gt;;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">LRUCache</span>(<span class="type">int</span> capacity) &#123;</span><br><span class="line">        m_capacity = capacity;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> key)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 查找key是否在哈希表中</span></span><br><span class="line">        <span class="keyword">typename</span> Map::iterator map_itor = m_map.<span class="built_in">find</span>(key);</span><br><span class="line">        <span class="comment">// 不存在，返回-1</span></span><br><span class="line">        <span class="keyword">if</span> (map_itor == m_map.<span class="built_in">end</span>())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果存在</span></span><br><span class="line">        <span class="comment">// 1.链表要将数据删除</span></span><br><span class="line">        <span class="comment">// 2.在将数据加入到链表队头</span></span><br><span class="line">        <span class="comment">// 目的是为了维护链表队头的hi最近访问的数据</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 取出哈希表的value值，也就是链表节点</span></span><br><span class="line">        <span class="keyword">typename</span> List::iterator list_itor = map_itor-&gt;second;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建新的键值对</span></span><br><span class="line">        std::pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; list_pair = std::<span class="built_in">make_pair</span>(list_itor-&gt;first, list_itor-&gt;second);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从链表中删除该节点</span></span><br><span class="line">        m_list.<span class="built_in">erase</span>(list_itor);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将数据加入到队头</span></span><br><span class="line">        m_list.<span class="built_in">push_front</span>(list_pair);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 更新哈希表</span></span><br><span class="line">        m_map[key] = m_list.<span class="built_in">begin</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> list_pair.second;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">put</span><span class="params">(<span class="type">int</span> key, <span class="type">int</span> value)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 查找key是否在哈希表中</span></span><br><span class="line">        <span class="keyword">typename</span> Map::iterator itor = m_map.<span class="built_in">find</span>(key);</span><br><span class="line">        <span class="comment">// 如果存在，则要将老数据从哈希表和链表中移除</span></span><br><span class="line">        <span class="keyword">if</span> (itor != m_map.<span class="built_in">end</span>())</span><br><span class="line">        &#123;</span><br><span class="line">            m_list.<span class="built_in">erase</span>(itor-&gt;second);</span><br><span class="line">            m_map.<span class="built_in">erase</span>(itor);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 插入到链表头</span></span><br><span class="line">        m_list.<span class="built_in">push_front</span>(std::<span class="built_in">make_pair</span>(key, value));</span><br><span class="line">        <span class="comment">// 将链表头放入hash表中</span></span><br><span class="line">        m_map[key] = m_list.<span class="built_in">begin</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 当链表大小超过阈值后，删除</span></span><br><span class="line">        <span class="keyword">if</span> (m_list.<span class="built_in">size</span>() &gt; m_capacity)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> end_key = m_list.<span class="built_in">back</span>().first;</span><br><span class="line">            m_list.<span class="built_in">pop_back</span>();</span><br><span class="line">            m_map.<span class="built_in">erase</span>(end_key);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> m_capacity;</span><br><span class="line">    List m_list;</span><br><span class="line">    Map m_map;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DLinkedNode</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key=<span class="number">0</span>, value=<span class="number">0</span></span>):</span><br><span class="line">        self.key = key</span><br><span class="line">        self.value = value</span><br><span class="line">        self.prev = <span class="literal">None</span></span><br><span class="line">        self.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LRUCache</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, capacity: <span class="built_in">int</span></span>):</span><br><span class="line">        self.cache = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="comment"># 使用伪头部和伪尾部节点    </span></span><br><span class="line">        self.head = DLinkedNode()</span><br><span class="line">        self.tail = DLinkedNode()</span><br><span class="line">        self.head.<span class="built_in">next</span> = self.tail</span><br><span class="line">        self.tail.prev = self.head</span><br><span class="line">        self.capacity = capacity</span><br><span class="line">        self.size = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, key: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> self.cache:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">        <span class="comment"># 如果 key 存在，先通过哈希表定位，再移到头部</span></span><br><span class="line">        node = self.cache[key]</span><br><span class="line">        self.moveToHead(node)</span><br><span class="line">        <span class="keyword">return</span> node.value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">put</span>(<span class="params">self, key: <span class="built_in">int</span>, value: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> self.cache:</span><br><span class="line">            <span class="comment"># 如果 key 不存在，创建一个新的节点</span></span><br><span class="line">            node = DLinkedNode(key, value)</span><br><span class="line">            <span class="comment"># 添加进哈希表</span></span><br><span class="line">            self.cache[key] = node</span><br><span class="line">            <span class="comment"># 添加至双向链表的头部</span></span><br><span class="line">            self.addToHead(node)</span><br><span class="line">            self.size += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> self.size &gt; self.capacity:</span><br><span class="line">                <span class="comment"># 如果超出容量，删除双向链表的尾部节点</span></span><br><span class="line">                removed = self.removeTail()</span><br><span class="line">                <span class="comment"># 删除哈希表中对应的项</span></span><br><span class="line">                self.cache.pop(removed.key)</span><br><span class="line">                self.size -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果 key 存在，先通过哈希表定位，再修改 value，并移到头部</span></span><br><span class="line">            node = self.cache[key]</span><br><span class="line">            node.value = value</span><br><span class="line">            self.moveToHead(node)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addToHead</span>(<span class="params">self, node</span>):</span><br><span class="line">        node.prev = self.head</span><br><span class="line">        node.<span class="built_in">next</span> = self.head.<span class="built_in">next</span></span><br><span class="line">        self.head.<span class="built_in">next</span>.prev = node</span><br><span class="line">        self.head.<span class="built_in">next</span> = node</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeNode</span>(<span class="params">self, node</span>):</span><br><span class="line">        node.prev.<span class="built_in">next</span> = node.<span class="built_in">next</span></span><br><span class="line">        node.<span class="built_in">next</span>.prev = node.prev</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">moveToHead</span>(<span class="params">self, node</span>):</span><br><span class="line">        self.removeNode(node)</span><br><span class="line">        self.addToHead(node)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeTail</span>(<span class="params">self</span>):</span><br><span class="line">        node = self.tail.prev</span><br><span class="line">        self.removeNode(node)</span><br><span class="line">        <span class="keyword">return</span> node</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>18 排序算法</title>
      <link href="/dsa/leetcode/18_sort/README/"/>
      <url>/dsa/leetcode/18_sort/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-排序算法"><a href="#1-排序算法" class="headerlink" title="1.排序算法"></a>1.排序算法</h1><p><a href="https://leetcode.cn/problems/sort-an-array/description/" title="912. 排序数组 - 力扣（LeetCode）">912. 排序数组 - 力扣（LeetCode）</a></p><ul><li><a href="https://www.cnblogs.com/onepixel/p/7674659.html" title="十大经典排序算法">十大经典排序算法</a></li><li><a href="https://www.bilibili.com/video/av25136272" title="9 种经典排序算法可视化动画">9 种经典排序算法可视化动画</a></li><li><a href="https://www.bilibili.com/video/av63851336" title="6 分钟看完 15 种排序算法动画展示">6 分钟看完 15 种排序算法动画展示</a></li></ul><h2 id="1-1-算法分类"><a href="#1-1-算法分类" class="headerlink" title="1.1 算法分类"></a>1.1 算法分类</h2><h3 id="（1）比较类排序："><a href="#（1）比较类排序：" class="headerlink" title="（1）比较类排序："></a>（1）比较类排序：</h3><p>通过比较来决定元素间的相对次序，由于其时间复杂度不能突破 <code>O(nlogn)</code>，因此也称为非线性时间比较类排序。</p><h3 id="（2）非比较类排序："><a href="#（2）非比较类排序：" class="headerlink" title="（2）非比较类排序："></a>（2）非比较类排序：</h3><p>不通过比较来决定元素间的相对次序，它可以突破基于比较类排序的时间下界，以线性时间运行，因此也被称为线性时间非比较类排序</p><p><img src="image/image_4smP0XFDau.png" alt=""></p><h2 id="1-2复杂度分析"><a href="#1-2复杂度分析" class="headerlink" title="1.2复杂度分析"></a>1.2复杂度分析</h2><p><img src="image/image_t8TWO1gqbX.png" alt=""></p><p><img src="image/image_WLAXNzD_mc.png" alt=""></p><h1 id="2-初级排序-O-n-2"><a href="#2-初级排序-O-n-2" class="headerlink" title="2.初级排序 O(n^2)"></a>2.初级排序 O(n^2)</h1><ol><li><strong>选择排序（Selection Sort)</strong> : 每次找最小值，然后放到待排序数组的起始位置</li><li>**插入排序 (Insertion Sort) **: 从前到后逐步构建有序序列；对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。</li><li>**冒泡排序  (Bubble Sort) **: 嵌套循环，每次查看相邻的元素如果逆序，则交换</li></ol><h2 id="2-1-选择排序"><a href="#2-1-选择排序" class="headerlink" title="2.1 选择排序"></a>2.1 选择排序</h2><h3 id="（1）算法原理"><a href="#（1）算法原理" class="headerlink" title="（1）算法原理"></a>（1）算法原理</h3><ul><li>每一趟在待排序元素中选取关键字最小（或最大）的元素加入有序子序列中</li><li>必须进行总共n-1趟处理</li></ul><p><img src="image/849589-20171015224719590-1433219824_4dinKAGBkn.gif" alt=""></p><h3 id="（2）性能分析"><a href="#（2）性能分析" class="headerlink" title="（2）性能分析"></a>（2）性能分析</h3><p><strong>空间复杂度</strong>：O(1)</p><p><strong>时间复杂度</strong></p><ul><li><strong>最好</strong>：原本有序O(n)</li><li><strong>最坏</strong>：原本逆序O(n^2)</li><li><strong>平均</strong>：O(n^2)</li></ul><p><strong>稳定性</strong>：稳定</p><h3 id="（3）代码实现"><a href="#（3）代码实现" class="headerlink" title="（3）代码实现"></a>（3）代码实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 选择排序 O(n^2)  超出时间限制</span></span><br><span class="line"><span class="comment">// 每一趟在待排序元素中选取关键字最小的元素加入有序子序列中</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">select_sort</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> len = nums.<span class="built_in">size</span>();</span><br><span class="line">    <span class="comment">// 记录最小元素位置</span></span><br><span class="line">    <span class="type">int</span> min_idx = <span class="number">-1</span>;</span><br><span class="line">    <span class="comment">// 遍历</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</span><br><span class="line">        min_idx = i;</span><br><span class="line">        <span class="comment">// 选择最小元素</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = i + <span class="number">1</span>; j &lt; len; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[j] &lt; nums[min_idx]) &#123;</span><br><span class="line">                min_idx = j;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 交换元素</span></span><br><span class="line">        <span class="keyword">if</span> (min_idx != i) &#123;</span><br><span class="line">            <span class="type">int</span> tmp = nums[min_idx];</span><br><span class="line">            nums[min_idx] = nums[i];</span><br><span class="line">            nums[i] = tmp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）算法分析"><a href="#（4）算法分析" class="headerlink" title="（4）算法分析"></a>（4）算法分析</h3><p>表现最稳定的排序算法之一，因为无论什么数据进去都是O(n2)的时间复杂度，所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。</p><h2 id="2-2-插入排序"><a href="#2-2-插入排序" class="headerlink" title="2.2 插入排序"></a>2.2 插入排序</h2><p>每次将一个待排序的记录按其关键字大小插入到前面已排好序的子序列中，直到全部记录插入完成。</p><h3 id="（1）性能分析"><a href="#（1）性能分析" class="headerlink" title="（1）性能分析"></a>（1）性能分析</h3><p><strong>空间复杂度</strong>：O(1)</p><p><strong>时间复杂度</strong></p><ul><li><strong>最好</strong>：原本有序O(n)</li><li><strong>最坏</strong>：原本逆序O(n^2)</li><li><strong>平均</strong>：O(n^2)</li></ul><p><strong>稳定性</strong>：稳定</p><p><img src="image/849589-20171015225645277-1151100000_dkaScGi6A-.gif" alt=""></p><h3 id="（2）直接插入排序"><a href="#（2）直接插入排序" class="headerlink" title="（2）直接插入排序"></a>（2）直接插入排序</h3><p>顺序查找插入位置，<strong>适用于顺序表、链表</strong></p><h4 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h4><ul><li>将0号位置放哨兵：要插入的元素</li><li><strong>每次将一个待排序的记录按其关键字插入到前面已排号的序列中</strong></li></ul><p><img src="image/image_tZWKYqOS6x.png" alt=""></p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2.1 直接插入排序 超时</span></span><br><span class="line"><span class="comment">// 将0号位置放哨兵：要插入的元素</span></span><br><span class="line"><span class="comment">// 每次将一个待排序的记录按其关键字插入到前面已排号的序列中</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">inert_sort</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> len = nums.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span> (len &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 依次将 2~n 插入到前面已排序的序列</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; len; i++) &#123;</span><br><span class="line">        <span class="comment">// i 小于前驱，将 i 插入有序表</span></span><br><span class="line">        <span class="keyword">if</span> (nums[i] &lt; nums[i - <span class="number">1</span>]) &#123;</span><br><span class="line">            <span class="type">int</span> tmp = nums[i];</span><br><span class="line">            <span class="type">int</span> j = i - <span class="number">1</span>;</span><br><span class="line">            <span class="comment">// 从后往前查找待插入得位置</span></span><br><span class="line">            <span class="keyword">for</span> (; j &gt;=<span class="number">0</span> &amp;&amp; tmp &lt; nums[j]; j--) &#123;</span><br><span class="line">                <span class="comment">// 向后移动元素</span></span><br><span class="line">                nums[j + <span class="number">1</span>] = nums[j];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 插入数据</span></span><br><span class="line">            nums[j + <span class="number">1</span>] = tmp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）折半插入排序"><a href="#（3）折半插入排序" class="headerlink" title="（3）折半插入排序"></a>（3）折半插入排序</h3><p>折半查找找到应插入的位置，<strong>仅适用于顺序表</strong></p><h4 id="算法思想-1"><a href="#算法思想-1" class="headerlink" title="算法思想"></a>算法思想</h4><p><strong>折半查找</strong></p><ul><li>在[low, high]之间找目标关键字，每次检查 mid = (low + high)/2</li><li>根据mid所指示元素与目标关键字的大小调整low或high，不断缩小low和high的范围</li><li>若low &gt; high，则查找失败</li></ul><p><strong>注意</strong></p><p>一直到low&gt;high时才停止折半查找。<strong>当mid所指元素等于当前元素时，应继续令low=mid+1，以保证“稳定性”。</strong> 最终应将当前元素插入到low所指的位置，即high+1.</p><h4 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2.2 折半插入排序 超时</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bin_insert_sort</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> left, mid, right;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; nums.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        <span class="type">int</span> tmp = nums[i];</span><br><span class="line"></span><br><span class="line">        left = <span class="number">0</span>;</span><br><span class="line">        right = i - <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 查找</span></span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">            mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="comment">// 查找左半部分</span></span><br><span class="line">            <span class="keyword">if</span> (nums[mid] &gt; tmp) &#123;</span><br><span class="line">                right = mid - <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 后移元素，空出插入位置</span></span><br><span class="line">        <span class="comment">// 注意范围，left指针指向选中的元素</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = i - <span class="number">1</span>; j &gt;= left; j--) &#123;</span><br><span class="line">            nums[j + <span class="number">1</span>] = nums[j];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 插入</span></span><br><span class="line">        nums[left] = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）希尔排序"><a href="#（4）希尔排序" class="headerlink" title="（4）希尔排序"></a>（4）希尔排序</h3><h4 id="算法思想-2"><a href="#算法思想-2" class="headerlink" title="算法思想"></a>算法思想</h4><ul><li><strong>先将排序表分割成若干如L[i, i+d, i+2d, …, i+kd] 的“特殊”子表，对各个子表进行直接插入排序</strong>。</li><li>缩小增量d，重复上述过程，直到d=1为止。</li></ul><p><img src="image/849589-20180331170017421-364506073_8W_-iqtlw-.gif" alt=""></p><h4 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h4><ul><li><strong>空间复杂度</strong>：O(1)</li><li><strong>时间复杂度</strong>：未知，但优于直接插入排序</li><li><strong>稳定性</strong>：不稳定</li><li><strong>适应性</strong>：仅可适用于顺序表</li></ul><p><img src="image/image_WvXjj_XbtC.png" alt=""></p><h4 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2.3 希尔排序</span></span><br><span class="line"><span class="comment">// 先将排序表分割成若干如L[i, i+d, i+2d, ..., i+kd] 的“特殊”子表，</span></span><br><span class="line"><span class="comment">// 对各个子表进行直接插入排序。</span></span><br><span class="line"><span class="comment">// 缩小增量d，重复上述过程，直到d=1为止。</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">shell_sort</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> len = nums.<span class="built_in">size</span>();</span><br><span class="line">    <span class="comment">// 步长</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> dk = len / <span class="number">2</span>; dk &gt;= <span class="number">1</span>; dk = dk / <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = dk + <span class="number">1</span>; i &lt; len; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[i] &lt; nums[i - dk]) &#123;</span><br><span class="line">                <span class="comment">// 将i插入有序增量子表</span></span><br><span class="line">                <span class="type">int</span> tmp = nums[i];</span><br><span class="line">                <span class="comment">// 元素后移</span></span><br><span class="line">                <span class="type">int</span> j = i - dk;</span><br><span class="line">                <span class="keyword">for</span> (; i &gt; <span class="number">0</span> &amp;&amp; tmp &lt; nums[j]; j -= dk) &#123;</span><br><span class="line">                    nums[j + dk] = nums[j];</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 插入</span></span><br><span class="line">                nums[j + dk] = tmp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="2-3-冒泡排序"><a href="#2-3-冒泡排序" class="headerlink" title="2.3 冒泡排序"></a>2.3 冒泡排序</h2><h3 id="（1）算法原理-1"><a href="#（1）算法原理-1" class="headerlink" title="（1）算法原理"></a>（1）算法原理</h3><ul><li>从后往前（或从前往后）两两比较相邻元素的值，若为逆序，则交换它们，直到序列比较完。称这个过程为“一趟”冒泡排序，最多只需n-1趟排序</li><li>每一趟排序后都可以使一个元素的移动到最终位置，以确定最终位置的元素在之后的处理中无需对比</li><li><p>如果某一趟排序过程中未发生“交换”，则算法可以提前结束</p><p><img src="image/849589-20171015223238449-2146169197_hKGqK-DB0Z.gif" alt=""></p></li></ul><h3 id="（2）性能"><a href="#（2）性能" class="headerlink" title="（2）性能"></a>（2）性能</h3><ul><li><strong>空间复杂度</strong>：O(1)</li><li><strong>时间复杂度</strong>：<ul><li>最好（有序）：O(n)</li><li>最差（逆序）：O(n^2)</li><li>平均：O(n^2)</li></ul></li><li><strong>稳定性</strong>：稳定</li><li><strong>适用性</strong>：顺序表、链表都可以</li></ul><p><img src="image/image__KJxr9p5Wk.png" alt=""></p><h3 id="（3）代码实现-1"><a href="#（3）代码实现-1" class="headerlink" title="（3）代码实现"></a>（3）代码实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2.4 冒泡排序 超时</span></span><br><span class="line"><span class="comment">// 从后往前（或从前往后）两两比较相邻元素的值，若为逆序，</span></span><br><span class="line"><span class="comment">// 则交换它们，直到序列比较完。称这个过程为“一趟”冒泡排序，</span></span><br><span class="line"><span class="comment">// 最多只需n-1趟排序</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 每一趟排序后都可以使一个元素的移动到最终位置，</span></span><br><span class="line"><span class="comment">// 以确定最终位置的元素在之后的处理中无需对比</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果某一趟排序过程中未发生“交换”，则算法可以提前结束</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bulle_sort</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> len = nums.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span> (len &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 提前退出标志位</span></span><br><span class="line">    <span class="type">bool</span> flag = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; len; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        flag = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; len - i - <span class="number">1</span>; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 前面的元素比后面的大，交换顺序</span></span><br><span class="line">            <span class="keyword">if</span> (nums[j] &gt; nums[j + <span class="number">1</span>])</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="type">int</span> tmp = nums[j];</span><br><span class="line">                nums[j] = nums[j + <span class="number">1</span>];</span><br><span class="line">                nums[j + <span class="number">1</span>] = tmp;</span><br><span class="line">                <span class="comment">// 数据交换标志位</span></span><br><span class="line">                flag = <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 若没有数据交换，提前退出</span></span><br><span class="line">        <span class="keyword">if</span> (!flag)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="3-高级排序-O-N-LogN"><a href="#3-高级排序-O-N-LogN" class="headerlink" title="3.高级排序 O(N*LogN)"></a>3.高级排序 O(N*LogN)</h1><h2 id="3-1-快速排序-（Quick-Sort）"><a href="#3-1-快速排序-（Quick-Sort）" class="headerlink" title="3.1 快速排序 （Quick Sort）"></a>3.1 快速排序 （Quick Sort）</h2><h3 id="（1）算法思想"><a href="#（1）算法思想" class="headerlink" title="（1）算法思想"></a>（1）算法思想</h3><ul><li>从待排序序列中任取一个元素为基准（枢轴）pivot</li><li>比它小的往前放，比它大的往后放，则pivot放在最中间位置，这个过程称为一趟快速排序</li><li>一次划分为左右两个子表，再分别递归两个子表重复上述过程，直到每个部分只有一个元素为止</li></ul><p><img src="image/849589-20171015230936371-1413523412_-VzyoUFs3c.gif" alt=""></p><h3 id="（2）实现"><a href="#（2）实现" class="headerlink" title="（2）实现"></a>（2）实现</h3><ul><li>right直至指向表尾，left指针指向表头，left指向元素存为基准元素pivot</li><li>从right开始，向前比较，比基准大，right—，继续比较</li><li>若比基准小，则right所指元素移动到left的位置</li><li>再从left指针所指元素和基准元素比较，比基准小，left++，继续比较</li><li>若比基准大，则left所指元素移动到right位置</li><li>……</li></ul><p>当dk=1时，就是直接插入排序</p><h3 id="（3）性能"><a href="#（3）性能" class="headerlink" title="（3）性能"></a>（3）性能</h3><h4 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h4><p><strong>最好情况</strong>：树的高度与递归调用深度一样：$O(log_2n)$</p><p><strong>最坏情况</strong>：n-1次调用，栈深度 n-1</p><p><strong>平均情况</strong>：$O(log_2n)$</p><p><strong>空间复杂度 = 递归调用层数</strong></p><h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><p>快速排序的时间复杂度 = O(n * 递归层数)，运行时间与划分是否对称有关</p><p><strong>最好情况</strong>：做平衡划分：$O(nlog_2n)$</p><p><strong>最坏情况</strong>：发生在两个区域分别包括n-1个元素和0个元素</p><ul><li>优化思路：尽量选取数组在中间的元素，从头尾各去一个，它们的中间值作为枢轴元素；或从表中随机选取；</li></ul><p><strong>平均情况</strong>：$O(nlog_2n)$</p><p><strong>快速排序是所有内部排序算法中平均性能最优的</strong></p><h4 id="稳定性"><a href="#稳定性" class="headerlink" title="稳定性"></a>稳定性</h4><p>不稳定</p><h4 id="适用性"><a href="#适用性" class="headerlink" title="适用性"></a>适用性</h4><p>仅适用于线性表为顺序存储的情况</p><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><ul><li>一次划分：确定一个元素的最终位置</li><li>一趟排序：可能确定多个元素的最终位置</li></ul><p><img src="image/image_qUzI9eFR3U.png" alt=""></p><h3 id="（4）代码实现"><a href="#（4）代码实现" class="headerlink" title="（4）代码实现"></a>（4）代码实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3.1 快速排序</span></span><br><span class="line"><span class="comment">// 一次划分区间</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">partite_region</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> left, <span class="type">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 当前表的第一个元素作为枢轴，对其划分</span></span><br><span class="line">    <span class="type">int</span> pivot = nums[left];</span><br><span class="line">    <span class="comment">// 循环条件</span></span><br><span class="line">    <span class="keyword">while</span> (left &lt; right)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// high向前寻找比枢轴点小的元素</span></span><br><span class="line">        <span class="keyword">while</span> (left &lt; right &amp;&amp; nums[right] &gt; pivot)</span><br><span class="line">            right--;</span><br><span class="line">        <span class="comment">// 将此小的元素移动到枢轴点左端</span></span><br><span class="line">        nums[left] = nums[right];</span><br><span class="line">        <span class="comment">// low向后寻找比枢轴点大的元素</span></span><br><span class="line">        <span class="keyword">while</span> (left &lt; right &amp;&amp; nums[left] &lt;= pivot)</span><br><span class="line">            left++;</span><br><span class="line">        <span class="comment">// 将此大的元素移动到枢轴点右端</span></span><br><span class="line">        nums[right] = nums[left];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 枢轴点元素放到最终位置</span></span><br><span class="line">    nums[left] = pivot;</span><br><span class="line">    <span class="comment">// 放回枢轴点元素</span></span><br><span class="line">    <span class="keyword">return</span> left;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">quick_sort</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> left, <span class="type">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (left &lt; right)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 划分区间</span></span><br><span class="line">        <span class="type">int</span> pivot_pos = <span class="built_in">partite_region</span>(nums, left, right);</span><br><span class="line">        <span class="comment">// 一次对两个子表进行递归排序</span></span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">quick_sort</span>(nums, left, pivot_pos - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">quick_sort</span>(nums, pivot_pos + <span class="number">1</span>, right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-2-归并排序（Merge-Sort）-分治"><a href="#3-2-归并排序（Merge-Sort）-分治" class="headerlink" title="3.2 归并排序（Merge Sort） - 分治"></a>3.2 归并排序（Merge Sort） - 分治</h2><p>把两个或多个有序的子序列合并为一个</p><p>2路归并（二合一）</p><ul><li>n个元素2路归并排序，归并趟数 =$ceil(log_2n)$</li><li>每趟归并的时间复杂度为O(n)，则算法的时间复杂度为 O(nlogn)</li><li>空间复杂度：O(n)，来自于辅助数组</li></ul><p>k路归并（K合一）</p><p>m路归并，每选一个元素需要对比关键字 n-1次</p><p><img src="image/849589-20171015230557043-37375010_4W3l1LpGNt.gif" alt=""></p><h3 id="（1）算法思想-1"><a href="#（1）算法思想-1" class="headerlink" title="（1）算法思想"></a>（1）算法思想</h3><ul><li>若low\&lt;high，则将序列从中间 mid=(low+high)/2分开</li><li>对左半部分 [low, mid] 递归进行归并排序</li><li>对有半部分 [mid+1, high] 递归进行归并排序</li><li>将左右两个有序序列Merge为一个</li></ul><p><img src="image/image_VUEgBt65Dh.png" alt=""></p><h3 id="（2）性能-1"><a href="#（2）性能-1" class="headerlink" title="（2）性能"></a>（2）性能</h3><ul><li><strong>空间复杂度</strong>：O(n)</li><li><strong>时间复杂度</strong>：O(nlogn)</li><li><strong>稳定性</strong>：稳定</li></ul><h3 id="（3）代码实现-2"><a href="#（3）代码实现-2" class="headerlink" title="（3）代码实现"></a>（3）代码实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3.1 归并排序</span></span><br><span class="line"><span class="comment">// 先排序左右子数组，然后合并两个有序子数组</span></span><br><span class="line"><span class="comment">// 1. 把长度为n的输入序列分成两个长度为n/2的子序列</span></span><br><span class="line"><span class="comment">// 2. 对这两个子序列分别采用归并排序:</span></span><br><span class="line"><span class="comment">// 3. 将两个排序好的子序列合并成一个最终的排序序列</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">merge</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> left, <span class="type">int</span> mid, <span class="type">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 辅助数组</span></span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">tmp</span><span class="params">(right - left + <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="comment">// // 表nums中的元素，全部复制到tmp中</span></span><br><span class="line">    <span class="comment">// for (int k = left; k &lt;= right; k++)</span></span><br><span class="line">    <span class="comment">//     tmp[k] = nums[k];</span></span><br><span class="line">    <span class="type">int</span> left_idx = left;</span><br><span class="line">    <span class="type">int</span> right_idx = mid + <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> tmp_index = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 比较tmp的左右两段中的元素,将较小值复制到L中</span></span><br><span class="line">    <span class="keyword">while</span> (left_idx &lt;= mid &amp;&amp; right_idx &lt;= right)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 两个元素相等时，优先使用靠前的那个（稳定性）</span></span><br><span class="line">        <span class="keyword">if</span> (nums[left_idx] &lt;= nums[right_idx])</span><br><span class="line">            tmp[tmp_index++] = nums[left_idx++];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            tmp[tmp_index++] = nums[right_idx++];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 若第一个表未检测完，复制</span></span><br><span class="line">    <span class="keyword">while</span>(left_idx &lt;= mid)</span><br><span class="line">        tmp[tmp_index++] = nums[left_idx++];</span><br><span class="line">    <span class="comment">// 若第二个表未检测完，复制</span></span><br><span class="line">    <span class="keyword">while</span> (right_idx &lt;= right)</span><br><span class="line">        tmp[tmp_index++] = nums[right_idx++];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 复制tmp到nums</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; tmp.<span class="built_in">size</span>(); k++) &#123;</span><br><span class="line">        nums[left + k] = tmp[k];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">merge_sort</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> left, <span class="type">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (left &lt; right)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">merge_sort</span>(nums, left, mid);</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">merge_sort</span>(nums, mid + <span class="number">1</span>, right);</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">merge</span>(nums, left, mid, right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-3-堆排序（Heap-Sort）"><a href="#3-3-堆排序（Heap-Sort）" class="headerlink" title="3.3 堆排序（Heap Sort）"></a>3.3 堆排序（Heap Sort）</h2><p>堆插入O(log N)，取最大/小值 O(1)</p><h3 id="（1）堆"><a href="#（1）堆" class="headerlink" title="（1）堆"></a>（1）堆</h3><p>堆是一种特殊的树，只要满足这两点，它就是一个堆</p><ul><li>堆是一个<strong>完全二叉树</strong>；</li><li>堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。</li></ul><p>对于每个节点的值都<strong>大于等于</strong>子树中每个节点值的堆，叫作“<strong>大顶堆</strong>”。对于每个节点的值都<strong>小于等于</strong>子树中每个节点值的堆，叫作“<strong>小顶堆</strong>”。</p><p><img src="image/image_5RMXjCIV71.png" alt=""></p><p>其中第1个和第2个是大顶堆，第3个是小顶堆，第4个不是堆。除此之外，从图中还可以看出来，对于同一组数据，可以构建多种不同形态的堆。</p><h3 id="（2）实现一个堆"><a href="#（2）实现一个堆" class="headerlink" title="（2）实现一个堆"></a>（2）实现一个堆</h3><p><strong>完全二叉树比较适合用数组来存储</strong>。用数组来存储完全二叉树是非常节省存储空间的。因为不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。</p><p>数组中下标为<code>i</code>的节点，左子节点就是下标为<code>i*2</code>的节点，右子节点就是下标为<code>i*2+1</code>的节点，父节点就是下标为<code>i/2</code>的节点。</p><h4 id="结构体定义及初始化"><a href="#结构体定义及初始化" class="headerlink" title="结构体定义及初始化"></a><strong>结构体定义及初始化</strong></h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> MaxSize 200</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">int</span> ElemType;</span><br><span class="line"><span class="comment">// 采用动态数组定义</span></span><br><span class="line"><span class="comment">// 注意：0号不存储元素</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span></span><br><span class="line">&#123;</span><br><span class="line">    ElemType* data;</span><br><span class="line">    <span class="type">int</span> length;</span><br><span class="line">    <span class="type">int</span> maxSize;</span><br><span class="line">&#125;HeapList;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注意：0号不存储元素</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">initHeap</span><span class="params">(HeapList&amp; L, <span class="type">int</span> initSize)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    L.data = (ElemType*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(ElemType) * (initSize+<span class="number">1</span>));</span><br><span class="line">    L.length = initSize;</span><br><span class="line">    L.maxSize = MaxSize;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="插入元素-堆化（-heapify）"><a href="#插入元素-堆化（-heapify）" class="headerlink" title="插入元素 - 堆化（ heapify）"></a>插入元素 - 堆化（ heapify）</h4><p>不满足堆要求，就需要进行调整，让其重新满足堆的特性，这个过程就叫做堆化。</p><p>堆化两种，从下往上和从上往下。</p><p>堆化方法：<strong>顺着节点所在的路径，向上或者向下，对比，然后交换</strong>。</p><p><strong>从下往上堆化**</strong>：** 以让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，就互换两个节点。</p><p><img src="image/image_MZTL6Ih2d1.png" alt=""></p><p>堆化（插入）代码实现</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从下往上堆化</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">insertElemHeap</span><span class="params">(HeapList&amp; L, ElemType data)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (L.length &gt;= L.maxSize)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    <span class="comment">// 将新插入数据放到最后</span></span><br><span class="line">    L.length++;</span><br><span class="line">    L.data[L.length] = data;</span><br><span class="line">    <span class="comment">// 自下往上堆化</span></span><br><span class="line">    <span class="comment">// 让新插入的结点与父节点比对大小，</span></span><br><span class="line">    <span class="comment">// 如果不满足子结点小于等于父结点的大小关系，就互换两个结点</span></span><br><span class="line">    <span class="type">int</span> i = L.length;</span><br><span class="line">    <span class="keyword">while</span> (i / <span class="number">2</span> &gt; <span class="number">0</span> &amp;&amp; L.data[i] &gt; L.data[i / <span class="number">2</span>])</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 交换两个元素</span></span><br><span class="line">        ElemType tmpElem = L.data[i];</span><br><span class="line">        L.data[i] = L.data[i / <span class="number">2</span>];</span><br><span class="line">        L.data[i / <span class="number">2</span>] = tmpElem;</span><br><span class="line">        <span class="comment">// 更新i下标</span></span><br><span class="line">        i = i / <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="删除堆顶元素"><a href="#删除堆顶元素" class="headerlink" title="删除堆顶元素"></a>删除堆顶元素</h4><p>任何节点的值都大于等于（或小于等于）子树节点的值，可以发现，<strong>堆顶元素存储的就是堆中数据的最大值或者最小值。</strong></p><p><strong>1）数组空洞</strong></p><p>当删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。然后再迭代地删除第二大节点，以此类推，直到叶子节点被删除。</p><p><img src="image/image_K-xgIHpzdK.png" alt=""></p><p><strong>2）从上往下堆化</strong></p><ul><li>把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。</li><li>对于不满足父子节点大小关系的，互换两个节点，</li><li>并且重复进行这个过程，直到父子节点之间满足大小关系为止。</li></ul><p><img src="image/image_M5400o1DIM.png" alt=""></p><p><strong>3）代码</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从上往下堆化,从标号为k的元素开始堆化</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">heapify</span><span class="params">(HeapList&amp; L, <span class="type">int</span> k, <span class="type">int</span> len)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 寻找父结点的两个子结点中最大的一个</span></span><br><span class="line">        <span class="type">int</span> maxPos = k;</span><br><span class="line">        <span class="keyword">if</span> (k * <span class="number">2</span> &lt;= len &amp;&amp; L.data[k] &lt; L.data[<span class="number">2</span> * k])</span><br><span class="line">            maxPos = k * <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (k * <span class="number">2</span> + <span class="number">1</span> &lt;= len &amp;&amp; L.data[maxPos] &lt; L.data[<span class="number">2</span> * k + <span class="number">1</span>])</span><br><span class="line">            maxPos = k * <span class="number">2</span> + <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 若子结点比自己都小，父结点就是最大的</span></span><br><span class="line">        <span class="keyword">if</span> (maxPos == k)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="comment">// 交换元素</span></span><br><span class="line">        ElemType tmpElem = L.data[k];</span><br><span class="line">        L.data[k] = L.data[maxPos];</span><br><span class="line">        L.data[maxPos] = tmpElem;</span><br><span class="line">        <span class="comment">// 更新</span></span><br><span class="line">        k = maxPos;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">removeHeapTopElem</span><span class="params">(HeapList&amp; L, ElemType&amp; elem)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (L.length == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    elem = L.data[<span class="number">1</span>];</span><br><span class="line">    L.data[<span class="number">1</span>] = L.data[L.length];</span><br><span class="line">    L.length--;</span><br><span class="line">    <span class="built_in">heapify</span>(L, <span class="number">1</span>, L.length);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）堆排序"><a href="#（3）堆排序" class="headerlink" title="（3）堆排序"></a>（3）堆排序</h3><p>可以把堆排序的过程大致分解成两个大的步骤，** 建堆和排序**。</p><h4 id="1）建堆"><a href="#1）建堆" class="headerlink" title="1）建堆"></a>1）建堆</h4><p>首先将数组原地建成一个堆。所谓“原地”就是，不借助另一个数组，就在原数组上操作。</p><ul><li>是从后往前处理数组，并且每个数据都是从上往下堆化。</li></ul><p><img src="image/image_PiC0G2KP3x.png" alt=""></p><h4 id="2）排序"><a href="#2）排序" class="headerlink" title="2）排序"></a>2）排序</h4><ul><li>建堆结束之后，<strong>数组中的数据已经是按照大顶堆的特性来组织的</strong>。</li><li>数组中的第一个元素就是堆顶，也就是最大的元素。</li><li>把它跟最后一个元素交换，那最大元素就放到了下标为n的位置。</li></ul><p><img src="image/image_r2YcAwQuWt.png" alt=""></p><p>代码</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3.3 堆排序</span></span><br><span class="line"><span class="comment">// 从下往上堆化</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">heapify</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> n, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 寻找父结点的两个子结点中最大的一个</span></span><br><span class="line">        <span class="type">int</span> max_pos = k;</span><br><span class="line">        <span class="keyword">if</span> (k * <span class="number">2</span> &lt; n &amp;&amp; nums[k] &lt; nums[k * <span class="number">2</span>])</span><br><span class="line">            max_pos = k * <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (k * <span class="number">2</span> + <span class="number">1</span> &lt; n &amp;&amp; nums[max_pos] &lt; nums[k * <span class="number">2</span> + <span class="number">1</span>])</span><br><span class="line">            max_pos = k * <span class="number">2</span> + <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 若子结点比自己都小，父结点就是最大的</span></span><br><span class="line">        <span class="keyword">if</span> (max_pos == k)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="comment">// 交换元素</span></span><br><span class="line">        <span class="type">int</span> tmp_elem = nums[k];</span><br><span class="line">        nums[k] = nums[max_pos];</span><br><span class="line">        nums[max_pos] = tmp_elem;</span><br><span class="line">        <span class="comment">// 更新</span></span><br><span class="line">        k = max_pos;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">heap_sort</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> len = nums.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span> (len == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 建堆</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = len / <span class="number">2</span> - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="built_in">heapify</span>(nums,len, i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 排序</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = len - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="type">int</span> tmp = nums[i];</span><br><span class="line">        nums[i] = nums[<span class="number">0</span>];</span><br><span class="line">        nums[<span class="number">0</span>] = tmp;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">heapify</span>(nums, i, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用优先队列实现</p><p><img src="image/image_cMgLQ8JEiG.png" alt=""></p><p><img src="image/image_bUhLzktmls.png" alt=""></p><h3 id="（4）复杂度"><a href="#（4）复杂度" class="headerlink" title="（4）复杂度"></a>（4）复杂度</h3><ul><li>整个堆排序的过程，都只需要极个别临时存储空间，所以<strong>堆排序是原地排序算法</strong>。</li><li>堆排序包括建堆和排序两个操作，<strong>建堆过程的时间复杂度是O(n)</strong>，<strong>排序过程的时间复杂度是O(nlog n)，</strong> 所以，<strong>堆排序整体的时间复杂度是O(nlog n)。</strong></li><li><strong>堆排序不是稳定的排序算法</strong>，因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序。</li></ul><h1 id="4-特殊排序-O-n"><a href="#4-特殊排序-O-n" class="headerlink" title="4.特殊排序 - O(n)"></a>4.特殊排序 - O(n)</h1><ol><li>**计数排序 (Counting Sort) **: 计数排序要求输入的数据必须是有确定范围的整数。将输入的数据值转化为键存储在额外开辟的数组空间中；然后依次把计数大于 1的填充回原数组</li><li><strong>桶排序 (Bucket Sort)</strong> : 假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序(有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排)。</li><li>**基数排序 (Radix Sort) **: 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集，依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。</li></ol><h2 id="4-1-计数排序"><a href="#4-1-计数排序" class="headerlink" title="4.1 计数排序"></a>4.1 计数排序</h2><p>计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。</p><ul><li>找出待排序的数组中最大和最小的元素；</li><li>统计数组中每个值为i的元素出现的次数，存入数组C的第i项；</li><li>对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）；</li><li>反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。</li></ul><p><img src="image/849589-20171015231740840-6968181_Dlwaja5OTv.gif" alt=""></p><h2 id="4-2-桶排序"><a href="#4-2-桶排序" class="headerlink" title="4.2 桶排序"></a>4.2 桶排序</h2><p>桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。</p><ul><li>设置一个定量的数组当作空桶；</li><li>遍历输入数据，并且把数据一个一个放到对应的桶里去；</li><li>对每个不是空的桶进行排序；</li><li>从不是空的桶里把排好序的数据拼接起来。</li></ul><p><img src="image/849589-20171015232107090-1920702011_KfCZQVN22D.png" alt=""></p><h2 id="4-3-基数排序（Radix-Sort）"><a href="#4-3-基数排序（Radix-Sort）" class="headerlink" title="4.3 基数排序（Radix Sort）"></a>4.3 基数排序（Radix Sort）</h2><ul><li>取得数组中的最大数，并取得位数；</li><li>arr为原始数组，从最低位开始取每个位组成radix数组；</li><li>对radix进行计数排序（利用计数排序适用于小范围数的特点）；</li></ul><p><img src="image/849589-20171015232453668-1397662527_lqdNquFK5f.gif" alt=""></p><h1 id="5-实战题目"><a href="#5-实战题目" class="headerlink" title="5.实战题目"></a>5.实战题目</h1><h2 id="5-1-合并区间"><a href="#5-1-合并区间" class="headerlink" title="5.1 合并区间"></a>5.1 合并区间</h2><p><a href="https://leetcode.cn/problems/merge-intervals/description/" title="56. 合并区间 - 力扣（LeetCode）">56. 合并区间 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以数组 intervals 表示若干个区间的集合，其中单个区间为 intervals[i] = [starti, endi] 。请你合并所有重叠的区间，并返回 一个不重叠的区间数组，该数组需恰好覆盖输入中的所有区间 。</span><br></pre></td></tr></table></figure><p>首先，将列表中的区间按照左端点升序排序。然后将第一个区间加入 merged 数组中，并按顺序依次考虑之后的每个区间：</p><ol><li>如果当前区间的左端点在数组 merged 中最后一个区间的右端点之后，那么它们不会重合，可以直接将这个区间加入数组 merged 的末尾；</li><li>否则，它们重合，需要用当前区间的右端点更新数组 merged 中最后一个区间的右端点，将其置为二者的较大值。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">merge</span>(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; intervals) &#123;</span><br><span class="line">        <span class="keyword">if</span> (intervals.<span class="built_in">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">sort</span>(intervals.<span class="built_in">begin</span>(), intervals.<span class="built_in">end</span>());</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; merged;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; intervals.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">            <span class="type">int</span> L = intervals[i][<span class="number">0</span>], R = intervals[i][<span class="number">1</span>];</span><br><span class="line">            <span class="keyword">if</span> (!merged.<span class="built_in">size</span>() || merged.<span class="built_in">back</span>()[<span class="number">1</span>] &lt; L) &#123;</span><br><span class="line">                merged.<span class="built_in">push_back</span>(&#123;L, R&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                merged.<span class="built_in">back</span>()[<span class="number">1</span>] = <span class="built_in">max</span>(merged.<span class="built_in">back</span>()[<span class="number">1</span>], R);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> merged;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="5-2-翻转对"><a href="#5-2-翻转对" class="headerlink" title="5.2 翻转对"></a>5.2 翻转对</h2><p><a href="https://leetcode.cn/problems/reverse-pairs/description/" title="493. 翻转对 - 力扣（LeetCode）">493. 翻转对 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给定一个数组 nums ，如果 i &lt; j 且 nums[i] &gt; 2*nums[j] 我们就将 (i, j) 称作一个重要翻转对。</span><br><span class="line"></span><br><span class="line">你需要返回给定数组中的重要翻转对的数量。</span><br></pre></td></tr></table></figure><ol><li>暴力：两个嵌套循环 : O(n^2)</li><li>merge-sort : O(nlogn)</li><li>树状数组（竞赛时用的多）</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">reversePairs</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.<span class="built_in">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">merge_sort</span>(nums, <span class="number">0</span>, nums.<span class="built_in">size</span>() - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">merge_sort</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> left, <span class="type">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (left &gt;= right) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> count = <span class="keyword">this</span>-&gt;<span class="built_in">merge_sort</span>(nums, left, mid) + <span class="keyword">this</span>-&gt;<span class="built_in">merge_sort</span>(nums, mid + <span class="number">1</span>, right);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 临时数组，排序后使用</span></span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">sorted</span><span class="params">(right - left + <span class="number">1</span>)</span></span>;</span><br><span class="line">        <span class="type">int</span> i = left;</span><br><span class="line">        <span class="type">int</span> t = left;</span><br><span class="line">        <span class="type">int</span> c = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = mid + <span class="number">1</span>; j &lt;= right; j++, c++) &#123;</span><br><span class="line">            <span class="keyword">while</span> (i &lt;= mid &amp;&amp; (<span class="type">long</span> <span class="type">long</span>)nums[i] &lt;= <span class="number">2</span> * (<span class="type">long</span> <span class="type">long</span>)nums[j]) &#123;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">while</span> (t &lt;= mid &amp;&amp; nums[t] &lt; nums[j]) &#123;</span><br><span class="line">                sorted[c++] = nums[t++];</span><br><span class="line">            &#125;</span><br><span class="line">            sorted[c] = nums[j];</span><br><span class="line">            count += (mid - i + <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (t &lt;= mid) &#123;</span><br><span class="line">            sorted[c++] = nums[t++];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; sorted.<span class="built_in">size</span>(); k++) &#123;</span><br><span class="line">            nums[left + k] = sorted[k];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>19 高级动态规划</title>
      <link href="/dsa/leetcode/19_advice_dp/README/"/>
      <url>/dsa/leetcode/19_advice_dp/README/</url>
      
        <content type="html"><![CDATA[<h1 id="1-动态规划"><a href="#1-动态规划" class="headerlink" title="1.动态规划"></a>1.动态规划</h1><h2 id="1-1-递归-amp-分治"><a href="#1-1-递归-amp-分治" class="headerlink" title="1.1 递归 &amp; 分治"></a>1.1 递归 &amp; 分治</h2><p>递归：函数自己调用自己</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">recursion</span>(<span class="params">level, param1, param2, ...</span>):</span><br><span class="line">  <span class="comment"># 1.recursion terminator (递归终止条件)</span></span><br><span class="line">  <span class="keyword">if</span> level &gt; MAX_LeVEL:</span><br><span class="line">    process_result</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2.process logic in current level (处理当前层逻辑)</span></span><br><span class="line">  process(level, data, ...)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3.drill down (下探到下一层)</span></span><br><span class="line">  self.recursion(level + <span class="number">1</span>, p1, ...)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 4.reverse the current level status if needed (清理当前层)</span></span><br></pre></td></tr></table></figure><p>分治：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">divide_conquer</span>(<span class="params">problem, param1, param2, ...</span>):</span><br><span class="line">  <span class="comment"># 1.recursion terminator (递归终止条件)</span></span><br><span class="line">  <span class="keyword">if</span> problem <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    print_result</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 2.prepare data (拆分问题)</span></span><br><span class="line">  data = prepare_data(problem)</span><br><span class="line">  subproblems = split_problem(problem, data)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3.conquer subproblems (调子问题的递归函数)</span></span><br><span class="line">  subresult1 = self.divide_conquer(subproblems[<span class="number">0</span>], p1, ...)</span><br><span class="line">  subresult1 = self.divide_conquer(subproblems[<span class="number">0</span>], p1, ...)</span><br><span class="line">  subresult1 = self.divide_conquer(subproblems[<span class="number">0</span>], p1, ...)</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 4.process and generate the final result (合并结果)</span></span><br><span class="line">  result = process_result(subresult1, subresult2, subresult3, ...)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 5.revert the current level status (回复当前层状态)</span></span><br></pre></td></tr></table></figure><h2 id="1-2-动态规划-Dynamic-Programming"><a href="#1-2-动态规划-Dynamic-Programming" class="headerlink" title="1.2 动态规划 Dynamic Programming"></a>1.2 动态规划 Dynamic Programming</h2><ol><li>“Simplifying a complicated problem by breaking it down into simpler sub-problems” (int a recursive manner)</li><li>Divide &amp; Conquer + Optimal substructure (分治 + 最优子结构)</li><li>顺推形式：动态递推</li></ol><p>DP顺推模板</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">function DP:</span><br><span class="line">  <span class="comment"># 二维情况</span></span><br><span class="line">  dp = [][]</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> i = <span class="number">0</span> ... M &#123;</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="number">0</span> ... N &#123;</span><br><span class="line">      dp[i][j] = _Fuction(dp[i<span class="string">&#x27;][j&#x27;</span>] ...)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> dp[M][N]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>难点：</p><ul><li>DP状态的定义：需要经验，现实问题定义为数组，有可能是一维、二维或三维；</li><li>状态转移方程怎么写：最简单情况：$ dp[i] = dp[i-1] + dp[i-2]  $；更多情况需要求最小值或者累加累减；从k个状态中找到它的最值</li></ul><h2 id="1-3-关键点"><a href="#1-3-关键点" class="headerlink" title="1.3 关键点"></a>1.3 关键点</h2><p>动态规划 和 递归或者分治 没有根本上的区别（关键看有无最优子结构）</p><p><strong>拥有共性：</strong> ​<strong>找到重复子问题</strong></p><p><strong>差异性：</strong> ​<strong>最优子结构、中途可以淘汰次优解</strong></p><h1 id="2-状态转移方程"><a href="#2-状态转移方程" class="headerlink" title="2.状态转移方程"></a>2.状态转移方程</h1><h2 id="2-1-爬楼梯问题"><a href="#2-1-爬楼梯问题" class="headerlink" title="2.1 爬楼梯问题"></a>2.1 爬楼梯问题</h2><p><a href="https://leetcode.cn/problems/climbing-stairs/description/" title="70. 爬楼梯 - 力扣（LeetCode）">70. 爬楼梯 - 力扣（LeetCode）</a></p><p>递推公式：$f(n) = f(n-1) + f(n-2), f(1)=1,f(0)=0$</p><p>递归：<code>O(2^n)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">n</span>):</span><br><span class="line">  <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> f(n - <span class="number">1</span>) + f(n - <span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>递归 + 记忆化搜索：<code>O(n)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">n</span>):</span><br><span class="line">  <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">if</span> n <span class="keyword">not</span> <span class="keyword">in</span> mem:</span><br><span class="line">    mem[n] = f(n - <span class="number">1</span>) + f(n - <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">return</span> mem[n]</span><br></pre></td></tr></table></figure><p>动态规划：<code>O(n)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">n</span>):</span><br><span class="line">  dp = [<span class="number">1</span>] * (n + <span class="number">1</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">    dp[i] = dp[i - <span class="number">1</span>] + dp[i - <span class="number">2</span>]</span><br><span class="line">  <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></table></figure><p>动态规划 + 内存优化：<code>O(n), O(1)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">n</span>):</span><br><span class="line">  x, y = <span class="number">1</span>, <span class="number">1</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">    y, x = x + y, y</span><br><span class="line">  <span class="keyword">return</span> y</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2-2-不同路径"><a href="#2-2-不同路径" class="headerlink" title="2.2 不同路径"></a>2.2 不同路径</h2><p><a href="https://leetcode.cn/problems/unique-paths/description/" title="62. 不同路径 - 力扣（LeetCode）">62. 不同路径 - 力扣（LeetCode）</a></p><p><img src="image/image_ilVcM5xgj1.png" alt=""></p><p>递归公式：$f(x, y) = f(x - 1, y) + f(x, y - 1)$</p><p>递归：<code>O(mn), O(mn)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, y</span>):</span><br><span class="line">  <span class="keyword">if</span> x &lt;= <span class="number">0</span> <span class="keyword">or</span> y &lt;= <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> x == <span class="number">1</span> <span class="keyword">and</span> y == <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> f(x - <span class="number">1</span>, y) + f(x, y - <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>递归 + 记忆化搜索：<code>O(mn), O(mn)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, y</span>):</span><br><span class="line">  <span class="keyword">if</span> x &lt;= <span class="number">0</span> <span class="keyword">or</span> y &lt;= <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> x == <span class="number">1</span> <span class="keyword">and</span> y == <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">if</span> (x, y) <span class="keyword">not</span> <span class="keyword">in</span> mem:</span><br><span class="line">    mem[(x, y)] = f(x - <span class="number">1</span>, y) + f(x, y - <span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span> mem[(x, y)]</span><br></pre></td></tr></table></figure><p>动态规划：<code>O(mn), O(mn)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, y</span>):</span><br><span class="line">  dp = [[<span class="number">0</span>] * (m + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)]</span><br><span class="line">  dp[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, y + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, x + <span class="number">1</span>):</span><br><span class="line">      dp[i][j] = dp[i - <span class="number">1</span>][j] + dp[j][i - <span class="number">1</span>]</span><br><span class="line">      </span><br><span class="line">  <span class="keyword">return</span> dp[y][x]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2-3-打家劫舍"><a href="#2-3-打家劫舍" class="headerlink" title="2.3 打家劫舍"></a>2.3 打家劫舍</h2><p><a href="https://leetcode.cn/problems/house-robber/description/" title="198. 打家劫舍 - 力扣（LeetCode）">198. 打家劫舍 - 力扣（LeetCode）</a></p><p>定义方式1：</p><ul><li><code>dp[i]</code>状态的定义：max $ of robbing <code>A[0 → i]</code></li><li>$dp[i] = max(dp[i - 2] + nums, dp[i - 1])$</li></ul><p>定义方式2：</p><ul><li><code>dp[i][0]</code>状态定义：max $ of robbing <code>A[0 → i]</code> 且没偷 <code>nums[i]</code></li><li><code>dp[i][1]</code>状态定义：max $ of robbing <code>A[0 → i]</code> 且偷了 <code>nums[i]</code></li><li>$dp[i][0] = max(dp[i - 1][0], dp[i - 1][1])$</li><li>$dp[i][1] = dp[i - 1][0] + nums[i]$&#x20;</li></ul><h2 id="2-4-最小路径和"><a href="#2-4-最小路径和" class="headerlink" title="2.4 最小路径和"></a>2.4 最小路径和</h2><p><a href="https://leetcode.cn/problems/minimum-path-sum/description/" title="64. 最小路径和 - 力扣（LeetCode）">64. 最小路径和 - 力扣（LeetCode）</a></p><p><code>dp[i][j]</code>的状态定义：<code>minPath(A[1→i][1→j])</code></p><p>$dp[i][j] = min(dp[i - 1][j], dp[i][j - 1] + A[i][j])$</p><h2 id="2-5-买卖股票的最佳时机"><a href="#2-5-买卖股票的最佳时机" class="headerlink" title="2.5 买卖股票的最佳时机"></a>2.5 买卖股票的最佳时机</h2><p><a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock/description/" title="121. 买卖股票的最佳时机 - 力扣（LeetCode）">121. 买卖股票的最佳时机 - 力扣（LeetCode）</a></p><p><a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock/solutions/8753/yi-ge-fang-fa-tuan-mie-6-dao-gu-piao-wen-ti-by-l-3/" title="121. 买卖股票的最佳时机 - 力扣（LeetCode）">121. 买卖股票的最佳时机 - 力扣（LeetCode）</a></p><p><img src="image/image_todtwyCvKI.png" alt=""></p><p>状态定义：<code>dp[i][k][0 or 1]</code> (0 ≤ i ≤ n - 1, i ≤ k ≤ K)</p><ul><li><code>i</code>为天数</li><li><code>k</code>为最多交易次数</li><li><code>[0, 1]</code>为是否持有股票</li><li>总状态数： <code>n*K*2</code>中状态</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="number">0</span> &lt;= i &lt; n:</span><br><span class="line">  <span class="keyword">for</span> <span class="number">1</span> &lt;= k &lt;= K:</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> &#123;<span class="number">0</span>, <span class="number">1</span>&#125;:</span><br><span class="line">      dp[i][k][s] = <span class="built_in">max</span>(buy, sell, rest)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>$dp[i][k][0] = max(dp[i - 1][k][0], dp[i - 1][k][1] + prices[i])$</p><ul><li><code>dp[i - 1][k][0]</code> : 选择 rest</li><li><code>dp[i - 1][k][1] + prices[i]</code> ： 选择sell</li><li>今天没有持有股票，有两种可能：<ol><li>昨天就没有持有股票，然后今天选择rest，多以今天还是没有持有股票</li><li>昨天持有股票，但是今天sell了，所以今天没有持有股票</li></ol></li></ul><p>$dp[i][k][1] = max(dp[i - 1][k][1], dp[i - 1][k - 1][0] - prices[i])$</p><ul><li><code>dp[i - 1][k][1]</code> : 选择rest</li><li><code>dp[i - 1][k - 1][0] - prices[i]</code> : 选择buy</li><li>今天持有股票，有两种可能：<ol><li>昨天就持有股票，但是今天选择rest，所以今天还持有股票</li><li>昨天本没有持有股票，但今天选择buy，所以今天持有股票</li></ol></li></ul><p>初始状态：</p><ul><li>$dp[-1][k][0] = dp[i][0][0] = 0$</li><li>$dp[-1][k][1] = dp[i][0][1] = -infinity$</li></ul><p>状态转移方程：</p><ul><li>$dp[i][k][0] = max(dp[i - 1][k][0], dp[i - 1][k][1] + prices[i])$</li><li>$dp[i][k][1] = max(dp[i - 1][k][1], dp[i - 1][k - 1][0] - prices[i])$</li></ul><h1 id="3-进阶DP习题"><a href="#3-进阶DP习题" class="headerlink" title="3.进阶DP习题"></a>3.进阶DP习题</h1><p>复杂度来源：</p><ol><li><strong>状态拥有更多维度</strong>（二维、三维、或者跟过、甚至需要压缩）</li><li><strong>状态方程更加复杂</strong></li></ol><h2 id="3-1-最小花费爬楼梯"><a href="#3-1-最小花费爬楼梯" class="headerlink" title="3.1 最小花费爬楼梯"></a>3.1 最小花费爬楼梯</h2><p><a href="https://leetcode.cn/problems/min-cost-climbing-stairs/description/" title="746. 使用最小花费爬楼梯 - 力扣（LeetCode）">746. 使用最小花费爬楼梯 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">给你一个整数数组 cost ，其中 cost[i] 是从楼梯第 i 个台阶向上爬需要支付的费用。一旦你支付此费用，即可选择向上爬一个或者两个台阶。</span><br><span class="line"></span><br><span class="line">你可以选择从下标为 0 或下标为 1 的台阶开始爬楼梯。</span><br><span class="line"></span><br><span class="line">请你计算并返回达到楼梯顶部的最低花费。</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minCostClimbingStairs</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; cost)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = cost.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (n &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(n + <span class="number">1</span>)</span></span>;</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        dp[<span class="number">1</span>] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">2</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            dp[i] = std::<span class="built_in">min</span>(dp[i - <span class="number">1</span>] + cost[i - <span class="number">1</span>], dp[i - <span class="number">2</span>] + cost[i - <span class="number">2</span>]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="3-2-编辑距离"><a href="#3-2-编辑距离" class="headerlink" title="3.2 编辑距离"></a>3.2 编辑距离</h2><p><a href="https://leetcode.cn/problems/edit-distance/description/" title="72. 编辑距离 - 力扣（LeetCode）">72. 编辑距离 - 力扣（LeetCode）</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">给你两个单词 word1 和 word2， 请返回将 word1 转换成 word2 所使用的最少操作数  。</span><br><span class="line"></span><br><span class="line">你可以对一个单词进行如下三种操作：</span><br><span class="line"></span><br><span class="line">- 插入一个字符</span><br><span class="line">- 删除一个字符</span><br><span class="line">- 替换一个字符</span><br></pre></td></tr></table></figure><ol><li>BFS + 剪枝（单词的长度范围）</li><li>DP<ol><li>状态定义：<code>dp[0..i][0..j]</code>， i表示第一个字符串匹配到第二个字符串的长度；j表示第二个字符串匹配到第一个字符串的长度；<code>word1.substr(0, i)</code> 与 <code>word2.substr(0, j)</code>之间的编辑距离<br>2.</li></ol></li></ol><p><strong>w1和w2的最后一个字符一样</strong></p><blockquote><p>w1 : …x (i)<br>w2 : …x (j)&#x20;</p></blockquote><p><code>edit_dist(w1, w2) = edit_dist(w1[0 : i -1], w2[0, j - 1])</code></p><p><code>edit_dist(i, j) = edit_dist(i - 1, j - 1)</code></p><p><strong>w1和w2的最后一个字符不一样</strong></p><blockquote><p>w1 : …x (i)<br>w2 : …y (j)&#x20;</p></blockquote><p><code>edit_dist(i, j) = ``min``(edit_dist(i - 1, j - 1) + 1 , edit_dist(i - 1, j ) + 1, edit_dist(i, j - 1) + 1)</code></p><ul><li><code>edit_dist(i - 1, j - 1) + 1</code> :  替换，编辑距离 + 1</li><li><code>edit_dist(i - 1, j) + 1</code> : 删除word1最后一个字符， 编辑距离 + 1</li><li><code>edit_dist(i , j - 1) + 1</code> : 删除 word2最后一个字符，编辑距离 + 1</li></ul><p>注意，针对第一行，第一列要单独考虑，我们引入 <code>&#39;&#39;</code> 下图所示：</p><p><img src="image/image_2Vw6IPFNce.png" alt=""></p><p>第一行，是 <code>word1</code> 为空变成 <code>word2</code> 最少步数，就是插入操作</p><p>第一列，是 <code>word2</code> 为空，需要的最少步数，就是删除操作</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minDistance</span><span class="params">(string word1, string word2)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n1 = word1.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> n2 = word2.<span class="built_in">size</span>();</span><br><span class="line">        std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(n1 + <span class="number">1</span>, std::<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n2 + <span class="number">1</span>, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 第一行</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n2; j++) &#123;</span><br><span class="line">            dp[<span class="number">0</span>][j] = dp[<span class="number">0</span>][j - <span class="number">1</span>] + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 第一列</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n1; i++) &#123;</span><br><span class="line">            dp[i][<span class="number">0</span>] = dp[i - <span class="number">1</span>][<span class="number">0</span>] + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n1; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n2; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (word1.<span class="built_in">at</span>(i - <span class="number">1</span>) == word2.<span class="built_in">at</span>(j - <span class="number">1</span>)) &#123;</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>];</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[i][j] = std::<span class="built_in">min</span>(std::<span class="built_in">min</span>(dp[i - <span class="number">1</span>][j - <span class="number">1</span>], dp[i - <span class="number">1</span>][j]), dp[i][j - <span class="number">1</span>]) + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[n1][n2];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> DSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSA </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 对比学习论文综述</title>
      <link href="/paper_reading/1.4.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/"/>
      <url>/paper_reading/1.4.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<p>对比学习在计算机视觉领域的发展历程，4个阶段：</p><ol><li><strong>百花齐放</strong>：InstDisc（instance discrimination）、CPC、CMC。方法、模型、目标函数、代理任务都还没有统一。</li><li><strong>CV双雄</strong>：MOCOv1、SimCLRv1、MOCOv2、SimCLRv2、CPC和CMC的延伸工作、SwaV，这个阶段发展非常迅速，以上这些工作间隔时间都很短，ImageNet上的最好成绩，基本上每个月都在被刷新。</li><li><strong>不用负样本</strong>：BYOL及其后续改进，SimSima把所有方法都归纳总结，融入到SimSima框架之中，算是<strong>卷积神经网络做对比学习的总结性工作</strong>。</li><li><strong>Transformer</strong>：MOCOv3、DINO，用Vision Transformer开展工作。对于自监督学习来说，无论是对比学习还是最新的掩码学习，都是用Vision Transformer做的</li></ol><h2 id="1-阶段一：百花齐放-x20"><a href="#1-阶段一：百花齐放-x20" class="headerlink" title="1.阶段一：百花齐放&#x20;"></a>1.阶段一：百花齐放&#x20;</h2><h2 id="1-1-InstDisc（instance-discrimination）"><a href="#1-1-InstDisc（instance-discrimination）" class="headerlink" title="1.1 InstDisc（instance discrimination）"></a>1.1 <strong>InstDisc（instance discrimination）</strong></h2><ul><li>论文名称：<strong>Unsupervised Feature Learning via Non-Parametric Instance Discrimination</strong></li><li>论文地址：<a href="https://arxiv.org/abs/1805.01978" title="https://arxiv.org/abs/1805.01978">https://arxiv.org/abs/1805.01978</a></li></ul><h3 id="（1）简介"><a href="#（1）简介" class="headerlink" title="（1）简介"></a>（1）简介</h3><p>instdisc：<strong>提出了代理任务：个体判别任务</strong>。是MOCO中反复提到的文献61，如果MOCO是一篇里程碑式的工作，那么InstDisc就是巨人的肩膀，就是MOCO提到的memory bank方法的论文。</p><p><strong>创新点</strong>：<strong>用个体判别+ NCEloss，做对比学习</strong>，取得了不错的无监督表征学习的结果。同时它还提出了用别的数据结构，去存大量的负样本。以及如何对特征进行动量的更新。</p><p>对后来的对比学习的工作气到了至关重要的推进作用。</p><h3 id="（2）方法"><a href="#（2）方法" class="headerlink" title="（2）方法"></a>（2）方法</h3><h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h4><p><strong>图1，**</strong>动机<strong> 受到有监督学习结果的启发：让相似图片聚集在一起的原因并不是他们有相似的语义标签，而是图片确实长得太像了。基于此提出了</strong>个体判别任务**，这种无监督学习方式，就是把按类别走的有监督信号推到了极致，就是把每个instance（图片）都看成一个类别，目标是能学一种特征，让我们能把每一张图片都区分开。</p><p><img src="image/image_sYatpEB83H.png" alt=""></p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p><strong>图2，**</strong>方法<strong> 简单来说就是想</strong>通过一个CNN把图片都编码成一个特征，希望这个特征能在最后的特征空间中尽可能的分开（每个图片都是自己的类）<strong>；训练CNN：</strong>对比学习**，正样本是图片本身，负样本是数据集中所有其他的图片（用<code>memory bank</code>存放大量的负样本）</p><p>做对比学习，大量的负样本特征到底应该存在哪呢？本文用了 memory bank 的形式：就是说把所有图片的特征全都存到memory bank 里，也就是一个<strong>字典</strong>（ImageNet数据集有128万的图片，也就是说memory bank里要存128万行，也就意味着每个特征的维度不能太高，否则存储代价太大了，本文用的是128维）</p><p><img src="image/image_0RTqQj01KQ.png" alt=""></p><h4 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a><strong>前向过程</strong></h4><ul><li>batch size：256（256个图片输入，也是<strong>正样本</strong>）</li><li>编码器：ResNet50（生成2048维度的特征）</li><li>降维：128维（每个图片的特征大小）</li><li><strong>负样本</strong>：从memory bank中随机抽（文中抽了4096个负样本）</li><li><strong>用NCEloss去算对比学习的目标函数</strong>，一旦更新完网络，就用这个minibatch中的数据样本所对应的特征，去更换原来memory bank中的特征，这样memory bank就得到了更新。反复这个过程，不停的更新memory bank，让最后学到的特征尽可能的有区分性。</li><li><strong>测试时，使用KNN进行分类</strong> 。获得了训练好的模型后，对于一张图片提取他的特征，将他和memorybank中所有的存储图片特征计算相似度，然后采用k近邻算法，返回最相似的k张图片。最后根据相似度权重投票，得到其类别c。</li></ul><h3 id="（3）训练细节"><a href="#（3）训练细节" class="headerlink" title="（3）训练细节"></a>（3）训练细节</h3><h4 id="实验超参数的设定"><a href="#实验超参数的设定" class="headerlink" title="实验超参数的设定"></a>实验超参数的设定</h4><p>算loss的时候，温度设置是0.07，选了4000个负样本，训练200个epoch，bs是256，起始的lr是0.03（MOCO延续了这些超参数的设定）</p><h4 id="Parametric-Classifier参数分类器-x20"><a href="#Parametric-Classifier参数分类器-x20" class="headerlink" title="Parametric Classifier参数分类器 &#x20;"></a>Parametric Classifier参数分类器 &#x20;</h4><p>在传统的参数softmax函数中，对图片x及特征 $v=f_\theta (x)$，被识别为第i类样例的概率为：</p><script type="math/tex; mode=display">P(i \mid \mathbf{v})=\frac{\exp \left(\mathbf{w}_{i}^{T} \mathbf{v}\right)}{\sum_{j=1}^{n} \exp \left(\mathbf{w}_{j}^{T} \mathbf{v}\right)}</script><p>其中 <code>v</code>是卷积网络输出的特征表示，<code>i</code> 是预测类别(实例级)，<code>w</code>是需要优化的权重向量。</p><h4 id="Non-Parametric-Softmax-Classifier-x20"><a href="#Non-Parametric-Softmax-Classifier-x20" class="headerlink" title="Non-Parametric Softmax Classifier &#x20;"></a>Non-Parametric Softmax Classifier &#x20;</h4><p>作者认为纯粹的参数w阻碍了个体之间的对比，于是文章采用的无参softmax：使用L2正则化的 $v_i^{T}$ 来替换  $w_i^{T}$ ，$   \tau $ 用来调整类别分布的集中程度：</p><script type="math/tex; mode=display">P(i \mid \mathbf{v})=\frac{\exp \left(\mathbf{v}_{i}^{T} \mathbf{v} / \tau\right)}{\sum_{j=1}^{n} \exp \left(\mathbf{v}_{j}^{T} \mathbf{v} / \tau\right)}</script><p>使用Mermory Bank V 来存储上述的 $v_j$，在每个iteration对应修改其值 $  f_i\to v_i $，在初始化时通过单位随机向量对V进行初始化。</p><h4 id="Noise-Contrastive-Estimation"><a href="#Noise-Contrastive-Estimation" class="headerlink" title="Noise-Contrastive Estimation"></a>Noise-Contrastive Estimation</h4><p>多分类问题转化为一组二分类问题，其中二分类任务是区分数据样本和噪声样本。&#x20;</p><p>由上式可知，计算瓶颈在于分母，需要枚举所有图片，这样的计算复杂度是无法接受的。为了解决这一问题，我们不再采用原先的采样方式，而是<strong>用随机负采样</strong>，即从噪音分布当中进行随机采样，真实样本和噪音分布的数据比为 m。 &#x20;</p><p>如果噪音分布当中采样n个数据，那么真实样本就采样n/m个数据（一般就为1个）。这样原先的多元问题就转化为了二元问题，则Memory bank中特征表示 v 对应于第i 个样例的概率为：</p><script type="math/tex; mode=display">P(i \mid \mathbf{v})=\frac{\exp \left(\mathbf{v}^{T} \mathbf{f}_{i} / \tau\right)}{Z_{i}}</script><script type="math/tex; mode=display">Z_{i}=\sum_{j=1}^{n} \exp \left(\mathbf{v}_{j}^{T} \mathbf{f}_{i} / \tau\right)</script><p>设定噪声分布为一个均匀分布 $P_n=1/n$ ，则<code>v</code>属于第<code>i</code>个个体的后验概率为：</p><script type="math/tex; mode=display">h(i, \mathbf{v}):=P(D=1 \mid i, \mathbf{v})=\frac{P(i \mid \mathbf{v})}{P(i \mid \mathbf{v})+m P_{n}(i)}</script><p>训练目标为最小化似然函数</p><script type="math/tex; mode=display">\begin{aligned} J_{N C E}(\boldsymbol{\theta}) & =-E_{P_{d}}[\log h(i, \mathbf{v})] \\ & -m \cdot E_{P_{n}}\left[\log \left(1-h\left(i, \mathbf{v}^{\prime}\right)\right)\right]\end{aligned}</script><p>其中 $P_d$ 指代真实数据分布，对$  P_d $而言$v$ 是 $x_i$  的特征；$v ′$ 是来自另一幅图片，从噪声分布 $P_n$中随机采样得到，$v$ 和$v ′$ 都是从Memory Bank中采样得到的。</p><p>在正向计算时, 分母项 $\sum_{j=1}^{n} \exp \left(\mathbf{v}_{j}^{T} \mathbf{f}_{i} / \tau\right)$的计算是无法避免的, 直接计算的计算量同样很大, 于是本文使用蒙特卡罗方法来估计这一项:</p><script type="math/tex; mode=display">Z \simeq Z_{i} \simeq n E_{j}\left[\exp \left(\mathbf{v}_{j}^{T} \mathbf{f}_{i} / \tau\right)\right]=\frac{n}{m} \sum_{k=1}^{m} \exp \left(\mathbf{v}_{j_{k}}^{T} \mathbf{f}_{i} / \tau\right)</script><h4 id="Proximal-Regularization"><a href="#Proximal-Regularization" class="headerlink" title="Proximal Regularization"></a>Proximal Regularization</h4><p>给模型训练加了一个约束，让memory bank中的特征进行动量式的更新</p><p>由于每个“类”只有1个样例，在每个epoch中，一个“类”只被访问一次，训练的过程比较不稳定。为了使训练更加平滑，在损失函数上增加一项针对v 的惩罚, 来稳定训练过程。</p><script type="math/tex; mode=display">-\log h\left(i, \mathbf{v}_{i}^{(t-1)}\right)+\lambda\left\|\mathbf{v}_{i}^{(t)}-\mathbf{v}_{i}^{(t-1)}\right\|_{2}^{2}</script><p>其中， $v_i^{(t)}=f_\theta(x_i)$ （第t次迭代时backbone的输出特征）， $V={v_i^{(t-1)}}$ 来自于memory bank。这样随着多次迭代，由于 ${v_i^{(t)}}-{v_i^{(t-1)}}$ 的加入，backbone和memory bank存储的特征就逐渐相同了，回到了原始的损失，加速了收敛。</p><p>所以Proximal Regularization相当于模型的训练加了一个约束，从而能让 memory bank 里的那些特征进行动量式的更新（当前时刻的输出和上一时刻的输入有关），跟 MoCo 的想法是非常一致的。</p><p><img src="image/image_tEyBuW166N.png" alt=""></p><h3 id="（4）总结"><a href="#（4）总结" class="headerlink" title="（4）总结"></a>（4）总结</h3><p><code>Inst Disc</code> 这篇论文也是一个里程碑式的工作：它不仅<strong>提出了个体判别这个代理任务</strong>，而且用这个代理任务和 NCE loss做对比学习，从而取得了不错的无监督表征学习的结果。同时它还提出了<strong>用别的数据结构存储这种大量的负样本，以及如何对特征进行动量的更新</strong>，所以真的是对后来对比学习的工作起到了至关重要的推进作用。</p><h2 id="1-2-InvaSpread"><a href="#1-2-InvaSpread" class="headerlink" title="1.2 InvaSpread"></a>1.2 InvaSpread</h2><ul><li>论文名称：<strong>Unsupervised Embedding Learning via Invariant and Spreading Instance Feature</strong></li><li>论文地址：<a href="https://arxiv.org/abs/1904.03436" title="Unsupervised Embedding Learning via Invariant and Spreading Instance Feature (arxiv.org)">Unsupervised Embedding Learning via Invariant and Spreading Instance Feature (arxiv.org)</a></li></ul><h3 id="（1）简介-1"><a href="#（1）简介-1" class="headerlink" title="（1）简介"></a>（1）简介</h3><p>这篇文章作者同样没有为自己的方法起名字，所以后面一般将其简称为<code>Inva Spread</code>。<code>Inva Spread</code>是一种端到端的训练方式，直接训练特征本身，<strong>无需额外的数据结构</strong>（比如上文的memory bank），提升了效率和准确度。作者还使用了新的采样方式，降低了计算复杂度。</p><p>简单来说，<strong>本文中的正负样本都来自同一个mini_batch</strong>。比如对于图片$x_i$ ，其正样本就是数据增强后的图片 ${x_{i}}’$，而负样本就是这个mini_batch中除了 $(x_i,{x_{i}}’)$ 之外的所有样本，而不是整个数据集中的所有其它样本。这样负样本数大大减少，可以不需要额外的数据结构来存储，就可以用一个编码器做端到端的训练了。</p><p><code>Inva Spread</code>可以看做是<code>SimCLR</code>的前身，但由于数据增强策略不足以及负样本数量太少，也没有<code>SimCLR</code>提出的mlp projector ，使得最终的训练效果不好，没有太大的影响力。</p><blockquote><p><code>Inva Spread</code>的作者太穷，没有TPU，只能选择<code>batch_size=256</code>来训练。这样每次迭代的负样本只有255*2个，数量太少，对比学习的效果不够好（也就是在MOCO中说过的字典太小）。而<code>SimCLR</code>的作者来自谷歌，可以使用大量的TPU，最终训练的<code>batch_size=8192</code>，足以达到不错的训练效果。</p></blockquote><h3 id="（2）方法-1"><a href="#（2）方法-1" class="headerlink" title="（2）方法"></a>（2）方法</h3><p>作者认为提升效率的方法就是直接优化特征本身，拒绝额外的数据结构，也就是用端到端的方式。但这样做会有两种阻碍：一是如果抛弃通过参数w来学习，也不采用memory bank利用时间差更新而让特征自己乘自己，就会使得网络得不到训练。二是不采用NCE等方式，训练的复杂度就太大了。</p><p>作者认为，相似图片通过编码器以后，它的特征应该很类似，不同的图片，它的特征出来就应该不类似，这就是题目中说的invariant和 spreading 。于是作者提出的孪生神经网络结构，有效地解决了这两个问题： &#x20;</p><p><img src="https://i0.hdslb.com/bfs/note/d7daa73fbb0adf3b131e7f2da5d7b83f117405ff.png@690w_!web-note.webp" alt=""></p><p>对比学习的思想：同样的图片通过编码器，特征应该很类似，不同的图片特征应该不类似。</p><p>Invariant：相似的图片，特征应该保持不变性</p><p>Spreading：不相似的图片和物体，特征应该尽可能的分散开</p><p>具体做法：</p><ul><li>代理任务：个体判别</li><li>正负样本的选取：</li></ul><p>前向过程：</p><p><img src="image/image_Z47dMLNDFP.png" alt=""></p><ul><li>设batch_size=256，即输入256张图片。经过数据增强，又得到了256张增强后的图片。这样每个batch有256个正样本和（256-1）*2个负样本。【和InstDisc不同，InstDisc正样本256，但负样本是从一个memory bank中抽出来的，负样本是4096，甚至可以更大】。为了能用一个编码器做端到端的训练，本文从同一个minibatch中选正负样本。【这就是MOCO中讲到的端到端的学习方式】</li><li>根据正负样本计算loss（NCE loss 的一个变体），然后更新网络参数。相似的特征在特征空间中尽可能的接近，与其他的特征应该尽可能的拉远。目标函数式NCEloss的变体。</li><li>训练结果表示在最后特征空间中，就是绿色的两个球靠近，和所有别的球远离；其余类似。</li></ul><h3 id="（3）总结"><a href="#（3）总结" class="headerlink" title="（3）总结"></a>（3）总结</h3><p>这篇论文，属于另一个流派，也就是<strong>端到端的学习</strong>，而且只用一个编码器，不需要借助外部的数据结构去存储大量的负样本，它的正负样本都来自于同一个 minibach。</p><p>既然它跟 SimCLR 这么像，为什么它没有取得那么好的结果呢？就是之前在MoCo那篇论文里反复强调过的，就是这个字典必须足够大，也就是说在做对比学习的时候，负样本最好是足够多，而本文的作者是没有 TPU 的，所以说它的 batch size 就是256，也就意味着它的负样本只有500多个，再加上它还缺少像 SimCLR 那样那么强大的数据增广以及最后提出的那个 mlp projector，所以说呢这篇论文的结果没有那么炸裂，自然也就没有吸引大量的关注，但事实上它是可以理解成 SimCLR 的前身</p><h2 id="1-3-CPC"><a href="#1-3-CPC" class="headerlink" title="1.3 CPC"></a>1.3 CPC</h2><ul><li>论文名称：<strong>Representation Learning with Contrastive Predictive Coding</strong></li><li>论文地址：<a href="https://paperswithcode.com/paper/representation-learning-with-contrastive" title="https://paperswithcode.com/paper/representation-learning-with-contrastive">https://paperswithcode.com/paper/representation-learning-with-contrastive</a></li></ul><p>一般机器学习分为<strong>判别式模型和生成式模型</strong>，个体判别显然是属于判别式范畴的，那肯定就会有一些生成式的代理任务，比如最常见的预测型的任务。</p><p><strong>CPC</strong>：contrastive predivtive coding，一个可以处理音频、图片、文字还可以使用在强化学习中的通用模型。用预测的代理任务做对比学习。</p><p>本文使用音频为输入，如下图所示：</p><p><img src="image/image_MQiFV8nQ3Y.png" alt=""></p><ul><li>对于一个输入序列$x$，当前时刻为$t$。$t$时刻输入经过编码器 $g_{enc}$ 得到编码特征$  {z_t} $。</li><li>${z_t}$ 经过自回归模型 $ g_{ar}  $（比如RNN/LSTM）得到输出$c_t$ （context representation，上下文特征，因为含有之前时刻的信息）。如果  $c_t$表示的足够好，包含之前所有时刻的信息，那么应该可以用来预测未来时刻的输出特征$z_{t+i}$ 。</li><li>对比学习的<strong>正样本</strong>就是未来的输入通过编码器以后得到的未来时刻的特征输出，<strong>负样本</strong>的定义很广泛，比如，可以任选输入，通过编码器得到输出，那预测应该是不相似的。。 &#x20;</li></ul><p>这套思想是很普适的，输入可以换成句子，用前面的单词去预测后面的单词的特征的输出；可以换成一系列的图片patch，左上到右下，可以用上半部分的图片特征预测下半部分的图片特征。</p><h2 id="1-4-CMC"><a href="#1-4-CMC" class="headerlink" title="1.4 CMC"></a>1.4 CMC</h2><ul><li>论文名称：<strong>Contrastive Multiview Coding</strong></li><li>论文地址：<a href="https://paperswithcode.com/method/contrastive-multiview-coding" title="https://paperswithcode.com/method/contrastive-multiview-coding">https://paperswithcode.com/method/contrastive-multiview-coding</a></li></ul><h3 id="（1）简介-2"><a href="#（1）简介-2" class="headerlink" title="（1）简介"></a>（1）简介</h3><p>CMC使用一个物体的多个视角来作为正样本。这个思想来自于人类对世界的感受、观察。</p><p>在摘要中，作者说人类观察这个世界是通过很多个不同视角的传感器，比如说眼睛或者耳朵，来给大脑提供不同的信号。每一个视角都是带有噪声的，而且有可能是不完整的。但是最重要的那些信息，比如物理性质，几何形状以及语义信息，在所有的这些视角中间共享。例如一只狗可以被看到、听到、感受到。</p><p>基于此，作者认为一个强大的特征，应该具有<strong>视觉不变性</strong>（不论是看到还是听到，都应该能判断出那是一只狗）。所以CMC目的，就是最大化同一个场景不同视角的互信息，并且可以扩展到任意数量的未知视角，且视角越多效果越好。</p><p>本文定义正样本的方式很广泛，一个物体的多个视角都可以被当做正样本。</p><p>第一个做多视角的对比学习，不仅证明了对比学习的灵活性，也证明了多视角、多模态的可行性。【接下来OpenAI就出了clip模型：如果有一个图片以及描述它的文本，这就是一个正样本对，用来做多模态的对比学习】</p><h3 id="（2）方法-2"><a href="#（2）方法-2" class="headerlink" title="（2）方法"></a>（2）方法</h3><h4 id="正负样本"><a href="#正负样本" class="headerlink" title="正负样本"></a>正负样本</h4><p><img src="image/image_gHxZpfn-RP.png" alt=""></p><p><strong>数据集</strong>：<code>NYU RGBD</code>，有四个视角，原始图像、深度信息（距离观察者的远近）、surface normal（表面法线）、物体的分割图像</p><p><strong>正样本</strong>：虽然不同的输入来自不同的传感器，或者来自不同的模态，但是所有的输入都对应的一个东西，他们就应该互为正样本，也就是，绿色的点在特征空间中应该非常接近。</p><p>负样本：如果随机选择一张图片，不配对的，得到的特征就应该远离。</p><p><strong>局限性</strong>：在处理不同视角/模态的时候，可能需要不同的编码器，因为不同的输入可能长得很不一样，这样计算代价就会高。比如在CLIP中，文本端用的是bert，图像用的是vit。transformer有可能同时处理不同模态的数据【MA-CLIP: Towards Modality-Agnostic Contrastive Language-Image Pre-training 用一个transformer同时处理两个模态，效果反而更好】不用针对每个数据去做特有的改进。</p><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>CPC可以看做是学习过去和未来两个视角，个体判别是学习一张图片的不同crops，但使用的却都是一种目标函数。本文使用的也是普通的NCELoss目标函数，但作者将其进行扩展以适应不同视角的需求，对比学习也扩展到了很多其他领域。</p><p><strong>两个视角目标函数（两视角对比着学）</strong>：&#x20;</p><script type="math/tex; mode=display">\mathcal{L}_{\text {contrast }}^{V_{1}, V_{2}}=-\underset{\left\{v_{1}^{1}, v_{2}^{1}, \ldots, v_{2}^{k+1}\right\}}{\mathbb{E}}\left[\log \frac{h_{\theta}\left(\left\{v_{1}^{1}, v_{2}^{1}\right\}\right)}{\sum_{j=1}^{k+1} h_{\theta}\left(\left\{v_{1}^{1}, v_{2}^{j}\right\}\right)}\right]</script><p>其中，$h_{\theta}\left(\left\{v_{1}, v_{2}\right\}\right)=\exp \left(\frac{f_{\theta_{1}}\left(v_{1}\right) \cdot f_{\theta_{2}}\left(v_{2}\right)}{\left|f_{\theta_{1}}\left(v_{1}\right)\right| \cdot\left|f_{\theta_{2}}\left(v_{2}\right)\right|} \cdot \frac{1}{\tau}\right)$</p><p>即固定 $v_{1}^{1}$  ，列举 $v_{2}^{j}$ ，同样的也可以反过来固定 $v_{2}^{1}$ 于是：</p><script type="math/tex; mode=display">\mathcal{L}\left(V_{1}, V_{2}\right)=\mathcal{L}_{\text {contrast }}^{V_{1}, V_{2}}+\mathcal{L}_{\text {contrast }}^{V_{2}, V_{1}}</script><p>这里的  $f_{\theta }^{1}$  和  $ f_{\theta }^{2}  $是两种backbone，不共享参数，这个和Spreading Instance是有区别的。 &#x20;</p><p><strong>多个视角目标函数，有两种范式</strong>：</p><ul><li>仅将一个视角和其他所有视角对比： $L_{c}=\sum_{j=2}^{M}L(V_{1},V_{j})$&#x20;</li><li>每个视角相互对比：$\mathcal{L}_{F}=\sum_{1 \leq i&lt;j \leq M} \mathcal{L}\left(V_{i}, V_{j}\right)$</li></ul><p><img src="image/image_hK-PMsysWQ.png" alt=""></p><p>cmc原班作者人马还用对比学习的思想做了一篇蒸馏的工作。对于teacher 模型和student 模型，不论用什么网络，不论这个网络是好是坏是大是小，只要你的输入是同一张图片，那得到的这个特征就应该尽可能的类似，即二者的输出尽可能的相似。通过这种方式把 teacher和student做成了一个正样本对，从而可以做对比学习。</p><h3 id="（3）总结-1"><a href="#（3）总结-1" class="headerlink" title="（3）总结"></a>（3）总结</h3><p><code>CMC</code>正负样本确定的方式由个体升级成了个体的不同的视角（如色彩模型）。它同样使用了NCE，但将其扩展以适应不同的视角。<code>CMC</code>采用多视角对比学习，证明了对比学习的灵活性，也同时证明了多视角多模态的可行性，为之后的CLIP工作（图文配对的多模态对比学习）打下了基础。</p><p>但是本文也有一个局限，即处理不同的视角（模态）时，可能需要不同的编码器，因为不同的输入特点不一样。 如果每个视角都有一个编码器，那么训练的成本就有点高（比如在CLIP里，文本编码器是BERT，图片编码器是ResNet或者ViT）。</p><p>所以这也是现在<code>Transformer</code>最吸引人的地方，这个结构可以同时处理文本和图片，那么就可以用一个解码器处理两种模态，而不用做针对每种数据去做特有的改进。今年在ICLR上发表的<a href="https://paperswithcode.com/paper/ma-clip-towards-modality-agnostic-contrastive" title="MA-CLIP">MA-CLIP</a>，就是用一个<code>Transformer</code>去同时处理两个输入模态，效果反而更好。</p><h2 id="1-5-总结"><a href="#1-5-总结" class="headerlink" title="1.5 总结"></a>1.5 总结</h2><p>第一阶段的四篇论文，使用的<strong>代理任务不同</strong>（个体判别、预测未来、多视角多模态）、<strong>目标函数不同</strong>（NCE、InfoNCE、NCE的其他变体）、<strong>模型不同</strong>（InvaSpread仅用一个编码器、InstDisc用一个编码器和memory bank、CPC是一个编码器和一个自回归模型、CMC有两个甚至多个编码器）任务不同（图像、视频、音频、文字、强化学习）</p><ul><li>InstDisc：一个编码器+memory bank，特征一致性比较差</li><li>Inva Spread：只使用一个编码器进行端到端训练，但是字典太小，负样本不够</li><li>CPC：一个编码器+一个自回归模型</li><li>CMC：有两个甚至多个编码器</li></ul><h1 id="2-阶段二：CV双雄（19年中-20年中）"><a href="#2-阶段二：CV双雄（19年中-20年中）" class="headerlink" title="2.阶段二：CV双雄（19年中-20年中）"></a>2.阶段二：CV双雄（19年中-20年中）</h1><p>CV双雄指的就是MOCO和SimCLR</p><h2 id="2-1-MOCO"><a href="#2-1-MOCO" class="headerlink" title="2.1 MOCO"></a>2.1 MOCO</h2><ul><li>论文名称：<strong>Momentum Contrast for Unsupervised Visual Representation Learning</strong></li><li>论文地址：<a href="https://paperswithcode.com/paper/a-simple-framework-for-contrastive-learning" title="https://paperswithcode.com/paper/a-simple-framework-for-contrastive-learning">https://paperswithcode.com/paper/a-simple-framework-for-contrastive-learning</a></li></ul><p>主要贡献：把之前的对比学习方法归纳总结成一个字典查询的问题。提出<strong>队列</strong>和<strong>动量更新</strong>的编码器构造一个又大又一致的字典，能帮助更好的对比学习。</p><p>MoCo跟Inst Disc是非常相似的：</p><ul><li>它用队列取代了原来的memory bank作为一个额外的数据结构去存储负样本</li><li>它用动量编码器去取代了原来loss里的约束项，从而能达到动量的更新编码器的目的，而不是动量的去更新特征，从而能得到更好的结果</li></ul><p>但是整体的出发点以及一些实现的细节都是非常类似的</p><p>MoCo 的这个实现细节：</p><ul><li>首先从模型的角度上来说，它用的是残差网络，它的基线模型都用的是Res 50，其实Inst Disc也用的是Res 50，模型上是一样的</li><li>最后每个图片的特征维度也沿用了128维</li><li>它也对所有的特征做了L2 归一化</li><li>至于目标函数，MoCo 采用的是info NCE，而不是像Inst Disc是NCE但是算loss用的温度也是0.07</li><li>数据增强的方式也是直接借鉴过来的</li><li>包括后面训练的学习率0.03，训练200个epochs这些也都是跟Inst Disc保持一致的</li></ul><p>所以，说MoCo是Inst Disc一个改进型工作也不为过，但是MoCo真正出色的地方其实有两点</p><ul><li>改进简单有效，而且有很大的影响力，这个动量编码器的改进一直沿用到了最新的工作，带来好的效果。</li><li>另外moco的写作也很精彩，自顶向下：并没有按照传统简单直白的写作方式，先对比过去的工作，谈局限性，然后提出自己的方法。</li></ul><p>MOCO论文写作思路</p><ul><li>引言中，第一段写CV和NLP的区别，以及到底为什么无监督学习在CV这边做的不好；第二段开始讲对比学习，直接把对比学习的方法总结成一个字典查找的问题；然后在CV和NLP大一统、对比学习被看做字典查找也大一统的大框架下，提出了MOCO这个框架，希望能用一个又大又一致的字典，去整体的提高对比学习的性能。</li><li>方法部分，没有模型总览图、没有说模型、任务。而是从目标函数入手，说我们用的是InfoNCE，先定义正负样本，然后网络结构，然后实现细节和伪代码。3.1中，为了让MOCO看起来更普适，没有直接定义输入是什么，也没有定义网络结构是什么样</li><li>什么样的输入都可以（图片、图片块CPC、上下文的图片块）</li><li>网络：query和key的编码器既可以相同（InvaSpread）、部分共享和完全不同的（CMC多个视角所以多个编码器）</li></ul><h2 id="2-2-SimCLR"><a href="#2-2-SimCLR" class="headerlink" title="2.2 SimCLR"></a>2.2 SimCLR</h2><ul><li>论文名称：<strong>A Simple Framework for Contrastive Learning of Visual Representations</strong></li><li>论文地址：<a href="https://paperswithcode.com/paper/a-simple-framework-for-contrastive-learning" title="Big Self-Supervised Models are Strong Semi-Supervised Learners">Big Self-Supervised Models are Strong Semi-Supervised Learners</a></li></ul><h3 id="（1）简介-3"><a href="#（1）简介-3" class="headerlink" title="（1）简介"></a>（1）简介</h3><p>介绍对比学习常用SimCLR当例子，因为它概念上更容易理解，方法也很容易解释，只不过就是batchsize 太大，一般人不好上手。</p><h3 id="（2）方法-3"><a href="#（2）方法-3" class="headerlink" title="（2）方法"></a>（2）方法</h3><p><img src="image/image_9yN5Ng0h2Z.png" alt=""></p><ul><li><code>x</code>：一个minibatch的图片。</li><li><code>xi</code>和<code>xj</code>：x经过不同的数据增强，他俩就是正样本。正样本个数就是batchsize ，负样本就是剩下的样本以及他们数据增强后的样本2（batchsize -1）。</li><li><code>f</code>函数：是编码器，两个编码器共享权重。</li><li><code>h</code>：编码器得到的特征。</li><li><code>g</code>函数：<strong>projector，就是一个全连接层跟着一个relu的激活函数</strong>。（就是这么的一个简单的错做，能让最后学到的特征在imagenet这个分类任务上提10个点）。只有在训练的时候用，做下游任务的时候，只用特征。这个g函数只是为了让模型训练的更好，为了公平对比在下游任务上不使用。</li><li>选用的损失函数是 <code>NT-Xent loss</code>（the normalized temperature-scaled cross entropy loss）：normalized是指在特征后面做了L2归一化，temperature-scaled：在loss上乘一个τ）正样本之间是否能达到最大一致性。所以和<code>infoNCE loss</code>也是非常接近的。</li></ul><script type="math/tex; mode=display">\ell_{i, j}=-\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_{i}, \boldsymbol{z}_{j}\right) / \tau\right)}{\sum_{k=1}^{2 N} \mathbb{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_{i}, \boldsymbol{z}_{k}\right) / \tau\right)}</script><p>前向过程：图片进入编码器编码，然后projector降维，最后算一个对比学习的loss。</p><p><strong>projector在训练时才使用，推理时直接去掉，只用特征h特征。</strong></p><h3 id="（3）SimCLR和InvaSpread"><a href="#（3）SimCLR和InvaSpread" class="headerlink" title="（3）SimCLR和InvaSpread"></a>（3）<code>SimCLR</code>和<code>InvaSpread</code></h3><ol><li>SimCLR用了更多的数据增强（<strong>裁剪、改变色彩</strong>、旋转、cutout、高斯噪声、高斯模糊、sobel滤波器）</li><li>加了一个<code>g函数</code>（可学习的非线性变换），就是一个MLP层。</li><li>用了更大的batchsize，而且训练的时间更久</li></ol><p><code>SimCLR</code>可以被认为是<code>inva spread</code>的改进工作。其最大创新点就是在图片编码特征之后加了一个<code>projector</code>，但就这么简简单单的一层mlp，能让模型在ImageNet 分类任务上直接涨了近10个点。</p><p>SimCLR框架中几乎所有单独的组件都出现在以前的工作中，尽管具体的实现可能有所不同。但SimCLR的优势不是任何单一设计的选择，而是把所有的技术结合起来得到的结果。我们提供了一个全面的比较，非常详细的消融实验，在附录C。</p><p>SimCLR中提出的很多技术，都对后续的工作产生了长远的影响：</p><ol><li>在编码器之后加一个MLP层（MOCOv2，BYOL）</li><li>数据增强技术</li><li>用LARS优化器去做大batchsize 的模型训练（BYOL）</li></ol><h3 id="（4）实验"><a href="#（4）实验" class="headerlink" title="（4）实验"></a>（4）实验</h3><h4 id="模型效果"><a href="#模型效果" class="headerlink" title="模型效果"></a><strong>模型效果</strong></h4><p><code>SimCLR (4×)</code> 这个模型可以在 ImageNet 上面达到 76.5% 的 Top 1 Accuracy，比当时的 SOTA 模型高了7个点。如果把这个预训练模型用 1%的ImageNet的标签给 Fine-tune 一下，借助这一点点的有监督信息，SimCLR 就可以再达到 85.5% 的 Top 5 Accuracy，也就是再涨10个点。</p><p><img src="image/image_nxwcorv49g.png" alt=""></p><h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a><strong>数据增强</strong></h4><p><img src="image/image_e2eywwhoCF.png" alt=""></p><p>作者试验了以上10种数据增强，比如随机裁剪、变换色彩、翻转、Cutout、高斯噪声、blur噪声等等；并做了如下的消融试验（除了最后一列，余下是两两组合）。最后发现随机的裁剪和随机色彩变换组合效果最好。 &#x20;</p><p><img src="image/image_mSNciKEaxF.png" alt=""></p><h4 id="Projection-head-及特征维度"><a href="#Projection-head-及特征维度" class="headerlink" title="Projection head 及特征维度"></a><strong>Projection head 及特征维度</strong></h4><p><img src="image/image_ToSQZKOZVe.png" alt=""></p><ul><li>linear ：只有全连接层，不接Relu激活函数</li><li>None：没有Projection head，直接做对比训练</li><li>non-linear ：本文的方法，加了Projection head。（后面跟Relu，所以是非线性）</li><li>可以发现使用Projection head，结果提了近10个点</li><li>最后z的维度不论是32、64还是2048其实都没太大区别，这就是为什么对比学习现在一般都选一个比较低的特征维度，因为128就够了。</li></ul><h2 id="2-3-MOCOv2"><a href="#2-3-MOCOv2" class="headerlink" title="2.3 MOCOv2"></a>2.3 MOCOv2</h2><ul><li>论文名称：<strong>Improved Baselines With Momentum Contrastive Learning</strong></li><li>论文连接：<a href="https://paperswithcode.com/paper/improved-baselines-with-momentum-contrastive" title="https://paperswithcode.com/paper/improved-baselines-with-momentum-contrastive">https://paperswithcode.com/paper/improved-baselines-with-momentum-contrastive</a></li></ul><h3 id="（1）简介-4"><a href="#（1）简介-4" class="headerlink" title="（1）简介"></a>（1）简介</h3><p>因为moco和SimCLR的效果实在太好，2020年就掀起了对比学习的狂潮。直到2020年底，vision transformer出来以后才逐渐消退。</p><p>发现SimCLR的效果很好，技术都是即插即用型。就在MOCO上做很简单的改动：把MLP projection head和更多的数据增强用起来，就刷新了imagenet上的最好成绩，比过了SimCLR。</p><h3 id="（2）改进策略"><a href="#（2）改进策略" class="headerlink" title="（2）改进策略"></a>（2）改进策略</h3><p><code>MoCov2</code>主要是借鉴了<code>SimCLR</code>而做的优化，比如引入了mlp projection head以及使用更多的数据增强。<code>MoCov2</code>刷新了ImageNet 上的最好成绩，比之前的<code>MoCo</code>以及最新的<code>SimCLR</code>都高很多 。其上传的日期是3月9日，离<code>SimCLR</code>的发布还不到一个月。</p><p><code>MoCov2</code>对比<code>MoCo</code>主要有4个改动：</p><ul><li>添加 projection head</li><li>使用更多的数据增强</li><li>训练时使用cosine的learning rate schedule</li><li>训练的epoch，从200增加到800</li></ul><h3 id="（3）实验"><a href="#（3）实验" class="headerlink" title="（3）实验"></a>（3）实验</h3><h4 id="改进策略效果对比"><a href="#改进策略效果对比" class="headerlink" title="改进策略效果对比"></a><strong>改进策略效果对比</strong></h4><p><img src="image/image_db8b1iDk2c.png" alt=""></p><p>上图列出了模型效果对比图。</p><ul><li>MLP表示增加projection head，可以看到只增加这一点，就提了近6个点</li><li>aug+和cos分别表示上面提到的数据增强和cosine schedule</li><li>灰色行是有监督baseline模型</li></ul><h4 id="与SOTA模型分类效果对比-x20"><a href="#与SOTA模型分类效果对比-x20" class="headerlink" title="与SOTA模型分类效果对比 &#x20;"></a>与SOTA模型分类效果对比 &#x20;</h4><p>下面是和<code>MoCov1</code>以及 <code>SimCLR</code> 在ImageNet数据集上分类效果对比。</p><p><img src="image/image_MHAZDfb6pM.png" alt=""></p><h4 id="硬件资源对比"><a href="#硬件资源对比" class="headerlink" title="硬件资源对比"></a>硬件资源对比</h4><p><img src="image/image_9FF6X_0m_v.png" alt=""></p><p><code>MOCO</code>非常省内存只需要5个G，而且训练只需要53个小时（imagenet这种规模的数据集上，两天多已经算很快了）</p><p>端到端（<code>InvaSpread</code>、<code>SimCLR</code>）：<code>SimCLR</code>在小batchsize的时候效果远不如<code>MOCOv2</code>（因为<strong>字典不够大，负样本不够多，导致对比学习的对比不是很有效</strong>，而且不仅效果低内存占用也明显高，训练时长也长了十几个小时，端到端要性能差不多batchsize就要4096，这对硬件要求太高了）</p><h2 id="2-4-SimCLRv2"><a href="#2-4-SimCLRv2" class="headerlink" title="2.4 SimCLRv2"></a>2.4 SimCLRv2</h2><ul><li>论文名称：Big Self-Supervised Models are Strong Semi-Supervised Learners</li><li>论文连接：<a href="https://paperswithcode.com/paper/big-self-supervised-models-are-strong-semi" title="Big Self-Supervised Models are Strong Semi-Supervised Learners">Big Self-Supervised Models are Strong Semi-Supervised Learners</a></li></ul><h3 id="（1）简介-5"><a href="#（1）简介-5" class="headerlink" title="（1）简介"></a>（1）简介</h3><p><code>SimCLRv2</code>的主要思想体现在其标题里，即大的自监督模型很适合做半监督学习。在摘要中，作者提出：一种从少量带标签数据+大量无标签数据中进行学习的方案是：<strong>无监督预训练（必须是大模型）+有监督微调</strong>，这种半监督学习的方案在ImageNet上极为有效，具体的可以总结为三步：</p><ol><li><code>pretrain</code>：在无标签数据上无监督训练（SimCLR对比学习）一个Big ResNet模型（模型大小至关重要）以学习广义视觉特征表达。</li><li><code>fine-tune</code>：在少量有标签数据上通过进行有监督的微调</li><li><code>distill</code>：用微调后的模型作为<code>teacher</code>模型，在之前的无标签数据集上生成伪标签，然后训练一个<code>student</code>模型进行自监督训练（蒸馏阶段采用KL散度）。</li></ol><p>微调后，作者发现：模型的任务已知预测属性可以进一步改善并蒸馏到一个更小的网络中。为此，作者对无标签数据进行了二次利用以促使学生网络尽可能的模拟老师网络的标签预测性能，且蒸馏阶段采用伪标签方式且不会造成额外的更多复杂度。</p><p>整个框架其实也是受启发于google的另外一篇工作 <a href="https://paperswithcode.com/method/noisy-student" title="Noisy Student">Noisy Student</a>。<code>noisy student</code>就是在<code>ImageNet</code>数据集上先训练了一个 teacher 模型，然后在<code>JFT 300M</code>那个数据集上生成了很多的伪标签，最后一起训练了一个student模型，其精度为88，霸榜ImageNet快一年。</p><p><code>SimCLRv2</code>在仅仅采用<code>1%/10%</code>有标签数据时，backbone使用ResNet50就取得了<code>73.9%/77.5%</code>的top-1精度。</p><p><strong>noisy student 的工作</strong></p><ul><li>因为noisy student就是在ImageNet数据集上先训练了一个 teacher 模型，然后在JFT 300M那个数据集上生成了很多的伪标签，最后一起训练了一个student模型，而这个 student 的模型算是 ImageNet 上的SOTA，大概是88点多的准确率，霸占了ImageNet上这个sota很长时间，大概有一年的时间</li><li>Vision Transformer就跟这个 noisy student 比过，因为截止到那个时候，noisy student还是ImageNet上的 SOTA</li></ul><h3 id="（2）算法"><a href="#（2）算法" class="headerlink" title="（2）算法"></a>（2）算法</h3><p>模型结构如下图所示，训练过程就是上面提的三步： &#x20;</p><p><img src="image/image_BJID1CVNMf.png" alt=""></p><ol><li>第一部分：SimCLRv2，怎样自监督或自监督的对比学习，去训练一个大的模型出来。</li><li>第二部分：一旦有了一个很好的模型，只需要一小部分有标签的数据，去做一下有监督的微调。</li><li>第三部分：微调结束了就相当于有了一个teacher模型，就可以用这个teacher模型去生成很多伪标签，这样就可以在更多的无标签数据上做自学习了。</li></ol><p>我们要看的是第一部分，看作者怎么把SimCLR改进了。</p><p><code>SimCLRv2</code>相比<code>SimCLRv1</code>有三处改进：</p><ul><li>大模型：backbone从<code>ResNet50</code>替换为<code>ResNet152+SK net</code> （selective kernels）</li><li><p>加深<code>protection head</code> ：从一层加到两层。 &#x20;</p><p>protection head在SimCLRv1和MOCOv2中都被证明很有用，所以作者考虑多家几层。最后发现加到两层效果就够了</p></li><li><p>引入了动量编码器：使用了类似<code>MOCO</code>的动量编码器，效果提升了一个点。 &#x20;</p><p>作者解释是，<code>SimCLR</code>模型的 batch_size已经够大了，也就是字典的大小和字典里特征一致性，SimCLR v2 都已经做的很好了。换成<code>MOCO</code>这种队列结构的动量编码器，虽然可训练的负样本更多，但是提升没有那么明显了。</p></li></ul><h3 id="（3）微调"><a href="#（3）微调" class="headerlink" title="（3）微调"></a>（3）<strong>微调</strong></h3><ul><li><code>SimCLRv1</code>在微调时，是去掉 $g(\cdot )$（projector层），只保留编码器 $f(\cdot )$ 进行微调，即 $f^{task}(x_{i})=W^{task}f(x_{i})$ ；</li><li><code>SimCLRv2</code>在微调时，是保留 $  g(\cdot ) $的第一层 ，即 $ f^{task}(x_{i})=W^{task}\cdot \sigma (W^{MLP}\cdot f(x_{i}))  $</li></ul><h2 id="2-5-SwAV"><a href="#2-5-SwAV" class="headerlink" title="2.5 SwAV"></a>2.5 SwAV</h2><ul><li>论文名称：<strong>Unsupervised Learning of Visual Features by Contrasting Cluster Assignment</strong></li><li>论文地址：<a href="https://paperswithcode.com/paper/unsupervised-learning-of-visual-features-by" title="https://paperswithcode.com/paper/unsupervised-learning-of-visual-features-by">https://paperswithcode.com/paper/unsupervised-learning-of-visual-features-by</a></li></ul><h3 id="（1）简介-6"><a href="#（1）简介-6" class="headerlink" title="（1）简介"></a>（1）简介</h3><p><code>SwAV</code>即swap assignment view的缩写，意思就是<strong>一张图片不同视角的特征可以互相预测，因为来自同一张图片的不同视角特征按道理来说都是相似的</strong>。具体的做法，就是<strong>将聚类加入到了对比学习中</strong>。（将匹配问题转为预测问题，预测时借助簇类中心）</p><p>作者认为之前的对比学习，直接拿所有图片的编码特征去做对比有点原始而且计算量太大，因为所有的图片都是自己的类。作者考虑，<strong>能不能不做近似，能不能借助一些先验信息</strong>，一些更简洁的东西比进行对比，而不是和所有负样本直接进行对比。由此作者提出了可以和聚类中心特征进行对比（128万张图片被聚成3000个簇类中心<code>cluster center</code>）。</p><blockquote><p>比如MoCo在ImageNet上训练那就有128万类，即使在计算loss时取近似，只是取队列编码器里的作为负样本，那负样本也有6万多个。  之前的一些聚类方法常常将<code>ImageNet</code>数据集聚成<code>3000</code>个簇类中心。</p></blockquote><p>作者选择聚类这个想法有两个原因。首先，聚类方法也是一种无监督的特征表示学习方式，其目标也是希望相似的物体聚在一起，不相似的物体尽量互相远离，这个思想与做法和对比学习都比较接近；第二就是论文一作之前是做聚类的，比如deep cluster，也是一篇很好的无监督学习论文。</p><h3 id="（2）算法-1"><a href="#（2）算法-1" class="headerlink" title="（2）算法"></a>（2）算法</h3><p><img src="image/image_58rJ20fxcL.png" alt=""></p><ol><li>左边是过去的对比学习方法：图片经过不同的数据增强，通过编码器得到特征，然后对特征做一个对比学习的loss。</li><li>右边是<code>SwAV</code>：认为特征和特征作对比，有点费资源，因为每个图片都是自己的类，那么剩下的都是负样本，负样本太大只能取近似，能不能不做近似？能不能借助一些先验信息，不去和大量的负样本比，而去跟一些更简洁的东西比呢？</li></ol><p>去跟聚类中心比，就是右图中的<code>prototypes C</code>（矩阵，维度是D×K，D是特征的维度，K聚类中心的个数）</p><p>前向过程：一个minibatch的图片，做两次数据增强，分别通过编码器得到两个特征，让特征和<code>prototype C</code>去生成一个目标，也就是Q1和Q2（相当于ground truth）。</p><p><img src="https://i0.hdslb.com/bfs/note/c0a019c916646c988442b5010605a6001b415064.png@832w_!web-note.webp" alt=""></p><p><strong>Swapped prediction</strong>：按道理x1和x2是同一张图片，他们是一对正样本，那么生成的特征应该很相似，Z1·C可以预测Q2，点乘的结果就是预测；ground truth就是聚类分类得到的Q1和Q2。<strong>通过换位预测的方法，SwAV可以对模型进行训练</strong>。</p><p>用聚类的好处是什么？</p><ol><li><strong>减少计算量</strong>：如果要和很多的负样本作类比，可能就需要成千上万的负样本，即使如此，也只是个近似。而现在如果只是和聚类中心对比，用几百甚至3000个聚类中心就足以表示了，因为其实也并没有那么多类，imagenet也就1000类，COCO才80类，所以3000个聚类中心就足够用了，这相当于几万个负样本来说，是小了很多的。</li><li><strong>聚类对比更加合理</strong>：这些聚类中心是有明确的语义含义的，之前只是随机抽样负样本做对比的话，可能类别不均衡甚至可能是个正样本的，所以不如使用聚类中心有效。</li></ol><h3 id="（3）实验-1"><a href="#（3）实验-1" class="headerlink" title="（3）实验"></a>（3）实验</h3><p><strong>1. 不同模型在ImageNet上的Top-1精度对比</strong></p><p>下面是将模型作为特征提取器后在ImageNet上训练不同epoch时的的top-1精度对比（backbone都是ResNet50）</p><p><img src="https://i0.hdslb.com/bfs/note/13468cddae64f959bf9381f9ae525e4d3c5717a4.png@1086w_!web-note.webp" alt=""></p><p>不仅比之前讲过的方法效果好，还要比接下来要讲的BYOL和SimSiam效果都要好。算是卷积神经网络里，用res50分刷的最高的一篇工作了。</p><p>对比做的是imagenet上的Linear classification，就是提前预训练好的模型，冻住主干网络，只训练最后的分类头（MLP层）。前六个不是对比学习的方法，结果都比较低。有了对比学习开始，MOCO就开始上60了。</p><p>SwAV是把主干网络冻住情况下做的，都已经非常逼近从头到尾都在imagenet上训练的有监督基线模型。</p><p>右图是只把res50变宽，SwAV的结果还能不停的涨。当用5倍的模型的时候，SwAV的结果已经和有监督的模型差距非常小。</p><p>SwAV的性能这么好的原因：</p><ol><li>和聚类的方法融合。</li><li><strong>multi-crop</strong>：一个trick</li></ol><h3 id="（4）Multi-crop增强"><a href="#（4）Multi-crop增强" class="headerlink" title="（4）Multi-crop增强"></a>（4）Multi-crop增强</h3><p>一个trick。之前的对比学习方法用的是两个crop，就是一个正样本对就是两个图片x1和x2，如图所示，图片先resize到$256<em>256$，然后随机crop两个$224</em>224$的图片当成x1和x2，因为两张图片都非常大，所以重叠的区域也非常多，他们代表一个正样本，总之就是两个crop。这么大的crop抓住的是整个场景的特征，如果想学习局部物体的特征该怎么办？</p><p><img src="https://i0.hdslb.com/bfs/note/265e989c98e89abd50f9e8379c1fda74b2bec542.png@484w_!web-note.webp" alt=""></p><p>所以最好<strong>能多个crop</strong>，就能关注到局部的物体了。但是增加crop（文中的view）模型的计算复杂度一下就提高了，相当于使用了更多的正样本，如何能使用更多的正样本，而又不增加更多的计算成本？</p><p><strong>方法</strong>：<strong>把crop变小，取两个较大的crop争取学到全局特征，然后为了增加正样本的数量，为了学习局部特征，再去随机选4个小一点的crop</strong>。正样本数量增多了，但是通过取舍，整体的计算代价是差不多的。</p><p><img src="image/image_QkuW_27VG-.png" alt=""></p><p>这个multi-crop技术很有用，而且不仅对SwAV有效。</p><p><img src="https://i0.hdslb.com/bfs/note/a5e273871333d06e1e3bfc01d432b6059c8523dd.png@1086w_!web-note.webp" alt=""></p><p>如图，基线模型就是2*224，用了multi-crop技术的效果。</p><ul><li>SimCLR涨了2.4个点。</li><li>聚类的方法用了这个技术，提点效果更显著。</li></ul><p>如果不用multi-crop这个技术，SwAV的效果和mocov2是差不多的，也就是说一个纯聚类的方法，或者聚类和对比学习结合的方法，其实并没有什么优势，真正提点的是multi-crop这个技术，而且这个技术非常普适，思想也很简单，就是全局和局部的特征都要关注。所以接下来的很多工作借鉴的都是multi-crop这个技术，而不是SwAV这篇工作本身。</p><h2 id="2-6-CPCv2"><a href="#2-6-CPCv2" class="headerlink" title="2.6 CPCv2"></a>2.6 CPCv2</h2><p>简单提一下。CPCv2其实也是融合了很多的技巧，它用了更大的模型、用了更大的图像块、做了更多方向上的预测任务，把batch norm 换成了 layer norm，而使用了更多的数据增强，所以这一系列操作下来，CPC v2直接就把CPC v1之前在 ImageNet 上40多的准确率一下就拔到70多。</p><h2 id="2-7-infoMin"><a href="#2-7-infoMin" class="headerlink" title="2.7 infoMin"></a>2.7 infoMin</h2><p>What Makes for Good Views for Contrastive Learning，到底选择什么样的视角才能对对比学习更好。</p><p>infomin原则：<strong>最小化互信息</strong>，以前都是最大化互信息，都是想要两个视角之间的互信息达到最大，本文是想要恰好合适的互信息，如果最后互信息比你所需要的互信息要多，那也是一种浪费，而且有可能泛化做的不好。如果互信息比需要的少，可能就达不到最优的性能。所以作者的意思就是，不能一味的最大化互信息，而是要不多不少刚刚好，按照infomin的原则，去选择合适的数据增强，然后拿到合适的对比学习视角，作者发现对于很多的方法都有提升。</p><h2 id="2-8-总结"><a href="#2-8-总结" class="headerlink" title="2.8 总结"></a>2.8 总结</h2><p>第二阶段，很多细节已经趋于统一了</p><ul><li>目标函数都是infoNCE或者其变体</li><li>模型都是用一个编码器后面加一个projection head</li><li>更强的数据增强</li><li>动量编码器</li><li>训练的更久</li><li>准确度逐渐逼近有监督的基线模型</li></ul><h1 id="3-第三阶段：不用负样本"><a href="#3-第三阶段：不用负样本" class="headerlink" title="3.第三阶段：不用负样本"></a>3.第三阶段：不用负样本</h1><p>其实在上一阶段已经有不用负样本的趋势了，比如<code>SwAV</code>就是用的聚类中心进行对比。接下来要讲的BYOL和SimSiam其实就是正样本自己在玩，已经没有负样本或者聚类中心这样明确的一个对比的东西去做对比了。</p><h2 id="3-1-BYOL"><a href="#3-1-BYOL" class="headerlink" title="3.1 BYOL"></a>3.1 BYOL</h2><ul><li>论文名称：Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning</li><li>论文连接：<a href="https://paperswithcode.com/paper/bootstrap-your-own-latent-a-new-approach-to-1" title="https://paperswithcode.com/paper/bootstrap-your-own-latent-a-new-approach-to-1">https://paperswithcode.com/paper/bootstrap-your-own-latent-a-new-approach-to-1</a></li><li>BYOL分析博客：<a href="https://imbue.com/research/2020-08-24-understanding-self-supervised-contrastive-learning/" title="Understanding self-supervised and contrastive learning with \&quot;Bootstrap Your Own Latent\&quot; (BYOL) - imbue">Understanding self-supervised and contrastive learning with “Bootstrap Your Own Latent” (BYOL) - imbue</a></li><li>回应博客论文：<a href="https://arxiv.org/abs/2010.10241" title="BYOL works even without batch statistics">BYOL works even without batch statistics</a></li></ul><h3 id="（1）简介-7"><a href="#（1）简介-7" class="headerlink" title="（1）简介"></a>（1）简介</h3><p><code>BYOL</code>就是论文标题<code>Boostrap Your Own Latent</code>的缩写。Latent、Hidden、Feature、Embedding其实都是特征的意思，就是各种花里胡哨的用法而已；Boostrap就是类似自我改造的意思。</p><p><code>BYOL</code>使用了一种新的对比学习方法（A New approach），即没有引入任何形式的负样本，而是<strong>用图片的编码特征（梯度更新）去预测自己的编码特征（动量更新）</strong>，模型就这样训练起来了。（<strong>相当于用一个视角的特征取预测另一个视角的特征，将匹配转为预测问题</strong>）。这种训练方式类似<code>SwAV</code>，但是这次连簇类中心都没了，所以听起来有点不可思议。后来还有一篇博文分析了<code>BYOL</code>，认为其实是在使用BacthNorm时引入了隐式的负样本进行对比学习。BYOL作者一听不高兴了，这样不是说明我的工作大大折扣了吗，所以立马写了一篇技术实验论文驳斥了这个说法，证明了对比学习完全不使用负样本是可行的（后面会详细介绍）。</p><p><strong>为什么不用负样本这么新奇？</strong></p><p>因为<strong>在对比学习中负样本是一个约束</strong>，在算目标函数的时候，只有正样本，那么目标只有一个：让所有相似的物体的特征也尽可能的相似，这时候就有一个很明显的捷径解，就是说如果一个模型，不论什么样的输入，都会返回相同的输出，那么出来的所有特征都是一模一样的，那这个去算对比学习的loss就都是0，意思就是模型直接躺平了根本不用学，直接用这个捷径解trivial solution，就能完美解决你的问题。只有加上负样本这个约束，不光相似的物体要有相似的特征，不相似的物体要有不相似的特征，这样模型才有动力继续学，因为如果输出的所有特征都一样，那么负样本这边loss就无穷大，所以模型必须想办法让正样本和负样本的loss都往下降，达到一个最优解。所以负样本在对比学习里是个必须的东西，能防止模型学到这个捷径解（model collapse或learning collapse）。</p><h3 id="（2）算法-2"><a href="#（2）算法-2" class="headerlink" title="（2）算法"></a>（2）算法</h3><p><img src="image/image_UA9d63gvhw.png" alt=""></p><p>前向过程：</p><ul><li>输入x经过两次不同的Aug得到  $v$，${v}’$&#x20;</li><li>编码特征<ul><li>上面的online分支 v 经过编码器$  f_{\theta }  $得到编码特征 $y_{\theta }$ ，$f_{\theta }$是梯度更新</li><li>下面的target分支  $v’$ 经过编码器 $f_{\xi }$ 得到编码特征  ${y_{\xi }}’$ ， $  f_{\xi } $ 和 $f_{\theta }$ 模型结构一样，但用的是动量更新的方式。也就是说， $ f_{\xi }  $引入了MoCo中的动量编码器，其参数和 $ f_{\theta }  $不同，但是结构一样。</li><li>如果这两个编码器都是ResNet50，则输出特征是2048维</li></ul></li><li>projection head<ul><li>使用类似<code>SimCLR</code>中一样的projection head $ g_{\xi }  $和 $ g_{\theta }  $（也是一个MLP，<code>BYOL</code>中也把这个结构叫<code>predictor</code>），将特征降到256维，得到特征$z_{\theta },{z_{\xi }}’$。</li><li>&#x20;$g_{\xi }$ 和 $g_{\theta}$分别是梯度更新和动量更新，但二者结构一样。</li></ul></li><li>对比预测<ul><li>在 SimCLR中，是在$z_{\\theta },{z_{\\xi }}’$ 之间做maximum agreement，即使不同增强后再编码和MLP映射后的特征尽可能的接近</li><li>在SwAV中，是将 $y_{\theta },{y_{\xi }}’$ 分别和K个簇类中心c计算相似度得到$q_\theta, q_\xi$ ，然后互相预测作对比学习（ $y_{\theta }$ 和相似度矩阵点乘的结果去预测$  q_\xi $ ，反之亦然）</li><li><code>BYOL</code>中，上分支使用<code>prediction head</code>（也是<code>predictor</code>结构）将  $z_{\theta }$ 映射为 $q_{\theta }(z_{\theta })$ ，然后用 $q_{\theta }(z_{\theta })$ 去预测 $ sg({z_{\xi }})’  $来进行对比学习，其中sg表示<code>stop-gradient</code>，因为下分支编码器是动量更新。</li><li>损失函数是<code>MSELoss</code>，即直接计算预测特征  $q_{\theta }(z_{\theta })$和标签  $sg({z_{\xi }})’$ 这两个向量之间的mse。</li></ul></li></ul><p><strong>推理</strong>： &#x20;</p><p>当训练完成只留下编码器$y_{\\theta }$，剩下所有的东西都被拿掉了。然后用这个编码器编码图片，输出维特征去做下游任务的推理。</p><p>对比：</p><ul><li>按过程来看，<code>BYOL</code>就是将上分支输入经过一个梯度更新的编码器和两个<code>predictor</code>得到的 $q_{\theta }(z_{\theta })$ ，去预测下分输入经过一个动量更新的编码器和一个<code>predictor</code>得到的$sg({z_{\xi }})’$ 。</li><li>所以可以看出<code>BYOL</code>使用了MoCo的动量编码器、SimCLR的<code>projection head</code>以及预测任务，但是没有负样本，目标函数也不一样。通过自己预测自己就学起来了。</li><li><code>BYOL</code>的两个分支叫online和target，其实就相当于<code>MoCo</code>中的query和key分支。</li></ul><h3 id="（3）为何不使用负样本这么重要"><a href="#（3）为何不使用负样本这么重要" class="headerlink" title="（3）为何不使用负样本这么重要"></a>（3）为何不使用负样本这么重要</h3><p><strong>在对比学习中，负样本是一个约束</strong>。如果在算目标函数的时候只有正样本，也就是让所有相似的物体的特征也尽可能的相似，此时就有一个很明显的捷径：模型输出恒等于输入，对比学习的oss永远都是0，模型直接就躺平（也叫模型坍塌<code>model collapse</code>，表示模型根本就没有在学习）。</p><p>只有加上负样本这个约束，即不相似的物体也要有不相似的特征，这样模型才会继续学习，否则负样本的loss就无穷大了。所以加入负样本能防止模型学到捷径，是必须的。</p><p><code>BYOL</code>之所以神奇就是它没有用负样本，正样本自己跟自己学最后在ImageNet上也达到了74.3的top-1准确率，也是相当高了。</p><h3 id="（4）不同对比学习模型projection-head结构对比"><a href="#（4）不同对比学习模型projection-head结构对比" class="headerlink" title="（4）不同对比学习模型projection head结构对比"></a>（4）不同对比学习模型projection head结构对比</h3><h4 id="SimCLR"><a href="#SimCLR" class="headerlink" title="SimCLR"></a><code>SimCLR</code></h4><ul><li>编码特征y经过projection head映射为z，z之间进行对比学习。</li><li>projection head结构如右图所示：<code>Linear（2048×2048）+BN+ReLU +Linear（2048×128）+BN</code>。 &#x20;</li></ul><p><img src="image/image_WcoZlATsH8.png" alt=""></p><h4 id="MoCov2（MoCo-v1没有用projection-head）-x20"><a href="#MoCov2（MoCo-v1没有用projection-head）-x20" class="headerlink" title="MoCov2（MoCo v1没有用projection head） &#x20;"></a><code>MoCov2</code>（MoCo v1没有用projection head） &#x20;</h4><p>MoCov2确实是用了projection head，就是 $g_θ$ ，但是$g_θ$ 里面是没有batch norm的，其结构是<code>Linear（2048×2048）+ReLU +Linear（2048×128）</code>。&#x20;</p><p>&#x20;</p><p><img src="image/image_X312pwJ05T.png" alt=""></p><h3 id="BYOL-x20"><a href="#BYOL-x20" class="headerlink" title="BYOL &#x20;"></a><code>BYOL</code> &#x20;</h3><p>$ g_{\xi },g_{\theta },q_{\theta }  $都是projection head，其结构为<code>Linear+BN+ReLU +Linear</code> &#x20;</p><p><img src="image/image_GaLN8uQmG7.png" alt=""></p><h3 id="（5）-BYOL被认为是使用了隐式负样本"><a href="#（5）-BYOL被认为是使用了隐式负样本" class="headerlink" title="（5） BYOL被认为是使用了隐式负样本"></a>（5） <code>BYOL</code>被认为是使用了隐式负样本</h3><p><code>BYOL</code>发布到arxiv之后，在reddit、twitter、知乎全都引起了剧烈的讨论，因为大家都觉得很不可思议；不用负样本，只是自己预测自己，模型的学习怎么能不坍塌。由此引出了一篇博文<a href="https://generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/" title="《Understanding self-supervised and contrastive learning with “Bootstrap Your Own Latent” (BYOL)》">《Understanding self-supervised and contrastive learning with “Bootstrap Your Own Latent” (BYOL)》</a>。</p><p>这篇博文的作者在复现<code>BYOL</code>时遗漏了一个小细节，即借用了 <code>MoCov2</code>的<code>projection head</code>导致<code>projection head</code>中没有加<code>batch norm</code>，最终模型坍塌。作者就觉得实在是太奇怪了，所以赶紧又做了一些额外的实验，如下表所示 ：</p><div class="table-container"><table><thead><tr><th>Name</th><th>Projection MLP Norm</th><th>Prediction MLP Norm</th><th>Loss Function</th><th>Contrastive</th><th>Performance 5</th></tr></thead><tbody><tr><td>Contrastive Loss</td><td>None</td><td>None</td><td>Cross Entropy</td><td>Explicit</td><td>44.1</td></tr><tr><td>BYOL</td><td>Batch Norm</td><td>Batch Norm</td><td>L2</td><td>Implicit</td><td>57.7</td></tr><tr><td>Projection BN Only</td><td>Batch Norm</td><td>None</td><td>L2</td><td>Implicit</td><td>55.3</td></tr><tr><td>Prediction BN Only</td><td>None</td><td>Batch Norm</td><td>L2</td><td>Implicit</td><td>48</td></tr><tr><td>No Normalization</td><td>None</td><td>None</td><td>L2</td><td>None</td><td>28.3</td></tr><tr><td>Layer Norm</td><td>Layer Norm</td><td>Layer Norm</td><td>L2</td><td>None</td><td>29.4</td></tr><tr><td>Random</td><td>—</td><td>—</td><td>—</td><td>None</td><td>28.8</td></tr></tbody></table></div><ul><li><code>Projection MLP Norm</code>：第二第三列这里指的是两层<code>Projector</code>有没有用归一化</li><li><code>Loss Function</code>：普通对比学习loss是交叉熵损失函数，而<code>BYOL</code>用的是<code>L2 loss</code>，即mse 损失函数。</li><li><code>performance</code>：测试模型性能是在一个STL-10的数据集上做的，不是 ImageNet，但衡量标准还是准确度。</li></ul><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><ul><li><code>random</code>：使用一个随机初始化的残差网络，没有经过任何训练，直接去抽特征。然后在这个特征上训练一个全连接层，最后的结果是28.8。所以这个结果是一个完全随机的结果。</li><li>正常的<code>BYOL</code>：两层<code>Projector</code>都使用BN，效果最好</li><li><code>BYOL</code>变体：只使用一层BN，模型起码 也有学到东西。如果是都用LN或者干脆都不用BN，模型坍塌，什么都没学到。</li></ul><h4 id="最终分析"><a href="#最终分析" class="headerlink" title="最终分析"></a>最终分析</h4><p>作者认为在<code>Projector</code>层使用BN之后，是计算了整个batch的均值和方差，这意味着是有信息泄露的（MoCo使用了 Shuffling BN ，就是为了防止这种信息泄露）。模型不光是正样本自己和自己学，还和batch norm产生的平均图片（mode，中值）对比着学，这种平均图片就类似 <code>SwAV</code>的聚类中心了。</p><p>所以说，这篇博客的作者认为batch norm是<code>BYOL</code>能够成功的关键，其实是做了一种隐式的对比学习，这个观点很快就被大家所接受了，因为听起来确实很合理，而且后续试验也都验证了这一点。batch norm确实至关重要，拿掉batch norm以后模型就是不好训练，对超参数的设置非常的敏感，稍有不慎它就啥也不学了。</p><h3 id="（6）-BN只是加强模型的稳定性，BYOL不使用负样本的思想是没问题的"><a href="#（6）-BN只是加强模型的稳定性，BYOL不使用负样本的思想是没问题的" class="headerlink" title="（6） BN只是加强模型的稳定性，BYOL不使用负样本的思想是没问题的"></a>（6） BN只是加强模型的稳定性，<code>BYOL</code>不使用负样本的思想是没问题的</h3><p>BYOL的作者看到博客就急了，如果真是这样的话，BYOL就还是没有逃脱出对比学习的范畴，它还是找了一个东西去做对比，其创新性就大大降低了。所以作者赶紧做实验，看看能不能找到BYOL 模型不坍塌的另外一种解释。最终又写了一篇论文进行回应。</p><p>这篇论文叫<a href="https://arxiv.org/abs/2010.10241" title="BYOL works even without batch statistics">BYOL works even without batch statistics</a>，即在没有batch norm的时候<code>BYOL</code>照样能工作，详细的消融实验结果如下表所示 ：</p><p><img src="image/image_krBi6HEodD.png" alt=""></p><p>作者是在encoder（比如ResNet50）和两层<code>Projector</code>里分布使用BN/LN和什么都不用去做对比实验，最后发现：</p><ul><li><strong>BN非常关键</strong>：只要是projector中没有BN的地方，<code>SimCLR</code>性稍微下降；但是<code>BYOL</code>全都模型坍塌了</li><li><strong>有BN也会坍塌</strong>：作者找到了特例（红色框），即使当projector有BN的时候，<code>BYOL</code> 还是训练失败了 。如果BN真的很关键，它真的提供了隐式负样本的对比学习的话，训练就不应该失败</li><li>完全没有BN，<code>SimCLR</code>也坍塌（最后三列的结果。要注意<code>SimCLR</code>只有一层projector）。这表明完全不用归一化，<code>SimCLR</code>这种使用负样本进行对比学习的方式也无法训练。</li></ul><p>最终结论：<strong>BN跟它原来的设计初衷一样，主要作用就是提高模型训练时的稳定性，从而不会导致模型坍塌</strong> 。作者进一步延伸，如果一开始就能让模型初始化的比较好，后面的训练即使离开了BN也没有问题。  作者为此又设计了一个实验，借鉴<code>BEiT</code>中的<code>group norm+weight standardization</code> （前者也是一种归一化方式，后者是一种模型初始化的方式，但都没有进行批量统计操作），BYOL的top-准确率可以达到74.1%，和原来精度可以认为是一样了（74.3%）。</p><h2 id="3-2-SimSiam"><a href="#3-2-SimSiam" class="headerlink" title="3.2 SimSiam"></a>3.2 SimSiam</h2><ul><li>论文名称：Exploring Simple Siamese Representation Learning</li><li>论文连接：<a href="https://paperswithcode.com/paper/exploring-simple-siamese-representation" title="Exploring Simple Siamese Representation Learning">Exploring Simple Siamese Representation Learning</a></li></ul><h3 id="（1）简介-8"><a href="#（1）简介-8" class="headerlink" title="（1）简介"></a>（1）简介</h3><p><code>SimSiam</code>即simple Siamese network（简单孪生网络）。在BYOL发布时，就已经有很多对比学习的分析性工作了。大家发现，对比学习的成功好像是被很多trick一点点堆起来的性能，比如projection head、更多的数据增强、使用用动量编码器、更大的 batch size等等，好像都缺一不可。这样因素太多就不方便分析，也不知道每个点到底带来了哪些贡献，所以凯明团队又再次出手，把整个过程化繁为简了一下，最后提出了SimSiam。SimSiam<strong>结构非常简单</strong>，<strong>不需要用负样本（结构类似 **</strong><code>BYOL</code><strong>**）、大的batch size，也不需要动量编码器</strong>。然而即使在这种情况下，模型效果也很好。</p><h3 id="（2）方法-4"><a href="#（2）方法-4" class="headerlink" title="（2）方法"></a>（2）方法</h3><p><strong>1. 模型结构：</strong> 具体的模型总览图如下图所示，整体结构非常类似<code>BYOL</code>： &#x20;</p><p><img src="image/image_UAmNwisnOn.png" alt=""></p><p>为什么叫孪生网络：两个编码器网络结构相同且共享参数，整体架构和BYOL非常像，不同只在于SimSiam没有用动量编码器。</p><p><strong>伪代码</strong>：</p><p><img src="image/image_4dK5YxiLvw.png" alt=""></p><ul><li>向过程：得到两个视角之后先过编码器得到特征，然后通过predictor去得到预测，两边都预测，所以是一个对称性的loss：既可以做p1预测z2，也可以做p2预测z1的任务，但因为加了两次，所以要除以2</li><li>然后就是梯度回传更新网络</li><li>D函数表示loss的计算：MSEloss</li></ul><p>结论：<strong>之所以SimSiam可以训练，不会模型坍塌，主要就是因为有stop gradient这个操作</strong></p><p>做了一个假设：因为有了stop gradient这个操作，所以可以把SimSiam想象成一个<code>expectation-maximization</code>（EM）算法。因为有了stop gradient这个操作，这一个训练过程或者说这一套模型参数其实就被人为的劈成了两份，相当于在解决两个子问题，模型的更新也是在交替进行的。</p><p>可以理解成一个k-means的聚类问题，Kmeans就是分两步走的，每次先要把所有的点分配给一些聚类中心，分配好了以后再去更新聚类中心，再重复做这个操作。从这个角度说，SimSiam和SwAV就有关系了。</p><h3 id="（3）模型总结"><a href="#（3）模型总结" class="headerlink" title="（3）模型总结"></a>（3）模型总结</h3><p><img src="image/image_dLLzjKirsx.png" alt=""></p><ol><li>SimCLR两边都有梯度回传，做的是一个对比任务（SimCLR依赖于负采样以避免“坍塌”）；</li><li>SwAV也是做对比任务，但并没有跟负样本比，而是和聚类中心比，聚类中心是通过SK算法得到的</li><li>BYOL不是对比任务，加了predictor变成一个预测任务，用左边去预测右边，同时他们还使用了动量编码器</li><li>SimSiam的整个左边和BYOL一模一样，不同的是右边没有用动量编码器而是共享参数了</li></ol><h3 id="（4）stop-gradient避免了模型坍塌"><a href="#（4）stop-gradient避免了模型坍塌" class="headerlink" title="（4）stop-gradient避免了模型坍塌"></a>（4）<code>stop-gradient</code>避免了模型坍塌</h3><p>作者做了一系列实验分析，发现SimSiam能够成功训练，而没有模型坍塌，主要是因为有<code>stop gradient</code>这个操作。</p><h4 id="stop-gradient操作的影响-x20"><a href="#stop-gradient操作的影响-x20" class="headerlink" title="stop gradient操作的影响 &#x20;"></a><code>stop gradient</code>操作的影响 &#x20;</h4><p><img src="image/image_o-B5CbZM5f.png" alt=""></p><ul><li>蓝色线表示使用<code>stop gradient</code>操作，红色线表示witdout stop-gradient</li><li>左图对比训练损失：不使用<code>stop gradient</code>时，优化器快速找到一个退化解，并且达到最小损失值− 1</li><li>中间图：验证退化解是由模型坍塌导致的。作者研究了$l_{2}$ 正则化输出$  z / | z |_{2} $的标准差std。如果输出坍塌为一个常数向量，那么它们在所有例子上的std对于每一个通道应当是0，中间图的红色曲线验证了这一点。如果输出z 具有零均值各向同性高斯分布，那么的标准差为$\frac{1}{\\sqrt{d}}$ ，中间图的蓝色曲线显示在带有stop-gradient的情况下，它的标准差接近于$  \frac{1}{\\sqrt{d}} $。</li><li>右图训练集acc：接KNN分类器时的验证集精度，没有<code>stop gradient</code>时精度为0</li><li>右表：ImageNet linear evaluation，w/ stop-grad操作试验超过了五次</li></ul><p>结论：<strong>上述实验表明，“坍塌”确实存在，但不足以证明是</strong>**<code>stop gradient</code>**<strong>避免了坍塌</strong></p><h4 id="batch-sizes和BN的影响-x20"><a href="#batch-sizes和BN的影响-x20" class="headerlink" title="batch sizes和BN的影响 &#x20;"></a>batch sizes和BN的影响 &#x20;</h4><p><img src="image/image_Cp_tGtUYqj.png" alt=""></p><ul><li>batch sizes=256时效果就够了</li><li>去掉所有BN后<code>SimSiam</code>依旧有学习，尽管精度只有34.6%。对backbone隐含层添加BN后精度则提升到了67.4%；</li><li><code>encoder f</code>最后的线性层输出（从2048降维）添加BN，精度可以进一步提升到68.1%；</li><li><code>Projector h</code>添加BN ，训练反而不稳定，loss波动太大</li></ul><p>结论：<strong>BN有助于训练优化，但主要是提高模型训练的稳定性，而非避免模型坍塌（见第一行结果）。</strong></p><h4 id="Similarity-Function和Symmetrization（对称性损失）-x20"><a href="#Similarity-Function和Symmetrization（对称性损失）-x20" class="headerlink" title="Similarity Function和Symmetrization（对称性损失） &#x20;"></a>Similarity Function和Symmetrization（对称性损失） &#x20;</h4><p>作者还试验了Similarity Function和Symmetrization，发现不使用余弦相似性，使用非对称损失模型都训练的还可以，只是性能会下降一点。所以这两点也和模型坍塌无关。</p><p>通过上面的一些列消融实验对比分析可知，优化器、BN、相似性函数、对称损失可能会影响精度，但与“坍塌”避免无关；对于避免“坍塌”起关键作用的是<code>stop-gradient</code>操作。</p><h3 id="（5）交替优化假设"><a href="#（5）交替优化假设" class="headerlink" title="（5）交替优化假设"></a>（5）交替优化假设</h3><p><code>SimSiam</code>到底在隐式的优化什么？作者认为可以将SimSiam当做是一个<a href="https://blog.csdn.net/v_JULY_v/article/details/81708386" title="EM算法">EM算法</a>。因为stop gradient操作将一套模型参数被人为劈成了两份，即需要解决两个子问题，模型的更新其实也是在交替进行的。</p><p>作者假设SimSiam是一种类似交替优化的方案后（其SGD更新间隔为1），基于该假设，此方案在多步SGD更新下应该同样有效。为此，作者设计了一组实验验证上述假设，结果见下表：</p><p><img src="image/image_zTGsT9Bt-W.png" alt=""></p><ul><li>这里1-step就代表<code>SimSiam</code></li><li>更多步的SGD更新甚至可以取得比<code>SimSiam</code>更优的结果</li></ul><p>结论：<strong>交替优化是一种可行的方案，而</strong>**<code>SimSiam</code>**<strong>是其特例。</strong></p><p>作者接下来又做了一些推导，到最后可以把<code>SimSiam</code>理解成是一个k-means聚类问题。在k-means中，也是分两步走的。每次先要把所有的点分配给一些聚类中心；分配完后再更新这些聚类中心。后面就是不断迭代这两个过程。</p><h3 id="（6）模型效果对比"><a href="#（6）模型效果对比" class="headerlink" title="（6）模型效果对比"></a>（6）模型效果对比</h3><h4 id="SimSiam与其他SOTA无监督学习方法效果对比"><a href="#SimSiam与其他SOTA无监督学习方法效果对比" class="headerlink" title="SimSiam与其他SOTA无监督学习方法效果对比"></a>SimSiam与其他SOTA无监督学习方法效果对比</h4><p><img src="image/image_nFaeLF3qJx.png" alt=""></p><p>在imagenet上的Linear classification</p><ul><li>只用MOCOv2和SimSiam能用256的bs，其他的工作都要用更大的bs</li><li>SimCLR和MOCOv2都要用负样本，BYOL完全没用，SwAV用的聚类中心</li><li>100epoch的时候SimSiam学的最好，说明学的很快，但是随着训练推进涨幅就小了，动量编码器很好用，很好提点。但是本文主要是把这些trick都拿掉，证明没有这些trick也能训练。</li></ul><h4 id="下图是迁移学习性能对比"><a href="#下图是迁移学习性能对比" class="headerlink" title="下图是迁移学习性能对比"></a>下图是迁移学习性能对比</h4><p><img src="image/image_UqZxvOCfgm.png" alt=""></p><p>下游任务上，前三个是物体检测，最后一个实例分割（CV人必做的两个下游任务）</p><ul><li>针对下游任务的迁移来说，MOCOv2和SimSiam是表现最好的。如果想尝试一些对比学习的工作，会用MOCOv2作为基线模型，因为训练快训练稳而且下游任务迁移的好。</li></ul><h1 id="4-第四阶段：Transformer"><a href="#4-第四阶段：Transformer" class="headerlink" title="4.第四阶段：Transformer"></a>4.第四阶段：Transformer</h1><h3 id="4-1-MoCo-v3"><a href="#4-1-MoCo-v3" class="headerlink" title="4.1 MoCo v3"></a>4.1 MoCo v3</h3><ul><li>论文名称：An Empirical Study of Training Self-Supervised Vision Transformers</li><li>论文连接：<a href="https://paperswithcode.com/paper/an-empirical-study-of-training-self" title="An Empirical Study of Training Self-Supervised Vision Transformers">An Empirical Study of Training Self-Supervised Vision Transformers</a></li></ul><h3 id="（1）简介-9"><a href="#（1）简介-9" class="headerlink" title="（1）简介"></a>（1）简介</h3><p>无监督的预训练（BERT/GPT等）已经彻底改变了NLP，自从Vision Transformer成功之后，将ViT引入CV领域的自监督训练已经是大势所趋了。但是使用ViT作为backbone会导致训练很不稳定，这种不稳定性是造成模型准确率降低的一个主要问题。本文作者发现**只需要做一点小小的改动（冻结ViT的<code>patch projection</code>**<strong>层），就能让这个训练变得更稳定、效果也更好</strong>。所以作者不得不写一篇论文来把这个发现告诉大家，也就是标题说的An Empirical Study （一个实验性的study ）。这篇论文是ICCV 21的一篇口头报告论文，但它的的影响力依旧很大。</p><h3 id="（2）伪代码"><a href="#（2）伪代码" class="headerlink" title="（2）伪代码"></a>（2）伪代码</h3><p>MoCo v3的架构，其实就相当于是MoCo v2和SimSiam 的一个合体。因为没有模型总览图，所以直接看伪代码：</p><p><img src="https://i0.hdslb.com/bfs/note/bb6653285db4b8baeb965d7481acabe7f85c5d6a.png@1224w_!web-note.webp" alt=""></p><ul><li>整体的框架来说，它还是有两个网络：query编码器和key编码器（动量编码器），目标函数是对比学习loss，所以说从这个角度讲，它是个<code>MoCov2</code></li><li>query编码器除了backbone之外，还有projection head和predictor，而是还算了对称性loss，即<code>loss = ctr(q1, k2) + ctr(q2, k1)</code>。所以从这个角度讲，它又是<code>SimSiam</code>。</li><li>所以说，从整体结构上来看，<code>MoCov3</code>就是<code>MoCov2</code>和<code>SimSiam</code>一个延伸工作。</li></ul><h3 id="（3）冻结patch-projection可以解决训练不稳定的问题-x20"><a href="#（3）冻结patch-projection可以解决训练不稳定的问题-x20" class="headerlink" title="（3）冻结patch projection可以解决训练不稳定的问题 &#x20;"></a>（3）冻结patch projection可以解决训练不稳定的问题 &#x20;</h3><p>下图是作者将backbone从一个残差网络换成了ViT之后，模型自监督训练曲线： &#x20;</p><p><img src="image/image_K-9YTx8UGR.png" alt=""></p><p>因为vision transformer的出现，作者就很想把卷积神经网络换成vit，看看自监督学习+vit就能取得NLP那边的成功呢？</p><p>实验：把骨干网络从Resnet换成vit，如图是vit自监督训练的训练曲线，作者发现：</p><ul><li>当batchsize比较小的时候，曲线比较平滑，效果也还行</li><li>当batchsize变大之后，准确度会突然掉下来又恢复，但是就不如原来的高了，最后的准确度也会差很多。按道理说大batchsize应该会有更好的结果，但是这里大batchsize的结果却只有69.7</li></ul><p>如果能解决这个问题，有可能就能使用更大的bs去训练一个更大的vit，从而得到更好的结果。</p><p>针对这个普遍出现的问题，提出一个小trick，非常普适</p><p>作者后来观察了一下模型训练时每一层梯度回传的情况。作者发现，每次准确度大幅下降时，模型第一层梯度也会有一个波峰。于是作者尝试将这一层的权重全部冻住，结果发现问题就解决了。而且很神奇的是这个trick不光是对<code>MoCov3</code>有用，它对<code>BYOL</code>和 <code>SimCLR</code>也有用。</p><blockquote><p>第一层就是<code>ViT</code>的<code>patch projection</code>层，会将图片分割成一个个patch，然后经过线性层映射为Pacth embedding。</p></blockquote><p>因为transformer简单扩展性又好，不改它就只能改开始的tokenization阶段，和结尾的目标函数</p><h2 id="4-2-DINO"><a href="#4-2-DINO" class="headerlink" title="4.2 DINO"></a>4.2 DINO</h2><ul><li>论文名称：Emerging Properties in Self-Supervised Vision Transformers</li><li>论文连接：<a href="https://paperswithcode.com/paper/emerging-properties-in-self-supervised-vision" title="Emerging Properties in Self-Supervised Vision Transformers">Emerging Properties in Self-Supervised Vision Transformers</a></li></ul><h3 id="（1）简介-10"><a href="#（1）简介-10" class="headerlink" title="（1）简介"></a>（1）简介</h3><p><code>DINO</code>这个名字，来自于它的题目self distillation with no labels，也就是无标签的自蒸馏方法（学生网络预测教师网络的输出）。本文和<code>MoCov3</code>一样，也是一种自监督训练Vision Transformer的方式，但作者使用另一种操作——centering，使ViT可以稳定训练。另外本文发现自监督训练为 Vision Transformer features提供了一些新的特性。</p><h4 id="研究动机"><a href="#研究动机" class="headerlink" title="研究动机"></a><strong>研究动机</strong></h4><p>CV领域，Vision Transfomer（ViT）虽然可以取得和convnets（卷积网络）相比拟的结果，但是还没有展现出足够的优势。比如，相比于convnets，ViT需要更多的计算资源和数据，但是他们的features并没有展现出独特的特性。</p><p>transformer在NLP中的成功的一个主要方面来自自监督预训练的应用（BERT，GPT)，因为自监督训练出来的特征会包含更丰富的语义和信息。另一方面，卷积网络的自监督学习在CV领域也表现出很大的潜力。受此启发，本文将ViT和自监督学习结合，并研究<strong>自监督预训练对ViT feature的影响</strong>。</p><blockquote><p>自监督学习通过利用句子中的词创建<code>pretext tasks</code> ，相比于有监督学习中每个句子对应一个label， <code>pretext task</code>提供了更加丰富的学习信号。类似的，图像层面的有监督学习将丰富的图片信息减少到单一的分类概念。</p></blockquote><h4 id="发现"><a href="#发现" class="headerlink" title="发现"></a><strong>发现</strong></h4><p>通过研究，本文发现自监督ViT features具有一些独有的特性</p><p>自监督ViT features 中包含清晰的图像语义分割信息，而这在有监督ViT和convnets中都没有类似的表现。 &#x20;</p><p><img src="image/image_wEIVRGasTn.png" alt=""></p><blockquote><p>一个完全不用任何标签信息训练出来的<code>Vision Transformer</code> ，将它的自注意力图进行可视化，会发现能非常准确的抓住每个物体的轮廓，效果甚至可以媲美对这个物体做分割。</p></blockquote><p>只使用一个比较小的ViT backbone（<code>ViT-S/8</code>），自监督训练出来的ViT features 就能在KNN分类器中表现的很好，ImageNet数据集的 top-1精度达到78.3%，超过之前的自监督方法。（也就是ViT features直接去做最近邻分类，连线性分类头或微调都不需要）。</p><p>另外在消融实验中证明，动量编码器、multi-crop 数据增强和更小的 ViT patches（计算量更高）都有重要的作用。</p><h3 id="（2）方法-5"><a href="#（2）方法-5" class="headerlink" title="（2）方法"></a>（2）方法</h3><p>模型结构图如下： &#x20;</p><p><img src="image/image_HVgObjmNdW.png" alt=""></p><ul><li>DINO具体做法：延续了BYOL</li><li>self-distillation：和BYOL一样自己和自己学</li><li>用student去预测teacher</li><li>centering：减掉batch样本的均值。为了避免模型坍塌，把整个batch里的样本算一个均值，减掉这个均值。</li></ul><p>伪代码：</p><p><img src="image/image_PJMs3GlA09.png" alt=""></p><p>和MOCOv3非常像，前向过程一模一样，目标函数多了一个centering操作防止模型坍塌。</p><p>前向过程可以看出，DINO也是自己预测自己（student要预测teacher，teacher的输出当成是ground truth ），所以叫自蒸馏。DINO其实就是延续的BYOL，只不过是换了个名字。</p><div class="table-container"><table><thead><tr><th>模型</th><th>左分支</th><th>右分支</th></tr></thead><tbody><tr><td>MoCo</td><td>query 编码器</td><td>key编码器</td></tr><tr><td>BYOL</td><td>online network</td><td>target network</td></tr><tr><td>BYOL</td><td>student network</td><td>teacher network</td></tr></tbody></table></div><h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h1><p><img src="image/image_zBm9VJ5xZo.png" alt=""></p><h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3><ul><li><code>InstDisc</code>提出了个体判别任务，提出用一个memory bank的外部数据结构去存储负样本，从而得到一个又大又一致的字典去做对比学习</li><li><code>InvaSpread</code>不用外部结构的另一条路，端到端学习。只用一个编码器，从而可以端到端学习，但因为受限于bs太小，所以性能不够好。</li><li><code>CPCv1</code>提出了infoNCEloss，CPCv1是一个预测型的代理任务，不仅可以做图像，还可以做音频、视频、文字和强化学习，是一个非常全能的结构。</li><li><code>CMC</code>把两个视角的任务扩展到了多个视角，给接下来的多视角或者多模态的对比学习打下了铺垫。</li><li><code>deep cluster</code>是基于聚类学习的，当时还没有对比学习</li></ul><h3 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h3><ul><li><code>MOCOv1</code>是InstDisc的延伸工作，把memory bank变成一个队列，把动量更新特征变成了动量更新编码器，从而能预训练一个很好的模型。moco也是第一个在很多视觉的下游任务上，让一个无监督预训练的模型比有监督预训练模型表现好的方法。属于使用外部数据结构的。</li><li><code>SimCLRv1</code>是端到端的延伸性工作，和InvaSpread很像，但是用了很多的技术：加大batchsize、用了更多数据增强、加了一个Projection head、训练更长时间。所有的技术堆起来，让SimCLR在imagenet上取得了非常好的结果。</li><li><code>CPCv2</code>，也把这些技术用了一遍，直接比CPCv1在imagenet上的结果高了三十几个点。</li><li><code>CMC</code>把这些都分析了一下，提出了一个infomin的原则：两个视角之间的互信息要不多不少才是最好的</li><li><code>MOCOv2</code>发现这些即插即用的技术效果很好，就拿来用。</li><li><code>SimCLRv2</code>主要做半监督学习。</li><li><code>SwAV</code>把聚类学习和对比学习结合起来的一个工作，取得了不错的效果，这个不错的结果主要来自于它提出的multicrop的技术，如果没有这个技术就和mocov2或者SimCLR结果差不多。</li></ul><h3 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h3><ul><li><code>BYOL</code>提出处理负样本太麻烦，不要负样本了，不用负样本做对比了。把对比任务变成预测任务，自己跟自己学就行了。目标函数也很简单就是MSEloss就训练出来了</li><li><code>BN BLOG</code>：说BYOL能工作是因为用BN，BN提供了隐式负样本，所以BYOL才能正常训练而不会模型坍塌。</li><li><code>BYOLv2</code>：说BN只是帮助了模型训练，如果用另一种方式能提供更好的模型初始化，BYOL不需要BN提供的batch的统计量也可以工作。</li><li><code>SimSiam</code>总结了之前的工作，因为之前的工作一直在堆技术，堆多了不好分析，领域就不好推进了。所以SimSiam化繁为简提出来一个很简单的孪生网络的学习方法，既不需要大的bs，也不需要动量编码器，也不需要负样本，照样能取得不错的结果。SImSiam提出的假设就是，stop gradient这个操作至关重要，因为有这个操作的存在，SImSiam可以看作是一种EM算法，通过逐步更新的方式避免模型坍塌。</li><li><code>barlos twins</code>更换了一个目标函数，把之前大家做的对比和预测变成了两个矩阵之间比相似性。但因为是21年3月提出的，很快就淹没在vit的洪流之中。</li></ul><h3 id="第四阶段"><a href="#第四阶段" class="headerlink" title="第四阶段"></a>第四阶段</h3><p>这两个工作都是把骨干网络换成了ViT。但是换成ViT之后训练不稳定或者不好训练。他们提出了各自的解决方法。两种方法都能有效的提高模型训练的稳健性，防止模型坍塌，让ViT用自监督的方式也能训练的很好。</p><ul><li><code>MOCOv3</code>：把patch Projection layer冻住</li><li><code>DINO</code>：把teacher网络的输出先做一下归一化（centering）</li></ul><div class="table-container"><table><thead><tr><th>模型</th><th>创新点</th><th>优势</th><th>局限性</th></tr></thead><tbody><tr><td>阶段一</td><td>百花齐放</td><td></td><td></td></tr><tr><td>Inst Disc</td><td>提出了个体判别的任务，对比学习loss，使用一个 memory bank的外部数据结构去存储负样本来做对比学习</td><td></td><td>特征一致性差</td></tr><tr><td>Inva Spread</td><td>只使用一个编码器而不需要额外的数据结构去存储负样本</td><td>可以进行端到端的对比学习</td><td>字典太小，对比学习效果不好</td></tr><tr><td>CPC v1</td><td>提出了infoNCE Loss，以及预测型的代理任务，其输入可以是图像、音频、视频、文字或加强学习</td><td>是一个非常全能的结构</td><td></td></tr><tr><td>CMC</td><td>把两个视角的任务扩展到了多个视角，为以后的多视角多模态对比学习打下了基础 。</td><td></td><td></td></tr><tr><td>阶段二</td><td></td><td></td><td></td></tr><tr><td>MoCov1</td><td>Inst Disc的延伸工作，使用队列结构代替 memory bank来存储负样本，使用动量更新编码器代替动量更新特征；把之前对比学习方法都归纳成字典查询问题；第一个让无监督预训练媲美有监督预训练的方法</td><td>字典大且特征一致性好，训练便宜</td><td></td></tr><tr><td>SimCLR v1</td><td>Inva Spread延伸工作。batch-size加大到8192，引入<code>projection head</code> ，使用更优的数据增强（随机裁剪和随机色彩变换）</td><td>端到端训练</td><td></td></tr><tr><td>CPC v2</td><td>引入SimCLR v1的几个技术，ImageNet精度直接从40多提到70多</td><td></td><td></td></tr><tr><td>MoCov2</td><td>相比 MoCov1，引入了<code>projection head</code>；使用更多数据增强、cosi调度器和更长的训练epoch</td><td></td><td></td></tr><tr><td>SimCLR v2</td><td>受noisy student影响，使用伪标签进行半监督训练。相比SimCLRv1使用了更大的backbone，动量编码器和两层的 <code>projection head</code></td><td></td><td></td></tr><tr><td>SwAV</td><td>结合聚类和对比学习，使得对比学习不再需要负样本（跟聚类中心对比）；使用<code>multi crop</code>技术</td><td></td><td></td></tr><tr><td>阶段三</td><td>不用负样本</td><td></td><td></td></tr><tr><td>BYOL</td><td>处理负样本实在是太过麻烦，所以完全舍弃负样本，自己预测自己（mse loss），也可以训练</td><td></td><td></td></tr><tr><td>SimSiam</td><td>化繁为简，使用孪生网络，不需要动量编码器、负样本、大的batch-size就可以训练。不过一个分支必须是stop gradient，这样交替 优化，类似K-means</td><td></td><td></td></tr><tr><td>阶段四</td><td>引入Vision Transformer</td><td></td><td></td></tr><tr><td>MoCov3</td><td>冻住<code>ViT</code>结结构中的patch projection layer就可以稳定训练</td><td></td><td></td></tr><tr><td>DINO</td><td>teacher网络的输出先做centering归一化也可以稳定训练</td><td></td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PaperReading </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer综述</title>
      <link href="/llms/transformer/0.Transformer%E7%BB%BC%E8%BF%B0/"/>
      <url>/llms/transformer/0.Transformer%E7%BB%BC%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer综述"><a href="#Transformer综述" class="headerlink" title="Transformer综述"></a>Transformer综述</h1><p><img src="image/image_L0kh34cyT-.png" alt=""></p><ul><li><strong>论文标题：</strong> A Survey of Transformers</li><li><strong>论文链接：</strong> <a href="https://arxiv.org/abs/2106.04554" title="https://arxiv.org/abs/2106.04554">https://arxiv.org/abs/2106.04554</a></li></ul><p>Transformer 在自然语言处理、计算机视觉、音频处理等许多人工智能领域都取得了巨大的成功，也吸引了学术界和行业研究人员的大量兴趣。到目前为止，已经有各种各样的 Transformer 变体（又名 X-former）被提出，但是，关于这些 Transformer 变体的系统而全面的文献综述仍然缺失。<strong>这篇综述对各种 X-former 进行了全面介绍</strong>**。**&#x20;</p><p>这篇综述首先简要介绍了原版 Transformer，然后提出了一种新的 X-former 分类法。接着从架构修改、预训练、应用三个角度介绍各种 X-former。最后，概述了未来研究的一些潜在方向。</p><h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.<strong>引言</strong></h1><p>Transformer 最初是作为机器翻译的 Seq2Seq 模型提出的。后来的工作表明，基于 Transformer 的预训练模型 (PTM) 可以在各种任务上实现 SOTA。因此，Transformer，特别是 PTM，已成为 NLP 中的首选架构。除了语言相关的应用，Transformer 还被 CV，音频处理甚至其他学科采用。在过去几年中提出了各种 Transformer 变体（又名** X-former**），这些 X-former 也从不同的角度改进了原版 Transformer。</p><ol><li><strong>模型效率</strong>。应用 Transformer 的一个关键挑战是其处理长序列的效率较低，这主要是由于 self-attention 的计算和内存复杂性。改进方法包括<strong>轻量化注意力模块（例如稀疏注意力）</strong>和<strong>分而治之的方法（例如循环和分层机制）</strong>。</li><li><strong>模型泛化</strong>。由于 Transformer 是一种灵活的架构，并且对输入数据的结构偏差几乎没有假设，因此很难在小规模数据上进行训练。改进方法包括<strong>引入结构偏差</strong>或<strong>正则化</strong>，对大规模未标记数据进行预训练等。</li><li><strong>模型适配</strong>。这一系列工作旨在使 Transformer 适应特定的下游任务和应用程序。</li></ol><p>在这篇综述中，旨在全面回顾 Transformer 及其变体。虽然我们可以根据上述观点来分类 X-former，但许多现有的 X-former 可能会解决一个或几个问题。例如，稀疏注意力不仅降低了计算复杂度，而且在输入数据上引入了结构先验以缓解小数据集上的过拟合问题。因此，对现有的各种 X-former 进行分类，并主要根据它们改进原版 Transformer 的方式提出新的分类法：架构修改、预训练和应用。</p><h2 id="1-1-原版-Transformer"><a href="#1-1-原版-Transformer" class="headerlink" title="1.1 原版 Transformer"></a>1.1 <strong>原版 Transformer</strong></h2><p><img src="image/image_aosDIAhxkY.png" alt=""></p><h2 id="1-2-Transformer-分类"><a href="#1-2-Transformer-分类" class="headerlink" title="1.2 Transformer 分类"></a>1.2 <strong>Transformer 分类</strong></h2><p>迄今为止，已经从三个角度提出了基于原版 Transformer 的各种模型：<strong>架构修改类型</strong>、<strong>预训练方法</strong>和<strong>应用程序</strong>。如图：</p><p><img src="image/image_aRx97ST0JC.png" alt=""></p><p>详细的 Transformer 分类如下图：</p><p><img src="image/image_sL3wJMtAcn.png" alt=""></p><h1 id="2-模型层面"><a href="#2-模型层面" class="headerlink" title="2.模型层面"></a>2.<strong>模型层面</strong></h1><h2 id="2-1-注意力机制"><a href="#2-1-注意力机制" class="headerlink" title="2.1 注意力机制"></a><strong>2.1 注意力机制</strong></h2><p>Self-attention 在 Transformer 中扮演着重要的角色，但在实际应用中存在两个挑战。</p><ol><li><strong>复杂性</strong>。self-attention 的复杂度是 $O\left(T^{2} \cdot D\right)$ 。因此，在处理长序列时 Attention 模块会成为瓶颈。</li><li><strong>结构先验</strong>。Self-attention 不假设对输入有任何结构性偏见。甚至顺序信息也需要从训练数据中学习。因此，无预训练的 Transformer 通常很容易在小型或中等规模的数据上过拟合。</li></ol><p>Attention 机制的改进可以分为几个方向：</p><ol><li><strong>稀疏注意力</strong>。这一系列工作将稀疏偏差引入 Attention 机制，从而降低了复杂性。</li><li><strong>线性化注意力</strong>。这一系列工作将注意力矩阵与核特征图分解，然后以相反的顺序计算注意力以实现线性复杂度。</li><li><strong>原型和内存压缩</strong>。这类方法减少了查询或键值记忆对的数量，以减少注意力矩阵的大小。</li><li><strong>低秩的自注意力</strong>。这一系列工作捕获了 Self-attention 的低秩属性。</li><li><strong>先验注意力</strong>。该研究领域探索用先验的注意力分布来补充或替代标准注意力。</li><li><strong>改进的多头机制</strong>。这一系列工作探索了多个不同的多头（Multi-head）机制。</li></ol><h3 id="（1）Sparse-Attention-稀疏注意力"><a href="#（1）Sparse-Attention-稀疏注意力" class="headerlink" title="（1）Sparse Attention 稀疏注意力"></a><strong>（1）Sparse Attention 稀疏注意力</strong></h3><p>在标准的自注意力机制中，每个 token 都需要关注所有其他 token。然而，据观察，对于经过训练的 Transformer，学习到的注意力矩阵 A 在大多数数据点上通常非常稀疏。因此，<strong>可以通过结合结构偏差来限制每个查询关注的查询键对的数量来降低计算复杂度</strong>。</p><p>从另一个角度来看，标准注意力可以被视为一个完整的二分图，其中每个查询从所有内存节点接收信息并更新其表示。稀疏注意力可以被认为是一个稀疏图，其中删除了节点之间的一些连接。基于确定稀疏连接的指标，我们将这些方法分为两类：基于位置的稀疏注意力和基于内容的稀疏注意力。</p><h4 id="1）基于位置的稀疏注意力"><a href="#1）基于位置的稀疏注意力" class="headerlink" title="1）基于位置的稀疏注意力"></a>1）<strong>基于位置的稀疏注意力</strong></h4><p>在基于位置的稀疏注意力中，注意力矩阵根据一些预定义的模式受到限制。虽然这些稀疏模式以不同的形式变化，但我们发现其中一些可以分解为一些原子稀疏模式。</p><p><strong>1、原子稀疏注意力</strong>：全局注意力（Global）、带状注意力（Band）、扩张注意力（Dilated）、随机注意力（Random）、块局部注意力（Block Local）；</p><p><img src="image/image_VQFqxhzbkK.png" alt=""></p><p><strong>2、复合稀疏注意力</strong>：现有的稀疏注意力通常由以上原子模式中的一种以上组成。 如图：</p><p><img src="image/image_ierRAYNnx0.png" alt=""></p><p><strong>3、拓展稀疏注意力</strong>：除了上述模式，一些现有的研究探索了特定数据类型的扩展稀疏模式。对于文本数据，有 BP-Transformer；还有一些视觉数据的扩展，如 Image Transformer 和 Axial Transformer。如图：</p><p><img src="image/image_SCf9JHyVdq.png" alt=""></p><h4 id="2）基于内容的稀疏注意力"><a href="#2）基于内容的稀疏注意力" class="headerlink" title="2）基于内容的稀疏注意力"></a><strong>2）基于内容的稀疏注意力</strong></h4><p>另一行工作基于输入内容创建稀疏图，即稀疏连接以输入为条件。构建基于内容的稀疏图的一种直接方法是选择那些可能与给定查询具有较大相似性分数的键。为了有效地构建稀疏图，我们可以递归到最大内积搜索 (MIPS) 问题，即尝试通过查询找到具有最大点积的键，而无需计算所有点积项。</p><h3 id="（2）线性注意力"><a href="#（2）线性注意力" class="headerlink" title="（2）线性注意力"></a><strong>（2）线性注意力</strong></h3><p>线性化注意力是一类用 $\phi(Q) \phi(K)^{\top}$近似或替换非标准化注意力矩阵 $\exp \left(Q K^{\top}\right)$ 的方法，其中 是按行方式应用的特征图。 因此，非归一化注意力矩阵的计算可以通过计算 $\phi(Q)\left(\phi(K)^{\top} V\right)$来线性化，如图：</p><p><img src="image/image_dgmdnDZlXq.png" alt=""></p><p>该模型通过聚合（由特征映射的）键和值的外积表示的关联来维护内存矩阵，然后通过将内存矩阵与具有适当归一化的特征映射查询相乘来检索值。 这种方法有两个关键组件：1）特征图 ；2）聚合规则。</p><h3 id="（3）-查询原型和内存压缩"><a href="#（3）-查询原型和内存压缩" class="headerlink" title="（3） 查询原型和内存压缩"></a><strong>（3） 查询原型和内存压缩</strong></h3><p>除了使用稀疏注意力或基于内核的线性注意力之外，还可以通过减少查询或键值对的数量来降低注意力的复杂性，这分别产生了查询原型和内存压缩方法。</p><p><img src="image/image_WN3mZaiOa1.png" alt=""></p><p><strong>使用原型查询的注意力</strong>**：** 在查询原型设计中，几个查询原型作为计算注意力分布的主要来源。该模型要么将分布复制到表示的查询的位置，要么用离散的均匀分布填充这些位置。</p><p><strong>压缩键值内存的注意力</strong>**：** 可以通过在应用注意力机制之前减少键值对的数量来降低复杂性。</p><h3 id="（4）低秩的自注意力"><a href="#（4）低秩的自注意力" class="headerlink" title="（4）低秩的自注意力"></a><strong>（4）低秩的自注意力</strong></h3><p>一些经验和理论分析报告称，自注意力矩阵通常是低秩的。这个属性的含义是双重的：1）低秩属性可以用参数化显式建模；2) 可以用低秩近似代替自注意力矩阵。</p><h3 id="（5）先验的注意力"><a href="#（5）先验的注意力" class="headerlink" title="（5）先验的注意力"></a><strong>（5）先验的注意力</strong></h3><p>注意力机制通常将预期的注意力值输出为向量的加权和，其中权重是值上的注意力分布。传统上，分布是从输入生成的（例如，原版 Transformer 中的 ）。</p><p>作为一般情况，注意力分布也可以来自其他来源，我们称之为先验的。预先注意分布可以补充或替代输入产生的分布。我们将注意力的这种表述抽象为具有先验的注意力，如图所示。在大多数情况下，两个注意力分布的融合可以通过在应用 softmax 之前计算对应于先验注意力和生成注意力的分数的加权和来完成。</p><p><img src="image/image_Vp1phi2QnJ.png" alt=""></p><p><strong>模型位置先验</strong>**：** 某些类型的数据（例如，文本）可以表现出对位置的强烈偏好。此属性可以显式编码为先验注意力。</p><p><strong>从下层模块先验</strong>**：** 在 Transformer 架构中，经常观察到相邻层的注意力分布相似。因此，很自然地将前一层的注意力分布作为注意力计算的先验。</p><p><strong>多任务适配器先验</strong>**：** 适配器是依赖于任务的训练模块，它们附加在预训练网络的特定位置，用于跨任务高效参数共享。</p><p><strong>仅注意力先验</strong>**：** 一些工作探索了使用独立于输入之间成对交互的注意力分布。 换句话说，他们的模型只利用了先验注意力分布。</p><h3 id="（6）改进的多头机制"><a href="#（6）改进的多头机制" class="headerlink" title="（6）改进的多头机制"></a><strong>（6）改进的多头机制</strong></h3><p>多头（Multi-head）注意力的吸引力在于能够共同关注来自不同位置的不同表示子空间的信息。然而，<strong>没有机制可以保证不同的注意力头确实捕捉到不同的特征</strong>。</p><p><strong>头部行为建模</strong>**：** 使用多头注意力的一个基本动机是允许模型共同关注来自不同位置的不同表示子空间的信息。然而，在 原版 Transformer 中，没有明确的机制来保证注意力头之间的不同行为，也没有任何机制让头相互交互。有 一系列工作致力于通过引入更复杂的机制来改进多头机制，这些机制指导不同注意力头的行为或允许跨注意力头的交互。</p><p><strong>跨度受限的多头</strong>**：** 原版注意力采用完全注意力跨度假设，其中查询可以关注所有键值对。然而，经常观察到，一些头主要将注意力集中在局部环境中，而其他一些头则关注更广泛的环境。因此，限制注意力范围可能对于局部性和效率方面都有益处。</p><p><strong>精细聚合的多头</strong>**：** 在每个注意力头计算其输出表示后，原版多头注意力将这些表示连接起来，然后对连接后的表示应用线性变换以获得最终的输出表示。有人可能会争辩说，这种简单的逐个聚合范式并没有充分利用多头注意力的表现力，而使用更复杂的聚合更为可取。因此有人提出了使用为胶囊网络设计的路由方法，注意力头的输出首先转化为输入胶囊，然后经过迭代路由过程得到输出胶囊。然后将输出胶囊连接起来作为多头注意力的最终输出。</p><h2 id="2-2-位置表示"><a href="#2-2-位置表示" class="headerlink" title="2.2 位置表示"></a><strong>2.2 位置表示</strong></h2><p>很容易验证卷积和循环网络不是置换等变的。然而，<strong>Transformer 中的自注意力模块和位置前馈层都是置换等变的</strong>，这可能在建模时成为一个问题，而不是需要输入结构的集合输入问题。例如，在对文本序列建模时，单词的顺序很重要，因此在 Transformer 架构中正确编码单词的位置至关重要。因此，<strong>需要额外的机制将位置信息注入到 Transformer 中</strong>。一种常见的设计是首先使用向量表示位置信息，然后将向量作为附加输入注入模型。</p><h3 id="（1）绝对位置表示"><a href="#（1）绝对位置表示" class="headerlink" title="（1）绝对位置表示"></a><strong>（1）绝对位置表示</strong></h3><p>在原版 Transformer 中，位置信息被编码为绝对正弦位置编码。对于每个位置索引，编码是一个向量，其中每个元素都是一个正弦函数，具有预定义频率的索引。</p><p>另一种表示绝对位置的方法是为每个位置学习一组位置嵌入。与手工制作的位置表示相比，学习嵌入更加灵活，因为位置表示可以通过反向传播适应任务。但是嵌入的数量被限制在训练前确定的最大序列长度，这使得这种方法不再具有归纳性，即无法处理比训练时看到的序列更长的序列。</p><h3 id="（2）相对位置表示"><a href="#（2）相对位置表示" class="headerlink" title="（2）相对位置表示"></a><strong>（2）相对位置表示</strong></h3><p>另一系列工作<strong>侧重于表示 token 之间的位置关系，而不是单个 token 的位置</strong>。直觉认为，在自注意力中输入元素（方向和距离）之间的成对位置关系可能比元素的位置更有益。遵循这一原则的方法称为相对位置表示。</p><h3 id="（3）其他表示"><a href="#（3）其他表示" class="headerlink" title="（3）其他表示"></a><strong>（3）其他表示</strong></h3><p>一些研究已经探索使用包含绝对和相对位置信息的混合位置表示。Transformer with Untied Position Encoding (TUPE) 将注意力分数的计算重新设计为内容到内容项、绝对位置到位置项和表示相对位置关系的偏置项的组合。</p><h3 id="（4）没有显式编码的位置表示"><a href="#（4）没有显式编码的位置表示" class="headerlink" title="（4）没有显式编码的位置表示"></a><strong>（4）没有显式编码的位置表示</strong></h3><p>Wang 等人没有明确引入额外的位置编码，建议通过将嵌入推广到位置上的连续（复值）函数来对词嵌入中的位置信息进行编码。</p><h3 id="（5）Transformer-decoder-的位置表示"><a href="#（5）Transformer-decoder-的位置表示" class="headerlink" title="（5）Transformer decoder 的位置表示"></a><strong>（5）Transformer decoder 的位置表示</strong></h3><p>值得注意的是，mask的self-attention不是置换等变的。因此，仅利用 Transformer 解码器的模型具有在不包含显式位置表示的情况下感知位置信息的潜力。语言建模任务的一些实证结果证实了这一点，作者发现删除位置编码甚至可以提高性能。</p><h2 id="2-3-层归一化"><a href="#2-3-层归一化" class="headerlink" title="2.3 层归一化"></a><strong>2.3 层归一化</strong></h2><p>层归一化（Layer Normalization, LN）以及残差连接被认为是一种<strong>稳定深度网络训练的机制</strong>（例如，减轻不适定梯度和模型退化）。 有一些工作可以分析和改进 LN 模块。</p><h3 id="（1）LN-的位置"><a href="#（1）LN-的位置" class="headerlink" title="（1）LN 的位置"></a><strong>（1）LN 的位置</strong></h3><p>在原版 Transformer 中，LN 层位于残差块之间，称为 post-LN。 后来的 Transformer 实现将 LN 层放在注意力或 FFN 之前的残差连接内，在最后一层之后有一个额外的 LN 来控制最终输出的大小，这被称为 pre-LN。<strong>Pre-LN 已被许多后续研究和实现所采用</strong>，区别如图所示。</p><p><img src="image/image_EdCZFdZwmi.png" alt=""></p><h3 id="（2）LN-的替代"><a href="#（2）LN-的替代" class="headerlink" title="（2）LN 的替代"></a><strong>（2）LN 的替代</strong></h3><p>徐等人凭经验观察到LN模块中的可学习参数在大多数实验中不起作用，甚至增加了过度拟合的风险。他们从受控实验中进一步得出结论，正向归一化不是 LN 适用于 Transformer 的原因。从分析和实验中可以得出结论，均值和方差的导数重新居中并重新缩放梯度，并在 LN 中发挥重要作用。因此，他们提出了 AdaNorm，一种没有可学习参数的归一化技术。</p><p>Nguyen 和 Salazar 建议用缩放的 l2 归一化替换 LN 模块。</p><p>沈等人讨论了为什么批归一化 (Bath Normalization, BN) 在文本数据的 Transformer 中表现不佳，并得出结论，BN 的显着性能下降源于与其批量统计相关的不稳定性。因此，他们提出了对 BN 进行三个修改的 PowerNorm (PN)：1）它放宽了零均值归一化；2) 它使用信号的二次均值，而不是方差；3) 它使用二次均值的运行统计，而不是使用每批统计。</p><h3 id="（3）无归一化的Transformer"><a href="#（3）无归一化的Transformer" class="headerlink" title="（3）无归一化的Transformer"></a><strong>（3）无归一化的Transformer</strong></h3><p>除了 LN，还有另一种机制可以构建更深层次的神经网络。ReZero <strong>用可学习的残差连接替换 LN 模块</strong>。验证表明用 ReZero 机制替换 Transformer 中的 LN 可以为输入信号引入更好的动态等距，并导致更快的收敛。</p><h2 id="2-4-位置前馈网络"><a href="#2-4-位置前馈网络" class="headerlink" title="2.4 位置前馈网络"></a><strong>2.4 位置前馈网络</strong></h2><p>位置前馈网络 (FFN) 层对于 Transformer 实现良好性能很重要。董等人观察到，简单地堆叠自我注意模块会导致等级崩溃问题，导致 token 均匀性归纳偏差，而前馈层是缓解此问题的重要构建块之一。各种工作都探索了对 FFN 模块的修改。</p><p><strong>FFN 中的激活函数</strong>**：** 原版 Transformer 采用整流线性单元 (ReLU) 激活来实现两个 FFN 层之间的非线性关系。随着时间的推移，一些研究探索了除 ReLU 之外的不同激活，比如 GELU 和 GLU。</p><p><strong>调整 FFN 以获得更大容量</strong>**：** 有几项工作专注于扩展 FFN 以获取更大的模型容量。基本思想是用具有更多参数的类似结构替换 FFN。</p><p><strong>删除 FFN 层</strong>**：** 值得注意的是，有人可能会争辩说，在某些情况下，可以完全删除 FFN 层，从而简化网络。</p><h1 id="3-架构层面"><a href="#3-架构层面" class="headerlink" title="3.架构层面"></a>3.<strong>架构层面</strong></h1><h2 id="3-1-使-Transformer-轻量化"><a href="#3-1-使-Transformer-轻量化" class="headerlink" title="3.1 使 Transformer 轻量化"></a><strong>3.1 使 Transformer 轻量化</strong></h2><p>除了在模块级别为减轻计算开销所做的努力之外，还有一些尝试通过更高级别的修改来使 Transformer 变得轻量级。</p><p>类似于<strong>将注意力分解为局部约束注意力和低阶全局注意力的低阶自注意力</strong>，Lite Transformer 建议将 Transformer 中的每个注意力模块替换为两个分支结构，其中一个分支使用注意力来捕获远程上下文，而另一个分支使用深度卷积和线性层来捕获局部依赖关系。该架构在模型大小和计算方面都是轻量级的，因此更适合移动设备。</p><p>Funnel Transformer 利用类似漏斗的 encoder 架构，其中隐藏序列的长度使用沿序列维度的池化逐渐减少，然后使用上采样恢复。与普通的 Transformer 编码器相比，该架构有效地减少了 FLOP 和内存。自然，人们可以使用这种架构使用相同的计算资源构建更深或更广的模型。</p><p>DeLighT 用 DeLighT 块替换了标准的 Transformer 块，该块由三个子模块组成：1）“扩展和减少”DeLightT 转换模块，以低计算要求学习更广泛的表示；2）单头自注意力学习成对交互；3）一个轻量级的“reduce-and-expand”FFN。他们还提出了一种逐块缩放策略，允许输入附近的更浅和更窄的块以及输出附近更宽和更深的块。诱导网络比普通 Transformer 深得多，但参数和操作更少。</p><h2 id="3-2-加强跨块连接"><a href="#3-2-加强跨块连接" class="headerlink" title="3.2 加强跨块连接"></a><strong>3.2 加强跨块连接</strong></h2><p>在原版 Transformer 中，每个块都将前一个块的输出作为输入并输出一系列隐藏表示。人们可能对创建更多路径感兴趣，输入信号可以沿着这些路径通过网络。Realformer 和 Predictive Attention Transformer 重用前一个块的注意力分布来引导当前块的注意力。这可以看作是在相邻的 Transformer 块之间创建了一条前向路径。</p><h2 id="3-3-自适应计算时间"><a href="#3-3-自适应计算时间" class="headerlink" title="3.3 自适应计算时间"></a><strong>3.3 自适应计算时间</strong></h2><p>与大多数神经模型一样，原版 Transformer 使用固定（学习过的）计算程序来处理每个输入。一个有趣且有前途的修改是使计算时间以输入为条件，即将自适应计算时间 (Adaptive Computation Time, ACT) 引入 Transformer 模型。此类修改可能会带来以下优势：</p><ol><li><strong>困难示例的特征细化</strong>。 对于难以处理的数据，浅层表示可能不足以完成手头的任务。 应用更多计算来获得更深入、更精细的表示会更理想。</li><li><strong>简单示例的效率</strong>。 在处理简单的示例时，浅层表示可能足以完成任务。 在这种情况下，如果网络可以学习使用减少的计算时间来提取特征，那将是有益的。</li></ol><p>如图所示是三种 ACT 范式：</p><p><img src="image/image_fwCt7S4oyi.png" alt=""></p><h2 id="3-4-分治策略的-Transformer"><a href="#3-4-分治策略的-Transformer" class="headerlink" title="3.4 分治策略的 Transformer"></a><strong>3.4 分治策略的 Transformer</strong></h2><p>序列长度上的自注意力的二次复杂度会显着限制一些下游任务的性能。例如，语言建模通常需要远程的上下文。另一种处理长序列的有效方法是使用分治策略，即<strong>将输入序列分解为可以由 Transformer 或 Transformer 模块有效处理的更细段</strong>。我们确定了两类有代表性的方法，循环和分层 Transformer，如图所示。这些技术可以被理解为 Transformer 模型的包装器，其中 Transformer 作为一个基本组件，被重用以处理不同的输入段。</p><p><img src="image/image_5ByAvofxwA.png" alt=""></p><h3 id="（1）循环-Transformer"><a href="#（1）循环-Transformer" class="headerlink" title="（1）循环 Transformer"></a><strong>（1）循环 Transformer</strong></h3><p>在循环 Transformer 中，会维护一个缓存以合并历史信息。在处理一段文本时，网络从缓存中读取作为附加输入。处理完成后，网络通过简单地复制隐藏状态或使用更复杂的机制来写入内存。</p><h3 id="（2）分层-Transformer"><a href="#（2）分层-Transformer" class="headerlink" title="（2）分层 Transformer"></a><strong>（2）分层 Transformer</strong></h3><p>分层 Transformer 将输入分层分解为更细粒度的元素。低级特征首先被送到 Transformer 编码器，产生输出表示，然后聚合（使用池化或其他操作）以形成高级特征，然后由高级 Transformer 处理。这类方法可以理解为一个层次抽象的过程。这种方法的优点有两个：1）分层建模允许模型以有限的资源处理长输入；2）它有可能产生更丰富的对任务有益的表征。</p><h2 id="3-5-探索替代架构"><a href="#3-5-探索替代架构" class="headerlink" title="3.5 探索替代架构"></a><strong>3.5 探索替代架构</strong></h2><p>尽管 Transformer 架构取得了成功，但人们可能会质疑当前的 Transformer 架构是否是最佳的。有趣的是，有几项研究探索了 Transformer 的替代架构。</p><p>卢等人将 Transformer 解释为多粒子动态系统中对流扩散方程的数值常微分方程 (Ordinary Differential Equation, ODE) 求解器，并设计 Macaron Transformer，它将每个 Transformer 块替换为 FFN-attention-FFN 变体。</p><p>Sandwich Transformer 探索重组注意力模块和 FFN 模块，使得注意力模块主要位于较低层，FFN 模块主要位于较高层。诱导模型在不增加参数、内存或训练时间的情况下，改善了多语言建模基准的困惑度。</p><p>掩码注意网络 (Mask Attention Network, MAN) 在每个 Transformer 块中的自注意模块之前添加了一个动态掩码注意模块。掩码以标记表示、标记和头部索引之间的相对距离为条件。所提出的动态掩码注意力被证明可以有效地对文本数据中的局部性进行建模，并且诱导模型在机器翻译和抽象摘要中始终优于Baseline模型。</p><p>值得注意的是，有一系列工作使用神经架构搜索 (NAS) 来搜索替代 Transformer 架构。Evolved Transformer (ET) 采用基于进化的架构搜索，标准的 Transformer 架构为初始群体提供种子。搜索到的模型在多个语言任务上表现出对 Transformer 的持续改进。作为另一项代表性工作，DARTSformer 应用可微架构搜索（DARTS），结合多分裂可逆网络和反向传播重建算法以提高内存效率。由此产生的模型始终优于标准 Transformer，并且与更大的 ET 模型相比具有优势，并且搜索成本显着降低。</p><h1 id="4-预训练-Transformer"><a href="#4-预训练-Transformer" class="headerlink" title="4.预训练 Transformer"></a>4.预训练 Transformer</h1><p>作为与卷积网络和循环网络的一个关键区别，Transformer 不对数据的结构做出任何假设。一方面，这有效地使 Transformer 成为一种非常通用的架构，具有捕获不同范围依赖关系的潜力。另一方面，这使得 Transformer 在数据有限时容易过拟合。缓解此问题的一种方法是在模型中引入归纳偏置。</p><p>最近的研究表明，在大型语料库上预训练的 Transformer 模型可以学习对下游任务有益的通用语言表示。这些模型是使用各种自监督的目标进行预训练的，例如，根据上下文预测掩码。在对模型进行预训练后，可以简单地在下游数据集上对其进行微调，而不是从头开始训练模型。为了说明在预训练中使用 Transformers 的典型方法，我们确定了一些预训练的 Transformer 并将它们分类如下：</p><ol><li><strong>仅 Encoder</strong>。一系列工作使用 Transformer Encoder 作为其主干架构。<strong>BERT</strong>是典型的 PTM，通常用于自然语言理解任务。它利用掩码语言建模 (MLM) 和下一句预测 (NSP) 作为自监督训练目标。RoBERTa 进一步调整了 BERT 的训练并删除了 NSP 目标，因为 NSP 被发现会损害下游任务的性能。</li><li><strong>仅 Decoder</strong>。一些研究侧重于对语言建模的 Transformer 解码器进行预训练。例如，生成式预训练 Transformer (<strong>GPT</strong>) 系列（即 GPT 、GPT-2 和 GPT-3 ）专门用于缩放预训练的 Transformer 解码器，并且最近说明了大规模 PTM 可以通过将任务和示例作为构造提示输入模型来在低资源场景下实现不错的功能。</li><li><strong>Encoder-Decodr</strong>。也有采用 Transformer Encodr-Decoder 作为整体架构的 PTM。BART 将 BERT 的去噪目标扩展到 Encoder-Decoder架构。使用 Encoder-Decoder 架构的好处是，诱导模型具备执行自然语言理解和生成的能力。**T5 **采用类似的架构，是最早在下游任务中使用特定于任务的文本前缀的研究之一。</li></ol><p>一些 Transformer 架构变体也可以应用于基于 Transformer 的 PTM。例如， BigBird 就是一个基于编码器的 PTM，它使用基于复合位置的稀疏注意力来启用长序列输入。GPT-3 在自注意力模块中使用交替的密集和局部带状稀疏注意力。Switch Transformer 是一种基于 Encoder 的 PTM，它用混合专家层替换了 FFN 层，并且可以增加参数数量，同时保持每个示例的 FLOPs 不变。</p><h1 id="5-Transformer的应用"><a href="#5-Transformer的应用" class="headerlink" title="5.Transformer的应用"></a>5.Transformer的应用</h1><p>Transformer 最初是为机器翻译而设计的，但由于其灵活的架构，已被广泛应用于 NLP 之外的各个领域，包括 CV 和音频处理。</p><ol><li><strong>自然语言处理</strong>。Transformer 及其变体已在 NLP 任务中得到广泛探索和应用，例如机器翻译、语言建模和命名实体识别。大量的努力致力于在大规模文本语料库上预训练 Transformer 模型，我们认为这是 Transformer 在 NLP 中广泛应用的主要原因之一。</li><li><strong>计算机视觉</strong>。Transformer 还适用于各种视觉任务，例如图像分类、物体检测、图像生成和视频处理。</li><li><strong>音频应用</strong>。Transformer 还可以扩展到与音频相关的应用，例如语音识别，语音合成，语音增强和音乐生成 。</li><li><strong>多模态应用</strong>。由于其灵活的架构，Transformer 还被应用于各种多模态场景，例如视觉问答、视觉常识推理、字幕生成、语音到文本翻译和文本到图像生成。</li></ol><h1 id="6-总结和未来方向"><a href="#6-总结和未来方向" class="headerlink" title="6.总结和未来方向"></a>6.总结和未来方向</h1><p>在这篇综述中，我们对 X-former 进行了全面概述，并提出了一个新的分类法。现有的大部分工作都是从不同的角度对 Transformer 进行改进，例如效率、泛化和应用。改进包括结合结构先验、设计轻量级架构、预训练等。</p><p>尽管 X-former 已经证明了它们在各种任务中的能力，但挑战仍然存在。除了当前的关注点（例如效率和泛化）之外，Transformer 的进一步改进可能在于以下几个方向：</p><ol><li><strong>理论分析</strong>。Transformer 的架构已被证明能够支持具有足够参数的大规模训练数据集。许多工作表明，Transformer 具有比 CNN 和 RNN 更大的容量，因此具有处理大量训练数据的能力。当 Transformer 在足够的数据上进行训练时，它通常比 CNN 或 RNN 具有更好的性能。一个直观的解释是 Transformer 对数据结构的先验假设很少，因此比 CNN 和 RNN 更灵活。然而，理论原因尚不清楚，我们需要对Transformer能力进行一些理论分析。</li><li><strong>超越注意力的更好的全局交互机制</strong>。Transformer 的一个主要优点是使用注意力机制来模拟输入数据中节点之间的全局依赖关系。然而，许多研究表明，对于大多数节点来说，完全注意是不必要的。在某种程度上，无法区分地计算所有节点的注意力是低效的。因此，在有效地对全局交互进行建模方面仍有很大的改进空间。一方面，self-attention 模块可以看作是一个具有动态连接权重的全连接神经网络，它通过动态路由聚合非局部信息。因此，其他动态路由机制是值得探索的替代方法。另一方面，全局交互也可以由其他类型的神经网络建模，例如记忆增强模型。</li><li><strong>多模态数据统一框架</strong>。在许多应用场景中，集成多模态数据对于提高任务性能是有用且必要的。此外，通用人工智能还需要能够捕捉不同模态之间的语义关系。由于 Transformer 在文本、图像、视频和音频方面取得了巨大成功，我们有机会构建一个统一的框架，更好地捕捉多模态数据之间的内在联系。然而，模内和跨模态注意力的设计仍有待改进。</li></ol><p>最后，我们希望这篇综述成为一个参考，以更好地了解 Transformer 的当前研究进展，并帮助读者进一步改进 Transformer 的各种应用。</p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 MoCo</title>
      <link href="/paper_reading/1.3.MoCo/"/>
      <url>/paper_reading/1.3.MoCo/</url>
      
        <content type="html"><![CDATA[<ul><li>论文名称：Momentum Contrast for Unsupervised Visual Representation Learning</li><li>论文连接：<a href="https://arxiv.org/pdf/1911.05722.pdf" title="1911.05722.pdf (arxiv.org)">1911.05722.pdf (arxiv.org)</a></li></ul><h1 id="0-基础知识"><a href="#0-基础知识" class="headerlink" title="0.基础知识"></a>0.基础知识</h1><h2 id="0-1-对比学习"><a href="#0-1-对比学习" class="headerlink" title="0.1 对比学习"></a>0.1 对比学习</h2><p>对比学习顾名思义就是对比着学习，模型不需要知道图片具体是什么，只需要知道哪些图片类似，哪些不类似。</p><p>假设有三张图片，两张是人类，一张是狗，假如这三张图片都通过一个网络，得到了三个特征。如果已经有了一个学习好的特征空间，那么学习到的三个特征就是特征空间里的三个点，我们希望对比学习做到的就是：<strong>能把类似图片的特征尽可能的靠近，不类似的图片的特征尽可能的远离</strong>。如果能做到，那么我们就学到了一个很好的特征。</p><p><img src="image/image_ephLeTRAV_.png" alt=""></p><p><strong>对比学习虽然不需要知道图片的标签信息，但还是需要知道哪些图片相似，哪些不相似，才能做模型训练。那为什么对比学习在视觉领域是一个无监督的训练方式呢？</strong></p><p>因为在视觉领域，通过设计一些巧妙的代理任务，从而人为的订立一些规则，这些规则可以用来定义哪些图片是相似的，哪些是不相似的。从而可以提供一个监督信号去训练模型，这就是所谓的自监督训练。</p><h2 id="0-2-最广泛应用的代理任务：instance-discrimination个体判别"><a href="#0-2-最广泛应用的代理任务：instance-discrimination个体判别" class="headerlink" title="0.2 最广泛应用的代理任务：instance discrimination个体判别"></a>0.2 <strong>最广泛应用的代理任务：instance discrimination个体判别</strong></h2><p>如果我们有一个没有标签的数据集，里面有n张图片：$x_1…x_n$，那么如何去定义相似呢？</p><p>instance discrimination的做法如下：随机选择一张图片，对其做随机剪裁+数据增广（即transformation），得到另外两张图，此时虽然已经不同了，但是因为来自同一张图片，他们的语义信息不应该发生变化。<strong>这两张图片被称作正样本，这个数据集里其余的图片被看做负样本。对于这个代理任务，它认为所有的图片自成一类。</strong></p><p><img src="image/image_jbtckL0Y95.png" alt=""></p><p><strong>对比学习中常见的实现方式：有了这个代理任务，有了这个去定义正负样本的规则之后，接下来就是通过一个模型，再得到一些特征，对这些特征使用一些常见的对比学习的目标函数就可以了，比如NCEloss</strong></p><p>对比学习最厉害的地方在于，它的<strong>灵活性</strong>：什么都可以比，哪个领域都能用，自然后来就扩展到了多模态领域，造就了后来openai的clip模型</p><h1 id="1-题目-amp-作者"><a href="#1-题目-amp-作者" class="headerlink" title="1.题目&amp;作者"></a>1.题目&amp;作者</h1><p>题目：动量对比学习的方法去做无监督的表征学习</p><p><strong>Momentum Contrast</strong></p><p>动量对比学习，动量可以从数学上理解为一种加权移动平均</p><script type="math/tex; mode=display">y_t=m \cdot y_{t-1}+(1-m) \cdot y_t</script><p>为了让当前时刻的输出$yt$，不完全依赖于当前时刻的输入$xt$。给上一个时刻的输出$yt$一个权重$m$（动量，0-1之间），去参与当前时刻的输出yt。如果动量m趋近于1的时候，当前时刻的输出yt的改变是非常缓慢的，此时1-m是趋近于0，也就是很少的依赖于当前的输入。如果m很小的话，就是当前的输出更多的依赖于当前的输入。</p><p><strong>moco就是利用了动量的这种特性，从而去缓慢的更新一个编码器，让中间学习的字典中的特征，尽可能的保持一致**</strong>。** ​</p><h1 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a>2.摘要</h1><p>用moco这个方法去做无监督的表征学习，虽然我们是基于对比学习的。但我们是从另一个角度去看对比学习，也就是把对比学习看作是一个<strong>字典查询的任务</strong>。具体来说，就是我们做了一个动态的字典，它由两部分组成：<strong><code>队列</code>**</strong>+<strong>**<code>一个移动平均的编码器</code></strong>，</p><p>（1）因为队列中的样本不需要做梯度回传，所以就可以往队列中放很多负样本，从而使这个字典变得很大；</p><p>（2）移动平均编码器：是为了让字典中的特征尽可能的保持一致。我们发现在无监督训练的过程中，如果有一个很大而且一致的字典，会对无监督的对比学习非常有好处。</p><p><strong>结果上的亮点</strong>：</p><p>imagenet数据集上的分类任务：如果用大家普遍采用linear protocol（是指如果预训练好了一个骨干网络，现在要把它用到不同的数据集上的时候，把他的骨干网络冻住backbone freeze，只去学习最后的全连接层，也就是那个分类头。这相当于把一个提前训练好的预训练模型当做一个特征提取器，只用它去抽特征，这样就可以间接的证明，之前预训练好的那个模型的特征到底学的好不好）去做测试，moco能取得和之前最好的无监督学习方式差不多，或者更好的结果。</p><p><strong>moco学习的特征是能很好的迁移到下游任务的</strong>（这是最大的卖点，因为我们之所以想做大规模的无监督预训练，就是为了学到一个很好的特征，然后这个特征拥有很好的迁移性，就可以在没有那么多标注数据的下游任务里获得很好的结果。）</p><p>moco能在7个下游任务上超越之前的有监督的预训练模型，用的模型都是一样的，只是训练方式不同，一个是用有监督带标签的训练，一个是无监督不带标签的数据训练。</p><p>这就意味着无监督和有监督的表征学习中间的鸿沟，对于很多视觉任务来说已经填上了。</p><h1 id="3-引言"><a href="#3-引言" class="headerlink" title="3.引言"></a>3.引言</h1><h2 id="3-1-研究动机"><a href="#3-1-研究动机" class="headerlink" title="3.1 研究动机"></a>3.1 研究动机</h2><p>GPT和BERT证明了无监督预训练在NLP领域的成功。但在视觉领域还是有监督的预训练占主导地位，或者无监督结果远不如有监督模型的效果。作者认为<strong>可能是因为视觉和文本之间截然不同的原始信号空间</strong>：</p><p>NLP任务中：<strong>原始的信号空间是离散的，是由单词或词根词缀表示的，有很强的语义信息</strong>。从而很容易的可以建立<strong>tokenize</strong>的字典（每个单词对应成特征），每个key看做一个类别，就有一个类似于标签的东西去帮助进行学习。就类似有监督学习的范式，有标签信息帮助进行无监督学习。所以nlp中无监督很好建模，建模好的模型也很好优化。</p><p>视觉：原始信号在连续而且高维的空间，不像单词一样有很强的语义信息，并不简洁，不适合建立字典，不好建模。所以无监督学习远不如有监督学习。</p><p>最近有一些无监督表征学习的方式，是基于对比学习的，而且取得了非常不错的效果，虽然这些工作的出发点，或者做法都不一样，但都可以被归纳为：<strong>在构造一个动态的字典</strong>。</p><p><strong>对比学习：</strong></p><ol><li>确定正负样本</li><li>输入编码器，得到特征输出</li></ol><p>对比学习就是让，正样本和锚点尽可能靠近，负样本对与锚点尽可能远离。</p><p><img src="image/image_I_5i5facb6.png" alt=""></p><p>为什么可以被归纳为在做一个动态字典：</p><ul><li>key：字典中特征</li><li>query：锚点特征</li></ul><p>具体来说，<strong>对比学习去训练一些编码器，进行字典查找，目的是让已经编码好的query</strong>，尽可能和匹配的特征相似，和负样本的特征远离。整个学习的框架，就是对比学习的框架了，那么只需要去最小化对比学习的目标函数就可以了。</p><p><img src="image/image_-TI1nNAsDa.png" alt=""></p><p>把对比学习当成动态字典，字典需要有两个特性：</p><ul><li><strong>大</strong>：能更好的从高维的视觉空间做抽样，字典中的key越多能表示的视觉特征越丰富</li><li><strong>训练时保持尽可能的一致性</strong>：字典中的key都应该用相同或者相似的编码器得到，这样就能保证在和锚点特征作对比的时候尽可能的保持一致</li></ul><h2 id="3-2-MoCo"><a href="#3-2-MoCo" class="headerlink" title="3.2 MoCo"></a>3.2 MoCo</h2><p>MoCo的目的：<strong>为了给无监督的对比学习构造一个又大又一致的字典</strong></p><p><img src="image/image_k_m8UT1a0E.png" alt=""></p><p>为什么队列表示：<strong>受限于显卡内存</strong>，如果字典太大那么就需要很多图片。为了让字典的大小和每次模型做前向过程时的batch size大小剥离开，使用了队列的数据结构。</p><p>具体来说，这个队列可以很大，但<strong>每次更新队列是逐步进行的</strong>，当用一个很小的batch size的时候，当前batch抽得的特征进入队列，把最早的mini-batch移出队列，这样就把训练时mini-batch的大小和队列的大小分开了，那么队列的大小可以设置的非常大。</p><p>如上述所说，每次迭代只有当前的batch是从当前时刻的编码器得到的，<strong>为了让所有的key都使用一致的编码器</strong>，提出了第二个改进：<strong>动量编码器</strong>。虽然动量编码器是由左侧编码器初始化来的，但是在模型训练过程中，如果选择很大的动量，那么动量编码器更新的是非常缓慢的，不会跟着左侧编码器快速的改变，从而保证了字典里所有的key都是由相似的编码器得到的。</p><p>如此，<strong>moco这个方法可以构建一个又大又一致的字典，从而无监督的去学习一个视觉表征</strong>。</p><h2 id="3-3-代理任务"><a href="#3-3-代理任务" class="headerlink" title="3.3 代理任务"></a>3.3 代理任务</h2><p>moco只是建立中间模型的方式，只是为对比学习提供了一个动态的字典，具体选择什么代理任务做自监督学习，完成模型的训练？</p><p>moco是非常灵活的，可以和很多代理任务搭配用。本文中为什么选择了简单的个体判别任务呢？简单+效果好，一句话概括这个代理任务就是：<strong>如果一个query和一个key是同一个图片的不同视角，那么就说q和k是配对的</strong>（能在动态字典里查到q对应的k）。用了这个代理任务之后，moco在imagenet数据集上做linear classification的时候能和之前最好的方法打平手或有更好的表现。</p><h2 id="3-4-结果"><a href="#3-4-结果" class="headerlink" title="3.4 结果"></a>3.4 结果</h2><p>moco能做到在很大的无标注数据集上，做完预训练之后，预训练好的特征能直接迁移到下游任务上。moco可以在偏向真实世界、亿级规模图片的数据集上工作的很好了。<strong>moco可以在很多任务上，把无监督学习和有监督表征学习之间的坑填平</strong>，甚至可以取代之前大家一直使用的imagenet预训练的模型。&#x20;</p><p>其实大家对于无监督学习还有另外的期待，就是当你用更大的数据集，用更大的模型，我们希望这个模型的提升是永无止境的，最好不要有性能保护的效应。作者为了保证实验的完整性，他在facebook自己的数据集上，也就是有10亿instagram图片的数据集上也去做了预训练，最后的结果还能提升，所以这就证明了moco是可以在一个更偏向于真实世界而且有亿级规模图片的数据集上（relatively uncurated scenario，因为这个数据集不像是ImageNet一样是精心挑选过，而且大部分图片都是只有一个物体在中间，而instagram的图片数据集就相对而言非常丰富了，而且也会展示出真实世界中数据的一些特性）工作的很好。这也证明了moco可以在很多视觉任务上很大程度的把无监督学习和有监督表征学习的坑填平，这句话的影响力无疑是巨大的，也就是说之前在ImageNet数据集上预训练的模型现在都可以换成moco无监督学习的模型。</p><h1 id="4-结论-amp-讨论"><a href="#4-结论-amp-讨论" class="headerlink" title="4.结论&amp;讨论"></a>4.<strong>结论&amp;讨论</strong></h1><p>主要是讨论部分，通过在ImageNet-1M和IG-1B上做实验发现，虽然数据集提升了1000倍，但是性能只提升了不到1个百分点，作者认为是大规模的数据集没有被很好地利用起来，他们觉得有一个更好的代理任务，有可能会解决这个问题。</p><p><img src="https://i0.hdslb.com/bfs/note/389e686cb16641511591d269a544c9494f87f1cf.png@670w_!web-note.webp" alt=""></p><p>最后作者希望moco能够对使用对比学习的代理任务有帮助，之所以强调对比学习，是因为moco设计的初衷就是去<strong>构造一个大的字典从而让正负样本能够更有效地去对比，提供一个稳定的自监督信号</strong>，最后去训练这个模型。</p><h1 id="5-相关工作"><a href="#5-相关工作" class="headerlink" title="5.相关工作"></a>5.<strong>相关工作</strong></h1><p>无监督学习/自监督学习一般有两个方向可以做，一个是<strong>代理任务</strong>，一个是<strong>目标函数</strong>。</p><p><strong>代理任务</strong>：代理任务就是那些没有实际应用场景的任务（比如检测/分割），这些代理任务的提出主要是<strong>为了学习到一个好的特征</strong>。</p><p>moco主要就是在目标函数上下功夫，它提出的又大又一致的词典呢，主要影响的是后面的infoNCE这个目标函数的计算，这里的目标函数主要是针对无监督学习的目标函数。</p><p><strong>常见的构建目标函数的方式</strong>**：衡量模型输出与**固定的目标之间的差距（eg. </p><ol><li><strong>生成式网络</strong>：eg. 自编码器：输入一张原图，通过编码器和解码器，想重建这张图，既可以用<code>L1 loss</code>也可以用<code>L2loss</code>。</li><li><strong>判别式网络</strong>：eg. eight position：把一张图片打成9宫格，编号，判断随机挑选的周边格位于中间格的哪个方位</li><li><strong>对比学习的目标函数</strong>：主要是去一个特征空间里，<strong>衡量各个样本对之间的相似性</strong>，目标是让相似物体的特征拉的尽量近，不相似物体之间的特征推开的尽量远。<strong>目标是在训练过程中不停改变的</strong>，是由编码器抽出来的数据特征来决定的（也就是moco中所说的字典）</li><li><strong>对抗性的目标函数</strong>：<strong>衡量的是两个概率分布之间的差异</strong>，主要是用来做无监督数据生成的。后来也有一些对抗性的方法，用来做特征学习了（因为如果能生成很好的图形，按道理说是已经学到了数据的底层分布，也就是模型学到的特征是不错的）</li></ol><p><strong>代理任务</strong>的多种形式：重建整张图、重建patch、九宫格方法、聚类</p><p><strong>对比学习vs代理任务的关系</strong>：不同的代理任务可以和一些形式的对比学习的目标函数配对使用的（CPC预测性的对比学习，用上下文信息预测未来）（CMC利用一个物体的不同视角做对比）</p><p>相关工作主要围绕代理任务和目标函数来展开，是因为这两个部分和有监督学习很不一样，相关工作也写的非常简单明了。</p><p>无监督学习没有GT，那就要靠代理任务自己去造，<strong>代理任务的作用就是去生成一个自监督的信号</strong>，然后去充当GT的作用，既然有了GT和输出y，那么我们就需要一个目标函数来指导模型学的更好，所以说这就是为什么moco这篇论文从代理任务和目标函数这两个角度去写相关工作的原因。</p><h1 id="6-Method"><a href="#6-Method" class="headerlink" title="6.Method"></a>6.<strong>Method</strong></h1><h2 id="6-1-损失函数"><a href="#6-1-损失函数" class="headerlink" title="6.1 损失函数"></a>6.1 损失函数</h2><p>假设有一个编码好的query $q$，一系列编码好的样本$k_0,k_1…$（看作字典里的key）。假设字典中只有一个key和query是匹配的（也可以拓展为有多个正样本对）。</p><p>对比学习的目标函数最好能满足如下要求：$q$和$k_+$相似的时候，loss的值比较低，$q$和其他$k$不相似的时候，loss的值也应该低。到这个状态，模型差不多就训练好了。反之，loss应该高，惩罚模型，让模型赶紧更新参数。</p><h3 id="（1）NCE-Loss"><a href="#（1）NCE-Loss" class="headerlink" title="（1）NCE Loss"></a>（1）NCE Loss</h3><p>noise contrastive estimation把超级多分类的问题变成二分类问题，就还可以很好的使用softmax操作。</p><ul><li><strong>noise contrastive</strong>：解决类别多的问题。因为类别数太多（每个图片是一个类），没法算softmax，所以没法算目标函数。</li><li>NCEloss把问题简化为一个二分类问题：数据类别和噪声类别。</li><li><strong>estimation</strong>：把所有剩下的图片都当做负样本，还是太多了，在数据集上选一些负样本算loss就可以了，只是一个估计。（如果样本选的很少，就不能近似，效果就会差）所以moco说要尽可能大的字典，因为越大就能提供一个越好的近似。</li></ul><h3 id="（2）InfoNCE-Loss"><a href="#（2）InfoNCE-Loss" class="headerlink" title="（2）InfoNCE Loss"></a>（2）InfoNCE Loss</h3><p>觉得2分类（数据样本和噪声样本）不太合理，应该看作多分类问题。</p><script type="math/tex; mode=display">\mathcal{L}_q=-\log\frac{\exp(q\cdot k_+/\tau)}{\sum_{i=0}^K\exp(q\cdot k_i/\tau)}</script><p>温度超参数$τ$：标量。一般是用来控制分布的形状的，设置的过大那么对比损失对所有的负样本一视同仁，导致模型学习没有轻重。如果设置的过小，互让模型只关注哪些特别困难的负样本，会导致模型很难收敛，或者学好的特征不好泛化；</p><p>求和公式上标K指的是负样本的数量，q.k其实相当于sofymax里面的logits。</p><p><strong>代理任务提供正负样本</strong></p><p>模型的输入query和key分别是query的输入和key的输入经过一个编码器得到的，至于模型到底是什么以及输入到底是什么，他们具体的实现由他们具体的代理任务决定，输入的xk和xq既可以是图片，也可以是图片块。</p><p>对于模型，作者说对于q和k 的编码器既可以是一样的也可以是不同的，还可以是部分参数共享的。</p><h2 id="6-2-MoCo"><a href="#6-2-MoCo" class="headerlink" title="6.2 MoCo"></a>6.2 MoCo</h2><p>对比学习是一种在高维的连续的输入（图片）上，构建字典的方式。这个字典是动态的，因为字典中的key是随机取样的，而且用来给这些key做编码的编码器，也是在训练过程中不停的改变。（与之前的有监督和无监督的方法都不一样，因为他们学习的都是固定的目标）。</p><p>作者认为，如果想学习一个好的特征，这个字典必须有两个特性：<strong>大而且一致</strong>，大的字典能包含很多语义丰富的负样本，从而有助于学到更有判别性的特征；一致性是为了避免模型的训练，避免学到一些trivial solution捷径解。</p><h3 id="（1）字典看作队列"><a href="#（1）字典看作队列" class="headerlink" title="（1）字典看作队列"></a>（1）字典看作队列</h3><p>把字典用队列的形式表示。字典是所有数据的子集，因为在算对比学习的目标函数的时候，只取近似而不是在整个数据集上算loss。</p><p>优点1：把字典的大小和mini-batch的大小剥离开，就可以在模型训练中使用比较标准的minibatch size，但是字典的大小可以非常大，非常灵活，而且可以当做超参数一样单独设置。</p><p>优点2：使用队列的数据结构可以让维护字典的计算开销非常小，根据队列的特性，每次移出队列的都是最早计算的那些mini-batch，这对对比学习来说是很有利的，因为从一致性的角度来说，最早计算的那些mini-batch的key是最过时的，也就是和最新的mini-batch算的key是最不一致的。</p><h3 id="（2）动量更新"><a href="#（2）动量更新" class="headerlink" title="（2）动量更新"></a>（2）动量更新</h3><p>队列让字典变得非常大，但是也<strong>因为非常长的队列，导致没办法给队列里所有的元素进行梯度回传了</strong>。也就是key的编码器没办法通过反向传播的方式去更新参数，每个iteration只对一个minibatch的负样本计算key，队列里的其他key都是过去时刻编码器计算的值。不能让query的编码器一直在更新，而key的编码器不动。</p><p><strong>方法1</strong>：每个训练iteration结束之后，<strong>把更新好的编码器参数</strong>$f_q$<strong>直接复制过来，给key的编码器</strong>$f_k$。结果不好，原因可能是一个快速改变的编码器，降低了这个队列里所有key的特征的一致性。也就是，假设每个minibatch，size就是1，每次只更新一个key，那么所有产生的key都是由不同的编码器产生的。这样，<strong>快速改变的编码器就会降低所有key之间的一致性</strong>。</p><p><strong>方法2</strong>：动量更新的方式。query编码器的参数$θ_q$，key编码器的参数$θ_k$，那么$θ_k$的更新方式如下：</p><script type="math/tex; mode=display">\theta_{\mathrm{k}} \leftarrow m \theta_{\mathrm{k}}+(1-m) \theta_{\mathrm{q}}</script><ul><li>$m$：动量参数，$[0,1]$</li><li>$θ_q$：是通过梯度反向回传来更新模型参数的</li><li>$θ_k$：除了刚开始是用$θ_q$初始化的，后面的更新主要是靠自己，因为如果动量m设置的很大，$θ_k$更新就非常缓慢了。</li></ul><p>因为使用了动量更新的方式，虽然队列中的key都是由不同的编码器产生得到的，但是因为这些编码器之间的区别太小，所以产生的这些key的一致性是非常强的。</p><h3 id="（3）与之前的方法对比"><a href="#（3）与之前的方法对比" class="headerlink" title="（3）与之前的方法对比"></a>（3）与之前的方法对比</h3><p>之前的对比学习的方法，都可以看作是字典查找，但他们都或多或少受限于字典的大小和字典一致性的问题。解释之前的方法怎么受限的，MoCo又是如何通过动量对比的方式去解决这些局限性。</p><p>总结之前的方法，归纳为两种架构：</p><p><img src="image/image_SBwzZhGZkd.png" alt=""></p><h4 id="端到端学习的方式（SimCLR）：图-a"><a href="#端到端学习的方式（SimCLR）：图-a" class="headerlink" title="端到端学习的方式（SimCLR）：图(a)"></a>端到端学习的方式（SimCLR）：图(a)</h4><p>顾名思义，就是<strong>编码器都可以通过梯度回传来更新模型参数，两个编码器可以是不同的网络</strong>，但是之前的工作都是使用相同的网络，为了简单起见MoCo中使用的是同一个模型，也就是Res50。为什么可以用同一个模型？因为正负样本都是来自于同一个minibatch，做一次前向就能得到所有样本的特征，而且这些样本是高度一致的。编码器都能用反向回传学习了，特征也高度一致了，听上去很好，但是<strong>局限性在于字典的大小</strong>。因为在端到端学习中，minibatch的大小和字典的大小是等价的，如果想要一个很大的字典，里面有成千上万个key的话，也就意味着minibatch size的大小必须也是成千上万，这就很难了，因为现在的GPU塞不下这么大的batchsize。即使能塞下这么大的batchsize，也不好优化，如果处理的不好，模型是很难收敛的。</p><p>优点：<strong>编码器可以实时更新</strong>，所以字典中的key一致性非常高。</p><p>缺点：<strong>字典大小=minibatch大小</strong>，所以字典不能过大，否则硬件内存吃不消。</p><h4 id="Memory-bank：图-b"><a href="#Memory-bank：图-b" class="headerlink" title="Memory bank：图(b)"></a>Memory bank：图(b)</h4><p>更关注字典的大，牺牲一些一致性。</p><p><strong>query有编码器，key没有编码器，把所有的特征都存到一起，每次模型训练的时候从中随机抽样很多key来当做字典</strong>，就是整个右侧的操作都是在线下执行的，所以完全不用担心硬件内存的问题，也就是这个字典可以抽样的很大。</p><p>但是<strong>特征一致性就处理的不好</strong>，假设memory bank中有128万个key。训练的时候是随机抽样当做字典的，假设抽出来$key1,key 2,key 3,…$ 去和query算loss，算完loss回传的梯度，更新了query编码器之后，<strong>用这个新的编码器在原来</strong>$key1,key 2,key 3,…$** 对应位置上去生成新的特征，放进memory bank中，以此更新memory bank**。</p><p>因为都是在不同时刻的编码器得到的，而且这些编码器都是通过梯度回传来很快的更新的，也就意味着得到的这些<strong>特征都缺乏一致性</strong>。</p><p>还存在另一个问题：因为memory bank中存了所有的图片，那就意味着模型训练了整整一个epoch，才能把整个memory bank更新一遍。当开始下一个epoch训练的时候，假设选了$key1,key 5,key 8,…$，那这三个key的特征已经不知道是上一个epoch哪个时间点算出来的特征了。也就<strong>导致query编码器产生的特征和key这边产生的特征差的非常远。</strong></p><h4 id="MoCo：图-c"><a href="#MoCo：图-c" class="headerlink" title="MoCo：图(c)"></a>MoCo：图(c)</h4><p>MoCo采用队列的方式去实现一个字典，从而使它不像端到端的学习一样，受限于batchsize的大小。同时，为了提高字典中特征的一致性，MoCo使用了动量编码器。其实从整体上看，MoCo的做法和memory bank的方法是更加接近的：</p><ol><li>都是只有query的编码器是通过梯度回传来更新模型参数的</li><li>字典都是采用了额外的数据结构存储（memory bank和队列），从而和batchsize剥离开了</li><li>memory bank这篇论文意识到了特征不一致带来的坏处了，所以增加了一个loss（proximal optimization）目的就是让训练变得更平滑。和MoCo中的动量更新有异曲同工之效的，只不过memory bank中动量更新的是特征，MoCo中动量更新的是编码器。</li><li><strong>MoCo的扩展性很好</strong>，可以在上亿级别的图像库上训练。但对于特别大的数据集，因为要把所有的特征都存到一个memory bank中，memory bank的方法就捉襟见肘。</li></ol><h2 id="6-3-伪代码"><a href="#6-3-伪代码" class="headerlink" title="6.3 伪代码"></a>6.3 伪代码</h2><p><img src="image/image_XPIvMOsbEr.png" alt=""></p><h2 id="6-4-Shuffling-BN"><a href="#6-4-Shuffling-BN" class="headerlink" title="6.4 Shuffling BN"></a>6.4 Shuffling BN</h2><p>因为用了BN，很可能导致batch样本中间的信息会泄露，因为BN要算这些样本的running mean和running variance，能通过泄露的信息很容易的找到正样本，而不需要学一个真正好的模型。因为BN大部分都是在当前GPU上算的，所以作者在做多卡训练之前先把样本的训练打乱，再送到所有的GPU上去，算完了特征再把顺序恢复来算最后的loss，这样就对loss没有影响了，但是每个GPU上的BN计算就不会存在泄露的问题。现在用Transformer就不用BN了，就用Layer norm了。</p><h1 id="7-实验"><a href="#7-实验" class="headerlink" title="7.实验"></a>7.<strong>实验</strong></h1><p><strong>无监督预训练数据集</strong>：ImageNet-1M，ImageNet类别数是1000，但是MOCO用的个体判别任务，所以类别数是1Million</p><p><strong>为了验证扩展性的数据集</strong>：Instagram-1B，真实世界的数据分布：长尾、不均衡，既有一个物体的，也有多个物体的图片，或者是场景层面的图片</p><p><strong>训练</strong>：SGD作为优化器；对于ImageNet数据集来说用的是标准batchsize：256，是在一台8卡机上训练的；如果用res50网络结构，训练200个epoch，大概需要53小时</p><h2 id="7-1-特征分类器迁移学习"><a href="#7-1-特征分类器迁移学习" class="headerlink" title="7.1 特征分类器迁移学习"></a>7.1 <strong>特征分类器迁移学习</strong></h2><p>完成模型预训练之后，把模型的backbone冻住，只把它当做一个特征提取器，在它上面训练一个全连接层去充当一个分类头，训练这个分类头用了100个epoch。在测试集上报告了1-crop，top-1的分类准确度。</p><p>网格搜索得到的学习率是30，这说明<strong>无监督学习到的特征分布和有监督学习到的，是非常不一样的</strong>。</p><h2 id="7-2-消融实验"><a href="#7-2-消融实验" class="headerlink" title="7.2 消融实验"></a>7.2 消融实验</h2><h3 id="（1）队列好处"><a href="#（1）队列好处" class="headerlink" title="（1）队列好处"></a>（1）队列好处</h3><p><img src="image/image_d3woJN5QGQ.png" alt=""></p><p>横坐标：负样本的个数，可以粗略理解为字典的大小；</p><p>纵坐标：ImageNet top-1准确率</p><p>8卡v100 32gb内存</p><ul><li>端到端：batchsize最大1024</li><li>memory bank： batchsize可以到最大，但是效果不好（因为特征不一致性）</li><li>MOCO：字典很大，性能到后期已经饱和了</li></ul><p>说明：<strong>MOCO性能最好，对硬件要求最低，扩展性也比较好的方法</strong></p><h3 id="（2）动量更新好处"><a href="#（2）动量更新好处" class="headerlink" title="（2）动量更新好处"></a>（2）动量更新好处</h3><p><img src="image/image_yd37Ve8ck4.png" alt=""></p><p>使用相对较大的动量，性能是最好的</p><p>说明缓慢更新的编码器，对对比学习是有好处的，因为能保持一致性</p><p>如果直接把query的编码器拿来用，会发现模型不能收敛了，loss一直震荡，从而导致训练失败。</p><h2 id="7-3-ImageNet效果比较"><a href="#7-3-ImageNet效果比较" class="headerlink" title="7.3 ImageNet效果比较"></a>7.3 ImageNet效果比较</h2><p><img src="image/image_0qqdUMnLVk.png" alt=""></p><p>都是把网络当做特征提取器，抽出来的特征再去训练一个全连接层当做分类头，最后得到这些结果。</p><p>上半部分不是对比学习，下半部分是对比学习。可以发现对比学习的效果确实不错。</p><p>无监督学习中，模型的大小是非常关键的，因为模型越大，一般效果就会越好。所以文中也列举了模型结构和模型参数大小，这样就可以做一个相对全面而且公平的比较了。</p><p>MOCO既能在小模型上得到最好的结果，也能在大模型上得到最好的结果。</p><h2 id="7-3-迁移学习"><a href="#7-3-迁移学习" class="headerlink" title="7.3 迁移学习"></a>7.3 迁移学习</h2><p>验证MOCO得到的特征，能不能在下游任务上得到好的迁移学习效果。</p><p>无监督学习最主要的目标：学习一个可以迁移的特征。用ImageNet去做有监督训练，最有影响力的时候，就是在下游任务上做微调，可以用这个预训练好的模型做模型的初始化，从而当下游任务只有很少的标注数据的时候，也能获得很好的效果。</p><p>用检测任务来做MoCo的无监督预训练模型和ImageNet有监督预训练模型之间的比较。</p><h3 id="（1）归一化"><a href="#（1）归一化" class="headerlink" title="（1）归一化"></a>（1）归一化</h3><p>无监督学习到的特征分布和有监督学习到的，是非常不一样的。要拿这个特征去做下游任务，不可能都做一遍网格搜索去找最佳学习率，就太麻烦了。</p><p>拿之前大家为有监督的预训练，已经设置好的超参数做微调，那就既可以做公平对比，也不用做网格搜索了。</p><p><strong>当特征分布不一致的时候，最常用的解决办法：</strong> ​<strong>归一化</strong>。</p><p>具体来说，就是整个模型都在微调，尤其是BN层用的是sync BN（synchronized batch norm）：把多机训练时候所有GPU上的BN的统计量都合起来，算完running mean、running Variance之后再做BN层的更新，就会让特征的归一化做的更彻底，也会让模型的训练更稳定。</p><p>同时在新加的层（做检测要用的FPN结构）中也用了BN，目的就是去调整值域的大小，从而好做特征的归一化。</p><p>只要做好了特征归一化，作者就发现，可以拿着有监督训练那边用的超参数来做微调了。</p><h3 id="（2）学习时长"><a href="#（2）学习时长" class="headerlink" title="（2）学习时长"></a>（2）学习时长</h3><p>当下游任务数据集足够大（如coco）的时候，可以不需要预训练，直接从随机初始化开始从头训练，效果一样很好，那么无论有监督还是无监督，是无所谓的，因为不需要预训练的模型去做模型初始化了，这样也就不能体现MoCo的优越性了。参考文献31中说的是前提是当训练足够长的时候，也就是训练短的时候预训练模型初始化还是有用的。</p><p>所以在moco中使用的是1×或2×的学习时长，在这个时候预训练还是非常有用的。这时候就可以比较MoCo训练的好，还是有监督的imageNet训练的好了。</p><p>上述铺垫就是想说明，当我们用MoCo的预训练模型做微调的时候，这和之前有监督预训练模型然后再微调的方式是一样的，好处就是，当在不同的数据集或者不同的任务上做微调的时候，就不用再调参了。</p><h3 id="（3）检测问题与有监督学习对比"><a href="#（3）检测问题与有监督学习对比" class="headerlink" title="（3）检测问题与有监督学习对比"></a>（3）检测问题与有监督学习对比</h3><p><img src="image/image_-cVy2eSG_A.png" alt=""></p><h3 id="（4）三种对比学习比较"><a href="#（4）三种对比学习比较" class="headerlink" title="（4）三种对比学习比较"></a>（4）三种对比学习比较</h3><p>三种对比学习方式，在下游任务上再做一次对比：之前的两种方式都没有超过有监督学习的结果，只有MoCo超越了。</p><p><img src="image/image_M--7ccXOt5.png" alt=""></p><p>MoCo在很多个下游任务上都超越了ImageNet的有监督预训练的模型，只在零星的几个任务上MoCo稍微差了一点，主要是集中在实例分割和语义分割的任务上。</p><p><strong>注意</strong>：MoCo在Instagram数据集上训练的模型，比在ImageNet上训练出来的模型好，在所有任务上都普遍好，这说明MoCo的扩展性好，更多的数据就能学到更好的模型，这和NLP那边得到的结论是一致的，也就达到了无监督学习的终极目标。</p>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PaperReading </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 GAN</title>
      <link href="/paper_reading/1.2.GAN/"/>
      <url>/paper_reading/1.2.GAN/</url>
      
        <content type="html"><![CDATA[<p>GAN Note</p><p>最新版本论文：<a href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf" title="Generative Adversarial Nets (neurips.cc)">Generative Adversarial Nets (neurips.cc)</a>（建议看这个）</p><p>arXiv版本论文：<a href="https://arxiv.org/pdf/1406.2661.pdf" title="1406.2661.pdf (arxiv.org)">1406.2661.pdf (arxiv.org)</a>（早期写的）</p><h1 id="1-标题-作者"><a href="#1-标题-作者" class="headerlink" title="1.标题 + 作者"></a>1.标题 + 作者</h1><p>近 5 年，GAN 上头条次数很多，Reddit 里 GAN 很火</p><p>使用GAN生成人脸网址：<a href="http://thispersondoesnotexist.com" title="thispersondoesnotexist.com">thispersondoesnotexist.com</a></p><p>加州法令：禁止换脸、禁止对政治人物骚操作，说未讲过的话</p><p><strong>GAN: 两个网络相互对抗</strong></p><ul><li>generative: ML模型分 discriminative(AlexNet, ResNet, Transformer) 和 generative</li><li>adversarial: 对抗</li><li>nets: networks 简写，非 native speaker 不建议使用简写</li></ul><h1 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a>2.摘要</h1><p>写作简洁，可直接搬运（wiki, textbook）</p><ul><li>创新工作：讲清楚自己是谁？</li><li>拓展工作：和别人的区别、创新</li></ul><p>本文的 framework ：estimating generative models via an adversarial process；simultaneously train two models 生成器G、判别器D</p><ul><li>G：让 D 犯错；让生成数据 尽可能靠近 原始分布；</li><li>D：</li></ul><p>在任意函数 G 和 D 的空间里，a unique solution exists，独一无二的解表明 G 找到了真实数据分布，D 无法分辨。</p><p>G 和 D 是 MLP，error backpropagation 训练，简单。无需 Markov chains or unrolled approximate inference networks 近似推理过程的展开。</p><h1 id="3-导言"><a href="#3-导言" class="headerlink" title="3.导言"></a>3.导言</h1><p>深度学习不等于深度神经网络，深度神经网络只是深度学习的其中之一。深度学习是对各种数据特征的分布（概率）表示（represent probability distributions）。</p><p>discriminative 模型发展很好，<strong>generative 模型发展困难、Why?</strong> difficulty in maximum likelihood estimation 计算最大似然概率的近似时，计算困难。</p><p>本文解决了generative模型发展的困难：不近似likelihood 函数，使用别的方法计算 得到更好的模型。</p><p>“adversarial nets” framework里面的G和D都是MLP；G的输入是随机噪声（random noise），映射到任何一个想拟合的分布。G和D使用MLD，不使用 Markov chains 分布采样，使用 backpropagation 训练网络，有计算上的优势。</p><h1 id="4-相关工作"><a href="#4-相关工作" class="headerlink" title="4.相关工作"></a>4.相关工作</h1><h2 id="4-1-早期生成模型"><a href="#4-1-早期生成模型" class="headerlink" title="4.1 早期生成模型"></a>4.1 早期生成模型</h2><p>构造带有参数的概率分布函数（“provided a parametric specification of a probability distribution function”），去逼近真实的分布函数，通过maximizing the log-likelihood去训练此概率分布函数。例如：“Boltzmann machine” 。</p><p><strong>存在问题</strong>：</p><ul><li>需要提前知道这个分布，才能构造带参的概率分布函数</li><li>计算这个分布函数困难，维度比较高</li></ul><p><strong>如何解决：</strong></p><ul><li>不需要提前知道这个分布，直接学习这个模型，近似数据分布</li></ul><p>Boltzmann machine 和 GAN 的 generative models 有什么区别</p><ul><li>Boltzmann machine：一定要学习一个分布，知道分布的均值、方差等一系列参数</li><li>GAN：直接学习一个模型，不需要知道其他参数</li></ul><p>GAN计算容易，但不知道真实的分布是什么样的。</p><h2 id="4-2-误差反向传递对GAN求解"><a href="#4-2-误差反向传递对GAN求解" class="headerlink" title="4.2 误差反向传递对GAN求解"></a>4.2 误差反向传递对GAN求解</h2><p>为什么误差传递对GAN有效：对f的期望起到等价于对F自己求导</p><script type="math/tex; mode=display">\lim\limits_{\sigma\to0}\nabla_{\boldsymbol{x}}\mathbb{E}_{\epsilon\sim\mathcal{N}(0,\sigma^2I)}f(\boldsymbol{x}+\epsilon)=\nabla_{\boldsymbol{x}}f(\boldsymbol{x}).</script><h2 id="4-3-相关模型"><a href="#4-3-相关模型" class="headerlink" title="4.3 相关模型"></a>4.3 相关模型</h2><h3 id="（1）VAEs"><a href="#（1）VAEs" class="headerlink" title="（1）VAEs"></a>（1）VAEs</h3><p>VAEs：使用随机反向传播训练“variational autoencoders” ，类似于GAN，但是在第二个神经网络不同，VAEs使用识别模型，进行近似推理（“recognition model that performs approximate inference” ）；GANs需要差异化数据，因此不能有模型离散数据。</p><h3 id="（2）NCE（“Noise-contrastive-estimation”-）"><a href="#（2）NCE（“Noise-contrastive-estimation”-）" class="headerlink" title="（2）NCE（“Noise-contrastive estimation” ）"></a>（2）NCE（“Noise-contrastive estimation” ）</h3><p>NCE：训练生成模型通过学习权重,使模型适用于不同的数据从一个固定的噪声分布。使用以前训练模型的噪声分布允许训练序列模型提高质量。</p><p>NCE的不足：关键指标的限制是其“鉴别器”被定义为噪声的概率密度分布的比例和分布的模型,因此需要通过评估和backpropagate密度的能力。</p><p>NCE 的损失函数复杂一点 —&gt; 求解性能不如 GAN</p><h3 id="（3）PM（“predictability-minimization”-）"><a href="#（3）PM（“predictability-minimization”-）" class="headerlink" title="（3）PM（“predictability minimization” ）"></a>（3）PM（“predictability minimization” ）</h3><p>Jürgen（LSTM作者） predictability minimization算法被埋没了，Jürgen 认为 GAN 是 reverse PM。</p><p>PM和GAN主要有三点区别：</p><ol><li>网络之间的竞争是唯一的，但训练网络是统一的。PM只是隐藏单元，鼓励一个神经网络统计独立完成的一些任务；</li><li>两个网络之间竞争的本质是不一样的。在PM中，两个网络的输出进行比较,一个网络试图使输出相似，另一个试图让输出不同，输出是一个标量。在GANs中，一个网络产生一个丰富、高维向量作为输入到另一个网络，并试图选择一个输入，其他网络不知道如何处理。</li><li>学习过程不同。PM被描述为一个优化问题的目标函数是最小化，学习最小的目标函数。GANs基于极大极小的策略，而不是一个优化的问题,和有一个价值函数,一个智能体寻求最大化和其他试图最小化。</li></ol><h3 id="（4）“adversarial-examples”"><a href="#（4）“adversarial-examples”" class="headerlink" title="（4）“adversarial examples”"></a>（4）“adversarial examples”</h3><p>adversarial examples 是基于例子发现通过使用梯度优化直接在输入一个分类网络,为了找到类似于数据分类错误的例子。</p><p>这种adversarial examples 的存在确实表明生成对抗网络训练可能是低效的，因为它们表明，可以使现代辨别网络自信地承认一个类不模仿任何human-perceptible属性的类。</p><h1 id="5-模型"><a href="#5-模型" class="headerlink" title="5.模型"></a>5.模型</h1><h2 id="5-1-G和D都是MLP"><a href="#5-1-G和D都是MLP" class="headerlink" title="5.1 G和D都是MLP"></a>5.1 G和D都是MLP</h2><ul><li>G：输入为随机噪声 $p_z(z)$ ，维度不确定，接近真实分布x的维度；在MLP中通过$G(z; \theta_g)$ 学习参数 $\theta_g$ ,输出生成的数据。把 <strong>z</strong> 映射成 <strong>x</strong></li><li>D：输入为G生成的数据或真实数据x；在MLP中通过$D(x;\theta_d)$ 学习参数$\theta_d$ ，输出为一维标量，若为1，则这张图片来自真实采样数据x；若为0，则输入来自生成模型。</li></ul><p>使用MLP的利弊：计算简单、但不真正分布。看到一个图片，很难找到对应的 z；只能反向操作，随机给一个 z ，生成一个像样的图片。</p><p>同时训练D和G：</p><ul><li>训练D 正确分类的概率：真实采样数据和来自G生成的数据</li><li>训练G 尽可能使D犯错，D(G(x))更大，更靠近1</li></ul><h2 id="5-2-value-function-V-G-D"><a href="#5-2-value-function-V-G-D" class="headerlink" title="5.2 value function V (G, D)"></a><strong>5.2</strong> value function V (G, D)</h2><script type="math/tex; mode=display">\min_G\max_DV(D,G)=\mathbb{E}_{x\sim p_{\mathrm{ditz}}(n)}\left[\log D(x)\right]+\mathbb{E}_{z\sim p_{x}(z)}\left[\log(1-D(G(z)))\right]</script><p>D 判断此图是生成的，$D(G(Z)) = 0,log( 1 - D(G(Z)) ) = log1 = 0$</p><p>D 判断此图是真实采样，$D(G(Z)) = 1, log( 1 - D(G(Z)) ) = log0 = -∞$</p><p>value function 有 min G max D 的双目标：（two-player minimax game Nash equilibrium：D和G都不能往前进步）</p><ul><li>D 使数据尽量分开</li><li>G 使数据尽量分不开</li></ul><p>min G 尽可能得使 D 判别器 犯错，无法判断来自 真是采样 or 生成数据。</p><p>训练 G 使得 D 无法区别 是真实采样数据 还是生成数据。</p><ul><li>完美情况：$log 1 + log 1 = 0$</li><li>不完美情况：$log(0 - 1) + log(0 -1) &lt; 0$ ——&gt; maximize D 正确分类的概率，objective 靠近 0，靠近完美情况</li></ul><h2 id="5-3-训练过程"><a href="#5-3-训练过程" class="headerlink" title="5.3 训练过程"></a>5.3 训练过程</h2><p><img src="image/image_z5Cyn28BbO.png" alt=""></p><p>绿色实线：G对噪声z的映射</p><p>蓝色虚线：D在数据空间上的判断线</p><p>黑色虚线：数据x的真实分布</p><p>训练过程</p><ul><li>step 1: G 把 均匀分布采样得到的噪声 z 映射到 绿色高斯分布</li><li>step 1 —&gt; 2: D 尽可能地去学习如何判断，i.e., 真实数据均值更靠左，将左侧的数据判断为真 1， 将右侧数据判断为假 0</li><li>step 2 —&gt; 3：G 尽可能地去糊弄 D，把高斯分布的均值中心往左挪动</li><li>final step: G 把 来自均匀分布的噪声，映射成和真实 data <strong>x</strong> 相似的高斯分布。D 对来自真实采样的数据 or G 生成的数据无法判断来源，概率都是 0.5，躺平———-</li></ul><h2 id="5-4-算法"><a href="#5-4-算法" class="headerlink" title="5.4 算法"></a>5.4 算法</h2><p><img src="image/image_kcWHFgMCSj.png" alt=""></p><h2 id="5-5-一些问题"><a href="#5-5-一些问题" class="headerlink" title="5.5 一些问题"></a>5.5 一些问题</h2><h3 id="（1）G迭代轮次k"><a href="#（1）G迭代轮次k" class="headerlink" title="（1）G迭代轮次k"></a>（1）G迭代轮次k</h3><p>k 不能太小 —&gt; 保证 判别器 D 有足够的更新</p><ul><li>D 要对 G 生成的不错的数据 有一定的判别能力，不然 G 很无聊的，都不想糊弄 D，轻而易举。</li><li>警察不给力，假钞不会被发现，抓不到造假者，无需提升工艺，游戏结束。</li></ul><p>k 不能太大 —&gt; D 过于完美，D(G(<strong>z</strong>^(i))) = 0，对值为 0 求导，G 梯度更新有困难</p><ul><li>警察超厉害，造假者产一点假钞就会被发现，端掉制造工厂；造假者赚不到钱，不能提升工艺，结束游戏。</li></ul><p>最好情况：双方实力相当、相爱相杀、一起进步。k 的设计目标：使得 D 和 G 的更新进度差不多</p><h3 id="（2）判断GAN收敛？"><a href="#（2）判断GAN收敛？" class="headerlink" title="（2）判断GAN收敛？"></a>（2）判断GAN收敛？</h3><p>min max 两项：一方不动、一方动；双方都在相互抖动；有很多 GAN 收敛相关的研究工作。</p><h3 id="（3）G的梯度更新问题"><a href="#（3）G的梯度更新问题" class="headerlink" title="（3）G的梯度更新问题"></a>（3）G的梯度更新问题</h3><p>早期 G 比较弱，生成数据和真实采样的数据差距很大。—&gt; D 判别器 很容易区分数据来源。 —&gt; $log( 1 - D(G(z)) )$ 变为 $log(1 - 0) = 0$ —&gt; 无法更新判别器 D 的参数。</p><p>改变 value function：$min log( 1 - D(G(z)) )$ —&gt; $max log( D(G(z)) )$</p><p>如果 D 判别得很好，$log( D(G(z)) ) = log( 0 )= -∞$ 带来其它问题，有后续的研究工作改进。</p><h1 id="6-理论"><a href="#6-理论" class="headerlink" title="6.理论"></a>6.理论</h1><h2 id="6-1-Proposition-1：全局最优解-p-g-p-data"><a href="#6-1-Proposition-1：全局最优解-p-g-p-data" class="headerlink" title="6.1 Proposition 1：全局最优解 $ p_g = p_{data} $"></a>6.1 Proposition 1：全局最优解 $ p_g = p_{data} $</h2><p>当固定G时，全局最优辨别器D为：$D^*G(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$</p><p>value function 的全局最优解 $p_g = p_{data}$ , 当且仅当 G 生成器学到的分布和真实数据分布一致。</p><ul><li>$p_{data}(x)$ : <strong>x</strong> 在真实数据分布中的概率 [0,1]</li><li>$p_g(x)$ : <strong>x</strong> 在生成器拟合的分布中的概率[0,1]</li><li>global optimum: $D_G^*(x) = 1 / 2$</li></ul><p>D 的训练过程：从真实数据分布和噪音变量的先验分布中采样数据，用 value function 训练一个二分类的分类器，但分类器输出的概率一直是 1 / 2 —&gt; D 什么都分不出来 —&gt; 分布是重合的</p><p><strong>two sample test 判断两块数据是否来自同一分布。</strong></p><ul><li>1 / 2 的启示：高维数据统计分布不好用时，训练一个分类器。</li><li>这个分类器能分开两块数据 —&gt; 不同分布</li><li>这个分类器不能分开两块数据 —&gt; 相同分布</li><li>分类器的应用：部署 训练集上训练好的模型 到另一个环境，用一个分类器检测 训练集和测试集 是否同一分布，避免部署的新环境和已有模型的不匹配。</li></ul><h2 id="6-2-Theorem-1"><a href="#6-2-Theorem-1" class="headerlink" title="6.2 Theorem 1"></a>6.2 <strong>Theorem 1</strong></h2><p>C(G) 取全局最小值时，if and only if 生成器 G 生成的数据分布和真实数据分布一样 p_g = p_data</p><p><img src="image/image_-BrC1zTJlu.png" alt=""></p><h2 id="6-3-Proposition-2"><a href="#6-3-Proposition-2" class="headerlink" title="6.3 Proposition 2 :"></a>6.3 Proposition 2 :</h2><p>当 G 和 D 有足够的容量的时候，在算法 1 的每一步，判别器 D 是可以达到最优解的，对 生成器 G 的优化：判别器 D 已经换成了 $D_G^*$ ，p_g会逼近p_data</p><script type="math/tex; mode=display">\mathbb{E}_{\boldsymbol{x}\sim p_{d a a}}[\log D_{G}^{*}(x)]+\mathbb{E}_{\boldsymbol{x}\sim p_{g}}[\log(1-D_{G}^{*}(x))]</script><h1 id="7-实验-总结"><a href="#7-实验-总结" class="headerlink" title="7.实验 + 总结"></a>7.实验 + 总结</h1><p>最后一列是GAN生成的</p><p><img src="image/image_OdX15IXI5k.png" alt=""></p><p>disad：训练难，G 和 D 需要比较好的均衡，否则生成的图片很难看。</p><p>adv：G 没有去看真实数据，没有试图拟合数据特征，生成的图片中边缘比较锐利。 ❌</p><h1 id="8-评价"><a href="#8-评价" class="headerlink" title="8.评价"></a>8.评价</h1><h2 id="8-1-写作：清晰明确，GAN-在做什么？"><a href="#8-1-写作：清晰明确，GAN-在做什么？" class="headerlink" title="8.1 写作：清晰明确，GAN 在做什么？"></a><strong>8.1 写作：清晰明确，GAN 在做什么？</strong></h2><p>Abstract: GAN 在干什么？</p><p>Intro: 短 + 故事性（why 需要 GAN，无需拟合似然函数的参数、计算简单）</p><p>Related works: GAN 与已有工作 adversarial examples 的区别。真正伟大的工作：不在于你的那些想法是否出现过，而在于你给大家展示，这个工作可以取得非常好的应用；让别人信服，有人 follow，把这个领域做大做强。</p><p>adversarial nets: value function 如何做优化</p><p>theoretical results: 证明 value function 为什么能得到最优解，求解算法 algorithm 1 差不多能得到最优解</p><p>experiments + pros &amp; cons + future works</p><p>开创性比较高的工作：适合本文的写法</p><p>创新度不够的工作：讲清楚和别人的区别、文章的贡献</p><h2 id="8-2-GAN-算法评论："><a href="#8-2-GAN-算法评论：" class="headerlink" title="8.2 GAN 算法评论："></a><strong>8.2 GAN 算法评论：</strong></h2><ul><li>无监督学习，无需标注数据</li><li>标签 by 数据来自真实采样 or 来自生成器拟合的</li><li>有监督学习的损失函数来训练无监督学习，训练高效</li><li>自监督学习的灵感来源 i.e., BERT</li></ul>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PaperReading </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 MAE</title>
      <link href="/paper_reading/2.8.MAE/"/>
      <url>/paper_reading/2.8.MAE/</url>
      
        <content type="html"><![CDATA[<h1 id="8-MAE"><a href="#8-MAE" class="headerlink" title="8.MAE"></a>8.MAE</h1><p>Masked Autoencoders Are Scalable Vision Learners</p><p>MAE：CV版的BERT</p><ul><li>论文链接：<a href="https://arxiv.org/pdf/2111.06377.pdf" title="https://arxiv.org/pdf/2111.06377.pdf">https://arxiv.org/pdf/2111.06377.pdf</a></li><li>论文代码：<a href="https://github.com/facebookresearch/mae" title="https://github.com/facebookresearch/mae">https://github.com/facebookresearch/mae</a></li><li>李沐讲解：<a href="https://www.bilibili.com/video/BV1sq4y1q77t" title="MAE 论文逐段精读【论文精读】_哔哩哔哩_bilibili">MAE 论文逐段精读【论文精读】_哔哩哔哩_bilibili</a></li></ul><p>MAE 2021.11.11提交 arxiv</p><p>知乎 百万 view; Reddit or Twitter 讨论不多</p><p>MAE 很新 —&gt; 如何在读比较新的文章 获取一些新的研究思路？</p><h1 id="0-和之前精读论文的关系？"><a href="#0-和之前精读论文的关系？" class="headerlink" title="0.和之前精读论文的关系？"></a>0.<strong>和之前精读论文的关系？</strong></h1><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><ul><li>一个<strong>纯基于注意力机制的编码器和解码器</strong></li><li>表现比 RNN 架构好，在机器翻译任务</li></ul><h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><ul><li>使用 一个 Transformer 编码器，拓展到更一般的 NLP 的任务</li><li>使用了 <strong>完型填空</strong> 的自监督的训练机制，不需要使用标号，去预测一个句子里面 不见 masked 的词 ，从而获取对文本特征抽取的能力</li><li>BERT 极大的扩展了 Transformer 的应用，在一个大规模的、没有标号的数据上 训练出非常好的模型出来</li></ul><h3 id="ViT"><a href="#ViT" class="headerlink" title="ViT"></a>ViT</h3><ul><li>将 Transformer 用到 CV 上面</li><li>把整个图片分割成很多 16 * 16 的小方块，每一个方块 patch 做成一个词 token，然后放进 Transformer 进行训练</li><li>证明：<strong>训练数据足够大</strong> （1,000万 或者 一个亿的训练样本）的时候，<strong>Transformer 的架构 精度 优于 CNN 架构</strong></li></ul><h3 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h3><ul><li>BERT 的一个 CV 的版本，<strong>基于 ViT ，BERT化</strong>，把整个训练拓展到没有标号的数据上面，<strong>通过完型填空来获取图片的一个理解</strong></li><li>MAE不是第一个将 BERT 拓展到 CV 上；但MAE 很有可能 未来影响最大</li><li><strong>BERT 加速了 Transformer 架构 在 NLP 的应用</strong></li><li><strong>MAE 加速 Transformer 在 CV 上的应用</strong></li></ul><p><img src="image/image_1OPUOhzy5W.png" alt=""></p><h1 id="1-标题-作者"><a href="#1-标题-作者" class="headerlink" title="1.标题 + 作者"></a><strong>1.标题 + 作者</strong></h1><h2 id="1-1-标题"><a href="#1-1-标题" class="headerlink" title="1.1 标题"></a>1.1 标题</h2><p>Masked Autoencoders Are Scalable Vision Learners</p><p>Masked Autoencoders 带掩码的自编码器 是可扩展的视觉学习器 scalable vision learners</p><p><strong>scalable</strong>：可扩展的，模型比较大</p><p><strong>efficient</strong>：算法特别快</p><p><strong>vision learners</strong>：≠ classifier，一个 backbone 的模型</p><p><strong>masked</strong>：来源于 BERT</p><ul><li>BERT 是 masked language model，带掩码的语言模型；完形填空</li><li>每次挖掉一些东西，然后去预测挖掉的东西</li></ul><p><strong>Auto-encoder</strong>：</p><ul><li>auto “自”，ML模型 auto 自模型；i.e., 自回归模型</li><li><strong>标号 y 和 样本 x 来自 同一个东西</strong>；语言模型中，每一次用前面那些词，去预测接下来的一个词；在另外一个样本里面，我们这个预测的词也是标号，也是成为另外一个样本的 x 本身</li><li>样本 x 和 标号 y 来自于同样的句子里面的词 —&gt; auto</li><li>NLP 里，语言模型是一大类，有没有 auto 大家都理解；Transformer、BERT 用的是 encoder，加不加 auto 大家都觉得没问题</li><li>但在CV 里 auto 自任务很少，因为图片（像素）的标号很少来自于图片本身，图片的标号更多是文本；encoder-decoder 在图片里用得很多，<strong>加 auto 在 encoder之前，MAE 的图片标号是图片本身，区分于其它工作</strong></li></ul><h2 id="1-2-标题模板"><a href="#1-2-标题模板" class="headerlink" title="1.2 标题模板"></a>1.2 <strong>标题模板</strong></h2><p><strong>强有力的句式</strong>：文章的结论总结成一句话，<strong>XX 是 好 XX</strong>，<strong>结论放在 title 里，句式相对客观</strong></p><p>i.e., GPT 系列 3篇文章</p><ul><li><strong>GPT1</strong>： Improving Language Understanding by Generative Pre-Training (Generative Pre-Train Model 就是GPT模型的名字由来）</li><li><strong>GPT2</strong>: Language Models are Unsupervised Multitask Learners</li><li><strong>GPT3</strong>: Language Models are Few-Shot Learners</li></ul><p><strong>Example</strong>：</p><p>我的手写笔是世界上 最好用来做笔记的笔 ❌</p><p>手写笔是一个做笔记很好的工具 ✔</p><ul><li>虽然手写笔是我第一个发明的</li><li>从读者角度讲述，手写笔为什么它是一个好东西？</li></ul><h2 id="1-3作者"><a href="#1-3作者" class="headerlink" title="1.3作者"></a>1.3<strong>作者</strong></h2><p>FAIR</p><p>一作：Kaiming 恺明 ResNet 一作 + Project lead （从公司拿资源做项目、管理者；不是一般最后的大老板）</p><p>*：equal technial contribution</p><p>最后两位作者：CV 物体检测的大佬</p><h1 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a><strong>2.摘要</strong></h1><h3 id="扩展-title-MAE"><a href="#扩展-title-MAE" class="headerlink" title="扩展 title MAE"></a><strong>扩展 title MAE</strong></h3><p>Masked Autoencoders Are Scalable Vision Learners —&gt; Masked AutoEncoders are scalable self-supervised learners for computer vision.</p><h3 id="MAE-简单"><a href="#MAE-简单" class="headerlink" title="MAE 简单"></a><strong>MAE 简单</strong></h3><p><strong>随机盖住图片里的一些块(patch, image 的一个块)，再重构 缺失的像素</strong>。</p><ul><li>思想来自于 BERT 带掩码的语言模型</li><li>这里不是 masked 词，而是图片的一个块 image，重构这个缺失块里面的所有像素</li></ul><h3 id="2个-core-designs"><a href="#2个-core-designs" class="headerlink" title="**2个 **core designs"></a>**2个 **<strong>core designs</strong></h3><h4 id="（1）asymmetric-encoder-decoder-architecture"><a href="#（1）asymmetric-encoder-decoder-architecture" class="headerlink" title="（1）asymmetric encoder-decoder architecture"></a>（1）<strong>asymmetric encoder-decoder architecture</strong></h4><ul><li>虽然 MAE 说自己是 masked autoencoder；任何一个模型都有一个编码器解码器；BERT 预测相对简单， 解码器：最后的一个全连接输出层</li><li>MAE 预测一个块里面的所有像素</li><li>MAE 的编码器 只关注 可见的 patches，节省计算时间</li><li>如果 一个 patch 被丢掉了，编码器不会对它进行编码</li><li>MAE a lightweight decoder 重构原始的像素</li></ul><h4 id="（2）MASK部分块，做自监督学习"><a href="#（2）MASK部分块，做自监督学习" class="headerlink" title="（2）MASK部分块，做自监督学习"></a>（2）MASK部分块，做自监督学习</h4><p><strong>遮住大量的块 (i.e., 75%) 重构像素是一个非显然 nontrivial and meaningful 有意义的 self-supervisory task 自监督任务</strong></p><ul><li>如果只遮住几块的话，插值法就可以得到缺失的像素，</li><li>遮住一大半的块，迫使模型学习更好的表征</li></ul><h3 id="更有效训练模型"><a href="#更有效训练模型" class="headerlink" title="更有效训练模型"></a>更有效训练模型</h3><p><strong>asymmetric encoder-decoder + 75% masked —&gt; train large models efficiently and effectively</strong></p><ul><li>大：比较有挑战的任务，不是求显然的解 nontrivial</li><li><strong>块</strong>：不看遮住的块，训练量是 1 / 4，加速 3 倍 or 以上</li></ul><h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><h4 id="（1）-ViT-Huge-模型-在-ImageNet-1K"><a href="#（1）-ViT-Huge-模型-在-ImageNet-1K" class="headerlink" title="（1） ViT-Huge 模型 在 ImageNet-1K"></a>（1） ViT-Huge 模型 在 ImageNet-1K</h4><p>最简单的 a vanilla ViT-Huge 模型 在 ImageNet-1K 87.8% 准确率</p><ul><li>ViT 的自监督学习，效果不那么好，所以没怎么展开</li><li>ViT 的作者认为还是需要 有标号的模型、用更大的训练集，效果更好</li><li>MAE 使用小数据集 ImageNet-1K 100w 图片，self-supervise 效果很好</li></ul><h4 id="（2）迁移学习效果"><a href="#（2）迁移学习效果" class="headerlink" title="（2）迁移学习效果"></a><strong>（2）迁移学习效果</strong></h4><p>MAE 主要用来做 迁移学习，在别的任务上表现好</p><p>shows promising scaling behavior</p><h1 id="3-关键图"><a href="#3-关键图" class="headerlink" title="3.关键图"></a><strong>3.关键图</strong></h1><h2 id="3-1-MAE结构图"><a href="#3-1-MAE结构图" class="headerlink" title="3.1 MAE结构图"></a>3.1 MAE结构图</h2><p>CV 最重要的一张图就是放在第一页的右上角</p><p><img src="image/image_W4Ue-sNM9A.png" alt=""></p><p><strong>预训练流程</strong>：input —&gt; patches —&gt; masked —&gt; unmasked patches in encoder —&gt; unmasked + masked 按位置排列 进 decoder —&gt; decoder 重构 masked patches 的像素</p><ul><li><strong>patches + masked</strong>：一张红色鸟图片进来，切成 patches，masked 块 (3/4) 是 灰色的。</li><li><strong>unmasked patches，encoder</strong>：没有 masked (1 / 4) 的块 进入 encoder (ViT)，得到每一块的特征（蓝色）。</li><li>encoder 的输出 和 masked tokens 按照在图片中的原始位置排列成一长条向量 （包含位置信息）。</li><li>长条向量 进入 decoder，解码器尝试重构缺失的像素信息，还原原始图片</li></ul><p><strong>encoder 比 decoder 高</strong>：计算量主要来自于 encoder，对图片的像素进行编码</p><p>优化 encoder by 编码器只用处理 unmasked patches，i.e., 一张图里 1/4 的像素，—&gt; 计算量降低</p><ul><li>Transformer 模型计算量特别大，几倍加速也很重要。</li></ul><p><strong>什么情况不需要解码器？</strong> 用 MAE 做一个 CV 的任务，只需要用编码器。一张图片进来，不需要做掩码，直接切成 patches 格子块，然后得到所有 patches 的特征表示，当成是这张图片的特征表达，用来做 CV 的任务。</p><h2 id="3-2-测试图片"><a href="#3-2-测试图片" class="headerlink" title="3.2 测试图片"></a>3.2 测试图片</h2><p><strong>图2：ImageNet 测试集图片</strong></p><p>三列分别是：80% masked tokens, MAE 重构的效果，ImageNet 真实图片</p><p><img src="image/image_eiECB2Wc86.png" alt=""></p><p>虽然细节有一点模糊，钟的指针、车的形状、狗、灯都还原的很好。</p><ul><li>图片尺寸只有那么高，分辨率有限</li></ul><p>Note： MAE 不一定对 所有图片的构造都那么好，图 2 是展示比较好的样例</p><p><strong>图3：COCO</strong></p><p>不同的数据集，效果也惊人。</p><p><img src="image/image_T6CHsManaq.png" alt=""></p><p>**图4 **同一张图片、masked patches 的不同比例 的还原效果</p><p>95%效果惊人，蘑菇🍄、斑马🦓、车🚗、西红柿 都还原出来了。</p><p><img src="image/image_HMe3yw7iJZ.png" alt=""></p><h1 id="4-结论"><a href="#4-结论" class="headerlink" title="4.结论"></a><strong>4.结论</strong></h1><p><strong>Simple algorithms that scale well are the core of deep learning</strong>. 简单 + 可以拓展很好的算法是 DL 的核心</p><p><strong>simple</strong>：作者的简单是在 ViT 基础上，MAE 提出来的东西相对简单</p><p><strong>scale well</strong>：能跑 大数据集</p><p>在NLP 领域自监督学习比较火，而在CV 里 有标号的预训练数据是主流。<strong>MAE 在 ImageNet 数据集上，通过自编码器学习到 可以媲美 有标号的 结果。</strong></p><p><strong>图片和语言的差别</strong></p><ul><li>a word in a sentence：一个词是语义单元，包含较多语义信息</li><li>a patch in an image：一定的语义信息，但不是一个语义的 segment，一个 patch 并不含有一个特定的物体，可能是多个物体的一小块 or 一个物体重叠的一块</li><li>即使图片和语言的 masked 的单元包含语义信息不同，MAE or Transformer 可以学到一个隐藏的比较好的语义表达</li></ul><p><strong>broader impacts</strong></p><p>如果工作出圈，对社会的影响？</p><ul><li>只用了图片本身信息学习，图片本身有 bias 的话，倾向于某一些图片 or 有一些不好的图片，可能会有负面的社会影响</li><li><strong>MAE 可以用来生成不存在的内容</strong>，MAE 是生成模型，生成原始的像素，和 GAN 类似，有误导大家的可能</li><li>如果要使用这个工作，请一定要考虑潜在的影响</li></ul><h1 id="5-导言"><a href="#5-导言" class="headerlink" title="5.导言"></a><strong>5.导言</strong></h1><h2 id="5-1-问题-amp-其他领域解决方案"><a href="#5-1-问题-amp-其他领域解决方案" class="headerlink" title="5.1 问题 &amp; 其他领域解决方案"></a>5.1 <strong>问题</strong> &amp; 其他领域解决方案</h2><p><strong>问题所在</strong>：深度学习飞速发展，但 CV 仍然以来百万级别、甚至更多有标注的数据</p><p><strong>大量有标注数据是必须的吗？其它领域怎么解决的？</strong></p><p><strong>NLP 的自监督学习很好</strong></p><ul><li>GPT、BERT 可以使用 无标注 的数据及逆行训练，得到千亿级别可学习的参数模型</li><li>GPT 系列，一个标准的语言模型</li><li>BERT 一个带掩码的自编码模型</li></ul><p><strong>CV 里已有的 maksed autoencoder 带掩码的自编码器</strong></p><ul><li>denoising autoencoder，一张图片里加入很多噪音，通过去噪来学习对这张图片的理解</li><li>最近也有很多工作将 BERT 应用于 CV</li></ul><p>但，作者认为 BERT 在 CV 领域的应用落后于 NLP。</p><h2 id="5-2-什么使得带掩码的自编码器模型在-CV-和-NLP-处理上的不一样呢"><a href="#5-2-什么使得带掩码的自编码器模型在-CV-和-NLP-处理上的不一样呢" class="headerlink" title="5.2 什么使得带掩码的自编码器模型在 CV 和 NLP 处理上的不一样呢"></a>5.2 什么使得带掩码的自编码器模型在 CV 和 NLP 处理上的不一样呢</h2><p><strong>What makes masked autoencoding different between vision and language？</strong></p><h3 id="（1）CV-使用-CNN，卷积窗口不好将-mask-放进去"><a href="#（1）CV-使用-CNN，卷积窗口不好将-mask-放进去" class="headerlink" title="（1）CV 使用 CNN，卷积窗口不好将 mask 放进去"></a><strong>（1）CV 使用 CNN，卷积窗口不好将 mask 放进去</strong></h3><p><strong>archtectural gap has been addressed by ViT</strong></p><ul><li>CNN 在一张图片上，使用一个卷积窗口、不断地平滑，来汇聚一些像素上面的信息 + 模式识别</li><li>Transformer 的一个 mask 对应的是一个特定的词，会一直保留，和别的词区分开来</li><li><strong>卷积上做掩码？</strong> 图片的一块盖住 by 像素替换成一个特定的值，卷积窗口扫过来、扫过去时，无法区分边界，无法保持 mask 的特殊性，无法拎出来 mask；最后从掩码信息很难还原出来</li><li><strong>卷积不好加入位置编码？</strong> 不那么充分。Transformer 需要位置编码：attention 机制没有位置信息；卷积自带位置信息，不断平移时，不需要加入位置信息</li></ul><h3 id="（2）语言和图片的信息密度不同"><a href="#（2）语言和图片的信息密度不同" class="headerlink" title="（2）语言和图片的信息密度不同"></a><strong>（2）语言和图片的信息密度不同</strong></h3><p>NLP 的一个词是一个语义的实体，一个词在字典里有很长的解释；一句话去掉几个词，任务很难，i.e., 完形填空 —&gt; BERT 的 mask 比例不能过高</p><p><strong>CV 的图片去掉一个块，通过对邻居的像素值进行插值还原</strong>。<strong>怎么让任务不那么 trivial 呢？</strong></p><ul><li><strong>随机去掉很高比例的块</strong>，极大降低图片的冗余性</li><li>这一块的周边块都被去掉了，这一块和很远的块的关系不那么冗余</li><li>nontrivial 任务，<strong>使 模型去看 一张图片的 holistic 全局信息，而不仅关注局部</strong></li></ul><h3 id="（3）The-autoencoder’s-decoder"><a href="#（3）The-autoencoder’s-decoder" class="headerlink" title="（3）The autoencoder’s decoder"></a>（3）The autoencoder’s decoder</h3><p>CV 还原图片的原始像<strong>素</strong>：低层次的表示</p><p><strong>NLP 还原句子里的词</strong>：语义层次更高，i.e., BERT 的一个全连接层还原词</p><p><strong>图片分类、目标检测的 decoder</strong>：一个全连接层</p><p><strong>语义分割（像素级别的输出）</strong>：一个全连接层不够，很有可能使用一个转置的卷积神经网络、来做一个比较大解码器。</p><h2 id="5-3-MAE-的想法"><a href="#5-3-MAE-的想法" class="headerlink" title="5.3 MAE 的想法"></a>5.3 <strong>MAE 的想法</strong></h2><p><strong>随机遮住大量的块，然后去重构这些被遮住的像素信息</strong>，让它使用一个非对称的编码器和解码器的机制。</p><p><strong>非对称</strong>：编码器和解码器看到的东西不一样</p><ul><li>编码器只看到可见块</li><li>解码器拿到编码器的输出之后，重构 masked patches</li></ul><p><strong>非对称的原因</strong>：<strong>大量 masked 块；编码器只看可见块，极大降低计算开销、减少内存消耗</strong></p><h2 id="5-4实验结果"><a href="#5-4实验结果" class="headerlink" title="5.4实验结果"></a><strong>5.4实验结果</strong></h2><p>MAE预训练，只使用 ImageNet-1K 100w 无标号数据，ViT-Large/-Huge 达到 ViT 需要 100倍于 ImageNet-1K 的数据 的效果。</p><p>迁移学习效果也很好，预训练的模型在 目标检测、实例分割、语义分割 的效果都很好。</p><p>和 NLP 类似的效果：在大量的没有标号的数据上，通过自监督学习训练出来模型，迁移学习效果不错</p><h2 id="5-5-导言总结"><a href="#5-5-导言总结" class="headerlink" title="5.5 导言总结"></a>5.5 导言总结</h2><p>2页，图片 + 使用了 问题 - 回答问题 - 引出想法 的写法</p><p><strong>更本质的问题？</strong></p><p>把 BERT 从 NLP 用到 CV 有什么问题？</p><p>MAE 算法为什么要设计成这样？</p><ul><li>ViT 解决 图片中的 mask</li><li>大量随机 masked 块，降低图片冗余度</li><li>非对称的自编码器-解码器</li></ul><p><strong>写作建议：</strong>&#x20;</p><p><strong>讲清楚，你为什么要这么做？你对这个问题的动机？</strong></p><ul><li>没有动机 就是技术报告了，i.e. AlexNet</li></ul><h1 id="6-相关工作"><a href="#6-相关工作" class="headerlink" title="6.相关工作"></a><strong>6.相关工作</strong></h1><h2 id="6-1带掩码的语言模型："><a href="#6-1带掩码的语言模型：" class="headerlink" title="6.1带掩码的语言模型："></a>6.1<strong>带掩码的语言模型</strong>：</h2><p>BERT, GPT</p><h2 id="6-2自编码器在-CV-的应用"><a href="#6-2自编码器在-CV-的应用" class="headerlink" title="6.2自编码器在 CV 的应用"></a><strong>6.2自编码器在 CV 的应用</strong></h2><ul><li>MAE 也是一种形式的 带去噪的自编码</li><li>masked patch 在这一个图片块里面加了很多噪声</li><li>和 传统的 DAE(Denoising autoencoder) 是很不一样的</li><li>MAE 基于 ViT、transformer 架构</li></ul><h2 id="6-3带掩码的自编码器在-CV-的应用"><a href="#6-3带掩码的自编码器在-CV-的应用" class="headerlink" title="6.3带掩码的自编码器在 CV 的应用"></a><strong>6.3带掩码的自编码器在 CV 的应用</strong></h2><ul><li>iGPT，GPT 在 image 的应用</li><li>ViT 最后一段，怎么用 BERT 方式训练模型</li><li>BEiT，BERT 在 image 上的应用</li><li>给每一个 patch 离散的标号，更像 BERT</li></ul><p>MAE 直接重构 原始的像素信息</p><h2 id="6-4-self-supervised-learning"><a href="#6-4-self-supervised-learning" class="headerlink" title="6.4 self-supervised learning"></a><strong>6.4 self-supervised learning</strong></h2><p>最近火 的 contrastive learning，使用数据增强</p><p>MAE 不需要数据增强（实验部分有讲）</p><p>相关工作总结：4个角度的对比，MAE 和它们都不同，但是<strong>没有特别讲 MAE 和每一个不一样的地方在哪里</strong>。</p><h1 id="7-MAE模型"><a href="#7-MAE模型" class="headerlink" title="7.MAE模型"></a><strong>7.MAE模型</strong></h1><h2 id="7-1-MAE总体架构"><a href="#7-1-MAE总体架构" class="headerlink" title="7.1 MAE总体架构"></a>7.1 MAE总体架构</h2><p>MAE 是一个简单的 自编码器：<strong>看到了部分的观察的数据，用观察到的部分数据 重构 完整的原始信号。</strong></p><p>所有自编码器的工作</p><ul><li>将观察到的信号 映射到一个潜在 latent 表示里面</li><li>潜在表示：语义空间的一个表示</li><li>解码器 用 潜表示 latent 重构原始信号</li></ul><p><strong>MAE 的自编码器 和 经典自编码器的不同？</strong></p><ul><li>asymmetric design 非对称结构</li><li>编码器只看 可见块</li><li>忽略不可见 masked 块，节省计算开销</li></ul><h2 id="7-2-掩码-mask"><a href="#7-2-掩码-mask" class="headerlink" title="7.2 掩码 mask"></a>7.2 <strong>掩码 mask</strong></h2><p>和 ViT 的一样图片 patches 化， i.e., 一张图片 九宫格分割，3 * 3，每一格 代表一个 patch，作为一个词 token。</p><p><strong>random sampling：</strong> 随机均匀采样块保留, 剩余块用 mask 掩码盖住。</p><p><img src="image/image_Kb5Ctp1-PG.png" alt=""></p><p><strong>MAE 的关键技术？</strong> 只采样少量的块，其余的块全覆盖，去掉图片 patches 之间的冗余度。—&gt; 增加任务的复杂度</p><h2 id="7-3-MAE-的编码器"><a href="#7-3-MAE-的编码器" class="headerlink" title="7.3 MAE 的编码器"></a>7.3 <strong>MAE 的编码器</strong></h2><ul><li>就是一个 ViT， 没有任何改动，只作用于 可见块</li></ul><p><strong>MAE 编码器如何作用于可见块呢？</strong></p><ul><li><strong>和 ViT 一样部分</strong>：每一个 patch 块拿出来，做一个线性投影；然后+ 位置信息 —&gt; token</li><li><strong>和 ViT 不一样</strong>部分：masked 块不进入 MAE 编码器。i.e., 随机采样概率 1 / 4， 25% 样本块进入 ViT，计算量减少</li></ul><h2 id="7-4-MAE-的解码器"><a href="#7-4-MAE-的解码器" class="headerlink" title="7.4 MAE 的解码器"></a>7.4 <strong>MAE 的解码器</strong></h2><p>要重构 masked 块的像素信息，需要看到 <strong>可见块（编码器对可见块的潜表示） 和 masked 块 （没有进入编码器）</strong>；<strong>通过一个共享的可以学到的向量来表示</strong>：每一个被盖住的块都表示成同样一个向量，此向量值可学习。</p><p><strong>解码器是另外一个 transformer</strong>，需要位置信息，不然无法区分对应哪一个 掩码masked tokens</p><p><strong>可见块的位置信息 question？</strong></p><ul><li>位置信息 要不要也对那些编码器过来的 潜在表示 也加上</li><li>因为可见块的潜表示其实本来已经加过一次了，那么这个地方要不要再加上一次？</li></ul><p><strong>解码器主要在pre-training用到</strong>；别的下游任务，解码器不需要，只需要编码器对图片编码得到潜表示 → 灵活，想用什么随便用</p><p><strong>解码器的架构大小</strong>：相对小，计算开销不到编码器的 1 / 10</p><h2 id="7-5-重构出原始的像素"><a href="#7-5-重构出原始的像素" class="headerlink" title="7.5 重构出原始的像素"></a><strong>7.5 重构出原始的像素</strong></h2><p>解码器的最后一层： a linear projection</p><ul><li>一个 patch 是 16 * 16 像素的话，线性层会投影到长为 256 的维度</li><li>再 reshape(16, 16), 还原原始像素信息</li><li>损失函数： MSE，像素值相减，再平方和</li><li>损失只作用于非可见块的损失，和 BERT 一样，可见块的图片编码器已经看到了，看到答案就不算正确率了</li></ul><p>对预测的像素做一次 normalization，使像素均值为 0 方差为 1，数值更稳定。</p><h2 id="7-6-简单实现"><a href="#7-6-简单实现" class="headerlink" title="7.6 简单实现"></a>7.6 <strong>简单</strong>实现</h2><ul><li>对每一个输入 patch 生成 a token：一个一个 patch 的线性投影 + 位置信息</li><li>随机采样：randomly shuffle 随机打断序列，把最后一块拿掉。从头部均匀的、没有重置的样本采样；25% 意味着 随机 shuffle， 只保留前 25%</li><li>解码时：append 跟以前长度一样的这些掩码的一些词源 mask tokens （一个可以学习的向量 + 位置信息），重新 unshuffle 还原到原来的顺序</li><li>MSE 算误差时，跟原始图的 patches 对应</li></ul><p>The decoder is applied to this full list (with positional embeddings added)。<strong>编码器处理可见块的潜表示需不需要再加位置信息？</strong></p><p><strong>shuffle 和 unshuffle 好处：</strong> 没有稀疏的操作，实现快，不影响 ViT 块的实现</p><h1 id="8-实验"><a href="#8-实验" class="headerlink" title="8.实验"></a><strong>8.实验</strong></h1><h2 id="8-1-ImageNet-的实验"><a href="#8-1-ImageNet-的实验" class="headerlink" title="8.1 ImageNet 的实验"></a>8.1 ImageNet 的实验</h2><p>在 ImageNet-1K 100万张图片 数据集上：先做自监督的预训练（不用标号，只拿图片）；然后再在同样的数据集上做有标号的监督训练</p><p><strong>做法：</strong>&#x20;</p><ul><li>end to end 的微调，允许改整个模型 所有的可学习的参数；</li><li>linear probing 允许改最后一层的线性输出层</li></ul><p><strong>结果</strong>：在验证集上报告 top-1 的精度，用 中心剪裁的 224*224 的图片</p><p><strong>Baseline:</strong> ViT-Large / 16, 16 * 16。ViT-Large 比 ResNet50 要大很多，很容易 overfitting</p><p><strong>比较的 3 种情况：</strong>&#x20;</p><ol><li><strong>scratch, original：76.5。</strong> ViT 所有的内容在 ImageNet-1K上训练, 结果不稳定 200 epoches</li><li><strong>scratch, our impl： 82.5</strong>。加入 strong regularization 。ViT 文章说 ViT 需要很大的数据才行；但是后来发现，小一点的数据 + 合适的正则化 也是可以的。</li><li><strong>baseline MAE：84.9</strong> 。<strong>先使用 MAE 做预训练，再在 ImageNet 上微调</strong> 50 epoches。数据集没变化，预训练和微调都是 ImageNet</li></ol><p><img src="image/image_8-bj9OSheZ.png" alt=""></p><p>MAE 纯从图片上学到不错的信息</p><h2 id="8-2-主要结果"><a href="#8-2-主要结果" class="headerlink" title="8.2**主要结果 **"></a>8.2**主要结果 **</h2><h3 id="（1）表1"><a href="#（1）表1" class="headerlink" title="（1）表1"></a>（1）<strong>表1</strong></h3><p><img src="image/image_gI9FjKrS6i.png" alt=""></p><p><strong>a：解码器的深度</strong>，多少个 Transformer 块; end to end fine-tuning 贵一点，但效果好</p><ul><li>全都 ft，深度和效果关系不大 84.x</li><li>只调 lin, 深度深一点好</li></ul><p><strong>b：解码器的宽度</strong>，每一个 token 表示成一个多长的向量，512 比较好。</p><p><strong>c：编码器要不要加入被盖住的 masked 块：</strong>&#x20;</p><ul><li><strong>不加</strong>很好，精度高、计算量更少</li><li>非对称的架构 精度好、性能好</li></ul><p><strong>d：重构的目标</strong></p><ul><li>每个像素的MSE</li><li><strong>每个像素的MSE + normalization 均值为0 方差为 1 效果好</strong></li><li>PCA 做一次降维</li><li>dVAE: BEiT 的做法，通过 ViT 把每一个块映射到一个离散的 token，像 BERT 一样的去做预测</li></ul><p><strong>e：怎么样做数据增强</strong>，MAE 对数据增强不敏感</p><ul><li>什么都不做</li><li>固定大小的裁剪</li><li><strong>随机大小的裁剪</strong></li><li>裁剪 + 颜色变化</li></ul><p><strong>f：怎么采样 被盖住的块</strong></p><ul><li><strong>随机采样 最简单最好</strong></li><li>按一块块的采样 50 %</li><li>按一块块的采样 75 %</li><li>网格采样</li></ul><h3 id="（2）表1-主结果内容的展开图"><a href="#（2）表1-主结果内容的展开图" class="headerlink" title="（2）表1 主结果内容的展开图"></a>（2）<strong>表1 主结果内容的展开图</strong></h3><p><img src="image/image_bAd5UreseH.png" alt=""></p><p>使用不同的掩码率的时候的效果：10%的块被遮住： 83.2%；超过 40%的块被遮住：精度大幅提升</p><p>只调最后一层：更敏感</p><h3 id="（3）表2-训练时间"><a href="#（3）表2-训练时间" class="headerlink" title="（3）表2 训练时间"></a>（3）<strong>表2 训练时间</strong></h3><ul><li>ViT-Large + 解码器只使用一层 Transformer 的块：84.8% 精度不错，耗时最少</li><li>带掩码的块 + 大的解码器，加速 3.7倍</li><li>ViT huge 加速也比较多</li></ul><p><img src="image/image_PQqBh6v7KJ.png" alt=""></p><p><strong>绝对时间</strong>：128个 TPU v3的 core， tensorflow 实现；训练时间是10个小时 和 大概是一天多，可以忍受。</p><h3 id="（4）图-6-表示的是不同的掩码采样策略的区别"><a href="#（4）图-6-表示的是不同的掩码采样策略的区别" class="headerlink" title="（4）图 6 表示的是不同的掩码采样策略的区别"></a>（4）<strong>图 6 表示的是不同的掩码采样策略的区别</strong></h3><ul><li>随机采样效果好</li><li>尽量的按照一块一块的来切</li><li>按照格点来切</li></ul><p><img src="image/image_imzlByBEKS.png" alt=""></p><h3 id="（5）图-7-预训练的轮数和微调的精度的对比"><a href="#（5）图-7-预训练的轮数和微调的精度的对比" class="headerlink" title="（5）图 7 预训练的轮数和微调的精度的对比"></a>（5）<strong>图 7 预训练的轮数和微调的精度的对比</strong></h3><p><img src="image/image_RH82Znla0z.png" alt=""></p><p>ImageNet-1K 上训练个 1,000 个数据轮，精度有提升，在一直训练一直学习，过拟合也没那么多严重，因为1,000轮是非常非常多的</p><ul><li>一般在 ImageNet 上训练， 200轮 enough</li></ul><h2 id="8-3-与之前工作相比"><a href="#8-3-与之前工作相比" class="headerlink" title="8.3 与之前工作相比"></a>8.3 与之前工作相比</h2><h3 id="1）表3：跟前面结果比-MAE-效果是最好的"><a href="#1）表3：跟前面结果比-MAE-效果是最好的" class="headerlink" title="(1）表3：跟前面结果比 MAE 效果是最好的"></a>(1）表3：跟前面结果比 MAE 效果是最好的</h3><p><img src="image/image_3zJC4VJ_er.png" alt=""></p><h3 id="（2）图8：跟-ViT-里面的结果比"><a href="#（2）图8：跟-ViT-里面的结果比" class="headerlink" title="（2）图8：跟 ViT 里面的结果比"></a>（2）图8：跟 ViT 里面的结果比</h3><p><img src="image/image_1_FEULjI3L.png" alt=""></p><ul><li>最上面的虚线：ViT 在 JFT 3亿标号的图片数据集合的效果</li><li>排第二的线：只使用 ImageNet-1K 也就是1/300数据的效果</li><li>两根线很接近，不能说这是一个很公平的比较</li><li>JFT数据集包括的类数远远大于 ImageNet</li><li>它们很多是一些 顾我自己 care 的一些目标，但是 ImageNet很多都是一些猫猫狗狗的图片</li><li>测试集也是 ImageNet，JFK 它多了很多很多 可能跟你那些标号不那么一样的图片</li></ul><p>把验证集换到一个 不那么跟 ImageNet 相像的数据集上，可能这个差距会大一点，<strong>主要比较的是算法，而不是数据集</strong></p><h2 id="8-4调编码器所有层的参数和最后一层的参数效果差距大"><a href="#8-4调编码器所有层的参数和最后一层的参数效果差距大" class="headerlink" title="8.4调编码器所有层的参数和最后一层的参数效果差距大"></a><strong>8.4调编码器所有层的参数和最后一层的参数效果差距大</strong></h2><p><strong>到底调多少层：</strong>&#x20;</p><ul><li>少，快，精度差</li><li>多，慢，精度好</li></ul><p><img src="image/image_fWp66tUXlw.png" alt=""></p><ul><li>x轴表示多少个Transformer块被调</li><li>y轴表示精度</li></ul><p><strong>调 4 - 5 层比较好</strong></p><ul><li>底层不调：底层学到的东西稍微是比较低层次一点，你可以换一个任务也不需要变太多</li><li>上面那些层，跟你的任务相关，要调</li></ul><h2 id="8-5迁移学习实验结果"><a href="#8-5迁移学习实验结果" class="headerlink" title="8.5迁移学习实验结果"></a><strong>8.5迁移学习实验结果</strong></h2><p>COCO 的目标检测和分割</p><ul><li>MAE 做它的主干网络 效果最好</li><li>跟之前的 ViT 和 BEiT 比</li></ul><p><img src="image/image_7UP3sg3HMj.png" alt=""></p><p>重构像素的比较</p><ul><li>像 BEiT 那样重构 or 用 dVAE 学出来的标号比 差别不大</li><li>重构原始的像素 简单、好</li></ul><p><img src="image/image_DYjn_jKdDU.png" alt=""></p><h1 id="9-评论"><a href="#9-评论" class="headerlink" title="9.评论"></a><strong>9.评论</strong></h1><h2 id="9-1-算法"><a href="#9-1-算法" class="headerlink" title="9.1 算法"></a>9.1 算法</h2><p><strong>MAE 算法不难</strong></p><ul><li>利用 ViT 来做跟 BERT 一样的自监督学习</li><li>ViT 文章已经做了这个事情了</li></ul><p><strong>MAE 在 ViT 的基础提升点</strong></p><ul><li>需要盖住更多的块，降低剩余块之间的冗余度，任务变得复杂一些</li><li>使用一个 Tranformer 架构的解码器，直接还原原始的像素信息，使得整个流程更加简单一点</li><li>加上在 ViT 工作之后的各种技术，训练更鲁棒</li></ul><p>以上三点 MAE 使得在 ImageNet-1K 这个数据集上使用自监督训练的效果超过了之前的工作</p><h2 id="9-2写作：简单，故事性好"><a href="#9-2写作：简单，故事性好" class="headerlink" title="9.2写作：简单，故事性好"></a>9.2<strong>写作：简单，故事性好</strong></h2><p>导言：为什么 MAE 这么做</p><p>非常详细的实验数据：整个 MAE 里面 所有可以调的超参数 它到底是什么意思？</p><p><strong>简单的想法、非常好的结果、详细的实验 → 一个非常高质量的工作</strong></p>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> PaperReading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 GNN</title>
      <link href="/paper_reading/1.1.GNN/"/>
      <url>/paper_reading/1.1.GNN/</url>
      
        <content type="html"><![CDATA[<p>文章链接：<a href="https://distill.pub/2021/gnn-intro/" title="A Gentle Introduction to Graph Neural Networks (distill.pub)">A Gentle Introduction to Graph Neural Networks (distill.pub)</a></p><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h1><p>图这个数据结构相对于之前讨论的文本（文本是一个序列）、图片（图片是一个矩阵），图相对来说更加复杂一点。</p><p>图是一个很一般化的架构，十几年前，研究者提出了针对图的神经网络（图神经网络，GNN），最近它们在能力和表达上都有增强。</p><p>图神经网络的实际应用（starting to，图神经网络还是一个比较新的领域，在应用上刚起步）</p><ul><li>药物的发现</li><li>物理模拟</li><li>虚假新闻检测</li><li>车流量的检测</li><li>推荐系统</li></ul><p>本文旨在探索和解释现代的图神经网络</p><ul><li>什么数据可以表示成一张图</li><li>图和别的数据有什么不同，为什么要做图神经网络，而不是使用最简单的卷积神经网络等</li><li>构建了一个GNN，看各模块的具体结构</li><li>提供了一个GNN的playground</li></ul><p>图神经网络所关注的重点</p><ul><li>怎样把所想要的信息表示成向量</li><li>这些向量是不是能够通过数据来学到</li></ul><h1 id="2-什么是图"><a href="#2-什么是图" class="headerlink" title="2.什么是图"></a>2.什么是图</h1><p>图是用来表示entity（实体）之间的关系</p><ul><li>实体就是一个点（node，顶点）</li><li>关系就是一个边（edge）</li></ul><h2 id="2-1-图的构成"><a href="#2-1-图的构成" class="headerlink" title="2.1 图的构成"></a>2.1 图的构成</h2><ul><li>V：顶点</li><li>E：边</li><li>U：整个图（全局信息）</li><li>attribute：每个顶点、每条边和整个图表示的信息</li></ul><p><img src="image/image_vkusQ-Lg5E.png" alt=""></p><h2 id="2-2-图的属性"><a href="#2-2-图的属性" class="headerlink" title="2.2 图的属性"></a>2.2 图的属性</h2><p><img src="image/image_9sWVqUhGoX.png" alt=""></p><ul><li>顶点（黄色）：可以用一个embeding（向量）来表示它里面的属性，一共有六个值，高矮表示值的大小</li><li>边（蓝色）：可以使用向量来表示，长度可以和顶点不一样，这里使用的是一个长度为8的向量，即边中所有的属性用一个长度为8的向量来表示</li><li>全局信息（粉色）：可以用一个长为5的向量来表示</li></ul><h2 id="2-3-图的分类"><a href="#2-3-图的分类" class="headerlink" title="2.3 图的分类"></a>2.3 图的分类</h2><ul><li>有向图：</li><li>无向图</li></ul><p><img src="image/image_dWh05ohDHe.png" alt=""></p><h1 id="3-数据如何表示为图"><a href="#3-数据如何表示为图" class="headerlink" title="3.数据如何表示为图"></a>3.数据如何表示为图</h1><h2 id="3-1-将图片表示成一张图"><a href="#3-1-将图片表示成一张图" class="headerlink" title="3.1 将图片表示成一张图"></a>3.1 将图片表示成一张图</h2><p>假设有一张高宽都为244的RGB三通道图，一般来说会把它表示成一个三个维度的tensor（在输入卷积神经网络的时候用的是tensor），但是也可以把它当作一个图，它的每个像素就是一个点，如果两个像素之间是连接关系的话，就连接一条边。</p><p><img src="image/image_CUI9zjBn_n.png" alt=""></p><ul><li>左边表示图片；</li><li>右边是表示成的邻接矩阵；</li><li>左边图片中00表示第0列第0行，它与邻接矩阵中的对应关系入下图中蓝色圆圈所示，这样就将图片中的每个像素都映射成了图上面的点；</li><li>中间的矩阵叫做邻接矩阵，它的每一行是一个顶点，每一列也是一个顶点，如果第 i 行和第 j 列之间值为1（图中蓝色的点）的话就表示第 i 个结点和第 j 个节点之间是有边的，每一个蓝色的点表示图中的一条边，剩下白色的点表示这些点之间是没有边的，所以叫做邻接矩阵，通常是一个很大的非常稀疏化的矩阵</li></ul><h2 id="3-2-将文本表示为图"><a href="#3-2-将文本表示为图" class="headerlink" title="3.2 将文本表示为图"></a>3.2 将文本表示为图</h2><p>文本可以认为是一条序列，可以把其中每个词表示成顶点，那么每个词和下一个词之间有一条有向边（上一个词和下一个词之间有一条指向下一个词的有向边，在图上面叫做有向的路）</p><p><img src="image/image_oSZYlH-Uh5.png" alt=""></p><h2 id="3-3-其他图形式数据"><a href="#3-3-其他图形式数据" class="headerlink" title="3.3 其他图形式数据"></a>3.3 其他图形式数据</h2><h3 id="（1）分子示意图"><a href="#（1）分子示意图" class="headerlink" title="（1）分子示意图"></a>（1）分子示意图</h3><p><img src="image/image_fUH-j2Z9yF.png" alt=""></p><ul><li>左图是一个分子的图片，由一些原子通过一些作用力连接在一起</li><li>每一个原子可以表示成图里面的顶点，如下图右图所示，如果原子之间是相连的就可以连成一条边</li></ul><h3 id="（2）人物关系网络图"><a href="#（2）人物关系网络图" class="headerlink" title="（2）人物关系网络图"></a>（2）人物关系网络图</h3><p><strong>《奥赛德》剧中所有人物之间的交互图（关系网络）</strong></p><p><img src="image/image_Iv7viFZsXf.png" alt=""></p><ul><li>在这个剧中任何人物如果在一个场景中同时出现的话，就会将其演绎成一条边</li><li>处在边缘的人物可能就是跟其他人交互比较少</li></ul><p><strong>空手道俱乐部的社交网络图</strong></p><p><img src="image/image_zXjHW8pWbP.png" alt=""></p><p>该俱乐部中有两个老师，每个老师会跟一些同学比赛，然后可以把每一个老师跟每一个同学做过比赛的情况放在一起，这样也会得到一个社交网络图</p><h2 id="3-4-实际中图的大小"><a href="#3-4-实际中图的大小" class="headerlink" title="3.4 实际中图的大小"></a>3.4 实际中图的大小</h2><p><img src="image/image_i2QiZCjo3H.png" alt=""></p><ul><li>grapgh表示图的个数</li><li>node表示顶点数</li><li>edge表示边的个数</li><li>mean表示平均每个点所连接的边的数量</li><li>min表示图中一个顶点所连接的边的数量的最小值</li><li>max表示图中一个顶点所连接的边的数量的最大值</li><li>每一行就是一张图</li><li>第一行是一个空手道俱乐部的社交网络图</li></ul><h1 id="4-图结构可以提出什么问题？"><a href="#4-图结构可以提出什么问题？" class="headerlink" title="4.图结构可以提出什么问题？"></a>4.图结构可以提出什么问题？</h1><p>总的来说就是三类问题</p><ul><li>graph-level：对整个图进行识别</li><li>node-level：对顶点的属性进行判断</li><li>edge-level：对边的属性进行判断</li></ul><h2 id="4-1-图层面（graph-level）"><a href="#4-1-图层面（graph-level）" class="headerlink" title="4.1 图层面（graph-level）"></a>4.1 图层面（graph-level）</h2><p><strong>目标</strong>：预测整个图的性质</p><p>下图左边是一些原始的图，任务是要找出哪些图中含有两个圆环（给定一张图，然后对图进行分类）：</p><p><img src="image/image__vxsatxPus.png" alt=""></p><ul><li>这里是一个很简单的问题，不需要使用到机器学习，只需要使用简单的编程，沿着图遍历一下就可以的</li><li>对于复杂一点的任务，可以借助图神经网络</li></ul><h2 id="4-2-顶点层面（node-level）"><a href="#4-2-顶点层面（node-level）" class="headerlink" title="4.2 顶点层面（node-level）"></a>4.2 顶点层面（node-level）</h2><p><strong>目标</strong>：预测顶点属于哪一个图</p><p>下图是上述空手道俱乐部的社交关系图，假如某一天两个老师不合决裂了，所有的学生要对这两个老师进行站队（站在某一个老师那边）</p><p><img src="image/image_EGgrSBPVD3.png" alt=""></p><h2 id="4-3-边层面（edge-level）"><a href="#4-3-边层面（edge-level）" class="headerlink" title="4.3 边层面（edge-level）"></a>4.3 边层面（edge-level）</h2><p>给定一张图片，先通过语义分割及那个图片中的人物、背景拿出来之后，然后判断人物之间的关系（顶点已经有了，需要判断顶点之间的边的属性）</p><p><img src="image/image_AZein8FA90.png" alt=""></p><p><img src="image/image_0AwICH559I.png" alt=""></p><h1 id="5-深度学习用在图中存在的问题"><a href="#5-深度学习用在图中存在的问题" class="headerlink" title="5.深度学习用在图中存在的问题"></a>5.深度学习用在图中存在的问题</h1><p>将神经网路用在图上面最核心的问题是：<strong>如何表示图使得它能够和神经网络兼容</strong></p><p>图上面有四种信息</p><ul><li>顶点的属性</li><li>边的属性</li><li>全局信息</li><li>连接性：每条边到底连接的是哪两个点</li></ul><p>前三个比较容易，因为都能够使用向量来进行表示，神经网络对向量是友好的，问题在于如何表示连接性。</p><h2 id="5-1-邻接矩阵表示存在问题"><a href="#5-1-邻接矩阵表示存在问题" class="headerlink" title="5.1 邻接矩阵表示存在问题"></a>5.1 邻接矩阵表示存在问题</h2><p>连接性可以用邻接矩阵来表示：如果有n个顶点的话，邻接矩阵就是 n*n 的矩阵，其中如果两个顶点直接相连的话值就为1，否则为0，这样就能得到一个方形的矩阵。</p><p>但是用邻接矩阵来表示连接性的话也存在很多问题</p><ul><li>邻接矩阵可能会非常大：按照正常的存放可能存不下，由于它是一个稀疏矩阵，可以用稀疏矩阵的形式来进行存储，使得它至少在存储上是可行的。但是如果处理稀疏矩阵的话，高效地处理稀疏矩阵比较困难，特别是将稀疏矩阵用在GPU上面，一直是一个比较难的问题</li><li>邻接矩阵其实将任何行或者列的顺序交换都不会影响，视觉上可能是不一样的，但是其实表示的是同一种东西，这就意味着如果设计一个神经网络不管使用哪种顺序，放进神经网络之后都应该保证最后的结果是一样的（因为不同的顺序的邻接矩阵其实表示的是同一张图），<strong>有点类似于化学中的同分异构体</strong></li></ul><p><img src="image/image_vzPLISr1Ch.png" alt=""></p><h2 id="5-2-GNN图存储方式"><a href="#5-2-GNN图存储方式" class="headerlink" title="5.2 GNN图存储方式"></a>5.2 GNN图存储方式</h2><p>如果说既想要存储高效，又想要排序不影响最终结果的话，文中提出了一种存储方法，如下图所示</p><p><img src="image/image_g4pAC8A4gf.png" alt=""></p><ul><li>图中一共有8个顶点，7条边</li><li>每个点的属性使用的是一个标量来表示（也可以换成向量，并不影响）</li><li>每一条边也是用一个标量来表示的（也可以换成向量，并不影响）</li><li>全局的信息也是用一个标量来进行表示的（也可以换成向量，并不影响）</li><li>邻接列表：长度和边数一样，第 i 项表示第 i 条边连接的两个顶点</li><li>存储高效：这样在存储上，只把边和所有属性存储下来</li><li>与顺序无关：可以把边的顺序任意打乱，只要把连接列表的顺序也进行相应的变化就可以了。同理。也可以将顶点的顺序全部打乱，只要将邻接列表中的数字做相应 的更新就可以了</li><li>所以这样存储即使存储高效的，也是与顺序无关的</li></ul><h1 id="6-图神经网络（GNN）"><a href="#6-图神经网络（GNN）" class="headerlink" title="6.图神经网络（GNN）"></a>6.图神经网络（GNN）</h1><h2 id="6-1-定义"><a href="#6-1-定义" class="headerlink" title="6.1 定义"></a>6.1 定义</h2><p>“A GNN is an optimizable transformation on all attributes of the graph (nodes, edges, global-context) that preserves graph symmetries (permutation invariances).” (Sanchez-Lengeling 等, 2021, p. 10)</p><p>GNN是对图上所有的属性（包括顶点、边和全局上下文）进行的可以优化的变换，这个变换能够<strong>保持住图的对称信息</strong>（对称信息指的是将顶点进行另外一种排序信息之后，整个结果不会发生改变）。</p><p>message passing（信息传递神经网络，GNN也可以用别的神经网络来进行描述，这个地方用的是信息传递的框架）</p><p>GNN的输入是图，输出也是图，它会对顶点、边和全局的向量进行变换，但是不会改变图的连接性（顶点和边的连接信息在进入图神经网络之后是不会改变的）</p><h2 id="6-2-构造简单的GNN"><a href="#6-2-构造简单的GNN" class="headerlink" title="6.2 构造简单的GNN"></a>6.2 构造简单的GNN</h2><p>如下图所示，对于顶点向量、边向量和全局向量分别构造MLP（多层感知机），MLP的输入大小和输出大小相同，取决于输入的向量。</p><p><img src="image/image_8ZHvnEGIXW.png" alt=""></p><ul><li>$f_{U_n}$ 表示全局向量的MLP</li><li>$f_{V_n}$ 表示顶点的MLP</li><li>$f_{E_n}$ 表示边的MLP</li></ul><p>这三个MLP组成了GNN的层。这个层的输入是图，输出还是图。它的作用是：<strong>对于顶点的向量、边的向量和全局的向量分别找到对应的MLP</strong>，然后将它们放进去得到它们的输出，作为其对应的更新，图进去再出来之后的属性都是被更新过的，但是整个图的结构没有发生变化（只对属性集变化，但是不改变图的结构）。</p><p>因为MLP是对每个向量独自作用的而不会考虑所有的连接信息，所以不管对整个顶点做任何排序信息都不会改变结果，所以这个最简单的层就满足了之前的两个要求，也可以将这些层叠加在一起构造成比较深的GNN。</p><p><strong>注意</strong>：不管有多少个顶点，只有一个全连接层（所有的顶点将会共享一个全连接层里面的参数）。在之前的GNN层里面，不管图有多大，一层里面也就是3个MLP，所有的顶点、边共享一个MLP，全局就一个就不用共享了</p><h2 id="6-3-GNN预测和-Polling"><a href="#6-3-GNN预测和-Polling" class="headerlink" title="6.3 GNN预测和 Polling"></a>6.3 GNN预测和 Polling</h2><h3 id="（1）通过最后一层的输出得到预测值"><a href="#（1）通过最后一层的输出得到预测值" class="headerlink" title="（1）通过最后一层的输出得到预测值"></a>（1）通过最后一层的输出得到预测值</h3><p>考虑最简单的情况：已知顶点向量，对每个顶点做预测。</p><p>和一般的神经网络没有太多区别，因为顶点已经表示成向量了，然后要对它做预测的话，比如前述的空手到俱乐部的问题，其实就是一个简单的二分类问题</p><ul><li>每个学生已经有了对应的向量表示</li><li>在后面加一个输出为2的全连接层，然后再加一个softmax就能得到输出了</li></ul><p>如果是做n类的话，就加一个输出大小是n的全连接层，然后再加一个softmax就能得到多类输出了；做回归的话就是单输出就行了。</p><p>如下图所示，给定最后一层的输出是一个图，然后对于每一个顶点进入到全连接层，然后得到输出，这样就会对顶点作分类</p><p><img src="image/image_AE3ULhLfpg.png" alt=""></p><p>考虑一个稍微复杂一点的情况：还是对顶点做预测，但是顶点并没有表示成向量</p><p><strong>pooling（汇聚）</strong>：和CNN中的pooling其实没有太多本质上的区别</p><h3 id="（2）某个点没有自己向量，如何得到它的向量并做预测？"><a href="#（2）某个点没有自己向量，如何得到它的向量并做预测？" class="headerlink" title="（2）某个点没有自己向量，如何得到它的向量并做预测？"></a>（2）某个点没有自己向量，如何得到它的向量并做预测？</h3><p>可以把跟这个点连接的那些边和全局的向量拿出来，就会拿出五个向量，如下图所示：</p><p><img src="image/image_v3To3YSjAF.png" alt=""></p><ul><li>箭头表示对应的向量</li><li>中间的值是一个全局向量，所以没有写出来</li><li>将5个向量全部加起来，就会得到代表这一个点的向量（这里假设所有的顶点、边和全局向量的维度是一样的，如果不一样的话则需要做投影）</li><li>最后加上一个全连接层就能得到输出了，如下图所示</li></ul><p><img src="image/image_GF3MkKa1W4.png" alt=""></p><blockquote><p>上图表示只有边的向量En，没有顶点的向量Vn，通过汇聚层（从边到顶点），每个顶点都会得到自己的向量，最后进入到顶点之间共享的输出层就能得到顶点的输出了</p></blockquote><h3 id="（3）假设没有边的向量只有顶点的向量但是想对每个边做预测"><a href="#（3）假设没有边的向量只有顶点的向量但是想对每个边做预测" class="headerlink" title="（3）假设没有边的向量只有顶点的向量但是想对每个边做预测"></a>（3）假设没有边的向量只有顶点的向量但是想对每个边做预测</h3><ul><li>可以把顶点的向量汇聚到边上（一条边连接两个顶点，可以把这两个顶点加起来，也可以加上全局向量），得到对应的边的向量；</li><li>然后进入边向量的输出层（所有的边共享一个输出层），最后得到边的输出。</li></ul><p><img src="image/image_yJf9uTtXhC.png" alt=""></p><h3 id="（4）假设没有全局的向量，但是有顶点的向量然后对整个图做预测"><a href="#（4）假设没有全局的向量，但是有顶点的向量然后对整个图做预测" class="headerlink" title="（4）假设没有全局的向量，但是有顶点的向量然后对整个图做预测"></a>（4）假设没有全局的向量，但是有顶点的向量然后对整个图做预测</h3><ul><li>可以把所有顶点向量加起来，得到对应的全局向量</li><li>然后进入全局的输出层，输出层会得到全局的输出</li></ul><p><img src="image/image_mUrdrlXJGP.png" alt=""></p><p>综上所述，不管缺乏哪一类的属性，都可以通过汇聚操作得到所缺失的属性的向量，最后得到预测值</p><h3 id="（5）总结"><a href="#（5）总结" class="headerlink" title="（5）总结"></a>（5）总结</h3><p>总结上面的内容，就能得到一个简单的GNN，如下图所示</p><p><img src="image/image_ucDSSyLxOE.png" alt=""></p><ul><li>给定一个输入图，首先进入一系列的GNN层（每个层里面有三个MLP，对应三种不同的属性），会得到一个保持整个图结构的输出（但是里面所有的属性已经进行了更新）</li><li>最后根据所要预测的属性添加合适的输出层</li><li>但如果缺失信息的话可以加入合适的汇聚层就可以完成预测了</li></ul><p>这是最简单的情况。虽然这个情况很简单，但是有很大局限性，<strong>主要的问题在于在GNN块中并没有使用图的结构信息</strong>（对每个属性做变换的时候，仅仅是每个属性进入自己对应的MLP，并没有体现出三者之间的相互联系的连接信息），导致并没有合理地把整个图的信息更新到属性里面，以至于最后的结果并不能够特别充分利用图的信息。</p><h2 id="6-4-信息传递（Passing-messages”）"><a href="#6-4-信息传递（Passing-messages”）" class="headerlink" title="6.4 信息传递（Passing messages”）"></a>6.4 信息传递（Passing messages”）</h2><p>对上述问题进行改进，使得能够将图的信息尽早放进去</p><p>信息传递，工作原理如下图所示</p><p><img src="image/image_-t9oigm9ry.png" alt=""></p><ul><li>首先将上图中橙色实心顶点的向量和它的邻居的两个顶点的向量加在一起得到一个汇聚的向量</li><li>再将这个汇聚的向量放进MLP得到该顶点向量的更新</li><li>其他顶点的操作同理（拿到每个顶点本身的向量以及它邻居的向量加到一起，最后得到进入MLP之前的汇聚向量）</li></ul><p>这个跟标准的图片上的卷积有点类似，等价于在图片上做卷积，但是卷积核窗口里面每个窗口的权重是要相同的（因为在卷积里面其实是每一个顶点和它邻居顶点的向量会做加权和，权重来自于卷积窗口），这里没有加权和，只是加，所以权重应该是一样的，但是通道还是保留了（卷积中有多输入通道和多输出通道，对应信息传递过程中MLP还是保留了），所以和卷积操作有点类似，但是还是有区别。</p><p>GNN层也可以通过堆叠来完成整个图的长距离信息传递过程。</p><p>下图表示一个顶点，顶点之间通过距离为1的邻居（一近邻）将它的信息传递过来，就 ρ 从 v 到 v 的汇聚过程，这也是最简单的信息传递的过程</p><p><img src="image/image_--5Dyf_813.png" alt=""></p><h2 id="6-5-学习边的表示"><a href="#6-5-学习边的表示" class="headerlink" title="6.5 学习边的表示"></a>6.5 学习边的表示</h2><p>再复杂一点，之前考虑过，假设确实某种属性，可以从别的属性汇聚过来，以弥补这个属性，同理，不需要等到最后一层在进行汇聚，可以在较早的时候在边和顶点之间汇聚信息。</p><p>下图展示了如何将顶点的信息传递给边，然后再把边的信息传递给顶点</p><p><img src="image/image_V7cbarubpv.png" alt=""></p><ul><li>首先通过ρ（V to E）把顶点的向量传递给边（把每个边连接的顶点信息加到边的向量中，假设维度不一样的话，会先做投影，投影到同样的维度然后再加进去，这样边就拿到了顶点的信息）</li><li>同样可以每个顶点可以把连接它的边的信息也加到自己上面，同样维度不同的话，可以先做投影（这里如果维度不一样的话，会做两次投影），维度一样的可以直接相加</li><li>另外也可以将它contact在一起，也就是并在一起（类似于矩阵的拼接操作）</li><li>这样就完成了顶点到边的传递、边到顶点的传递之后，再进入到各自的MLP做更新</li></ul><p>这里是先把顶点的信息传递给边做更新，然后再把更新过信息的边的信息汇聚到顶点再做更新，如下图左图所示。如果反过来的话会得到不一样的结论如下图右图所示</p><p><img src="image/image_7cntjpR-Fa.png" alt=""></p><p>之前顶点和边之间不交换任何信息，各自进行更新，所以谁先进行更新不影响，但是现在先更新顶点还是先更新边会导致不一样的结果，<strong>目前来说还没有一致的结论说说先更新谁比较好，只是说这两种会导致不一样的结果</strong>。</p><p>其实可以交替更新，如下图所示：同时顶点汇聚到边然后边汇聚到顶点，汇聚之后先不要相加，然后再回来一次，就相当于两边的信息都有了，等价于两个信息同时汇聚，最终向量可能会宽一点，这样就不用担心到底是先做谁的汇聚了。</p><h2 id="6-6-增加全局信息"><a href="#6-6-增加全局信息" class="headerlink" title="6.6 增加全局信息"></a>6.6 增加全局信息</h2><p>之前有一个问题：因为每次只去看自己的邻居，<strong>假设图比较大而且连接没那么紧密的时候，会导致信息从一个点传递到一个很远的点需要走很长的步**</strong>。** ​</p><p>解决方案：加入<strong>master node</strong>（或者叫<strong>context vector</strong>），这个点是虚拟的点，<strong>可以跟所有的顶点相连，也可以跟所有的边相连</strong>（这里在图上面不好说，因为一个顶点是没办法跟边相连的，在这里是一个抽象的概念），它其实就是U，U跟所有的V里面的东西是相连的，跟E里面所有的东西也是相连的，所以如果想要把顶点的信息汇聚给边的时候，也会把U汇聚过来，因为U跟边是相连的（同理，汇聚顶点的时候，也会把U和它相连的E连过来），最后自己更新U的时候，会把所有的顶点信息和边的信息都拿过来，最后完成汇聚再做更新。</p><p><img src="image/image_dL4G8HXEsI.png" alt=""></p><p>到此，这三类属性都学到了对应的向量，而且这些向量在早期就已经进行了大量的信息传递，最后在做预测的时候就可以只用本身的向量，也可以把相邻的那些边的向量也拿过来，甚至是把跟他相邻的那些顶点的向量也拿过来，以及全局向量，也就是说不仅使用本身的向量，还可以将别的跟本身相关的东西都拿过来一起做预测。</p><p>对于来自于不同类别的属性，可以加在一起，合并在一起也可以。有一点像注意力机制（attention就是说将当前query的顶点和跟他相近的东西都拿过来，可以直接用图的信息将这些相近的向量拿过来，这样就完成了基于消息传递的图神经网络的工作）。</p><p>一个顶点的信息基于其他三个嵌入：邻接顶点、相连的边、全局信息</p><p><img src="image/image_mYDEtNJw5P.png" alt=""></p><h1 id="7-实验"><a href="#7-实验" class="headerlink" title="7.实验"></a>7.实验</h1><p>这部分是一个GNN的playgrround，作者吧GNN的训练程序嵌入到了javascript里面，然后做了一个比较小的分子图的预测数据集，使得可以调节图神经网络里面不同的超参数，来观察实际的训练效果。</p><p>可调节的超参数：</p><ul><li>图神经网络的层数：1层～4层</li><li>汇聚操作的方式：mean（每个向量按元素做平均，对应卷积神经网络中的average pooling）、sum（每个向量按元素求和，求和在卷积神经网络中用的不多）、max（每个向量按元素取最大值，对应卷积神经网络中的max pooling）</li><li>顶点的size：25、50、100（也可以取消勾选不学）</li><li>边的size：5、10、20（也可以取消勾选不学）</li><li>全局向量的size：25、50、100（也可以取消勾选不学）</li></ul><p>每次改变一个超参数，它就会重新对数据进行一次训练，训练使用的是50个epoch，训练完之后会显示auc（越大越好），每次可以调节不超参数得到不一样的结果</p><p><img src="image/image_GCM95wNL6e.png" alt=""></p><p>下图右侧是对所有数据点的预测：真实值的颜色用边框表示，预测值用实心里面的颜色表示；边框有两个颜色，一个是红色，一个是蓝色，表示两类；里面填的东西是预测值；如果边框和实心都是红色或者蓝色的话，则预测是正确的，否则的话就是错误的；这里是对每个分子图的向量投影到二维之后可视化的效果。</p><p>左边可以做预测，实时地对给定的图做预测；可以对图进行改变，可以多连一点边出来或者改变某一个节点的原子；改变之后可以实时观察预测值是多少然后在整个数据点上长什么样子</p><h2 id="7-1-模型可以学习参数的大小和最后测试的auc之间的关系"><a href="#7-1-模型可以学习参数的大小和最后测试的auc之间的关系" class="headerlink" title="7.1 模型可以学习参数的大小和最后测试的auc之间的关系"></a>7.1 模型可以学习参数的大小和最后测试的auc之间的关系</h2><p><img src="image/image_bBR1uMcVK6.png" alt=""></p><ul><li>每一个点就是一个特定的超参数在模型训练之后得到的结果，所以一个点就是一个模型</li><li>x轴是参数的个数</li><li>y轴表示auc，auc越高越好</li><li>整体来说，模型参数变高的时候，模型的auc的上限是上涨的，就算模型很大，参数也得调的比较好，不然也可能达不到很好的效果（也可能跟小模型具有一样的效果）</li></ul><h2 id="7-2-超参数对整个模型的影响"><a href="#7-2-超参数对整个模型的影响" class="headerlink" title="7.2 超参数对整个模型的影响"></a>7.2 超参数对整个模型的影响</h2><p>每一个属性向量的长度（顶点、边、全局），如下图:</p><p><img src="image/image_6eaQALE6Yj.png" alt=""></p><ul><li>每一个向量有三个可选的地方，对每一个特定的长度，变换别的参数之后平均下来的auc如上图所示</li><li>图中中间的横线表示的是一个中值，矩形表示25%和75%的quantile（分位数），就是说在这个上面只有25%的点比这条线要高，只有25%的点比这条线要低，垂线的最顶端表示最大值，垂线的最底端表示最小值（具体含义可以看概率论中的箱线图）</li><li>中值应该越高越好</li><li>整个图形也不希望特别长，越长表示越敏感，越短表示越稳定&#x20;</li><li>对于顶点来说，从25到50，中值向上升了一点，但是整体来看，敏感度没有太大的变化，整体来说50比25要好一点</li><li>对于边来说也是稍微大一点好一些</li><li>全局值的话也是稍微大一点好一些</li><li>整体来说都不是特别明显，这些点的方差比较大</li></ul><h2 id="7-3-不同的层数对精度的影响"><a href="#7-3-不同的层数对精度的影响" class="headerlink" title="7.3 不同的层数对精度的影响"></a>7.3 不同的层数对精度的影响</h2><p><img src="image/image_8tfbKTFCQ8.png" alt=""></p><ul><li>左图中x轴表示可以学习的参数个数，y轴是测试的精度</li><li>右图是一个boxplot（箱线图）</li><li>红色表示1层GNN层</li><li>蓝色表示2层GNN层</li><li>绿色表示3层GNN层</li><li>紫色表示4层GNN层</li><li>当层数越低的时候，可学习的参数就越少</li><li>从精度上看，左图好像是耦合在一起的，但是从箱线图来看，当层数增加的时候，中值还是在增加，但是方差还是挺大的，所以在左图中看起来是耦合的</li><li>可以将层数调高一点，但是也的将剩下的参数调的够好才行</li></ul><h2 id="7-4-不同的聚合操作（求和、求平均、取max）对精度的影响"><a href="#7-4-不同的聚合操作（求和、求平均、取max）对精度的影响" class="headerlink" title="7.4 不同的聚合操作（求和、求平均、取max）对精度的影响"></a>7.4 不同的聚合操作（求和、求平均、取max）对精度的影响</h2><p><img src="image/image_m-TEz6sT_V.png" alt=""></p><ul><li>基本上可以发现没什么区别，从箱线图可以发现这三个几乎是等价的，在这个数据集上，随便用哪一个都行，对结果的影响不是很大</li></ul><h2 id="7-5-在哪些属性之间传递信息对精度的影响"><a href="#7-5-在哪些属性之间传递信息对精度的影响" class="headerlink" title="7.5 在哪些属性之间传递信息对精度的影响"></a>7.5 在哪些属性之间传递信息对精度的影响</h2><p><img src="image/image_TqCKeVeNOy.png" alt=""></p><ul><li>绿色表示不传递任何信息，也就是最简单的GNN，没有任何消息传递，从箱线图中可以看到它的精度是最差的，中值、方差都比较小</li><li>箱线图的最右边表示在顶点、边、全局之间都传递信息，所有能传递信息的都进行传递，它的中值是最高的，但是如果参数没调好，下面有三个out layer，如下图所示</li><li>箱线图中倒数第二个表示在顶点和全局之间传递信息，而没有用边，效果也是不错的，这就意味着边（也就是连接原子之间的那些键）在传递信息上的帮助并不是很大</li><li>但是传递的信息越少，基本上就是效果越差</li></ul><h2 id="7-6-总结"><a href="#7-6-总结" class="headerlink" title="7.6 总结"></a>7.6 总结</h2><p>总的来说，GNN对超参数还是比较敏感的，能调的超参数也很多</p><ul><li>GNN的层数</li><li>每一个属性的大小</li><li>汇聚的操作方式</li><li>信息的传递方式</li></ul><p>上述因素都对参数传递造成了很大的影响</p><h1 id="8-相关技术"><a href="#8-相关技术" class="headerlink" title="8.相关技术"></a>8.<strong>相关技术</strong></h1><h2 id="8-1-其他类型图"><a href="#8-1-其他类型图" class="headerlink" title="8.1 其他类型图"></a>8.1 其他类型图</h2><p><strong>除了上文中提到的图，其实还有很多别的图</strong>，如下图所示</p><p><img src="image/image_9JAxODST6e.png" alt=""></p><ul><li>左图是一个multi graph：顶点之间可以有多条边，比如左图中相同顶点之间不同颜色的边表示不同种类的边（红色的无相边、绿色的有相边）</li><li>右图表示图可能是分层的：其中有一些顶点可以能是一个子图（hypernode）</li><li>不同的图结构会对神经网络做信息汇聚的时候产生影响</li></ul><h2 id="8-2-图采样和做batch"><a href="#8-2-图采样和做batch" class="headerlink" title="8.2 图采样和做batch"></a>8.2 图采样和做batch</h2><p>假设有很多层的时候，最后一层的顶点就算只看它的一近邻，最后顶点由于有很多层消息传递，所以它实际上是能看到一个很大的图，在图连通性足够的情况下，最后这个顶点有可能会看到整个图的信息。</p><p>在计算梯度的时候，需要把整个forward中所有的中间变量存下来，<strong>如果最后一层的顶点想要看整个图的话，意味着在算梯度的时候需要把整个图的中间结果都存下来，就导致计算量大到无法承受，因此就需要对图进行采样</strong>。在图中每次采样一个小图出来，在这个小图上做信息的汇聚，这样算梯度的时候就只需要将小图上的中间结果记录下来就行了。</p><h3 id="（1）采样方法"><a href="#（1）采样方法" class="headerlink" title="（1）采样方法"></a>（1）采样方法</h3><p>四种采样方法，具体哪一种使用效果比较好，取决于整个图的形状</p><h4 id="Random-node-sampling"><a href="#Random-node-sampling" class="headerlink" title="Random node sampling"></a>Random node sampling</h4><p>表示随机采样一些点，然后将这些点最近的邻居找出来；</p><p>采样了4个黄色的点，然后把它们的邻居（红色点）找了出来，在做计算的时候，只在这个子图上做计算，通过控制每次采样的点的数量，可以防止采样的子图过大，以保证内存能够将所有的中间变量存下来。</p><p><img src="image/image_VE2YwqV1HW.png" alt=""></p><h4 id="Random-walk-sampling"><a href="#Random-walk-sampling" class="headerlink" title="Random walk sampling"></a>Random walk sampling</h4><p>表示随机游走采样：从某个一个顶点开始，然后随机在图中找到一条边，然后沿着这条边走到下一个顶点，按照这种方式在图中随机游走。可以通过规定随机游走的步数，来得到子图</p><p><img src="image/image_3fmnRQI4nS.png" alt=""></p><h4 id="Random-walk-with-neighborhood"><a href="#Random-walk-with-neighborhood" class="headerlink" title="Random walk with neighborhood"></a>Random walk with neighborhood</h4><p>表示以上两种方式的结合：先随即走三步，然后把这三步每个点的邻居找出来</p><p><img src="image/image_I9N1f3frd4.png" alt=""></p><h4 id="Diffusion-Sampling"><a href="#Diffusion-Sampling" class="headerlink" title="Diffusion Sampling"></a>Diffusion Sampling</h4><p>表示取一个顶点，然后将它的一近邻、二近邻、三近邻往前走k步做一个宽度遍历，然后把所得到的子图拿出来。</p><p><img src="image/image_BIWidmZiPL.png" alt=""></p><h3 id="（2）如何做batch"><a href="#（2）如何做batch" class="headerlink" title="（2）如何做batch"></a>（2）如何做batch</h3><p>从性能上考虑，不希望对每一个顶点逐步逐步更新，这样每一步的计算量太小，不利于并行，希望是像别的神经网络一样，将小样本做成小批量，这样来对大的矩阵或者tensor做运算。</p><p>这里存在一个问题：<strong>每一个顶点的邻居的个数是不一样的，如何将这些顶点的邻居通过合并变成一个规则的张量是一个具有挑战性的问题</strong>。</p><h2 id="8-3-归纳偏好（inductive-biases）"><a href="#8-3-归纳偏好（inductive-biases）" class="headerlink" title="8.3 归纳偏好（inductive biases）"></a>8.3 归纳偏好（inductive biases）</h2><p>任何一个神经网络，或者说任何一个机器学习的模型中都存在假设</p><ul><li>卷积神经网络假设的是空间变换的不变性</li><li>循环神经网络假设的是时序的延续性</li><li>图神经网络假设的是：保持了图的对称性（不管怎样交换顶点的顺序，GNN对图的作用都是保持不变的）</li></ul><h2 id="8-4-不同汇聚操作比较"><a href="#8-4-不同汇聚操作比较" class="headerlink" title="8.4 不同汇聚操作比较"></a>8.4 不同汇聚操作比较</h2><p>汇聚操作包括求和、求平均或者求max，但是其实没有一种是特别理想的</p><p><img src="image/image_u-nHt6We4s.png" alt=""></p><ul><li>右图表示max和mean不能区分这两个网络</li><li>因此没有一个聚合操作是比另外一个更好的，也就是说，在实际中，这三个聚合操作都差不多，所以在实际应用中，应该具体分析哪一种更加适合</li></ul><h2 id="8-5-GCN作为子图的函数近似"><a href="#8-5-GCN作为子图的函数近似" class="headerlink" title="8.5 GCN作为子图的函数近似"></a>8.5 GCN作为子图的函数近似</h2><p>GCN是graph convolutional network（图卷积神经网络），就是上面提到的那个带汇聚操作的图神经网络</p><ul><li>GCN如果有k个层，每一层都是看一个它的邻居的话，就等价于在卷积神经网络里面有k层3*3的卷积。如果是这样的话，每一个最后一层看到的顶点是一个大小为k的子图（最远的顶点距离当前这个点的距离是k，因为每经过一层就相当于往前看了一步），这样的话可以认为每个点都是看以自己为中心往前走k步的子图的信息的汇聚</li><li>所以从一定程度上来说，GCN可以认为是：n个以自己为中心往前走k步的子图，最后求embeding</li></ul><h2 id="8-6-顶点和边对偶"><a href="#8-6-顶点和边对偶" class="headerlink" title="8.6 顶点和边对偶"></a>8.6 顶点和边对偶</h2><p>图论：可以把点变成边、边变成点，邻接关系表示保持不变，这种变换在GNN上也可以使用</p><h2 id="8-7-图卷积和矩阵乘法的关系"><a href="#8-7-图卷积和矩阵乘法的关系" class="headerlink" title="8.7 图卷积和矩阵乘法的关系"></a>8.7 图卷积和矩阵乘法的关系</h2><p>如何高效实现整个图神经网络的关键点</p><p>在图上做卷积或者做random work，等价于将它的邻接矩阵拿出来做矩阵乘法，page rank就是在一个很大的图上面随机游走，将邻接矩阵和一个向量不断地做乘法</p><h2 id="8-8-graph-attention-network"><a href="#8-8-graph-attention-network" class="headerlink" title="8.8 graph attention network"></a>8.8 graph attention network</h2><p>在图上做汇聚的时候是每个顶点和它邻接的顶点的权重加起来，但是如果是做卷积的话是做加权和，同理，在图上也可以做加权和，但是需要注意卷积的权重是跟位置相关的（每个3*3的窗口固定的点上有固定的权重），而对于图来说不需要有位置信息（因为每个顶点的邻居个数不变，而且邻居是可以随意打乱顺序的，权重对位置信息是不敏感的）,所以可以用注意力机制，权重取决于两个顶点向量之间的关系，而不是顶点的位置，在使用attention之后，可以给每个顶点一个权重，再按这个权重加起来就得到了一个graph attention network</p><h1 id="9-评论"><a href="#9-评论" class="headerlink" title="9.评论"></a><strong>9.评论</strong></h1><h2 id="9-1写作"><a href="#9-1写作" class="headerlink" title="9.1写作"></a>9.1写作</h2><ul><li>首先介绍了什么是图（图的属性（顶点、边、全局）应该用向量来表示），然后介绍了现实生活中的数据如何表示成图，怎样对图（顶点、边、整个图）做预测，机器学习的算法用到图上的时候有一些怎样的挑战</li><li>然后开始讲GNN：首先对GNN下定义（GNN就是对属性做变换，但是不改变图的结构），然后给了一个最简单的例子（用三个全连接层对每个属性单独做变换，再加一个输出层来做预测），再介绍如何解决数据的缺失问题（聚合操作来弥补缺失的属性）</li><li>然后开始介绍什么是真正意义上的GNN，在每一层中通过汇聚操作将整个信息传递过来（每个顶点去看它邻接顶点的信息，以及看它邻接边的信息或者是全局信息），在每个层中如果能够充分地汇聚图上的信息，GNN就能很有效地对整个图的结构进行发掘</li><li>接下来是实验部分：对不同的超参数在数据集上得出了很多结果，然后总结了每个超参数对结果的影响</li><li>最后对GNN相关的问题进行了展开</li><li>这个文章的特点是图很漂亮，制作精美，而且基本上都是交互图（交互图可以把很多信息都放进去，使得图既不是很脏，读者又能通过图获得很多东西）</li></ul><h2 id="9-2-缺点"><a href="#9-2-缺点" class="headerlink" title="9.2 缺点"></a>9.2 缺点</h2><ul><li>图既是优点也是缺点，这种交互图制作非常困难，导致写作门槛比较高</li><li>作者可以避免用公式和代码来对一个问题进行描述，而是大量地使用图和文字。有时候使用公式其实更加简洁准确，代码可以体现出很多细节上的东西，最好能结合公式、代码、图多维度描写</li><li>最后一节宽度的展开有一点画蛇添足，前面已经讲得很好了，但是后面又做了展开，每一个展开并没有讲清楚细节</li></ul><h2 id="9-3-图神经网络"><a href="#9-3-图神经网络" class="headerlink" title="9.3 图神经网络"></a>9.3 图神经网络</h2><ul><li>图是一个非常强大的工具，基本上所有的数据都能够表示成图，但也存在问题：在图上进行优化非常困难，因为图是稀疏架构，每个结构都是动态结构，使得计算比较困难</li><li>图神经网络对超参数是非常敏感的</li><li>上述两个原因导致图神经网络的门槛比较高，在工业上的应用依然比较少</li></ul><p>左图表示max不能区分这两个网络</p>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PaperReading </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ChatGPT 相关核心算法</title>
      <link href="/paper_reading/255.1.ChatGPT%20%E7%9B%B8%E5%85%B3%E6%A0%B8%E5%BF%83%E7%AE%97%E6%B3%95/"/>
      <url>/paper_reading/255.1.ChatGPT%20%E7%9B%B8%E5%85%B3%E6%A0%B8%E5%BF%83%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="1-ChatGPT-相关核心算法"><a href="#1-ChatGPT-相关核心算法" class="headerlink" title="1.ChatGPT 相关核心算法"></a>1.ChatGPT 相关核心算法</h1><p>ChatGPT 的卓越表现得益于其背后多项核心算法的支持和配合。本文将分别介绍作为其实现基础的 Transformer 模型、激发出其所蕴含知识的Prompt/Instruction Tuning 算法、其涌现出的思维链能力、以及确保其与人类意图对齐的基于人类反馈的强化学习算法。</p><h1 id="1-基于Transformer的预训练语言模型"><a href="#1-基于Transformer的预训练语言模型" class="headerlink" title="1.基于Transformer的预训练语言模型"></a>1.基于Transformer的预训练语言模型</h1><p>ChatGPT 强大的基础模型采用 Transformer 架构， Transformer 是一种基于自注意力机制的深度神经网络模型，可以高效并行地处理序列数据。</p><p>原始的 Transformer 模型包含两个关键组件：编码器和解码器。编码器用于将输入序列映射到一组中间表示，解码器则将中间表示转换为目标序列。编码器和解码器都由多层的注意力模块和前馈神经网络模块组成。其中自注意力模块可以学习序列中不同位置之间的依赖关系，即在处理每个位置的信息时，模型会考虑序列中其他所有位置上的信息，这种机制使得 Transformer模型能够有效地处理长距离依赖关系。在原始 Transformer 模型基础上，相继衍生出了三类预训练语言模型：编码预训练语言模型、解码预训练语言模型和编解码预训练语言模型。</p><h2 id="1-1-编码预训练语言模型（Encoder-only-Pre-trained-Models）"><a href="#1-1-编码预训练语言模型（Encoder-only-Pre-trained-Models）" class="headerlink" title="1.1 编码预训练语言模型（Encoder-only Pre-trained Models）"></a>1.1 编码预训练语言模型（Encoder-only Pre-trained Models）</h2><p>这类模型在预训练过程中只利用原始 Transformer 模型中的编码器。相应的预训练任务通常选用掩码语言建模任务（Masked Language Modeling）， 即掩码住（用特殊字符 [MASK] 替换）输入句子中一定比例的单词后，要求模型根据上下文信息去预测被遮掩的单词。其中有有代表性的工作包括  BERT， ALBERT， RoBERTa 等。</p><ul><li><strong>BERT</strong>：BERT 模型是最经典的编码预训练语言模型，其通过掩码语言建模和下一句预测任务，对 Transformer 模型的参数进行预训练。 &#x20;</li><li><strong>ALBERT</strong>：ALBERT 是一个轻量化的 BERT 模型，通过分解词向量矩阵和共享 Transformer 层参数来减少模型参数个数。 &#x20;</li><li><strong>RoBERTa</strong>：相较于 BERT 模型， RoBERTa 在预训练阶段，采用了更多的语料以及动态掩码机制（不同轮次同一样本掩码不同的单词），去掉了下一 句预测任务，同时采用了更大的批大小。</li></ul><h2 id="1-2-解码预训练语言模型（Decoder-only-Pre-trained-Models）"><a href="#1-2-解码预训练语言模型（Decoder-only-Pre-trained-Models）" class="headerlink" title="1.2 解码预训练语言模型（Decoder-only Pre-trained Models）"></a>1.2 解码预训练语言模型（Decoder-only Pre-trained Models）</h2><p>GPT (Generative Pre-trained Transformer) 是由 OpenAI 提出的只有解码器的预训练模型。相较于之前的模型，不再需要对于每个任务采取不同的模型架构，而是用一个取得了优异泛化能力的模型，去针对性地对下游任务进行微调。在本章节将介绍 GPT 系列模型，包括 GPT-1、 GPT-2 和  GPT-3。</p><h3 id="（1）GPT-1"><a href="#（1）GPT-1" class="headerlink" title="（1）GPT-1"></a>（1）GPT-1</h3><p>GPT-1 在文章“Improving Language Understanding by Generative PreTraining”[1] 中被提出。在 GPT 被提出之前，大多数深度学习方法都需要大量人工标注的高质量数据，但是标注数据的代价是巨大的，这极大程度上限制了模型在各项任务性能的上限。<strong>如何利用容易获取的大规模无标注数据来为模型的训练提供指导成为 GPT-1 中需要解决的第一个问题</strong>。另外自然语言处理领域中有许多任务依赖于自然语言在隐含空间中的表征，不同任务对应的表征很可能是不同的，这使得根据一种任务数据学习到的模型很难泛化到其他任务上。因此<strong>如何将从大规模无标注数据上学习到的表征应用到不同的下游任务成为 GPT-1 需要解决的第二个问题</strong>。</p><p>针对第一个问题， GPT-1 中使用了自左到右生成式的目标函数对模型进行预训练。这个目标函数可以简单理解为给定前  $i - 1$ 个 token，对第 $i$ 个 token 进行预测。基于这样的目标函数， GPT-1 就可以利用无标注的自然语言数据进行训练，学习到更深层次的语法信息与语义信息。 &#x20;</p><p>针对第二个问题，在完成了无监督的预训练之后， GPT-1 接着使用了有标注的数据进行有监督的微调使得模型能够更好地适应下游任务。给定输入token 序列$x1, x2, …, xm$ 与标签 y 的数据集，对模型的参数进行再次训练调  整，用到的优化模型是在给定输入序列时预测的标签最接近真实值</p><p>具体来说， GPT-1 在大规模无标注语料库上预训练之后，再利用有标注数据在特定的目标任务上对模型参数进行微调，实现了将预训练中获得的知识迁移到下游任务。</p><p>GPT-1 的结构很简单，由 12 层 Transformer Block（自注意力模块和前馈神经网络模块）叠加而成。下图是 GPT-1 原文中的总览图，左侧是 GPT-1 的架构以及训练时的目标函数；右侧是对于不同任务上进行微调时模型输入与输出的改变。</p><p><img src="image/image_uhbwwNZc2j.png" alt=""></p><h3 id="（2）GPT-2"><a href="#（2）GPT-2" class="headerlink" title="（2）GPT-2"></a>（2）GPT-2</h3><p>与 GPT-1 中的通过预训练-微调范式来解决多个下游任务不同， GPT- 2 更加侧重于 <strong>Zero-shot</strong> 设定下语言模型的能力。 Zero-shot 是指模型在下游任务中不进行任何训练或微调，即模型不再根据下游任务的数据进行参数上的优化，而是根据给定的指令自行理解并完成任务。 &#x20;</p><p>简单来讲， GPT-2 并没有对 GPT-1 的模型架构进行创新，而是在 GPT- 1 的基础上引入任务相关信息作为输出预测的条件，将 GPT-1 中的条件概  率 $p(output|input)$ 变为 $p(output|input; task)$；并继续增大训练的数据规模以及模型本身的参数量，最终在 Zero-shot 的设置下对多个任务都展示了巨大的潜力。 &#x20;</p><p>虽然 GPT-2 并没有模型架构上的改变，但是其将任务作为输出预测的条件引入模型从而在 Zero-shot 的设置下实现多个任务的想法一直延续至今。这样的思想事实上是在传达只要模型足够大，学到的知识足够多，任何有监督任务都可以通过无监督的方式来完成，即任何任务都可以视作生成任务。</p><h3 id="（3）GPT-3"><a href="#（3）GPT-3" class="headerlink" title="（3）GPT-3"></a>（3）GPT-3</h3><p>GPT-3使用了与 GPT-2 相同的模型和架构。文中为了探索模型规模 对于性能的影响，一共训练了 8 个不同大小的模型，并将最大的具有 1750 亿参数的模型称为 GPT-3。</p><p>GPT-3 最显著的特点就是大。大体现在两方面，一方面是模型本身规模大，参数量众多，具有 96 层 Transformer Decoder Layer，每一层有 96 个 128 维的注意力头，单词嵌入的维度也达到了 12,288；另一方面是训练过程中使用到的数据集规模大，达到了 45TB。在这样的模型规模与数据量 的情况下， GPT-3 在多个任务上均展现出了非常优异的性能，延续 GPT-2 将无监督模型应用到有监督任务的思想， GPT-3 在 Few-shot， One-shot 和 Zero-shot 等设置下的任务表现都得到了显著的提升。</p><h2 id="1-3-基于编解码架构的预训练语言模型（Encoder-decoder-Pretrained-Models）"><a href="#1-3-基于编解码架构的预训练语言模型（Encoder-decoder-Pretrained-Models）" class="headerlink" title="1.3 基于编解码架构的预训练语言模型（Encoder-decoder Pretrained Models）"></a>1.3 基于编解码架构的预训练语言模型（Encoder-decoder Pretrained Models）</h2><p>基于编码器的架构得益于双向编码的全局可见性，在语言理解的相关任务上性能卓越，但是因为无法进行可变长度的生成，不能应用于生成任务。</p><p>基于解码器的架构采用单向自回归模式，可以完成生成任务，但是信息只能从左到右单向流动，模型只知“上文”而不知“下文”，缺乏双向交互。针对 以上问题，一些模型采用序列到序列的架构来融合两种结构，使用编码器提取出输入中有用的表示，来辅助并约束解码器的生成。</p><ul><li><strong>BART</strong>：BART 的具体结构为一个双向的编码器拼接一个单向的自回归解码器，采用的预训练方式为输入含有各种噪声的文本，再由模型进行去噪重构。在解码器部分， BART 每一层对编码器的最后一层的隐藏表示执行交叉注意力机制以聚合关键信息。 BART 在维基百科和 BookCorpus 数据集上训练，数据量达 160GB。 &#x20;</li><li><strong>T5</strong>：BART 为了兼顾不同任务设计了复杂的预训练任务，针对如何在多个任务中实现优秀的迁移性能这一问题，谷歌研究者提出了一种新的范式：将所有自然语言处理任务统一成“文本到文本”的生成任务。 T5 通过在输入之前加入提示词，实现了用单个模型解决机器翻译、文本摘要、问答和分类等多个任务。针对迁移学习需要的巨量、高质量和多样的预训练数据， T5 在谷歌专门构造的 C4 数据集上进行训练。 &#x20;</li><li><strong>Switch Transformers</strong>：随着语言模型的深入研究，参数量的增加可以显著提高模型的性能，但随之而来的就是应用时越来越大的运算量。 SwicthTransformer 将混合专家网络（Mixture-of-Experts， MoE）的条件运算思想引入 Transformer 的全连接层，实现增加模型的尺寸而不增加推理时的运算量。</li></ul><h1 id="2-提示学习与指令精调"><a href="#2-提示学习与指令精调" class="headerlink" title="2.提示学习与指令精调"></a>2.提示学习与指令精调</h1><h2 id="2-1-提示学习概述"><a href="#2-1-提示学习概述" class="headerlink" title="2.1 提示学习概述"></a>2.1 提示学习概述</h2><p><strong>提示学习（Prompt Learning）</strong> 简单来说是通过一些方法编辑下游任务的输入，使其形式上模拟模型预训练过程使用的数据与任务。比如做情感分类任务时，监督学习的做法是输入“我今天考砸了”，模型输出分类的分数或分布，而提示学习的做法则是在“我今天考砸了”后拼接上自然语言描述“我感觉很 ____”，让模型生成后面的内容，再根据某种映射函数，将生成内容匹配到某一分类标签。 &#x20;</p><p>可以看出，提示学习这种方式拉近了测试分布与预训练分布的距离，进而可以利用大规模预训练语言模型在预训练过程中习得的强大语言建模能力，使其不经过微调就可以在各种下游任务上取得很好的结果。后续更有工作提出了自动提示搜索和连续提示的方法，使得提示本身也可以微调，使其  有了更好的灵活性。</p><p>相较于提示学习， <strong>指令精调（Instruction Tuning）</strong> 可以说是提示学习的加强版。两种学习方法的本质目标均是希望通过编辑输入来深挖模型自身所蕴含的潜在知识，进而更好的完成下游任务。</p><p>而与提示学习不同的是，指令学习不再满足于模仿预训练数据的分布，而是希望通过构造“指令  （Instruction）”并微调的方式，学习人类交互模式的分布，使模型更好的理解人类意图，与人类行为对齐；在指令学习中，模型需要面对的不再是单纯的补全任务，而是各种不同任务的“指令”，即任务要求。模型需要根据不同的任务要求，做出相匹配的正确回复。“指令”举例如下： &#x20;</p><ul><li>请将下面这句话翻译成英文“ChatGPT 都用到了哪些核心技术？” &#x20;</li><li>请帮我把下面这句话进行中文分词“我太喜欢 ChatGPT 了!” &#x20;</li><li>请帮我写一首描绘春天的诗词，诗词中要有鸟、花、草。 &#x20;</li></ul><p>从样例中可以看出，原本自然语言处理中的经典任务，经过任务要求的包装后，就变成了更符合人类习惯的“指令”。</p><p>研究表明，当“指令”任务的种类达到一定量级后，大模型甚至可以在没有见过的零样本（Zero-shot） 任务上有较好的处理能力。因此，指令学习可以帮助语言模型训练更深层 次的语言理解能力，以及处理各种不同任务的零样本学习能力。 OpenAI  提出的 InstructGPT 模型使用的就是指令学习的思想， ChatGPT 沿袭了InstructGPT 的方法。</p><h2 id="2-2-ChatGPT中的指令学习"><a href="#2-2-ChatGPT中的指令学习" class="headerlink" title="2.2 ChatGPT中的指令学习"></a>2.2 ChatGPT中的指令学习</h2><p>ChatGPT 所用到的指令学习数据集的构造方法和训练方法与 InstructGPT 大致相同，因此介绍 InstructGPT 构造“指令”数据集的细节。 &#x20;</p><p>InstructGPT 的“指令”数据集由两部分构成，<strong>其中一部分收集于全球用户使用 OpenAI 的 API 后的真实人机交互数据，这些数据在使用之前都经过了信息去重和敏感信息过滤；另一部分数据则来自于人工标注</strong>。</p><p>为了使标注人员能够标注出高质量的数据集， OpenAI 通过前期的审核和面试，聘请了一个由 40 人组成的标注团队。在这些人工标注的数据中，总共分为三类，其一是为了增加数据集中任务的多样性，由标注人员写出任意任务的 “指令”；其二是小样本（Few-shot）数据，由标注人员写出“指令”和一些对应的问答对，用于训练模型的小样本学习（Few-shot learning）能力；其三  是在 OpenAI API 中已有的用例，标注人员模仿这些用例写出相类似的“指令”数据。这些数据包含了语言模型中常见的任务类型（生成、问答、聊天、改写、总结、分类等），其中 45.6% 的“指令”为生成任务类型，在所有类型中占比最大。 &#x20;</p><p>InstructGPT 通过在构造的”指令”数据集上进行有监督微调（Supervised fne-tuning, SFT）和基于人工反馈的强化学习（Reinforcement Learning  from Human Feedback, RLHF）以使模型与人类需求对齐。 &#x20;</p><p>在实验结果上，将运用指令学习后且含有 175B 参数的 InstructGPT 模 型，在指令学习的经典数据集 FLAN、 T0 上进行精调后发现， InstructGPT 模型对比 FLAN、 T0 两个模型在效果上均有一定程度的提升。</p><h1 id="3-思维链（Chain-of-Thought，-COT）"><a href="#3-思维链（Chain-of-Thought，-COT）" class="headerlink" title="3.思维链（Chain of Thought， COT）"></a>3.思维链（Chain of Thought， COT）</h1><p>人类在解决数学应用题这类复杂推理任务的过程中，通常会将问题分解为多个中间步骤，并逐步求解，进而给出最终的答案，例如求解问题“小华每天读 24 页书， 12 天读完了《红岩》一书，小明每天读 36 页书，几天可以读完《红岩》？”，人会将问题分解为（1）“红岩共 24*12=288（页）”、（2）“小明可以用 288÷36=8(天)”。受此启发，谷歌研究人员 Jason Wei（现 OpenAI 员工）等提出了思维链，通过在小样本提示学习的示例中插入一系列中间推理步骤，有效提升了大规模语言模型的推理能力，图中展示模型通过产生思维链来正确求解数学应用题。 &#x20;</p><p><img src="image/image_2r1PDTQZLa.png" alt=""></p><p>相较于一般的小样本提示学习，思维链提示学习有几个吸引人的性质： &#x20;</p><ol><li>在思维链的加持下，模型可以将需要进行多步推理的问题分解为一系列的中间步骤，这可以将额外的计算资源分配到需要推理的问题上。 &#x20;</li><li>思维链为模型的推理行为提供了一个可解释的窗口，使通过调试推理路径来探测黑盒语言模型成为了可能。 &#x20;</li><li>思维链推理应用广泛，不仅可以用于数学应用题求解、常识推理和符号操作等任务，而且可能适用任何需要通过语言解决的问题。 &#x20;</li><li>思维链使用方式非常简单，可以非常容易地融入语境学习（in-context learning），从而诱导大语言模型展现出推理能力。</li></ol><h1 id="4-基于人类反馈的强化学习（Reinforcement-Learning-with-Human-Feedback，-RLHF）"><a href="#4-基于人类反馈的强化学习（Reinforcement-Learning-with-Human-Feedback，-RLHF）" class="headerlink" title="4.基于人类反馈的强化学习（Reinforcement   Learning with Human Feedback， RLHF）"></a>4.基于人类反馈的强化学习（Reinforcement   Learning with Human Feedback， RLHF）</h1><p>RLHF 是 ChatGPT/InstrcutGPT 实现与人类意图对齐，即按照人类指令尽可能生成无负面影响结果的重要技术。该算法在强化学习框架下实现，大体可分为以下两个阶段：</p><h3 id="（1）奖励模型训练"><a href="#（1）奖励模型训练" class="headerlink" title="（1）奖励模型训练"></a>（1）奖励模型训练</h3><p>该阶段旨在获取拟合人类偏好的奖励模型。奖励模型以提示和回复作为输入，计算标量奖励值作为输出。奖励模型的训练过程通过拟合人类对于不同回复的倾向性实现。具体而言，首先基于在人类撰写数据上精调的模型，针对同一提示采样多条不同回复。然后，将回复两两组合构成一条奖励模型训练样本，由人类给出倾向性标签。最终，奖励模型通过每条样本中两个回复的奖励值之差计算倾向性概率拟合人类标签，进而完成奖励模型的训练。</p><h3 id="（2）生成策略优化"><a href="#（2）生成策略优化" class="headerlink" title="（2）生成策略优化"></a>（2）生成策略优化</h3><p>给定习得的奖励模型， ChatGPT/InstructGPT 的参数将被视为一种策略，在强化学习的框架下进行训练。首先，当前策略根据输入的查询采样回复。然后，奖励模型针对回复的质量计算奖励，反馈回当前策略用以更新。值得注意的是，为防止上述过程的过度优化，损失函数同时引入了词级别的 KL 惩罚项。此外，为了避免在公开 NLP 数据集上的性能退化，策略更新过程兼顾了预训练损失。</p>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> PaperReading </tag>
            
            <tag> GPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 GPT-4</title>
      <link href="/paper_reading/2.7.GPT-4/"/>
      <url>/paper_reading/2.7.GPT-4/</url>
      
        <content type="html"><![CDATA[<h1 id="8-GPT-4"><a href="#8-GPT-4" class="headerlink" title="8.GPT-4"></a>8.GPT-4</h1><p>GPT-4 Technical Report：<a href="https://arxiv.org/pdf/2303.08774.pdf" title="2303.08774.pdf (arxiv.org)">2303.08774.pdf (arxiv.org)</a></p><p>OpenAI博客：<a href="https://openai.com/research/gpt-4" title="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a></p><p>博客基本是99页技术报告的缩略版，<strong>2023.3.14</strong>发布。</p><h1 id="0-疯狂的三月（202303）"><a href="#0-疯狂的三月（202303）" class="headerlink" title="0.疯狂的三月（202303）"></a>0.疯狂的三月（202303）</h1><ul><li><strong>03-08</strong> 微软发布Visual ChatGPT，聊天时可以用图片，并可以根据文字对图片进行修改<ul><li>论文：<a href="https://arxiv.org/abs/2303.04671" title="https://arxiv.org/abs/2303.04671">https://arxiv.org/abs/2303.04671</a></li><li>代码：<a href="https://github.com/microsoft/visual-chatgpt" title="https://github.com/microsoft/visual-chatgpt">https://github.com/microsoft/visual-chatgpt</a></li></ul></li><li><strong>03-09</strong> 微软宣布将要发布大型多模态模型GPT4</li><li><strong>03-09</strong> 10亿规模的模型GigaGAN推出<ul><li>论文：<a href="https://arxiv.org/abs/2303.05511" title="https://arxiv.org/abs/2303.05511">https://arxiv.org/abs/2303.05511</a></li></ul></li><li><strong>03-13</strong> 斯坦福大学推出7B的Alpaca模型<ul><li>代码：<a href="https://github.com/tatsu-lab/stanford_alpaca" title="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></li></ul></li><li><strong>03-14</strong> GPT4推出</li><li><strong>03-14</strong> 谷歌公布PALM模型的API使用</li><li><strong>03-14</strong> Anthropic介绍大型语言模型Claude，主打安全性</li><li><strong>03-14</strong> Adapt.ai公布他们的模型也能够使用工具</li><li><strong>03-15</strong> Midjourney推出第五代模型，可以完美处理手部细节</li><li><strong>03-15</strong> pytorch2.0发布</li><li><strong>03-16</strong> 微软公布ChatGPT加持的Copilot&#x20;</li></ul><p>GPT-4发布了99页的一份报告，但没有任何技术细节；</p><p>pytorch创始人对GPT-4评价</p><p><img src="image/image_KV2nvCVxS1.png" alt=""></p><h1 id="1-导言"><a href="#1-导言" class="headerlink" title="1.导言"></a>1.导言</h1><h2 id="1-1-GPT-4-简介"><a href="#1-1-GPT-4-简介" class="headerlink" title="1.1 GPT-4 简介"></a>1.1 GPT-4 简介</h2><p>GPT-4是<strong>一个多模态的模型</strong>，<strong>能够接受文本或者是图片的输入，最后输出纯文本</strong>。</p><p>GPT-4在真实世界中与人还是存在差距，但是在很多具有专业性或者学术性的数据集或者任务上面上，GPT-4有时候能够达到甚至超过人类的水平</p><p>GPT-4<strong>基本能够达到类人的表现</strong>，在事实性、可控性和安全性上有了很大的进步。</p><p>OpenAI花费了6个月通过对抗测试项目和ChatGPT的经验对齐（align）GPT-4，取得了有史以来最好的（虽然远非完美）的真实性、可控性。</p><p>GPT-4能够通过律师考试资格证考试，且能在所有参加考试的人中排名前10%（GPT-3.5在同样的考试中无法通过，且只能排到最后10%）</p><h2 id="1-2-GPT-4训练的稳定性"><a href="#1-2-GPT-4训练的稳定性" class="headerlink" title="1.2 GPT-4训练的稳定性"></a>1.2 GPT-4<strong>训练的稳定性</strong></h2><p>在此次GPT-4的训练过程中，训练表现出了前所未有的稳定性</p><ul><li>训练稳定</li><li>硬件设施没有出错</li><li>训练不会中断，一次训练直接跑到底</li><li>loss没有跑飞</li><li>更重要的是，可以<strong>准确预测模型训练的结果</strong>（通过在小规模计算成本下训练出来的模型可以准确地预估扩大计算成本之后模型的最终性能）</li></ul><p>对于大模型来讲，如果每次跑完训练才知道结果（参数的好坏，改进是否有效），花销比较大，一般会在较小的模型或者较小的数据集上做消融实验，验证之后再去大模型上进行实验</p><p>对于语言模型来讲，由于语言的扩展较大，所以导致在小规模模型上做的实验可能有效，但是换到大模型上就达不到想要的结果了；并且大模型上特有的涌现能力在小模型上无法观测</p><h1 id="2-训练过程"><a href="#2-训练过程" class="headerlink" title="2.训练过程"></a>2.训练过程</h1><p>与之前的GPT模型类似，GPT-4也是通过<strong>预测文章中下一个词的方式（Language Modeling Loss）去训练的</strong>，训练所用到的数据是公开数据（网络数据和公司所购买的数据）</p><ul><li>数据集非常大，包含了非常多的内容，比如数学问题的正确的解和不正确的解、弱推理、强推理、自相矛盾或者保持一致的陈述、各种意识形态和想法，以及更多的纯文本数据</li><li>因为在大量的数据集上训练过，而且有的时候是在不正确的答案上训练过，所以预训练模型（Base Model）有些时候的回答跟想要得到的回答相差很远。为了能跟人的意图尽可能保持一致，并且更加安全可控，所以使用<strong>RLHF</strong>（Reinforcement Learning with Human Feedback）的方法对模型进行了微调。</li></ul><p><strong>模型的能力看起来像是从预训练的过程中得到的</strong>，后续RLHF所进行的微调并不能够提高在测试中的性能（如果没有好好调参，甚至会降低测试的性能）。</p><ul><li>模型所谓的涌现的能力靠堆数据、堆算力，然后用简单的Language Modeling Loss堆出来的</li></ul><p>但是，<strong>RLHF用来对模型做控制，让模型更加清楚人类的意图，并且按照人类所能接受的方式做出回答</strong>。</p><ul><li>这个预训练模型甚至需要prompt engineering才知道需要回答问题</li></ul><h1 id="3-可预测的扩展性（Predictable-scaling）"><a href="#3-可预测的扩展性（Predictable-scaling）" class="headerlink" title="3.可预测的扩展性（Predictable scaling）"></a>3.可预测的扩展性（<strong>Predictable scaling</strong>）</h1><p>GPT-4的关键问题在于<strong>如何构建深度学习的infrastructure</strong>，然后准确地进行扩大</p><ul><li>主要原因是在大模型上是<strong>不可能做大规模的模型调参的</strong>，首先需要很多的算力，其次需要很长的训练时间。如果增加训练机器的数量，训练的稳定性也不能保证，多机器的并行训练很容易导致Loss跑飞。</li></ul><p>OpenAI<strong>研发出来了一套整体的infrastructure和优化方法</strong>，可以在多个尺度上的实验上达到稳定预测。为了验证，利用内部的代码库在GPT-4模型刚开始训练的时候，就已经可以准确地预测到GPT-4最终训练完成的Loss（预测结果是由另外一个Loss外推出去的，用了比原始所需计算资源小一万倍的计算资源上用同样的计算方法训练出来的模型）。</p><p><img src="image/image_g0WjhKBwfb.png" alt=""></p><ul><li>图中绿色的点是GPT-4最终的Loss的结果</li><li>纵坐标可以理解成Loss的大小，单位是Bits per word</li><li>横坐标表示所使用的算力（这里将数据集的大小、模型的大小全部混在一起，表示总体训练一个模型所需要的算力），越往左，模型的训练代价越小</li></ul><p>OpenAI通过将不同训练代价下的Loss点进行拟合，从而准确得到GPT-4最终的Loss。在同等的资源下，可以以更快的速度尝试更多的方法，最后得到更优的模型。</p><p>还有一些能力是不能完全准确预测的：</p><ul><li>inverse scaling prize竞赛，<strong>专门给大模型找茬，用来测试是否存在一些任务是小模型做的好，大模型反而做不好的</strong>，而且最好能够找到那些任务（随着计算成本的增加，任务的结果越来越差）</li></ul><p><img src="image/image_WbDXvkADXK.png" alt=""></p><ul><li><strong>hindsight neglect</strong>：过去做一件事情的时候，使用很理性地判断做出一个决断，这个决断按道理来讲是正确的，但是运气不好导致最终的结果不是很好，那么如果回到过去，是继续选择当初选择的理性做法还是愿意赌一把选择一个更冒险的方式。</li><li>按道理来讲，每次做选择都应该按照最理性的方式做选择，但是大模型在这种情况下出现了一个很有意思的现象：<strong>随着模型越来越大，反而越来越不理性，会根据最后的结果来判断到底应不应该做出决定</strong>。</li><li>GPT-4的准确度达到了100%，从<strong>侧面说明了可能GPT-4已经拥有了一定的推理能力</strong>，不会受到最后结果的影响</li></ul><p><strong>hindsight neglect举例</strong></p><blockquote><p>两种示例：<br>1、张三玩一个游戏，有90%概率赢100块，10%概率输掉10块。张三玩后赢了，回到过去是否应该玩？<br>2、张三玩一个游戏，有10%概率赢10块，90%概率输掉100块。张三玩后赢了，回到过去是否应该玩？<br>从理性分析，这里就是计算数学期望，显然示例1中期望是89块，张三应该选择玩；示例2中期望是-89块，张三即使之前运气好赢了，也应该选择不玩。GPT4在这类问题中能够给出合理的行为。&#x20;</p></blockquote><h3 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a><strong>拓展</strong></h3><p>训练的稳定性多么难能可贵——斯坦福MLSYS 在MetaAi怎样用三个月的时间做了一个跟GPT-3同等大小的语言模型（OPT-175Billion）</p><ul><li>地址：<a href="https://www.bilibili.com/video/BV1XT411v7c9?t=1283.6" title="https://www.bilibili.com/video/BV1XT411v7c9?t=1283.6">https://www.bilibili.com/video/BV1XT411v7c9?t=1283.6</a></li><li>模型虽然性能一般，但是整个过程干货比较多</li></ul><p><img src="https://i0.hdslb.com/bfs/note/0980caa519c2cc5f2496a9ad2fdb9db2d630f9cc.png@690w_!web-note.webp" alt=""></p><ul><li>OPT-175Billion在整个一个多月的训练过程中，因为各种各样的原因（机器崩掉，网络中断、Loss跑飞等），中间一共中断了五十多次，图中的每一段就代表跑的一段</li><li>训练一个大的模型的工程复杂度是难以想象的</li></ul><h1 id="4-能力"><a href="#4-能力" class="headerlink" title="4.能力"></a>4.能力</h1><p>在日常对话中，GPT-3.5和GPT-4的区别是非常小的，但是这个区别随着任务难度的增加慢慢会体现出来。<strong>GPT-4更加可靠，更加具有创造力，而且能够处理更加细微的人类的指示</strong>。</p><p>为了弄清楚这两个模型之间的区别，OpenAI设计了一系列的benchmark，包含很多之前专门为人类设计的模拟考试，使用了最近公开的一些数据，比如奥赛题目、AP（美国高中的一些大学先修课中的问题、购买的执照考试的版权数据）。<strong>在这些考试上没有做过特殊的训练</strong>：</p><ul><li>可能有一些问题是之前在模型预训练的过程中被模型见过的，这里OpenAI为了澄清，他们跑了两个版本：<strong>一个版本是模型直接考试然后汇报分数</strong>；<strong>另一个版本虽然采用同样的模型，但是把在预训练数据集中出现的问题拿掉，只在那些模型可能没见过的问题上再做一次测试</strong>，最后取这两次的分钟较低的那一次来作为GPT-4的分数。希望这么做能更加具有说服力。</li><li>这里的问题去重并没有说明具体的方法</li><li>GPT-4能在众多的考试中都取得较好的结果，说明其参加考试的能力还是不错的</li><li>GPT-4考试结果如下图所示</li></ul><p><img src="image/image_ltq9ehaSmj.png" alt=""></p><ul><li>柱状图是按照GPT-3.5的性能从低到高进行排列的</li><li>GPT-3.5在最右侧的AP Environmental Science中表现是最好的</li><li>淡绿色（no vision）表示没有使用图片</li><li>图中可以看出GPT-4在有了图片加持之后，在有些考试上还能获得更大的进步</li><li>在AP Caculus BC、AMC12、Codeforces Rating、AMC10上表现较差，<strong>GPT系列在数学上的表现比较差</strong>。</li><li>此外，虽然GPT-4能够修改文案，修改语法、润色文章，但是<strong>在高中英语文学课上以及高中英语语言本身的考试上得分都比较差</strong>。GPT系列的模型虽然能够生成大段大段的文字，但是它所写出来的东西很多时候就是翻来覆去地说话，都是一些空话大话，非常冠冕堂皇，并没有真正的思考，从而形成深刻的洞见。</li></ul><p>具体的考试结果如下图所示</p><p><img src="image/image_aEYox6s4vW.png" alt=""></p><p>GPT-4在传统的benchmark上的性能测试结果如下图所示</p><p><img src="image/image_W4oNc0KqOJ.png" alt=""></p><p><strong>GPT-4在多语言方面的能力</strong></p><ul><li>GPT-4在多语言上已经做得很好了，不仅是英语语系中的各种语言，对中文的支持也是不错的（<strong>能够识别拼音的输入，简体/繁体的转换也能够处理</strong>）</li><li>OpenAI为了进行测试，将MMLU全部进行了翻译（将14000多个多选题用微软的翻译全部翻译成不同的语言），通过测试发现，在26个语言中，其中24个语言中的测试结果GPT-4都要优于GPT-3.5和其他的一些大模型（Google的Chinchilla、PaLM），而且甚至在那些没有什么训练语料库的语言（Latvian、Welsh、Swahili）上表现也很好</li><li>测试结果如下图所示</li></ul><p><img src="image/image_NNr3nz9qaQ.png" alt=""></p><h1 id="5-视觉输入"><a href="#5-视觉输入" class="headerlink" title="5.视觉输入"></a>5.视觉输入</h1><h2 id="5-1-简介"><a href="#5-1-简介" class="headerlink" title="5.1 简介"></a>5.1 简介</h2><p>GPT-4是一个多模态的模型，可以接受图片作为输入；GPT-4可以允许用户任意自定义视觉或者语言任务，不管用户输入的是文本、图片或者是图片和文本混合的形式，GPT-4都能生成文本（自然语言、代码）。</p><p>但是目前图像输入还是内测阶段，暂时不对大众开放。OpenAI目前只选择了一家合作伙伴Be My Eyes来测试视觉功能；初衷是将图片转化成文字，然后再转成语音，从而为盲人提供更加便利的生活。</p><h2 id="5-2-示例"><a href="#5-2-示例" class="headerlink" title="5.2 示例"></a>5.2 示例</h2><p>1、用户输入几张照片，然后询问GPT-4这几张照片搞笑的地方在哪里？</p><ul><li>很多时候GPT-4都能给出解释，而且是一步一步的解释为什么搞笑。</li></ul><p><img src="https://i0.hdslb.com/bfs/note/bab8006b463e96405df9d75084d08eed0548e14c.png@690w_!web-note.webp" alt=""></p><p>2、图中是一个截图，并不是机器能够直接阅读的，需要内部自己做一个OCR才能让模型知道图片中到底是什么内容（截图中是一道法语描述的物理题）</p><ul><li>GPT-4用英语进行了一步一步的解释，最后得出答案</li></ul><p><img src="https://i0.hdslb.com/bfs/note/d49338b9ffd2bdb57ba93edef20f74e671667092.png@690w_!web-note.webp" alt=""></p><p>3、将一篇论文直接输入进GPT-4，让它输出对论文的总结</p><p><img src="https://i0.hdslb.com/bfs/note/6e2587661cd2b923b1185fff2b394711fe811a31.png@690w_!web-note.webp" alt=""></p><ul><li>GPT-4能够很好地总结所输入的论文</li><li>最近github上也发布了几个工具，通过调用OpenAI或者其他的模型，用户输入一个pdf，工具直接输出文章的摘要，而且也可以在里面随意地进行搜索（交互式地询问，而不用一个一个地找）（<a href="https://www.chatpdf.com/" title="https://www.chatpdf.com">https://www.chatpdf.com</a>）</li></ul><h2 id="5-3-多模态性能"><a href="#5-3-多模态性能" class="headerlink" title="5.3 多模态性能"></a>5.3 多模态性能</h2><p><img src="image/image_DjQQBk12nL.png" alt=""></p><p>测试结果虽然一般，不如在NLP中的测试结果那么惊艳，但是这些分数并不能完全代表GPT-4的能力，因为还在持续不断地发掘GPT-4更多的能力。</p><h1 id="6-可操作性（Steerability）"><a href="#6-可操作性（Steerability）" class="headerlink" title="6.可操作性（Steerability）"></a>6.可操作性（<strong>Steerability</strong>）</h1><p><strong>定义语言模型的行为</strong>，让语言模型按照用户所想要的方式进行答复。相比于ChatGPT，ChatGPT的人格是固定的，每次都是同样的语调语气，回复的风格也是一致的；最新的GPT-4开发了一个新功能，除了发给它的prompt（描述用户需求的文字），前面<strong>添加了System Message</strong>。System Message可以定义AI使用什么样的语气语调进行对话。</p><p>System Message是由整个Community发现的。</p><p><strong>Steerability举例</strong></p><p>作为一个苏格拉底式的辅导员，回复永远都应该是保持苏格拉底的风格，即永远不告诉学生真正的答案，而是询问一些启发式的问题，通过暗示来进行辅导让学生自己意识到问题的解决方式，从而培养学生自己解决问题的能力。在这个过程中，将难度较大的问题进行拆分，在学生能够听懂的水平上进行因材施教</p><p><img src="https://i0.hdslb.com/bfs/note/13982ee16d4954c7f0a5640ea781c35056528c4c.png@690w_!web-note.webp" alt=""></p><h1 id="7-限制（Limitations）"><a href="#7-限制（Limitations）" class="headerlink" title="7.限制（Limitations）"></a>7.限制（<strong>Limitations</strong>）</h1><p>1、在能力和局限性方面，GPT-4和之前的GPT系列模型差不多，还是<strong>不能完全可靠，有的时候还是会瞎编乱造，扭曲事实</strong>，并且推理的时候也可能会出错。因此在使用这些大模型的时候还是需要更加小心谨慎，尤其是在一些高风险的领域（法律、金融、新闻、政治）中</p><ul><li>虽然这些问题依然存在，但是GPT-4跟之前其他的模型以及外面的模型相比，在安全性上已经大幅提高了</li><li>在OpenAI内部专门用来进行对抗性测试的Evaluation Benchmark上，GPT-4比之前的GPT-3.5的得分要高出40%以上，提升显著</li></ul><p><img src="image/image_avufJ_S9LW.png" alt=""></p><ul><li>图中纵坐标表示准确度，横坐标表示OpenAI内部所使用的benchmark所涉及的领域。</li></ul><p>2、GPT-4本身还会<strong>有各种各样的偏见</strong>，目前已经取得了一些进步，但是还有很多需要做的。</p><p>3、GPT-4一般是<strong>缺少2021年9月份之后的知识</strong>，因为预训练数据就是截止到2021年9月份。但是ChatGPT有很多个版本，可能后续微调或者RLHF的时候，可能包含更新之后的数据，所以有时候也能正确回答2021年之后的一些问题。</p><p>4、GPT-4在很多的领域里都表现出强大的能力，取得很高的分数，但是<strong>有时候会犯一些非常简单的推理错误</strong>，看上去有点不可思议。如果用户故意输入一些虚假的陈述，GPT-4还<strong>非常容易上当受骗</strong>。</p><p>5、在一些特别困难的问题上，GPT-4跟人差不多，都会出现安全隐患，可能会写出不正确的代码。但是<strong>GPT-4哪怕有的时候预测错误了，也会非常自信</strong>。</p><p>通过研究发现，这是因为经过预训练之后，GPT-4的model calibration做的非常完美</p><p><strong>calibration（校准）的定义</strong></p><p><img src="image/image_nZhBC1WysJ.png" alt=""></p><ul><li>从图中能够看出，模型经过了完美的矫正（可以因为预训练的语料库比较大，已经掌握了客观事实的规律，因此模型对自己产生的结果比较自信）</li><li>但是<strong>经过后处理（Instructed Tuning或者是RLHF）之后，calibration的效果就没有了</strong>，模型的校准就没有处理前好了（可能是经过RLHF之后，模型更接近于人，具备一定的主观性，因此校准性能就下降了）。</li></ul><h1 id="8-Risks-amp-mitigations"><a href="#8-Risks-amp-mitigations" class="headerlink" title="8.Risks &amp; mitigations"></a>8.<strong>Risks &amp; mitigations</strong></h1><p>1、<strong>Red Teaming</strong>（对抗测试）：通过找各个领域的专家询问模型该问和不该问的问题，希望让模型知道哪些应该回答，哪些不该回答，通过<strong>人力的过程搜集数据，从而提升GPT-4的能力，能够拒绝不合理的要求</strong>。</p><p>2、<strong>GPT-4还利用自己来提升安全性的要求，在后续的RLHF的训练过程中</strong>，又新加了一个专门做安全的reward signal</p><ul><li>这个reward signal是从自己已经预训练好的GPT-4模型开始，通过<strong>分类器</strong>分类当前prompt到底是不是sensitive，是不是存在危险，可能不应该进行回答</li><li>通过reward signal让RLHF更加智能，让模型更加贴合人的意图，而且更加安全</li></ul><p>这种减少risk的方式能够显著提升GPT-4的安全性能，和GPT-3.5相比，<strong>对于那些不该回答的问题，GPT-4能比GPT-3.5少回答82%的问题</strong>。</p><p><strong>安全性问题举例</strong></p><p><img src="image/image_W2NdaGa_yw.png" alt=""></p><p>总的来说，模型层面的干扰技巧能够很大程度上防止模型生成不好的行为，但是也不能完全阻止，<strong>总归是能找出各种各样的漏洞，还有很多的工作要做</strong>。</p><h1 id="9其他"><a href="#9其他" class="headerlink" title="9其他"></a>9其他</h1><h2 id="9-1-AGI"><a href="#9-1-AGI" class="headerlink" title="9.1 AGI"></a>9.1 AGI</h2><p>微软拿早期的GPT-4模型，做了许多测试，发了一篇100多页论文：</p><ul><li>论文题目：Aparks of Artificial General Intelligence：Early experiments with GPT-4</li><li>论文地址：<a href="https://arxiv.org/abs/2303.12712" title="https://arxiv.org/abs/2303.12712">https://arxiv.org/abs/2303.12712</a></li></ul><p><strong>GPT-4潜在能力</strong></p><p>1、图像生成</p><ul><li>用户给GPT-4一些指示，生成一些能够作画的代码，然后使用这些代码直接生成图画，从而变相地进行图像生成（简单作画）</li><li>不光能够生成画，还能够不断地对生成的画（实际上是代码）进行改进（GPT-4不光是可以根据用户的指示不断进化从而得到更好的结果，同时GPT-4的模型自己也在进化）</li></ul><p>2、音乐生成</p><p>3、使用工具</p><h2 id="9-2-GPTs对劳动力影响"><a href="#9-2-GPTs对劳动力影响" class="headerlink" title="9.2 GPTs对劳动力影响"></a>9.2 GPTs对劳动力影响</h2><ul><li>论文名称：GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models</li><li>论文地址：<a href="https://arxiv.org/abs/2303.10130" title="https://arxiv.org/abs/2303.10130">https://arxiv.org/abs/2303.10130</a></li></ul><p>1、大概有80%的美国劳动力会因为大语言模型的到来而受到影响，大概是平时工作中10%的任务（剩余90%的任务仍然需要人来完成），19%的工人会发现有50%的工作有可能会被影响（AI至少能够完成50%以上的工作任务）</p><p>2、受影响比较多的工作</p><ul><li>如果有做<strong>科研（基础科学研究）的能力</strong>或者<strong>思维比较缜密，能够快速做出合理的决定</strong>，这些技能点大语言模型暂时还不具备</li><li>和大语言模型冲突的技能点：写代码、写文章。凡是和这两个技能点相关的工作可能会收到较大的影响</li></ul><p>哪些职业会受到较大的影响：</p><p><img src="https://i0.hdslb.com/bfs/note/c930468b4e6b78571358af74a35e578a4077e6c7.png@690w_!web-note.webp" alt=""></p><p>3、基本不受影响的职业</p><ul><li>这些职业都是需要<strong>实际动手操作</strong>的，在机械臂和机器人成熟之前，必须要真人进行操作，因此这些工种暂时是不会受到大语言模型影响的。</li></ul><p><img src="https://i0.hdslb.com/bfs/note/a391fa82ccd20e7788c7dc3808304282d271baec.png@690w_!web-note.webp" alt=""></p><h2 id="9-3-Yann-Lecun报告（20230324）"><a href="#9-3-Yann-Lecun报告（20230324）" class="headerlink" title="9.3 Yann Lecun报告（20230324）"></a>9.3 Yann Lecun报告（20230324）</h2><p><strong>Do Large language models need sensory grounding for meaning and understanding?</strong></p><p>现在的很多大语言模型还是需要很多的改进，现在还不能称之为智能</p><ul><li>现在的大语言模型虽然性能非常惊人，但是同时也会范一些非常愚蠢的错误</li><li>大语言模型对真实的世界一无所知，因为他们没有常识，也不能计划自己的输出（模型都是一个token一个token地生成然后向外输出的，而且每次生成的都不一样）</li></ul><h2 id="9-4未来研究方向"><a href="#9-4未来研究方向" class="headerlink" title="9.4未来研究方向"></a>9.4<strong>未来研究方向</strong></h2><p>现在机器学习还有很多的问题悬而未决，而且现在大语言模型遇到的问题其实跟30年前机器学习领域遇到的问题还是一样的，现在依然不知道大语言模型到底是怎样工作、怎么泛化的：</p><ul><li>如何从<strong>单语言到多语言</strong>？</li><li>为什么会具有<strong>涌现</strong>的能力？</li><li>如何提高模型<strong>做推理的能力</strong>（尤其是做因果推理）？</li><li>需要更多的方式<strong>阻止语言模型生成有害的文字</strong>或者<strong>带来比较坏的社会影响</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> PaperReading </tag>
            
            <tag> GPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 InstructGPT</title>
      <link href="/paper_reading/2.6.InstructGPT/"/>
      <url>/paper_reading/2.6.InstructGPT/</url>
      
        <content type="html"><![CDATA[<h1 id="6-InstructGPT"><a href="#6-InstructGPT" class="headerlink" title="6.InstructGPT"></a>6.InstructGPT</h1><p><strong>ChatGPT</strong></p><ul><li>Chat GPT 既没有发表在 NeurlPS 上面，也没有发表在 EMNLP ，甚至连一篇论文都没有</li><li>InstructGPT是微调的GPT-3.5模型</li></ul><h1 id="0-前言ChatGPT"><a href="#0-前言ChatGPT" class="headerlink" title="0.前言ChatGPT"></a>0.前言ChatGPT</h1><h2 id="0-1-ChatGPT-的四个应用"><a href="#0-1-ChatGPT-的四个应用" class="headerlink" title="0.1 ChatGPT 的四个应用"></a>0.1 <strong>ChatGPT 的四个应用</strong></h2><p><strong>官方</strong>给出四个使用的场景</p><p><strong>1、ChatGPT asks the clarifying questions to debug code</strong></p><p><img src="https://i0.hdslb.com/bfs/note/ed69197fc78382b5d08657a075377980f5ff6432.png@690w_!web-note.webp" alt=""></p><p><strong>2、ChatGPT initially refuses to answer a question that could be about illegal activities but responds after the user clarifies their intent</strong></p><ul><li>ChatGPT 能在<strong>安全性</strong>上避免进行一些非法的回答</li></ul><p><img src="https://i0.hdslb.com/bfs/note/3c51e62e1a2efe82d4ed35ff83544dc734e61a77.png@690w_!web-note.webp" alt=""></p><p><strong>3、ChatGPT is able to understand the reference (“it”) to the subject of the previous question (“fermat’s little theorem”)</strong></p><ul><li>ChatGPT 是能够理解上下文的，它能够记住之前的问题（它能够做一个 <strong>8000 词</strong>的上下文，也就是说如果回答是在 8000 词以内的话是能够联系上下文的）</li></ul><p><img src="https://i0.hdslb.com/bfs/note/ac04884a9e9c9f0b19e7a57df1f0e6a72ec87b8e.png@690w_!web-note.webp" alt=""></p><p><strong>4、ChatGPT provides responses to follow-up instructions</strong></p><ul><li>ChatGPT 是能够理解自己的<strong>局限性</strong>的，它明白自己有哪些事情是自己做不到的</li></ul><p><img src="https://i0.hdslb.com/bfs/note/1478eb9585a75dd1660127990c4f8f4ec8d53162.png@690w_!web-note.webp" alt=""></p><p>以上是官方给出的应用样例，还有一些其他的应用：</p><ul><li>把它伪装成一个操作系统，让它来执行代码</li></ul><p>GPT-3发布之后的一两年之内，出现了上百种应用，和 GPT-3 相比，ChatGPT 是基于<strong>对话</strong>的形式，而且是多轮对话，ChatGPT 更加自然一点，符合人的交互习惯，所以不出意外的话，未来也会出现越来越多的应用。</p><p>（根据 OpenAI 的一贯作风，它会先发布模型，过几个月之后再发论文，目前只有模型和博客，论文暂时还没有发布）</p><h2 id="0-2-ChatGPT-Methods"><a href="#0-2-ChatGPT-Methods" class="headerlink" title="0.2 ChatGPT Methods"></a>0.2 ChatGPT <strong>Methods</strong></h2><p>ChatGPT 用的是跟 InstructGPT 相同的方法</p><ul><li>InstructGPT 其实跟 GPT123 更相近，它的数据格式是一个 prompt</li><li>ChatGPT 的输入是一个对话的形式，所以说<strong>在数据收集上面和 InstructGPT 有一点不同</strong>：在标注数据的时候需要做成<strong>多轮对话</strong>的形式</li></ul><p><img src="https://i0.hdslb.com/bfs/note/9fe1a809c927f38ef6fd450c13c8cd5375286663.png@690w_!web-note.webp" alt=""></p><p>（这张图和 Instruct GPT 也是相同的）</p><p>ChatGPT 是在 GPT3.5系列的基础上进行微调得来的，这里的 GPT3.5 应该就是在GPT-3 代码的基础上进行修改得到的</p><h2 id="0-3-InstructGPT"><a href="#0-3-InstructGPT" class="headerlink" title="0.3 InstructGPT"></a>0.3 <strong>InstructGPT</strong></h2><ul><li>OpenAI 的工作都是基于前面的工作，工作是具有<strong>连续性</strong>的</li><li>InstructGPT 这篇文章发表于 <strong>2022 年 3 月 4 日</strong></li><li>论文标题：Training language models to follow instructions with human feedback</li><li>论文链接：<a href="https://arxiv.org/abs/2203.02155" title="https://arxiv.org/abs/2203.02155"><strong>https://arxiv.org/abs/2203.02155</strong></a></li></ul><p>其他参考资料：</p><ul><li><a href="https://openai.com/blog/chatgpt/" title="https://openai.com/blog/chatgpt/">https://openai.com/blog/chatgpt/</a></li><li>统计学中的信噪比怎么理解，<a href="https://www.zhihu.com/question/37522785" title="https://www.zhihu.com/question/37522785">https://www.zhihu.com/question/37522785</a></li></ul><h1 id="1-标题、作者"><a href="#1-标题、作者" class="headerlink" title="1.标题、作者"></a>1.标题、作者</h1><p>Training language models to follow instructions with human feedback（训练语言模型，使得它们能够服从人类的一些指示）</p><p>语言模型每次是给定一段东西，然后去预测下一个词，它是一个自监督模型，所以认为它是没有标号的。如果想让语言模型去解释某一个概念的话，就需要文本中出现过类似的东西，因此<strong>模型的行为取决于文本搜集的好坏</strong>。一般用来训练的文本大概都是几十亿、几百亿的词，所以具体里面有什么东西是不清楚的，只是大概知道文本质量的好坏，然后进行一定的清洗。因此模型的精细度是不够的，所以对整个模型的控制比较弱，一般就是大力出奇迹，把数据输入进去，得到什么样的模型就是什么</p><p>这样的问题在于：</p><ul><li><strong>有效性</strong>： 如果想让模型去做某个事情，但是模型始终学不会怎么办？因为文本中没有相应的东西。</li><li><strong>安全性</strong>： 模型输出一些不应该输出的东西怎么办？这对于大公司来讲将会造成很大的灾难</li></ul><p>最简单的办法就是<strong>标注一些数据</strong>，所以这篇文章的省流版本就是<strong>标注一点数据，然后将语言模型做一次微调，这样就能获得更好的效果</strong></p><ul><li>整个 OpenAI 或者说现在这些大的模型都是号称往无监督或者是自监督的方向发展，现在如果说还是需要进行数据标注，效果会很好，如果这么说的话，就是自相矛盾了，所以文章需要进行包装</li></ul><p><strong>Author</strong></p><p><img src="https://i0.hdslb.com/bfs/note/8c20b9e5f0b1d0c9421436130b7abb4779504209.png@1098w_!web-note.webp" alt=""></p><ul><li>作者基本上都是 OpenAI 的员工，带“ *”的是主要作者</li></ul><h1 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a>2.摘要</h1><p>把语言模型变大并不能代表它们会更好地按照用户的意图来做事情，大的语言模型很可能会生成一些<strong>不真实的</strong>、<strong>有害的</strong>或者是<strong>没有帮助</strong>的答案。换句话说，这些模型没有和用户站在一起（目标一致，达成合作）</p><ul><li>如果读者的关注点主要在研究上面，就可能会低估这一段话的重要性。因为在研究上，很多时候训练一个模型，在标准数据集上把整个分数刷上去就行了。但是在工业上的部署，也就是在 AI 模型的落地上面，<strong>安全性</strong>和<strong>有效性</strong>是非常重要的。</li><li>比如一个机器学习的产品，因为有一些地方没有做到位，从而引发争议导致整个产品下线，这种例子很多：</li><li>2015 年有用户反馈 Google 的照片服务将黑人的标签识别成了 Gorilla（大猩猩） ，导致 Google 紧急上线将 Gorilla 这个标签在模型中删掉。三年之后，Google photos 还是把 Gorilla 整个标签去掉了，也就是说，如果照片中有真的 Gorilla 的话，Google 是不会将它识别出来的；</li><li>2021 年纽约时报报道说 Facebook 因为它的 AI 算法把它的黑人视频加了一个灵长类动物的标签而道歉，跟之前的黑猩猩事件如出一辙；</li><li>微软发布的一个小冰聊天机器人的英文版在推特上发布 16 小时之后，用户发现它有一点<strong>种族歧视</strong>的语言，然后微软就紧急将它下架了，然后重新训练一个模型上线之后结果又乱讲话，最后导致整个产品被下线；</li><li>最近的例子，Meta 发布了一个叫做 Galactica 的模型（<a href="https://galactica.org/explore/，由">https://galactica.org/explore/，由</a> paper with code 团队发布），它能够做很多学术相关的事情，比如讲一个公式翻译成一个语言来进行描述或者说将一段代码用数学公式写出来，以及解决数学题，在模型发布不久之后，就有人发现这个模型<strong>会生成一些错误的或者是有偏见的但是听上去很正确的东西</strong>，他认为这个是一个非常危险的事情，等于是在一本正经的胡说八道并且使别人相信了，这些批评导致模型在发布的三天之后，Meta 就将这个模型下架了。。。</li></ul><p>所以当将一个机器学习的模型部署到产品中的时候需要非常小心，需要特别注意它出错的地方，避免在公关上出现问题。通常对于简单的分类问题来说相对会好一点，只需要将标号中一些有争议性的标号拿掉，但是语言模型的输出<strong>特别灵活</strong></p><ul><li>一方面研究者享受这种灵活性带来的巨大的应用场景</li><li>另一方面，这种灵活性的输出，导致出错的概率会更大</li></ul><p>GPT-3 发布这么久，有出过什么事情吗？</p><ul><li>其实是有的，只是 OpenAI 作为一个创业公司，媒体对创业公司的容忍度相对来讲会高一些</li><li>但是如果是一些大厂，比如 Google 这样的大公司，把 GPT-3 这种模型做成一个产品的形式时，一旦出现什么问题，就会出现很大的公关问题</li><li>事实上，ChatGPT 已经在安全性上做了很多工作，避免去回答一些非法的问题，实际上大家早就找到了各种可能性来绕开这些限制。</li><li>其实有很多的可能性让ChatGPT发表一些不适当的言论，只是现在 OpenAI 的口碑比较良好，如果说换一个大厂将同样的模型发表出来，结果就不一样了</li></ul><p>这篇文章中<strong>展示了怎样对语言模型和人类的意图之间做 align</strong> ，具体使用的方法是<strong>使用人类的反馈进行微调（fine-tuning with human feedback）</strong></p><ul><li><strong>注意这里使用的是 human feedback，不是使用的带标签的数据</strong></li></ul><p>具体做法是写了很多的 <strong>prompt</strong> ，在** OpenAI 的 API 上收集到各种问题，然后用标注工具将这些问题的答案写出来，这样就标注了一个数据集，然后在这个数据集上对 GPT-3 的模型做微调**。</p><p>然后又收集了一个数据集，这个数据集就是对每个模型的输出（问它一个问题，它可能会输出很多模型，因为它是一个概率采样的问题）<strong>进行人工标注</strong>，标注出好坏的顺序，有了这个顺序之后，再用<strong>强化学习</strong>继续训练出一个模型，这个模型就叫做 <strong>InstructGPT</strong></p><p>所以作者主要做了<strong>两件事情</strong>：</p><ul><li>首先标注了一些数据，将问题和答案都写出来然后训练一个模型</li><li>接下来又做了一个排序的数据集，然后用强化学习再训练出一个模型</li></ul><p>因此一共有两个模型，结果证明在人类的评估上面， InstructGPT （有标号的数据集）1.3B 的模型参数要好过最大的 GPT-3，也就是175B，也就是说<strong>一个 1% 大小的模型的效果反而更好一点</strong>。另外 InstructGPT 能在真实性上更高地降低有害的答案出现的概率。在公开的 NLP 数据集上，它的性能也没有显著的下降。</p><p>因此，等价于是<strong>说 InstructGPT更小，但是效果更好</strong>，而且在一些别的公开数据集上性能也没有变差；当然，不得不承认的是 InstructGPT 还是会犯一些简单的错误。</p><p>整个摘要的核心思想是说，作者标记了一个数据集，然后在这个数据集上比 GPT-3 要小 100 倍的模型然后在上面做微调的效果比最大的 GPT-3 模型的效果可能还要好一些</p><ul><li>对于这个结果，也不是特别意外，因为标注的数据集信噪比更加好一点，所以学习起来更加简单一点，不需要那么大的模型</li><li><strong>transformer 模型就是对整个数据做压缩</strong>，把整个数据信息压缩进模型的参数，信噪比越高，而且标注的信息和最后要评估的数据集可能更近一点，所以就导致不需要压缩那么多的东西</li><li><strong>信噪比</strong>：方差即不确定性，不确定性即信息。也就是说对于一组样本来说，其方差越大，代表样本中含有的信息越多，所以可以将方差看作信息量的一个度量。因此，信噪比的统计学含义就是：能够被模型解释的信息与不能够被模型解释的信息之比。</li><li>虽然 OpenAI 和一些大的厂都是在说，训练一个特别大的模型，根本不需要标注，效果特别好，实际上在实用上来讲，如果这个方向一路走到底的时候，<strong>计算能力不一定能吃得消</strong>，而且数据可能增长到某个程度之后，可能覆盖的地方还是存在问题，而在那些想要的特性但是模型做不到的地方适当加入一些人类的标注，其实相对来讲更加划算</li></ul><p>所以一个好的方法需要平衡算力的需求和人类标注的代价</p><h1 id="2-导论"><a href="#2-导论" class="headerlink" title="2.导论"></a>2.导论</h1><p>导论就是摘要中所讲的故事的一个稍微详细的版本，首先讲问题，然后讲方法，最后讲结果</p><h2 id="2-1-存在的问题"><a href="#2-1-存在的问题" class="headerlink" title="2.1 存在的问题"></a>2.1 存在的问题</h2><p>大的语言模型能够<strong>通过提示的方式把任务作为输入</strong>，但是这些<strong>模型也经常会有一些不想要的行为</strong>，比如说捏造事实，生成有偏见的、有害的或者是没有按照想要的方式来，这是因为整个语言模型训练的目标函数有问题。</p><ul><li><strong>实际的目标函数</strong>：语言模型的目标函数是在网上的文本数据中预测下一个词，即给定一个文本中的一段话，然后预测这段话后面的词</li><li><strong>我们希望的目标函数</strong>：根据人的指示来生成安全的、有帮助的答案</li><li>两个目标函数其实是<strong>不一样的</strong>，所以作者把真正训练的目标函数和所想要让这个模型做的事情之间的差距叫做语言模型目标函数是没有 align。</li></ul><p>所以这篇文章的目的就是让语言模型更好一点：</p><ul><li>希望语言模型能够更有帮助性，能够解决想让它解决的事情</li><li>能够更加真诚，不要捏造事实，要实事求是</li><li>无害，既不要生成让人反感的输出，也不要生成一些可能对别人造成危害的输出</li></ul><h2 id="2-2-实现方法"><a href="#2-2-实现方法" class="headerlink" title="2.2 实现方法"></a>2.2 实现方法</h2><p><strong>基于人类反馈的强化学习</strong>（reinforcement learning from human feedback，<strong>RLHF</strong>）基于人类反馈的强化学习</p><p>图二（InstructGPT 怎样从 GPT-3 一步一步训练而来的，<strong>一共标注了两个标注数据集，生成了三个模型</strong>）</p><p><img src="https://i0.hdslb.com/bfs/note/c1e7ac722ae5e6cbad26981c57238a31b3ca17bf.png@1098w_!web-note.webp" alt=""></p><h3 id="Step-1：收集样本数据，有监督微调"><a href="#Step-1：收集样本数据，有监督微调" class="headerlink" title="Step 1：收集样本数据，有监督微调"></a>Step 1：收集样本数据，有监督微调</h3><ol><li>首先找了各种<strong>人来写各种各样的问题</strong>（这个问题在 GPT 中叫做 <strong>prompt</strong> ，具体来说就是向一个 6 岁的小孩解释什么是月亮；这些问题也可能是来自之前用户在向 GPT-3 提交的各种问题中筛选出来的）</li><li>然后<strong>继续让人写答案</strong>（比如说例子中问题的答案就是一些人去了月球。。。）</li><li>在有了问题和答案之后，就可以将这两个拼成一段话，然后<strong>在这个上面对 GPT-3 进行微调</strong></li></ol><p>因此，虽然这是人类标注的数据，但是在 GPT 眼中都是一样的，都是给定一段话然后预测下一个词，所以在微调上跟之前的在别的地方做微调或者是做预训练没有任何区别</p><p>GPT-3 的模型在人类标注的数据上微调出来的模型叫做 <strong>有监督的微调（supervised fine-tuning）</strong>，这是训练出来的第一个模型，其实训练出来的这个模型也能用，但是它的问题在于生成答案是一件很贵的事情，所以很难让人把所有各式各样的答案都写出来</p><h3 id="Step-2-收集排序数据，训练奖励模型"><a href="#Step-2-收集排序数据，训练奖励模型" class="headerlink" title="Step 2 : 收集排序数据，训练奖励模型"></a>Step 2 : 收集排序数据，训练奖励模型</h3><ol><li><strong>给定一个问题</strong>，让上一步训练好的预训练<strong>模型 SFT 生成答案</strong></li><li>GPT 每一次预测一个词的概率，可以根据这个概率采样出很多答案，通常来说可以用 beam search</li><li>这里生成了四个答案，然后把这<strong>四个答案的好坏进行人工标注，进行排序标注</strong></li><li>有了这些排序之后，再训练一个<strong>奖励模型（Reward Model，RM）</strong>，这个模型是说给定 prompt 得到输出，然后对这个输出生成一个分数，可以认为这个分数是一个奖励或者是打分，使得对答案的分数能够满足人工排序的关系（大小关系保持一致），一旦这个模型生成好之后，就能够对生成的答案进行打分</li></ol><h3 id="Step-3：使用RM模型优化SFT模型"><a href="#Step-3：使用RM模型优化SFT模型" class="headerlink" title="Step 3：使用RM模型优化SFT模型"></a>Step 3：使用RM模型优化SFT模型</h3><ol><li>继续微调之前训练好的 SFT模型，使得它生成的答案能够尽量得到一个比较高的分数，即每一次将它生成的答案放进 RM 中打分，然后优化 SFT 的参数使得它生成的答案在 RM 中获得更高的分数。</li></ol><p><strong>备注</strong>：两次对模型的微调：GPT3模型 → SFT模型 → RL模型，其实这里始终都是同一个模型，只是不同过程中名称不同。</p><ul><li><strong>需要SFT模型的原因</strong>： GPT3模型不一定能够保证根据人的指示、有帮助的、安全的生成答案需要人工标注数据进行微调。</li><li><strong>需要RM模型的原因</strong>：标注排序的判别式标注成本远远低于生成答案的生成式标注。</li><li><strong>需要RL模型的原因</strong>：在对SFT模型进行微调时生成的答案分布也会发生变化，会导致RM模型的评分会有偏差，需要用到强化学习.</li></ul><p>最后训练出来的模型就叫做 InstructGPT ，它是 GPT-3 经过以上三个步骤训练得来的。</p><p>从技术要点上来看，<strong>有以下几个技术</strong>：</p><ol><li>第一步中的数据标注的实现</li><li>第二步中的数据排序的实现</li><li>微调和 GPT-3 的微调是一样的</li><li>RM 模型的训练</li><li>有了 RM 模型之后，如何通过强化学习来训练</li></ol><h2 id="2-3-结果描述"><a href="#2-3-结果描述" class="headerlink" title="2.3 结果描述"></a>2.3 结果描述</h2><p>最后是关于结果的一些描述：</p><ol><li>标注人员觉得 InstructGPT 的答案要比 GPT-3 的答案明显要好很多；</li><li>InstructGPT 在真实性上要比 GPT-3 好一些；</li><li>InstructGPT 在生成有害的输出上要比 GPT-3 好一点，因为它可以说不想回答某一个问题，但是在偏见（比如性别歧视）上并没有太大的提升；</li><li>在做微调的时候通常是根据某一个目标做微调，可能会使得模型在一些别的任务上的性能会下降。作者的做法是在做强化学习的时候，将最原始的目标函数拿回来，使得虽然在做完微调之后在这种 QA 上面做的更好一点，但是在一些其他的任务，比如说公有的 NLP 数据集上也不至于说性能下降很多</li><li>虽然在整个过程中进行了人工标注，但是<strong>标注非常有主观性</strong>，因为是写一段文字或者是判断两段话的好坏，作者找了一些没有标注数据参与训练的标注人员，只是从结果的角度去评估 InstructGPT 的话他们还是觉得 InstructGPT 要比 GPT-3 好一些（人与人之间的喜好是有一定的相关性的）</li><li>作者将 GPT-3 在 InstructGPT 的数据和其他的公用数据集 FLAN 和 T0 上进行了微调，最后比较发现，还是在自己的数据上微调出来的效果会好一些，也就是说别人的数据可能和自己的数据在分布上不太一致，所以意味着<strong>微调对数据还是比较敏感的</strong>。</li><li>作者标注了大量的问题，但是因为语言模型比较灵活，不可能将所有的问题都标注出来，所以作者发现<strong>虽然标注的问题里面只有少部分是总结代码或者是问代码相关的问题</strong>，<strong>在训练完之后发现实际的模型在这方面的表现还是不错的</strong>，也就是说所训练出来的模型其实是有一些泛化性的，因此这也意味着其实也没有必要一定要将所有不同的问答类型全部标注，模型根据之前的先验知识<strong>具有一定的泛化性</strong>。</li><li>模型也还是<strong>会犯一些简单的错误</strong>，因为文中所展示的都是一些模型所表现出来的比较出乎意料的东西，但是可能在一些大家习以为常的地方很可能会出错，所以在这一点上可以认为 InstructGPT 或者说甚至现在的 ChatGPT 多多少少还是像一个玩具，而不是一个工具。<ul><li>工具不需要惊喜，但是需要保证可用性，不能在一些正常的地方出错</li></ul></li></ol><h1 id="3-方法"><a href="#3-方法" class="headerlink" title="3.方法"></a>3.方法</h1><p>本文所使用的方法就是前面工作的方法，只是前面的工作主要用在文本样式的一致性和渐进式总结（类似于问答的场景下），方法本身没有本质上的区别（这些技术虽然都是 openAI 前面的研究，但是并不是 InstructGPT 的原创，这些技术之前就有了，只不过 InstructGPT 使用这些技术在一个新的数据集上重新训练了一下）</p><h2 id="3-1-Dataset"><a href="#3-1-Dataset" class="headerlink" title="3.1 Dataset"></a>3.1 <strong>Dataset</strong></h2><h3 id="（1）Prompt-数据集"><a href="#（1）Prompt-数据集" class="headerlink" title="（1）Prompt 数据集"></a>（1）Prompt 数据集</h3><p>首先<strong>标注人员写了很多的问题</strong>，这些问题包括：</p><ul><li><strong>Plain</strong>：让标注人员写任何的问题</li><li><strong>Few-shot</strong>：让标注人员写一个指令，有各种不同的指令，然后里面有后续的一些问题回答</li><li><strong>User-based</strong>：用户提供了一些想要支持的应用场景，然后将其构建成任务</li></ul><p>有了这些<strong>最初构建出来的 prompt 之后，作者训练了第一个 InstructGPT 模型</strong>，得到这个模型之后，将其放在 playground 中供大家使用。大家在使用的过程中可能又会提出一些问题，然后又把这些问题采集回来，并进行筛选。</p><ul><li>对每个用户最多采用 200 个问题</li><li>在划分训练集、验证集、测试集的时候是根据用户的 ID 来划分的（这个也很重要，当收集了很多来自不同用户的各种问题之后，不能把这些问题放在一起进行随机划分，因为一个用户可能会问一些类似的问题，如果这个问题同时出现在训练集和测试集中，就会造成数据污染，所以按照用户进行划分更加公平）</li><li>如果问题中包含了很多的用户信息，比如出现了人名，就将其过滤掉</li></ul><p>通过这个方法就<strong>得到了更多的 prompt</strong>。</p><h3 id="（2）三个模型的数据集"><a href="#（2）三个模型的数据集" class="headerlink" title="（2）三个模型的数据集"></a>（2）三个模型的数据集</h3><p>在有了这些 prompt 之后就产生了三个不同的数据集，数据集之间可能共享了一些问题：</p><ul><li><strong>SFT 数据集</strong>：让标注人员直接写答案。用来训练 SFT 模型的数据集中有 13000 个样本。</li><li><strong>RM 数据集</strong>：用来训练一个 RM 模型，只需要进行排序就可以了。用来训练 RM 模型的数据集中有 33000 个样本。</li><li><strong>PPO 数据集</strong>：用来训练强化模型，也就是 InstructGPT 。这个时候就不需要标注（标注来自于 RM 模型的标注）。用来训练 InstructGPT 模型的数据集中有 31000 个样本。</li></ul><h3 id="（3）一些数据集的例子"><a href="#（3）一些数据集的例子" class="headerlink" title="（3）一些数据集的例子"></a>（3）一些数据集的例子</h3><p>表 1 展示了 prompt 数据集中使用 API 的用户的用途分布情况，最多的是生成一些东西，其次是一些开放性的回答、头脑风暴等</p><p>表 2 中展示了一些例子</p><ul><li><strong>头脑风暴</strong>：列出五个能够使我保持对事业的热情的五个想法</li><li><strong>生成类</strong>：生成一个短故事</li><li><strong>重写</strong>：给定百老汇 show 的总结，将其中的要点列出来</li></ul><p><img src="image/image_Ft2wxlyP0i.png" alt=""></p><p>在文章的附录 A 中提供了大量的 prompt 的例子</p><p><img src="https://i0.hdslb.com/bfs/note/796218543084aaa9378b96b0aa3d1b9a5c143bec.png@1138w_!web-note.webp" alt=""></p><h2 id="3-2-数据标注"><a href="#3-2-数据标注" class="headerlink" title="3.2 数据标注"></a>3.2 数据标注</h2><p>作者在 Upwork（美国招聘合同工常用的网站） 和 ScaleAI（一个数据标注公司） 上招了一个 40 人组成的团队，在附录 B 中有对人员的筛选过程进行详细的描述</p><p>在标注的过程中，希望能够做到，尽量将帮助性排在第一位；在评测的时候尽量把真实性和无害性排在第一位。标注数据的时候给的指示和最终评估的时候不同。</p><p><strong>作者和标注人员紧密合作</strong>，因为整个任务相对来说还是比较开放的，而且比较难，所以需要不断地与标注人员进行沟通。</p><p>这些<strong>标注人员的一致性还是比较高</strong>的：72% 左右的情况下，大家是相互同意对方的一些评测。这就意味着这个任务可能具有二相性，但是大家的意向基本一致</p><p>如果之前没有做过<strong>数据标注</strong>，而且需要找人进行数据标注的话，可以参考作者所采用的方法，他的描述还是比较详细的</p><ul><li>特别是在附录中提供了很多的模板，又可能能够直接套用</li><li>作者还提供了标注网页的 UI 的样式，可以进行参考</li></ul><h2 id="3-3-模型"><a href="#3-3-模型" class="headerlink" title="3.3 模型"></a>3.3 模型</h2><p>总共有三个模型：</p><h3 id="（1）Supervised-fine-tuning（SFT）"><a href="#（1）Supervised-fine-tuning（SFT）" class="headerlink" title="（1）Supervised fine-tuning（SFT）"></a>（1）Supervised fine-tuning（<strong>SFT</strong>）</h3><p>等价于<strong>将 GPT-3 模型标注好的 prompt 和答案进行重新训练</strong>，总共训练了 16 个 epoch</p><p>因为数据比较少，总共只有 13000 个数据，所以 GPT 的模型训练一个 epoch 就过拟合了。这个模型也不是直接使用，而是<strong>用来初始化后面的模型</strong>，所以作者发现过拟合其实是没有问题的，对后面还能起到一定的帮助作用</p><h3 id="（2）Reward-Modeling（RM）"><a href="#（2）Reward-Modeling（RM）" class="headerlink" title="（2）Reward Modeling（RM）"></a>（2）Reward Modeling（<strong>RM</strong>）</h3><p>将 GPT-3 模型最后的 unembedding layer 去掉，在prompt和回复上面训练出来一个模型，并输出一个标量奖励。</p><ul><li>正常 GPT 进入最后一个输出层之后，放进 softmax 输出一个概率。现在 softmax 可以不用，在后面加上一个线性层来投影，即将所有词的输出投影到一个值上面，就是一个输出为 1 的线性层，就可以输出一个标量的分数。</li></ul><p>这里使用的是一个 6B 大小的RM，没有用最大的 175B</p><ul><li>作者发现 175B 大小的模型训练起来不是特别稳定（在比较大的模型训练，其实不稳定是它的一个比较大的痛点，而且现在也没有特别好的解决方案）。如果模型训练不稳定的话，在后面 RL 里面训练会比较麻烦</li><li>此外，用小一点的模型也能够节省算力</li></ul><p>因为输入的标注是排序，而不是让用户标注的值，仅仅是一个顺序，因此需要将这个顺序转换成一个值，作者使用的损失函数是排序中常见的 Pairwise-ranking loss：</p><script type="math/tex; mode=display">\operatorname{loss}(\theta)=-\frac{1}{\left(\begin{array}{c}K \\ 2\end{array}\right)} E_{\left(x, y_{w}, y_{l}\right) \sim D}\left[\log \left(\sigma\left(r_{\theta}\left(x, y_{w}\right)-r_{\theta}\left(x, y_{l}\right)\right)\right)\right]</script><p><img src="image/image_8BKdsA8hoo.png" alt=""></p><p><strong>损失函数参数解释：</strong></p><ul><li>$D$：第二个数据集，人工对答案进行排序。</li><li>$x$：第二个数据集D中的问题，每个问题对应K个答案，答案的顺序已经人工标注好了。</li><li>$y_w$和$y_l$：x对应的K个答案中的两个，其中yw排序比yl高，因为是一对，所以叫pairwise。</li><li>$r_\theta(x,y)$：即需要训练的RM模型，对于输入的一对x和y得到的标量分数。</li><li>$\theta$：需要优化的参数。&#x20;</li></ul><p><strong>损失函数理解：</strong></p><ol><li>$x$和$y_w$这一对问题和答案，放进RM模型中算出一个分数$r_\theta(x,y_w)$</li><li>$x$和$y_l$这一对问题和答案，放进RM模型中算出一个分数$r_\theta(x,y_l)$</li><li>因为人工标注出$y_w$的排序要比$y_l$高，$r_\theta(x,y_w)$得到的分数应该比$r_\theta(x,y_l<br>)$得到的分数高，所以$r_\theta(x,y_w) - r_\theta(x,y_l)$这个差值要越大越好</li><li>把相减后的分数通过sigmoid，那么这个值就在-1到1之间，并且我们希望$\sigma(r_\theta(x,y_w) - r_\theta(x,y_l))$越大越好</li><li>这里相当于将排序问题转换为了分类问题，即$\sigma(r_\theta(x,y_w) - r_\theta(x,y_l))$越接近1，表示$y_w$比$y_l$排序高，属于1这个分类，反之属于-1这个分类。所以这里就用logistic loss，由于是二分类，也相当于是交叉熵损失函数。</li><li>对于每个问题有$K$个答案，所以前面除以$C(K,2)$，使得loss不会因为K的变化而变化太多。</li><li>最后是最小化$loss(\theta)$，就是要最大化$r_θ(x,y_w)-r_θ(x,y_l)$这个值，即如果一个答案的排序比另一个答案排序高的话，我们希望他们通过RM模型得到的分数之差能够越大越好。</li></ol><p><strong>对于K的选择，为什么选9，而不选择4？</strong></p><ol><li>进行标注的时候，需要花很多时间去理解问题，但答案和答案比较相近，所以4个答案排序要30秒，但9个答案排序可能40秒就够了。加上看问题的时间，K=9花的时间可能比K=4多了30%。同时C(9,2)=36，C(4,2)=6，即K=9生成的问答对是K=4的6倍，等于说K=9比K=4只多花了30%的时间，但是能够标注的信息量却是他的6倍，<strong>非常划算</strong>。</li><li>K=9时，每次计算loss都有36项$r_θ(x,y)$要计算，这个RM模型计算比较贵，但可以通过重复利用之前算过的值，使得只要计算9次就行，这样就<strong>可以剩下很多时间</strong>。</li></ol><p><strong>标注时为什么不选择只标注最好的那个，而是进行排序？</strong></p><ul><li>K=4 ，在标注的时候只标注最好的一个，也就是说从 4 个答案中选出最好的答案，在计算损失的时候就不是 pairwise ，因为没有两两比较信息 <strong>，将一个二分类的逻辑回归问题变成了一个多分类的 softmax</strong> ，等于是在从 4 个值里面选出最大的值</li><li>K=4的时候是在4个答案中只标注最好的那一个，标注方便很多，这时候计算loss时变成了一个多分类的softmax。但是这样做有一个问题，就是容易<strong>overfitting</strong>。所以K=9时，保留了排序的信息，从而解决overfitting的问题。&#x20;</li></ul><p>现在改成了全部答案的排序使得整个问题变得复杂一点：不是要学习得到一个分数然后选出最大的值，而是说要学一个分数使得整个 9 个答案的排序能够保留下来。所以标号变多了之后，发现过拟合就会好一些，这也是作者对之前的一些方法进行改动的原因</p><h3 id="（3）Reinforcement-learning（RL）"><a href="#（3）Reinforcement-learning（RL）" class="headerlink" title="（3）Reinforcement learning（RL）"></a>（3）Reinforcement learning（<strong>RL</strong>）</h3><p>这里用到的模型是强化学习中的 PPO ，</p><ul><li>强化学习中的算法有很多，PPO 是其中之一</li><li>使用 PPO 是因为 PPO 也是 OpenAI 之前的工作，PPO 的作者也在本文的作者之列，所以挑选了一个自己比较熟悉的</li></ul><p>PPO 模型简单来讲就是在下面的目标函数上进行随机梯度下降：</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{objective}(\phi)= & E_{(x, y) \sim D_{\pi_{\phi}^{\mathrm{RL}}}}\left[r_{\theta}(x, y)-\beta \log \left(\pi_{\phi}^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right]+ \\ & \gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_{\phi}^{\mathrm{RL}}(x)\right)\right]\end{aligned}</script><p><img src="image/image_9EPjYcnizy.png" alt=""></p><p>这个目标函数和之前的主要区别是：<strong>数据分布是随着模型的更新变化的，在强化学习中称为环境会发生变化</strong></p><p><strong>参数解释：</strong></p><ul><li>$\pi^{SFT}$：SFT模型。之前在标好的问题和答案的数据上面用监督的微调训练出来的模型</li><li>$\pi_{\phi}^{\mathrm{RL}}$：强化学习中，模型叫做Policy，$\pi_{\phi}^{\mathrm{RL}}$就是需要调整的模型，即最终的模型。初始化是$\pi^{SFT}$。</li><li>$(x, y) \sim D_{\pi_{\phi}^{\mathrm{RL}}}$：x是第三个数据集中的问题，y是x通过$\pi_{\phi}^{\mathrm{RL}}$模型（当前模型）得到的答案。</li><li>$r_{\theta}(x, y)$：对问题x，答案y进行打分的RM模型。<strong>希望这个分数是最大的，优化这一项</strong>。</li><li>$\pi_{\phi}^{\mathrm{RL}}(y \mid x)$：问题x通过$\pi_{\phi}^{\mathrm{RL}}$得到答案y的概率，即对于每一个y的预测和它的softmax的输出相乘。</li><li>$\pi^{\mathrm{SFT}}(y \mid x)$：问题x通过$\pi^{\mathrm{SFT}}$得到答案y的概率。</li><li>$x∼D_{pretrain}$：x是来自GPT3预训练模型的数据。</li><li>$\beta$、$ \gamma<br>  $：调整系数。</li></ul><p><strong>目标函数理解：</strong> 优化目标是使得目标函数越大越好，$objective(\phi)$可分成三个部分，打分部分+KL散度部分+GPT3预训练部分</p><ol><li>将第三个数据集中的问题$x$，通过$\pi^{SFT}$模型得到答案$y$</li><li>把一对$(x,y)$送进RM模型进行打分，得到$r_{\theta}(x, y)$，即第一部分打分部分，这个分数越高就代表模型生成的答案越好</li><li>在每次更新参数后，$\pi_{\phi}^{\mathrm{RL}}$会发生变化，x通过$\pi_{\phi}^{\mathrm{RL}}$生成的y也会发生变化，而$r_{\theta}(x, y)$打分模型是根据$\pi^{SFT}$模型的数据训练而来，如果$\pi_{\phi}^{\mathrm{RL}}$和$\pi^{SFT}$差的太多，则会导致$r_{\theta}(x, y)$的分数估算不准确。因此需要通过<strong>KL散度</strong>来计算$\pi_{\phi}^{\mathrm{RL}}$生成的答案分布和$\pi^{SFT}$生成的答案分布之间的距离，使得两个模型之间不要差的太远。</li><li>我们希望两个模型的差距越小越好，即KL散度越小越好，前面需要加一个负号，使得$objective(\phi)$越大越好。这个就是KL散度部分。</li><li>如果没有第三项，那么模型最终可能只对这一个任务能够做好，在别的任务上会发生性能下降。所以第三部分就把原始的GPT3目标函数加了上去，使得前面两个部分在新的数据集上做拟合，同时保证原始的数据也不要丢，这个就是第三部分GPT3预训练部分。</li><li>当$\gamma=0$时，这个模型叫做PPO，当$\gamma$不为0时，这个模型叫做PPO-ptx。InstructGPT更偏向于使用PPO-ptx。</li><li>最终优化后的$\pi_{\phi}^{\mathrm{RL}}$模型就是InstructGPT的模型。&#x20;</li></ol><p>之前已经标好了数据，为了么要训练一个 $r_θ$ 之后再训练一个模型出来，为什么不直接训练？</p><ul><li>主要原因是<strong>标注的只是一个排序，而不是标注的答案</strong></li><li>给定一个模型，然后生成多个输出，由标注人员进行排序，再计算梯度，然后再对模型进行更新；下一次又生成新的数据，然后进行标注，这在 RL 中比较常见，叫做在线学习。如果想要做成在线学习的形式就需要实时对模型的生成结果进行排序，会造成人力或者是算力的浪费。所以在这个地方需要学习一个函数来替代掉这个人， $r_θ$  其实就是在学习人的排序从而给模型实时的反馈，这就是为什么这里需要训练两个模型。</li></ul><p>整个 RL 模型简单来说<strong>就是一个 PPO 的目标函数加上一个原始的 GPT-3 的目标函数结合在一起</strong>。可以看到它还是一个相对来讲比较简单的 RL 算法，其实比作者之前的工作还要简单一点</p><ul><li>在之前的工作中尝试在 RL 里面多走几个来回，现在只是在之前预训练好的模型之后，通过 RL 模型再跑一步，中间不需要人工进行数据标注</li><li>作者在实际操作过程中发现，这样对有一些任务有效果，但是对有些任务没有必要</li><li>对于一些比较复杂的任务，比如缩写任务，因为 y 的变化可能会比较大，所以重新进行标注可能会好一点</li><li>但相对来讲比较简单一点的任务，在 $r_\theta$ 变化没有那么大的情况下其实没有太大的必要</li></ul><h2 id="3-4小结"><a href="#3-4小结" class="headerlink" title="3.4小结"></a>3.4<strong>小结</strong></h2><p><img src="image/image_DoBbf7iBvy.png" alt=""></p><p>InstructGPT总共干了三件事情：</p><p>1、<strong>数据</strong>：将 prompt 和答案标出来，然后用最正常的 GPT 微调出一个模型</p><p>2、训练一个<strong>奖励模型</strong>去拟合人对模型中多个输出之间的排序，训练好之后将其放入到强化学习的框架</p><p>3、通过<strong>强化学习模型</strong>调整 SFT 模型，使得输出的结果在排序上更符合人的喜好</p><h1 id="4-结果"><a href="#4-结果" class="headerlink" title="4.结果"></a>4.结果</h1><p><img src="image/image_IC1H-53ElB.png" alt=""></p><ul><li>有三个不同大小的模型：原始的 GPT-3 ，1.3B~175B</li><li>y 轴表示和 175B 的 SFT 模型相比的胜率，正常的话是一半一半</li><li>GPT-3 在 prompt 上做比较多的调整，可以从图中看到有提升，但是跟有标注的比还是比较远的</li><li>实验结果也验证了导言中所说的用一个 1% 的模型，其实是能够打败 175B 的模型（是在一个特定的测试集上面，因为这个测试集和训练数据集是有一定的耦合性的）</li><li>在 GPT-3 这篇论文中也提到过，在没有看过任何训练数据的情况下，使用大力出奇迹的方式出来的模型，比用过训练数据的模型可能效果还要好一些，但也只是针对一些相对来讲比较简单的任务</li><li>在这篇文章中，整个任务是比较复杂的，prompt 比较长，而且答案也并不简单，在看过标注信息的前提下，提升还是比较大的</li></ul><h1 id="5-讨论"><a href="#5-讨论" class="headerlink" title="5.讨论"></a>5.讨论</h1><p>1、作者认为整个三个模型的训练的代价和预训练相比，相对来讲比较低</p><ul><li>因为<strong>样本比较少</strong>，就算是使用了 175B 的模型，样本数也足足小了几万倍或者几十万倍</li></ul><p>2、<strong>局限性</strong></p><ul><li>数据是由 40 个合同工标注出来的，这个模型的行为和这 40 个人是息息相关的，不一定能代表以后所有的用户，所以后续还需要招一些人或者是用户来提升模型的能力</li><li>比如这里面的数据主要是英语，所以在别的语言上肯定是有一定的差距的</li><li>在模型上面也不是完全安全，还是会出现各种问题</li></ul><h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6.总结"></a>6.<strong>总结</strong></h1><p>从技术上来讲，InstructGPT是一个比较<strong>实用</strong>的技术，它提供了一个方法：给定一个比较大的语言模型，怎样通过标注一些数据能迅速地提升它在某一个你所关心领域上的性能，使其能够达到一个实用的阶段</p><ul><li>这也给想<strong>用生成模型做产品</strong>提供了一个<strong>实际可操作</strong>的思路</li></ul><p>作者在一开始提到了三个目标：想要语言模型更加有<strong>帮助性</strong>，说更多的<strong>真话</strong>，以及<strong>无害性</strong></p><ul><li>实际上这篇文章主要还是在讲帮助性，也讲了整个数据标注的时候，也是优化帮助性</li><li>所以从<strong>创新性**</strong>和<strong>**完成度</strong>的角度，这篇文章一般，也为只是优化了一个相对来讲比较简单的目标，而没有同时考虑到剩下两个目标去显式地优化它们</li></ul><p>另外后面这个 <strong>RL 模型可能也是没有必要做的</strong></p><ul><li>可以选择在训练第一个模型的时候多标注一点数据，或者说可以用文本合成这样的方法来快速增大数据</li><li>这样做的好处就是在做微调的时候，直接将之前的代码拿过来用就可以了，而不需要去做 RL ，RL模型所带来的一系列复杂度的东西可以转移到数据上面，因为对数据进行处理相对来讲比较简单，那么整个模型的训练和部署就会变得更加简单</li><li>因此从<strong>实用性</strong>的角度来看，这样做的效果可能会更好一些</li></ul><p>所以其实就是显式地优化了帮助性这一个目标，使用了相对来讲比较复杂的 RL 算法也没有成功地说明使用它的<strong>必要性</strong>。</p><p>从一个创业公司的角度来讲，需要尽快地把产品做出来，效果做上去，其他的东西可能就没有那么重要，但是同样的做法，OpenAI 也许能做，但是别的公司不一定能做，在<strong>安全性</strong>上没有做太多完善的情况下，很有可能会引发公关危机。</p><h1 id="7-其他"><a href="#7-其他" class="headerlink" title="7.其他"></a>7.其他</h1><h2 id="7-1-指示学习（Instruct-Learning）和提示（Prompt-Learning）学习"><a href="#7-1-指示学习（Instruct-Learning）和提示（Prompt-Learning）学习" class="headerlink" title="7.1 指示学习（Instruct Learning）和提示（Prompt Learning）学习"></a>7.1 指示学习（Instruct Learning）和提示（Prompt Learning）学习</h2><p>指示学习是谷歌Deepmind的Quoc V.Le团队在2021年的一篇名为《Finetuned Language Models Are Zero-Shot Learners》文章中提出的思想。指示学习和提示学习的目的都是去挖掘语言模型本身具备的知识。不同的是Prompt是激发语言模型的<strong>补全能力</strong>，例如根据上半句生成下半句，或是完形填空等。Instruct是激发语言模型的理解能力，它通过给出更明显的指令，让模型去做出正确的行动。我们可以通过下面的例子来理解这两个不同的学习方式：</p><ol><li><strong>提示学习</strong>：给女朋友买了这个项链，她很喜欢，这个项链太____了。</li><li><strong>指示学习</strong>：这句话的情感是非常正向的：给女朋友买了这个项链，她很喜欢。</li></ol><p>指示学习的优点是它经过多任务的微调后，也能够在其他任务上做zero-shot，而提示学习都是针对一个任务的。泛化能力不如指示学习。我们可以通过下图来理解微调，提示学习和指示学习。</p><p><img src="image/image_IkklR__48e.png" alt=""></p><h2 id="7-2-人类反馈的强化学习"><a href="#7-2-人类反馈的强化学习" class="headerlink" title="7.2 人类反馈的强化学习"></a>7.2 人类反馈的强化学习</h2><p>强化学习通过奖励（Reward）机制来指导模型训练，奖励机制可以看做传统模型训练机制的损失函数。奖励的计算要比损失函数更灵活和多样（AlphaGO的奖励是对局的胜负），这带来的代价是奖励的计算是不可导的，因此不能直接拿来做反向传播。强化学习的思路是通过对奖励的大量采样来拟合损失函数，从而实现模型的训练。同样人类反馈也是不可导的，那么我们也可以将人工反馈作为强化学习的奖励，基于人类反馈的强化学习便应运而生。</p><p>RLHF最早可以追溯到Google在2017年发表的《Deep Reinforcement Learning from Human Preferences》，它通过人工标注作为反馈，提升了强化学习在模拟机器人以及雅达利游戏上的表现效果。</p><p><img src="image/image_y36te5mFvs.png" alt=""></p><p>InstructGPT/ChatGPT中还用到了强化学习中一个经典的算法：OpenAI提出的最近策略优化（<a href="https://arxiv.org/pdf/1707.06347.pdf" title="Proximal Policy Optimization，PPO">Proximal Policy Optimization，PPO</a>）。PPO算法是一种新型的Policy Gradient算法，Policy Gradient算法对步长十分敏感，但是又难以选择合适的步长，在训练过程中新旧策略的的变化差异如果过大则不利于学习。PPO提出了新的目标函数可以在多个训练步骤实现小批量的更新，解决了Policy Gradient算法中步长难以确定的问题。其实TRPO也是为了解决这个思想但是相比于TRPO算法PPO算法更容易求解。</p>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> PaperReading </tag>
            
            <tag> GPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 Swin Transformer</title>
      <link href="/paper_reading/2.4.Swin%20Transformer/"/>
      <url>/paper_reading/2.4.Swin%20Transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="4-Swin-Transformer"><a href="#4-Swin-Transformer" class="headerlink" title="4.Swin Transformer"></a>4.Swin Transformer</h1><p><strong>Swin transformer: Hierarchical vision transformer using shifted windows</strong></p><p>论文链接：<a href="https://arxiv.org/pdf/2103.14030.pdf">https://arxiv.org/pdf/2103.14030.pdf</a></p><p>官方代码库：<a href="https://github.com/microsoft/Swin-Transformer" title="microsoft/Swin-Transformer">microsoft/Swin-Transformer</a></p><p>论文解读视频：<a href="https://www.bilibili.com/video/BV13L4y1475U" title="https://www.bilibili.com/video/BV13L4y1475U">https://www.bilibili.com/video/BV13L4y1475U</a></p><h1 id="0-Swim-Transformer简介"><a href="#0-Swim-Transformer简介" class="headerlink" title="0.Swim Transformer简介"></a>0.Swim Transformer简介</h1><p>Swin Transformer是 ICCV 21的最佳论文，它之所以能有这么大的影响力主要是因为在 ViT 之后，<strong>Swin Transformer通过在一系列视觉任务上的强大表现 ，进一步证明了Transformer是可以在视觉领域取得广泛应用的</strong>。</p><p>更新时间线：</p><ul><li>2021年3月传到 arxiv上的</li><li>2021年4月份代码库放出</li><li>2021年5月12号又放出来了自监督版本的Swin Transformer—moby，从方法上和性能上其实和MoCo v3和DINO都差不多，只是换了个骨干网络</li><li>接下来过了一个月，Swin Transformer就被用到了视频领域，推出了Video-Swin-Transformer，在一系列数据集上都取得了非常好的效果；</li><li>7月初的时候，因为看到了有 MLP Mixer 这篇论文，把 Swin 的思想用到了 MLP 里，推出了 Swin MLP</li><li>8月初的时候，把 Swin Transformer 用到了半监督的目标检测里，然后取得了非常好的效果</li><li>10月份的时候获得了ICCV 的最佳论文奖</li><li>12月份受到了 BEiT 和 MAE 的推动，用 Swin Transformer 基于掩码自监督学习的方式做了一个叫 SimMIM 的论文</li></ul><p>所以说在这大半年的时间里，原作者团队就以每个月一篇论文的速度，基本把视觉领域所有的任务都刷了个遍，而且 Swin Transformer 不光应用范围广，效果也非常的炸裂。</p><p>所以说，在 Swin Transformer 作者团队不懈的努力下，Swin Transformer 在大部分视觉领域很多数据集上都取得了最好的结果，这就导致 Swin Transformer 成了视觉领域一个绕不开的Basline，接下来再想在这些数据集上刷分或者说再想发这些领域的论文，多多少少都得提到 Swin Transformer 或者跟它比，所以说它的影响力是巨大的。</p><h1 id="1-题目"><a href="#1-题目" class="headerlink" title="1.题目"></a>1.<strong>题目</strong></h1><p>Swin Transformer是一个用了移动窗口的层级式的Vision Transformer</p><ul><li>Swin：来自于 Shifted Windows ，S 和 win，Shifted Window（移动窗口）也是 Swin Transformer这篇论文的主要贡献</li><li>层级式 Hierarchical</li></ul><p>其实 Swin Transformer就是<strong>想让 Vision Transformer像卷积神经网络一样</strong>，也能够分成几个 block，也能做层级式的特征提取，从而导致提出来的特征有多尺度的概念</p><p>作者团队来自 MSRA</p><ul><li>MSRA 经常被誉为是研究者的黄埔军校，从里面出来了一众大佬，而且产出了一系列非常有影响力的工作，比如说大家耳熟能详的、现在单篇引用已经超过10万的 ResNet，也是四位作者都在 MSRA 的时候完成的工作</li></ul><h1 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a>2.<strong>摘要</strong></h1><p>这篇论文提出了一个新的 Vision Transformer 叫做 Swin Transformer，它可以被用来作<strong>为一个计算机视觉领域一个通用的骨干网络</strong></p><ul><li>之所以这么说，是因为ViT 在结论的部分指出，他们那篇论文只是做了分类任务，把下游任务比如说检测和分割留给以后的人去探索，所以说在 ViT 出来之后，大家虽然看到了Transformer在视觉领域的强大潜力，但是并不确定Transformer能不能把所有视觉的任务都做掉，所以 Swin Transformer这篇论文的研究动机就是想告诉大家用 Transformer没毛病，绝对能在方方面面上取代卷积神经网络，接下来大家都上 Transformer 就好了</li></ul><p>但是<strong>直接把Transformer从 NLP 用到 Vision 是有一些挑战的</strong>，这个挑战主要来自于两个方面</p><ul><li><strong>一个就是尺度上的问题</strong>。因为比如说现在有一张街景的图片，里面有很多车和行人，里面的物体都大大小小，那这时候代表同样一个语义的词，比如说行人或者汽车就有非常不同的尺寸，这种现象在 NLP 中就没有</li><li><strong>另外一个挑战是图像的 resolution太大了</strong>，如果要以像素点作为基本单位的话，序列的长度就变得高不可攀，所以说之前的工作要么就是用后续的特征图来当做Transformer的输入，要么就是把图片打成 patch 减少这个图片的 resolution，要么就是把图片画成一个一个的小窗口，然后在窗口里面去做自注意力，所有的这些方法都是为了减少序列长度</li></ul><p>基于这两个挑战，本文的作者就提出了 hierarchical Transformer，它的特征是通过一种叫做<strong>移动窗口的方式学来的</strong></p><ul><li>移动窗口的好处：不仅带来了更大的效率，因为跟之前的工作一样，现在自注意力是在窗口内算的，所以这个序列的长度大大的降低了；同时通过 shifting 移动的这个操作，能够让相邻的两个窗口之间有了交互，所以上下层之间就可以有 cross-window connection，从而变相的达到了一种全局建模的能力</li></ul><p>这种层级式的结构不仅非常灵活，<strong>可以提供各个尺度的特征信息，同时因为自注意力是在小窗口之内算的**</strong>，所以说它的计算复杂度是随着图像大小而线性增长，而不是平方级增长，** 这其实也为作者之后提出 Swin V2 铺平了道路，从而让他们可以在特别大的分辨率上去预训练模型</p><p><strong>因为 Swin Transformer 拥有了像卷积神经网络一样分层的结构，有了这种多尺度的特征，所以它很容易使用到下游任务里</strong>，所以在这篇论文里，作者不光是在 ImageNet-1K 上做了实验，而且达到了非常好的准确度87.3；而且还在密集预测型的任务上，比如说物体检测、物体分割上取得了很好的成绩，比如说在 COCO 上刷到58.7的 AP，比之前最好的方法高了2.7个点；然后在语义分割上，ADE上 也刷到了53.5，比之前最好的方法高了3.2个点。所以<strong>这种基于 Transformer 的模型在视觉领域是非常有潜力的</strong>。</p><p>为了凸显这篇文章的贡献，也就是 Shifted Windows 移动窗口的作用，这个版本又加了一句话：对于 MLP 的架构用 shift window 的方法也能提升，这句话其实这个版本才加入的，之前第一个版本就是投稿上那篇论文其实没有这句话，因为当时还没有 MLP Mixer 这篇论文。</p><h1 id="2-引言"><a href="#2-引言" class="headerlink" title="2.引言"></a>2.<strong>引言</strong></h1><h2 id="2-1-ViT-vs-Swin-Transformer"><a href="#2-1-ViT-vs-Swin-Transformer" class="headerlink" title="2.1 ViT vs Swin Transformer"></a>2.1 ViT vs Swin Transformer</h2><p>引言的前两段其实跟 ViT 非常一致，都是先说在视觉领域，之前卷积神经网络是主导地位，但是Transformer在 NLP 领域用的这么好，所以也想把Transformer用到视觉领域里面</p><p>但因为 ViT 已经把这件事干了，所以说Swin Transformer在第三段的开始说他们的研究动机，是<strong>想证明Transformer是可以用作一个通用的骨干网络，就是对所有视觉的任务，不光是分类，在检测、分割视频上也都能取得很好的效果</strong></p><p><img src="image/image_JKaiVONDev.png" alt=""></p><ul><li>图一如上图所示，作者先说了一下 Vision Transformer，把它放在右边做对比</li><li>Vision Transformer就是把图片打成 patch，因为 ViT 里用的 patch size 是16*16的，所以说这里的16 ×，也就意味着是16倍的下采样率，这也就意味着每一个 patch，也就是每一个 token，自始至终代表的尺寸都是差不多的；每一层的Transformer block 看到token的尺寸都是16倍下采样率。虽然它<strong>可以通过这种全局的自注意力操作，达到全局的建模能力，但是它对多尺寸特征的把握就会弱一些</strong></li><li>对于视觉任务，尤其是下游任务比如说检测和分割来说，<strong>多尺寸的特征是至关重要的</strong>，比如说对目标检测而言，运用最广的一个方法就是 FPN（a feature pyramid network：当有一个分层式的卷积神经网络之后，每一个卷积层出来的特征的 receptive field （感受野）是不一样的，能抓住物体不同尺寸的特征，从而能够很好的处理物体不同尺寸的问题；</li></ul><p><img src="image/image_oudLTkMxch.png" alt=""></p><ul><li>对于物体分割任务来说，那最常见的一个网络就是 UNet，UNet 里为了处理物体不同尺寸的问题，提出来一个叫做 skip connection 的方法，当一系列下采样做完以后，去做上采样的时候，不光是从 bottleneck 里去拿特征，还从之前也就是每次下采样完之后的东西里去拿特征，这样就把那些高频率的图像细节又全都能恢复出来了，当然分割里大家常用的网络结构还有 PspNet 、DeepLab，这些工作里也有相应的处理多尺寸的方法，比如说使用空洞卷积、使用 psp 和 aspp 层</li></ul><p><img src="image/image_pfoTWg4yhg.png" alt=""></p><ul><li>总之，对于计算机视觉的下游任务，尤其是<strong>密集预测型的任务（检测、分割），有多尺寸的特征是至关重要的</strong></li></ul><p>但是在** ViT 里处理的特征都是单一尺寸**，而且是 low resolution，也就是说自始至终都是处理的16倍下采样率过后的特征，所以说，它可能就不适合处理这种密集预测型的任务，同时对 ViT 而言，<strong>自注意力始终都是在最大的窗口上进行，也就是说始终都是在整图上进行的，所以它是一个全局建模，它的复杂度是跟随图像的尺寸进行平方倍的增长</strong>，像检测和分割领域，一般现在常用的输入尺寸都是800乘以800或者1000乘1000，之前虽然用 patch size 16能处理 224*224 的图片，但是当图片变到这么大的时候，即使用patch size16，序列长度还是会上千，计算复杂度还是难以承受的</p><p>所以基于这些挑战，作者提出了 Swin Transformer，<strong>Swin Transformer 其实是借鉴了很多卷积神经网络的设计理念以及先验知识</strong></p><ul><li>比如说为了减少序列的长度、降低计算复杂度，<strong>Swin Transformer采取了在小窗口之内算自注意力</strong>，而不是像 ViT 一样在整图上算自注意力，这样只要窗口大小是固定的，自注意力的计算复杂度就是固定的，整张图的计算复杂度就会跟图片的大小而成的线性增长关系，就是说图片增大了 x 倍，窗口数量也增大了 x 倍，计算复杂度也就乘以 x，而不是乘以 x 的平方</li><li>这个就算是<strong>利用了卷积神经网络里的 Locality 的 Inductive bias，就是利用了局部性的先验知识</strong>，同一个物体的不同部位或者语义相近的不同物体还是大概率会出现在相连的地方，所以即使是在一个 Local，一个小范围的窗口算自注意力也是差不多够用的，全局计算自注意力对于视觉任务来说，其实是有点浪费资源的</li><li>另外一个挑战是如何生成多尺寸的特征，卷积神经网络为什么会有多尺寸的特征？主要是因为有 Pooling （池化）这个操作，池化能够增大每一个卷积核能看到的感受野，从而使得每次池化过后的特征抓住物体的不同尺寸，所以类似的 ，<strong>Swin Transformer也提出来了一个类似于池化的操作叫做 patch merging</strong>，就是把相邻的小 patch 合成一个大 patch，这样合并出来的这一个大patch其实就能看到之前四个小patch看到的内容，它的感受野就增大了，同时也能抓住多尺寸的特征</li></ul><p>所以图一左边所示，Swin Transformer 刚开始的下采样率是4倍，然后变成了8倍、16倍，之所以刚开始是4×的，是因为最开始的 patch 是4乘4大小的，一旦有了多尺寸的特征信息，有了这种4x、8x、16x的特征图，那自然就可以把这些多尺寸的特征图输给一个 FPN，从而就可以去做检测了</p><ul><li>同样的道理，有了这些多尺寸的特征图以后，也可以把它扔给一个 UNET，然后就可以去做分割了</li><li>所以这就是作者在这篇论文里反复强调的，<strong>Swin Transformer是能够当做一个通用的骨干网络的，不光是能做图像分类，还能做密集预测性的任务</strong></li></ul><h2 id="2-2移动窗口操作"><a href="#2-2移动窗口操作" class="headerlink" title="2.2移动窗口操作"></a>2.2移动窗口操作</h2><p>第四段主要就开始讲 Swin Transformer一个关键的设计因素——<strong>移动窗口的操作</strong>，如下图中所示</p><p><img src="image/image_7B_yBK15wf.png" alt=""></p><ul><li>如果在 Transformer 第 L 层把输入或者特征图分成小窗口的话，就会有效的降低序列长度，从而减少计算复杂度</li><li>图中每一个灰色的小 patch 是最基本的元素单元，也就是图一中4*4的 patch；每个红色的框是一个中型的计算单元，也就是一个窗口</li><li>在 Swin Transformer 这篇论文里，一个小窗口里面默认有七七四十九个小patch的</li></ul><h3 id="（1）shift-的操作"><a href="#（1）shift-的操作" class="headerlink" title="（1）shift 的操作"></a>（1）<strong>shift 的操作</strong></h3><p><img src="image/image_gkSUSXKyzh.png" alt=""></p><ul><li>如果用一个大的蓝色的正方形来描述整体的特征图，其实** shift 操作就是往右下角的方向整体移了两个 patch，也就变成了像图中右图的格式**</li><li>然后在新的特征图里把它再次分成四方格，如图中右图所示</li><li>最后 shift 完就能得到图中Layer 1+1的结果了</li></ul><h3 id="（2）效果"><a href="#（2）效果" class="headerlink" title="（2）效果"></a>（2）效果</h3><p>这样的好处是<strong>窗口与窗口之间可以进行互动</strong>，因为如果按照原来的方式，就是没有 shift，这些窗口之间都是不重叠的，如果每次自注意力的操作都在小的窗口里头进行了，<strong>每个窗口里的 patch 就永远无法注意到别的窗口里的 patch 的信息，这就达不到使用 Transformer 的初衷</strong></p><ul><li>因为Transformer的初衷就是更好的理解上下文，如果窗口都是不重叠的，那自注意力真的就变成孤立自注意力，就没有全局建模的能力</li><li>但如果加上 shift 的操作，每个 patch 原来只能跟它所在的窗口里的别的 patch 进行交互，但是 shift 之后，这个 patch就可以跟新的窗口里的别的 patch就进行交互了，而这个新的窗口里所有的 patch 其实来自于上一层别的窗口里的 patch，这也就是作者说的能起到 cross-window connection，就是窗口和窗口之间可以交互了</li></ul><p>再配合上之后提出的 <strong>patch merging，合并到 Transformer 最后几层的时候，每一个 patch 本身的感受野就已经很大了</strong>，就已经能看到大部分图片了，然后再加上移动窗口的操作，它所谓的窗口内的局部注意力其实也就变相的等于是一个全局的自注意力操作了。<strong>这样就是既省内存，效果也好</strong>。</p><h3 id="（3）结果"><a href="#（3）结果" class="headerlink" title="（3）结果"></a>（3）结果</h3><p>第五段作者再次展示了一下结果，因为 Swin Transformer 的结果确实非常好，最后一段作者就展望了一下，作者说他们<strong>坚信一个 CV 和NLP 之间大一统的框架是能够促进两个领域共同发展的</strong>。</p><ul><li>确实如此，因为人在学习的过程中也是一个多模态的学习过程，但 Swin Transformer还是利用了更多视觉里的先验知识，从而在视觉任务上大杀四方</li><li>但是在模型大一统上，也就是 unified architecture 上来说，其实 ViT 还是做的更好的，因为它真的可以什么都不改，什么先验信息都不加，就能让Transformer在两个领域都能用的很好，这样模型不仅可以共享参数，而且甚至可以把所有模态的输入直接就拼接起来，当成一个很长的输入，直接扔给Transformer去做，而不用考虑每个模态的特性。</li></ul><h1 id="3-结论"><a href="#3-结论" class="headerlink" title="3.结论"></a>3.<strong>结论</strong></h1><p>这篇论文提出了 Swin Transformer，它是一个层级式的Transformer，而且它的计算复杂度是跟输入图像的大小呈线性增长的</p><p>Swin Transformerr 在 COCO 和 ADE20K上的效果都非常的好，远远超越了之前最好的方法，所以作者说基于此，希望 Swin Transformer 能够激发出更多更好的工作，尤其是在多模态方面</p><p>因为在Swin Transformer 这篇论文里最关键的一个贡献就是基于 Shifted Window 的自注意力，它对很多视觉的任务，尤其是对下游密集预测型的任务是非常有帮助的，但是如果 Shifted Window 操作不能用到 NLP 领域里，其实在模型大一统上论据就不是那么强了，所以作者说接下来他们的未来工作就是要把 Shifted Windows用到 NLP 里面，而且如果真的能做到这一点，那 Swin Transformer真的就是一个里程碑式的工作了，而且模型大一统的故事也就讲的圆满了</p><h1 id="4-相关工作"><a href="#4-相关工作" class="headerlink" title="4.相关工作"></a>4.<strong>相关工作</strong></h1><p>跟 ViT 的相关工作非常相似，作者先大概讲了一下卷积神经网络，然后又讲了一下自注意力或者 Transformer 是如何用来帮助卷积神经网络的，最后纯 Transformer 用来做视觉里的骨干网络</p><h1 id="5-方法"><a href="#5-方法" class="headerlink" title="5.方法"></a>5.<strong>方法</strong></h1><p>主要分为两大块</p><ul><li>大概把整体的流程讲了一下，主要就是过了一下前向过程，以及提出的 patch merging 操作是怎么做的</li><li>基于 Shifted Window 的自注意力，Swin Transformer怎么把它变成一个transformer block 进行计算</li></ul><p>模型总览图如下图所示</p><p><img src="image/image_nqLFiAeDtR.png" alt=""></p><h2 id="5-1-前向过程"><a href="#5-1-前向过程" class="headerlink" title="5.1 前向过程"></a>5.1 <strong>前向过程</strong></h2><h3 id="（1）阶段1：Patch-Projection-Swin-Transformer-block"><a href="#（1）阶段1：Patch-Projection-Swin-Transformer-block" class="headerlink" title="（1）阶段1：Patch Projection + Swin Transformer block"></a>（1）阶段1：Patch Projection + Swin Transformer block</h3><p>224*224*3 → (patch) 56*56*48 → (STB) 56*56*96</p><ul><li>假设说有一张$224<em>224</em>3$（ImageNet 标准尺寸）的输入图片</li><li>第一步就是像 ViT 那样<strong>把图片打成 patch</strong>，在 Swin Transformer 这篇论文里，它的 patch size 是$4<em>4$，而不是像 ViT 一样$16</em>16$，所以说它经过 patch partition 打成 patch 之后，得到图片的尺寸是$56<em>56</em>48$，56就是224/4，因为 patch size 是4，向量的维度48，因为$4<em>4</em>3$，3 是图片的 RGB 通道</li><li>打完了 patch ，接下来就要做 <strong>Linear Embedding</strong>，也就是说要把向量的维度变成一个预先设置好的值，就是 Transformer 能够接受的值，在 Swin Transformer 的论文里把这个超参数设为 <strong>c</strong>，对于 Swin tiny 网络来说，也就是上图中画的网络总览图，它的 <strong>c 是96</strong>，所以经历完 Linear Embedding 之后，输入的尺寸就变成了$56<em>56</em>96$，前面的56*56就会拉直变成<strong>3136</strong>，变成了序列长度，后面的96就变成了每一个token向量的维度，其实 Patch Partition 和 Linear Embedding 就相当于是 ViT 里的Patch Projection 操作，而在代码里也是用一次卷积操作就完成了，</li><li>第一部分跟 ViT 其实还是没有区别的，但紧接着区别就来了</li><li>首先序列长度是3136，对于 ViT 来说，用 patch size $16<em>16$，它的序列长度就只有196，是相对短很多的，这里<strong>的3136就太长了</strong>，是目前来说Transformer不能接受的序列长度，所以 <em>*Swin Transformer 就引入了基于窗口的自注意力计算</em></em>，每个窗口按照默认来说，都只有七七四十九个 patch，所以说序列长度就只有49就相当小了，这样就解决了计算复杂度的问题</li><li>所以也就是说， stage1中的<strong>swin transformer block 是基于窗口计算自注意力的</strong>，现在暂时先把 transformer block当成是一个黑盒，只关注输入和输出的维度，对于 Transformer 来说，如果不对它做更多约束的话，Transformer输入的序列长度是多少，输出的序列长度也是多少，它的输入输出的尺寸是不变的，所以说在 stage1 中经过两层Swin Transformer block 之后，输出还是$56<em>56</em>96$</li><li>到这其实 Swin Transformer的第一个阶段就走完了，也就是先过一个 Patch Projection 层，然后再过一些 Swin Transformer block，接下来如果想要有多尺寸的特征信息，就要构建一个层级式的 transformer，也就是说需要一个像卷积神经网络里一样，有一个类似于池化的操作</li></ul><p>这篇论文里作者就提出 <strong>Patch Merging</strong> 的操作，Patch Merging 其实在之前一些工作里也有用到，它很像 Pixel Shuffle 的上采样的一个反过程，Pixel Shuffle 是 lower level 任务中很常用的一个上采样方式</p><h3 id="（2）阶段2：Patch-Merging-Swin-Transformer-block"><a href="#（2）阶段2：Patch-Merging-Swin-Transformer-block" class="headerlink" title="（2）阶段2：Patch Merging + Swin Transformer block"></a>（2）阶段2：<strong>Patch Merging</strong> + Swin Transformer block</h3><p>操作举例如下图所示</p><p><img src="image/image_hY3FNqS_l5.png" alt=""></p><ul><li>假如有一个张量， <strong>Patch Merging 顾名思义就是把临近的小 patch 合并成一个大 patch</strong>，这样就可以起到下采样一个特征图的效果了</li><li>这里因为是想下采样两倍，所以说在选点的时候是每隔一个点选一个，也就意味着说对于这个张量来说，每次选的点是1、1、1、1</li></ul><p><img src="image/image_zsuDxTcIuu.png" alt=""></p><ul><li>其实在这里的1、2、3、4并不是矩阵里有的值，而是给它的一个序号，同样序号位置上的 patch 就会被 merge 到一起，这个序号只是为了帮助理解</li><li>经过隔一个点采一个样之后，原来的这个张量就变成了四个张量，也就是说所有的1都在一起了，2在一起，3在一起，4在一起，如果原张量的维度是 h * w * c ，当然这里 c 没有画出来，经过这次采样之后就得到了4个张量，每个张量的大小是 h/2、w/2，它的尺寸都缩小了一倍</li><li>现在把这四个张量在 c 的维度上拼接起来，也就变成了下图中红线所画出来的形式，张量的大小就变成了 h/2 * w/2 * 4c，相当于用空间上的维度换了更多的通道数</li></ul><p><img src="image/image_v0OPccOiaZ.png" alt=""></p><ul><li>通过这个操作，就把原来一个大的张量变小了，就像卷积神经网络里的池化操作一样，为了跟卷积神经网络那边保持一致（不论是 VGGNet 还是 ResNet，一般在池化操作降维之后，通道数都会翻倍，从128变成256，从256再变成512），所以这里也只想让他翻倍，而不是变成4倍，所以紧接着又再做了一次操作，就是在 c 的维度上用一个1乘1的卷积，把通道数降下来变成2c，通过这个操作就能把原来一个大小为 $ h<em>w</em>c  $的张量变成$  h/2 <em> w/2 </em>2c  $的一个张量，<strong>也就是说空间大小减半，但是通道数乘2，这样就跟卷积神经网络完全对等起来了</strong></li></ul><p>整个这个过程就是 Patch Merging，经历过这次Patch Merging操作之后，输出的大小就从$56<em>56</em>96$变成了$28<em>28</em>192$，经过stage2中的 Transformer block，尺寸是不变的，所以出来之后还是28*28*192</p><p>这样第二阶段也就完成了，</p><h3 id="（3）阶段3，4：重复Patch-Merging-Swin-Transformer-block"><a href="#（3）阶段3，4：重复Patch-Merging-Swin-Transformer-block" class="headerlink" title="（3）阶段3，4：重复Patch Merging + Swin Transformer block"></a>（3）阶段3，4：重复<strong>Patch Merging</strong> + Swin Transformer block</h3><p>第三和第四阶段都是同理，都是先进来做一次Patch Merging，然后再通过一些 Swin Transformer block，所以维度就进一步降成了$14<em>14</em>384$以及$7<em>7</em>768$</p><p>这里其实会发现，特征图的维度真的跟卷积神经网络好像，因为如果回想残差网络的多尺寸的特征，就是经过每个残差阶段之后的特征图大小也是56*56、28*28、14*14，最后是7*7</p><p><strong>而且为了和卷积神经网络保持一致，Swin Transformer这篇论文并没有像 ViT 一样使用 CLS token</strong>，ViT 是给刚开始的输入序列又加了一个 CLS token，所以这个长度就从196变成了197，最后拿 CLS token 的特征直接去做分类，但 Swin Transformer 没有用这个 token，它是像卷积神经网络一样，<strong>在得到最后的特征图之后用global average polling，就是全局池化的操作，直接把7*7就取平均拉直变成1了</strong></p><ul><li>作者这个图里并没有画，因为 Swin Transformer的本意并不是只做分类，它还会去做检测和分割，所以说它只画了骨干网络的部分，没有去画最后的分类头或者检测头，但是如果是做分类的话，最后就变成了1*768，然后又变成了1*1,000</li><li>如果是做ImageNet的话，这样就完成了整个一个分类网络的前向过程</li></ul><p><img src="image/image_NLJDRHwW2E.png" alt=""></p><p>所以看完整个前向过程之后，就会发现 Swin Transformer 有四个 stage，还有类似于池化的 patch merging 操作，自注意力还是在小窗口之内做的以及最后还用的是 global average polling，所以说** Swin Transformer 这篇论文真的是把卷积神经网络和 Transformer 这两系列的工作完美的结合到了一起**，也可以说它是披着Transformer皮的卷积神经网络</p><h2 id="5-2-基于自注意力的滑动窗口"><a href="#5-2-基于自注意力的滑动窗口" class="headerlink" title="5.2 基于自注意力的滑动窗口"></a>5.2 基于自注意力的滑动窗口</h2><p>这篇论文的主要贡献就是基于窗口或者移动窗口的自注意力，这里作者又写了一段研究动机，就是为什么要引入窗口的自注意力，其实跟之前引言里说的都是一个事情，就是说<strong>全局自注意力的计算会导致平方倍的复杂度</strong>，同样当去做视觉里的下游任务，尤其是密集预测型的任务，或者说遇到非常大尺寸的图片时候，这种全局算自注意力的计算复杂度就非常贵了，所以就用窗口的方式去做自注意力</p><h3 id="（1）窗口划分举例"><a href="#（1）窗口划分举例" class="headerlink" title="（1）窗口划分举例"></a>（1）<strong>窗口划分举例</strong></h3><p>原图片会被平均的分成一些没有重叠的窗口，拿第一层之前的输入来举例，它的尺寸就是56*56*96，也就说有一个维度是56*56张量，然后把它切成一些不重叠的方格，也就是下图中用橘黄色表示的方格</p><p><img src="image/image_oFgjSrkws-.png" alt=""></p><ul><li>每一个橘黄色的方格就是一个窗口，但是<strong>这个窗口并不是最小的计算单元，最小的计算单元其实还是之前的那个 patch</strong>，也就意味着每一个小窗口里其实还有 m * m 个 patch，在 Swin Transformer 这篇论文里一般 m 默认为7，也就是说，<strong>一个橘黄色的小方格里有七七四十九个小 patch</strong></li><li>现在所有自注意力的计算都是在这些小窗口里完成的，就是说序列长度永远都是七七四十九</li><li>原来大的整体特征图到底里面会有多少个窗口呢？其实也就是每条边56/7就8个窗口，也就是说一共会有8*8等于64个窗口，<strong>就是说会在这64个窗口里分别去算它们的自注意力</strong></li></ul><h3 id="（2）基于窗口的自注意力模式的计算复杂度"><a href="#（2）基于窗口的自注意力模式的计算复杂度" class="headerlink" title="（2）基于窗口的自注意力模式的计算复杂度"></a>（2）<strong>基于窗口的自注意力模式的计算复杂度</strong></h3><p>说到底，基于窗口的自注意力计算方式能比全局的自注意力方式省多少呢？在Swin Transformer这篇论文里作者就给出了一个大概的估计，它给出了两个公式如下所示</p><script type="math/tex; mode=display">\begin{gathered}\Omega(\mathrm{MSA})=4h w C^{2}+2(h w)^{2}C,   ~~~~~~~~~~(1) \\\Omega(\mathrm{W-MSA})=4h w C^{2}+2M^{2}h w C, (2) \end{gathered}</script><ul><li>公式（1）对应的是标准的多头自注意力的计算复杂度</li><li>每一个图片大概会有 h*w 个 patch，在刚才的例子里，h 和 w 分别都是56，c 是特征的维度</li><li>公式（2）对应的是基于窗口的自注意力计算的复杂度，这里的 M 就是刚才的7，也就是说一个窗口的某条边上有多少个patch</li></ul><p><strong>公式推算</strong></p><h4 id="以标准的多头自注意力为例"><a href="#以标准的多头自注意力为例" class="headerlink" title="以标准的多头自注意力为例"></a>以标准的多头自注意力为例</h4><p><img src="image/image_20dVZAf0Ai.png" alt=""></p><ul><li>如果现在有一个输入，自注意力首先把它变成 q k v 三个向量，这个过程其实就是原来的向量分别乘了三个系数矩阵</li><li>一旦得到 query 和 k 之后，它们就会相乘，最后得到 attention，也就是自注意力的矩阵</li><li>有了自注意力之后，就会和 value 做一次乘法，也就相当于是做了一次加权</li><li>最后因为是多头自注意力，所以最后还会有一个 projection layer，这个投射层会把向量的维度投射到我们想要的维度</li></ul><p>如果这些向量都加上它们该有的维度，也就是说刚开始输入是 h*w*c</p><ul><li>首先，$ to_q_k_v()  $函数相当于是用一个 $h<em>w</em>c$ 的向量乘以一个 $c<em>c$ 的系数矩阵，最后得到了 $h</em>w<em>c$。所以每一个计算的复杂度是 $h</em>w<em>c^2$，因为有三次操作，所以是三倍的 $h</em>w*c^2$</li><li>然后，算自注意力就是 $h<em>w</em>c$乘以 k 的转置，也就是 $c<em>h</em>w$，所以得到了 $h<em>w</em>h<em>w$，这个计算复杂度就是$(h</em>w)^2*c$</li><li>接下来，自注意力矩阵和value的乘积的计算复杂度还是 $(h<em>w)^2</em>c$，所以现在就成了2*$(h<em>w)^2</em>c$</li><li>最后一步，投射层也就是$h<em>w</em>c$乘以 c*c 变成了 $h<em>w</em>c$ ，它的计算复杂度就又是 $h<em>w</em>c^2$</li><li>最后合并起来就是最后的公式（1）</li></ul><h4 id="基于窗口的自注意力计算复杂度"><a href="#基于窗口的自注意力计算复杂度" class="headerlink" title="基于窗口的自注意力计算复杂度"></a>基于窗口的自注意力计算复杂度</h4><ul><li>因为在每个窗口里算的还是多头自注意力，所以可以直接套用公式（1），只不过高度和宽度变化了，现在高度和宽度不再是$  h <em> w $，而是变成窗口有多大了，也就是 $M</em>M$，也就是说现在 h 变成了 M，w 也是 M，它的序列长度只有 M * M 这么大</li><li>所以当把 M 值带入到公式（1）之后，就得到计算复杂度是$4 <em> M^2 </em> c^2 + 2 <em> M^4 </em> c$，这个就是在一个窗口里算多头自注意力所需要的计算复杂度</li><li>那我们现在一共有 $ h/M * w/M  $个窗口，现在用这么多个窗口乘以每个窗口所需要的计算复杂度就能得到公式（2）了</li></ul><p>对比公式（1）和公式（2），虽然这两个公式前面这两项是一样的，只有后面从 $(h<em>w)^2$变成了 $M^2 </em> h <em> w$，看起来好像差别不大，但其实如果仔细带入数字进去计算就会发现，<strong>计算复杂的差距是相当巨大的</strong>，因为这里的 $ h</em>w  $如果是56*56的话， $ M^2  $其实只有49，所以是相差了几十甚至上百倍的</p><h2 id="5-3-移动窗口"><a href="#5-3-移动窗口" class="headerlink" title="5.3 移动窗口"></a>5.3 移动窗口</h2><p>这种基于窗口计算自注意力的方式虽然很好地解决了内存和计算量的问题，<strong>但是窗口和窗口之间没有通信，这样就达不到全局建模了</strong>，也就文章里说的会限制模型的能力，所以最好还是要有一种方式能让窗口和窗口之间互相通信起来，这样效果应该会更好，因为具有上下文的信息，所以作者就提出移动窗口的方式</p><p><strong>移动窗口就是把原来的窗口往右下角移动一半窗口的距离</strong>，如果Transformer是上下两层连着做这种操作，先是 window再是 shifted window 的话，就能起到窗口和窗口之间互相通信的目的了</p><p>所以说在 Swin Transformer里， transformer block 的安排是有讲究的，每次都是先要做一次基于窗口的多头自注意力，然后再做一次基于移动窗口的多头自注意力，这样就达到了窗口和窗口之间的互相通信。</p><script type="math/tex; mode=display">\begin{aligned}&\hat{\mathbf{z}}^{l}=\mathbf{W-MSA}\left(\mathrm{LN}\left(\mathbf{z}^{l-1}\right)\right)+\mathbf{z}^{l-1}, \\&\mathbf{z}^l=\mathbf{MLP}\left(\mathrm{LN}\left(\hat{\mathbf{z}}^l\right)\right)+\hat{\mathbf{z}}^l, \\&\hat{\mathbf{z}}^{l+1}=\mathbf{SW-MSA}\left(\mathrm{LN}\left(\mathbf{z}^{l}\right)\right)+\mathbf{z}^{l}, \\&\mathbf{z}^{l+1}=\mathrm{MLP}\left(\mathrm{LN}\left(\hat{\mathbf{z}}^{l+1}\right)\right)+\hat{\mathbf{z}}^{l+1},\end{aligned}</script><p>如下图所示</p><p><img src="https://i0.hdslb.com/bfs/note/de79e43763a634933ea6eb1eae348a68b7e93e78.png" alt=""></p><ul><li>每次输入先进来之后先做一次 Layernorm，然后做窗口的多头自注意力，然后再过 Layernorm 过 MLP，第一个 block 就结束了</li><li>这个 block 结束以后，紧接着做一次Shifted window，也就是基于移动窗口的多头自注意力，然后再过 MLP 得到输出</li><li>这两个 block 加起来其实才算是 Swin Transformer 一个基本的计算单元，这也就是为什么stage1、2、3、4中的 swin transformer block 为什么是 <em>2、</em> 2、<em>6、</em> 2，也就是一共有多少层 Swin Transformer block 的数字总是偶数，<strong>因为它始终都需要两层 block连在一起作为一个基本单元，所以一定是2的倍数</strong></li></ul><p>到此，Swin Transformer整体的故事和结构就已经讲完了，<strong>主要的研究动机就是想要有一个层级式的 Transformer，为了这个层级式，所以介绍了 Patch Merging 的操作</strong>，从而能像卷积神经网络一样把 Transformer 分成几个阶段，<strong>为了减少计算复杂度，争取能做视觉里密集预测的任务，所以又提出了基于窗口和移动窗口的自注意力方式，</strong> 也就是连在一起的两个Transformer block，最后把这些部分加在一起，就是 Swin Transformer 的结构</p><p>其实作者后面还讲了两个点</p><ul><li>一个是怎样提高移动窗口的计算效率，他们采取了一种非常巧妙的 <strong>masking</strong>（掩码）的方式</li><li>另外一个点就是这篇论文里没有用绝对的位置编码，而是用<strong>相对的位置编码</strong></li></ul><p>但这两个点其实都是为了提高性能的一些技术细节，跟文章整体的故事已经没有多大关系了</p><h2 id="5-4-移动窗口存在的问题"><a href="#5-4-移动窗口存在的问题" class="headerlink" title="5.4 移动窗口存在的问题"></a>5.4 移动窗口存在的问题</h2><p><img src="image/image_6i8O1pKGsn.png" alt=""></p><ul><li>上图是一个基础版本的移动窗口，就是把左边的窗口模式变成了右边的窗口方式</li><li>虽然这种方式已经能够达到窗口和窗口之间的互相通信了，但是会发现一个问题，就是原来计算的时候，特征图上只有四个窗口，<strong>但是做完移动窗口操作之后得到了9个窗口，窗口的数量增加了，而且每个窗口里的元素大小不一</strong>，比如说中间的窗口还是4*4，有16个 patch，但是别的窗口有的有4个 patch，有的有8个 patch，都不一样了，<strong>如果想做快速运算，就是把这些窗口全都压成一个 patch直接去算自注意力，就做不到了，因为窗口的大小不一样</strong></li><li>有一个<strong>简单粗暴的解决方式就是把这些小窗口周围再 pad 上0</strong> ，把它照样pad成和中间窗口一样大的窗口，这样就有9个完全一样大的窗口，这样就还能把它们压成一个batch，就会快很多</li><li><strong>但是这样的话，无形之中计算复杂度就提升了</strong>，因为原来如果算基于窗口的自注意力只用算4个窗口，但是现在需要去算9个窗口，复杂度一下提升了两倍多，所以还是相当可观的</li><li>那怎么能让第二次移位完的窗口数量还是保持4个，而且每个窗口里的patch数量也还保持一致呢？作者提出了一个非常巧妙的掩码方式。</li></ul><h2 id="5-5-掩码masking"><a href="#5-5-掩码masking" class="headerlink" title="5.5 掩码masking"></a>5.5 掩码<strong>masking</strong></h2><h3 id="（1）掩码过程"><a href="#（1）掩码过程" class="headerlink" title="（1）掩码过程"></a>（1）掩码过程</h3><p>当通过普通的移动窗口方式，得到9个窗口之后，现在不在这9个窗口上算自注意力，<strong>先再做一次循环移位（ cyclic shift ）</strong></p><p><img src="image/image_nG4ywVPKDm.png" alt=""></p><ul><li>经过这次循环移位之后，原来的窗口（虚线）就变成了现在窗口（实线）的样子，那如果在大的特征图上再把它分成四宫格的话，我在就又得到了四个窗口，意思就是说移位之前的窗口数也是4个，移完位之后再做一次循环移位得到窗口数还是4个，<strong>这样窗口的数量就固定了，也就说计算复杂度就固定了</strong></li><li>但是新的问题就来了，虽然对于移位后左上角的窗口（也就是移位前最中间的窗口）来说，里面的元素都是互相紧挨着的，他们之间可以互相两两做自注意力，但是<strong>对于剩下几个窗口来说，它们里面的元素是从别的很远的地方搬过来的，所以他们之间，按道理来说是不应该去做自注意力，也就是说他们之间不应该有什么太大的联系</strong></li><li>解决这个问题就需要一个很常规的操作，也就是<strong>掩码操作</strong>，这在Transformer过去的工作里是层出不穷，很多工作里都有各式各样的掩码操作</li><li>在 Swin Transformer这篇论文里，作者也巧妙的设计了几种掩码的方式，<strong>从而能让一个窗口之中不同的区域之间也能用一次前向过程，就能把自注意力算出来，但是互相之间都不干扰</strong>，也就是后面的 masked Multi-head Self Attention（<code>MSA</code>）</li><li>算完了多头自注意力之后，还有最后一步就是需要<strong>把循环位移再还原回去</strong>，也就是说需要把A、B、C再还原到原来的位置上去，原因是还需要保持原来图片的相对位置大概是不变的，整体图片的语义信息也是不变的，如果不把循环位移还原的话，那相当于在做Transformer的操作之中，一直在把图片往右下角移，不停的往右下角移，这样图片的语义信息很有可能就被破坏掉了</li></ul><p>所以说整体而言，上图介绍了一种高效的、批次的计算方式，比如说本来移动窗口之后得到了9个窗口，而且窗口之间的patch数量每个都不一样，为了达到高效性，为了能够进行批次处理，先进行一次循环位移，把9个窗口变成4个窗口，然后用巧妙的掩码方式让每个窗口之间能够合理地计算自注意力，最后再把算好的自注意力还原，就完成了基于移动窗口的自注意力计算</p><h3 id="（2）掩码操作举例"><a href="#（2）掩码操作举例" class="headerlink" title="（2）掩码操作举例"></a><strong>（2）掩码操作举例</strong></h3><p>下图为已经经过循环位移的图片，可以分成4个窗口；假设特征图大小为14*14 个patch，划分成4个窗口，每个窗口里面应该有7*7个patch。图中0~8并不是真正的内容，而是用的一种序号，主要用于区分不用区域。</p><p><img src="image/image_zVN8ANUFf6.png" alt=""></p><ul><li>window 0：只有区域0，是相邻，可以做自注意力计算</li><li>window 1：区域1和2不相邻，不应该做自注意力计算</li><li>window 2：区域3和6不相邻，不应该做自注意力计算</li><li>window 3：区域4，5，7，8都不相邻，不能互相做自注意力计算</li></ul><h4 id="以window-2为例，计算自注意力，即掩码如何加"><a href="#以window-2为例，计算自注意力，即掩码如何加" class="headerlink" title="以window 2为例，计算自注意力，即掩码如何加"></a>以window 2为例，计算自注意力，即掩码如何加</h4><p>将window 2拉直为一个向量，从左往右，再往下。先得到的的元素为3号位元素，下面为6号。4*7=28个3号元素；3*7=21个6号元素，向量长度位49，维度位c。</p><p>转置之后做矩阵乘法，结果简单用<code>33</code>表示。</p><p><img src="image/image_Lln_PkVt26.png" alt=""></p><p>得到矩阵的右上角和左下角的自注意力是我们不需要的，设计一个掩码模板，如下图绿色所示，<strong>不需要的区域，掩码为很小的数（负数），将mask和计算后的矩阵相加，再经过softmax操作后，不需要的地方为0了</strong>。</p><p><img src="image/image_L9_7YX8iqH.png" alt=""></p><h4 id="window-1掩码计算示例"><a href="#window-1掩码计算示例" class="headerlink" title="window 1掩码计算示例"></a>window 1掩码计算示例</h4><p>计算结果中，11和22是需要的值，但是12和21是不需要的自注意力值。</p><p><img src="image/image_3YNATQZyyj.png" alt=""></p><h3 id="（3）掩码可视化"><a href="#（3）掩码可视化" class="headerlink" title="（3）掩码可视化"></a>（3）<strong>掩码可视化</strong></h3><p>作者通过这种巧妙的循环位移的方式和巧妙设计的掩码模板，从而实现了只需要一次前向过程，就能把所有需要的自注意力值都算出来，而且只需要计算4个窗口，也就是说窗口的数量没有增加，计算复杂度也没有增加，非常高效的完成了这个任务</p><p><img src="https://i0.hdslb.com/bfs/note/bd3aa5ded3e7605bb8b584d2a518e361d60e5c44.png" alt=""></p><h2 id="5-6-相对位置偏执（论文中没有）"><a href="#5-6-相对位置偏执（论文中没有）" class="headerlink" title="5.6 相对位置偏执（论文中没有）"></a>5.6 相对位置偏执（论文中没有）</h2><p>关于相对位置偏执，论文里也没有细讲，就说了参考的哪些论文，然后说使用了相对位置偏执后给够带来明显的提升。根据论文中的表4可以看出，在Imagenet数据集上如果不使用任何位置偏执，top-1为80.1，但使用了相对位置偏执（rel. pos.）后top-1为83.3，提升还是很明显的。</p><p>根据论文中的公式，相对位置偏执是在Q和K匹配之后加上的B</p><script type="math/tex; mode=display">\mathrm{Atention}(Q,K,V)=\mathrm{SoftMax}(QK^T/\sqrt{d}+B)V,</script><p>如下图，假设输入的feature map高宽都为2，那么<strong>首先可以构建出每个像素的绝对位置</strong>（左下方的矩阵），对于每个像素的绝对位置是使用行号和列号表示的。比如蓝色的像素对应的是第0行第0列所以绝对位置索引是 $(0,0)$，接下来再看看相对位置索引。首先看下蓝色的像素，在蓝色像素使用q与所有像素k进行匹配过程中，是以蓝色像素为参考点。然后用<strong>蓝色像素的绝对位置索引与其他位置索引进行相减，就得到其他位置相对蓝色像素的相对位置索引</strong>。例如黄色像素的绝对位置索引是$  (0,1) $，则它相对蓝色像素的相对位置索引为$  (0, 0) - (0, 1)=(0, -1) $。那么同理可以得到其他位置相对蓝色像素的相对位置索引矩阵。同样，也能得到相对黄色，红色以及绿色像素的相对位置索引矩阵。接下来将<strong>每个相对位置索引矩阵按行展平</strong>，并拼接在一起可以得到下面的4x4矩阵 。</p><p><img src="image/image_g2EfkEf-H4.png" alt=""></p><p>请注意，这里描述的一直是<strong>相对位置索引</strong>，并不是<strong>相对位置偏执参数</strong>。因为后面会根据相对位置索引去取对应的参数。比如说黄色像素是在蓝色像素的右边，所以相对蓝色像素的相对位置索引为$(0, -1)$。绿色像素是在红色像素的右边，所以相对红色像素的相对位置索引为$  (0, -1) $。<strong>可以发现这两者的相对位置索引都是</strong>$(0, -1)$<strong>，所以他们使用的相对位置偏执参数都是一样的</strong>。其实讲到这基本已经讲完了，但在源码中作者为了方便把二维索引给转成了一维索引。具体这么转的呢，有人肯定想到，简单啊直接把行、列索引相加不就变一维了吗？比如上面的相对位置索引中有(0, -1)和(-1,0)在二维的相对位置索引中明显是代表不同的位置，但如果简单相加都等于-1那不就出问题了吗？接下来我们看看源码中是怎么做的。首先在原始的相对位置索引上加上<code>M-1</code>(M为窗口的大小，在本示例中M=2)，加上之后索引中就不会有负数了。</p><p><img src="image/image_WkkYs1lm76.png" alt=""></p><p>接着将所有的行标都乘上2M-1。</p><p><img src="image/image_5-wXv82n3m.png" alt=""></p><p>最后将行标和列标进行相加。这样即保证了相对位置关系，而且不会出现上述 $0+(-1)=(-1)+0$的问题了，是不是很神奇。</p><p><img src="image/image_U4HrqBULvX.png" alt=""></p><p>刚刚上面也说了，之前计算的是<strong>相对位置索引</strong>，并不是<strong>相对位置偏执参数</strong>。真正使用到的可训练参数$ \hat{B}  $是保存在relative position bias table表里的，这个表的长度是等于$  (2M-1) \times (2M-1) $的。那么上述公式中的相对位置偏执参数B是根据上面的相对位置索引表根据查relative position bias table表得到的，如下图所示。</p><p><img src="image/image_SXVbR9qR4T.png" alt=""></p><h2 id="5-7-Swin-Transformer的变体"><a href="#5-7-Swin-Transformer的变体" class="headerlink" title="5.7 Swin Transformer的变体"></a>5.7 Swin Transformer的变体</h2><p>在方法的最后一节也就是3.3节，作者大概介绍了一下他们提出的 Swin Transformer的几个变体</p><p>下图（表7）是原论文中给出的关于不同Swin Transformer的配置，T(Tiny)，S(Small)，B(Base)，L(Large)，其中：</p><ul><li><code>win. sz. 7x7</code>表示使用的窗口（Windows）的大小</li><li><code>dim</code>表示feature map的channel深度（或者说token的向量长度）</li><li><code>head</code>表示多头注意力模块中head的个数</li></ul><p><img src="image/image_aQ8U20Ca17.png" alt=""></p><ul><li>Swin Tiny：C=96, layer numbers = {2,2,6,2}</li><li>Swin Small： C= 96, layer numbers ={2,2,18,2}</li><li>Swin Base：C= 128, layer numbers ={2,2,18,2}</li><li>Swin Large： C= 192, layer numbers ={2,2,18,2}</li></ul><p>Swin Tiny的计算复杂度跟 ResNet-50 差不多，Swin Small 的复杂度跟 ResNet-101 是差不多的，这样主要是想去做一个比较公平的对比</p><p>这些变体之间有哪些不一样呢？，其实主要不一样的就是<strong>两个超参数</strong></p><ul><li>一个是向量维度的大小 c</li><li>另一个是每个 stage 里到底有多少个 transform block</li></ul><p>这里其实就跟残差网络就非常像了，残差网络也是分成了四个 stage，每个 stage 有不同数量的残差块。</p><h1 id="6-实验"><a href="#6-实验" class="headerlink" title="6.实验"></a>6.<strong>实验</strong></h1><h2 id="6-1-分类实验"><a href="#6-1-分类实验" class="headerlink" title="6.1 分类实验"></a>6.1 分类实验</h2><p>首先是分类上的实验，这里一共说了两种预训练的方式</p><ul><li>第一种就是在正规的ImageNet-1K(128万张图片、1000个类)上做预训练</li><li>第二种方式是在更大的ImageNet-22K（1,400万张图片、2万多个类别）上做预训练</li></ul><p>当然不论是用ImageNet-1K去做预训练，还是用ImageNet-22K去做预训练，最后测试的结果都是在ImageNet-1K的测试集上去做的，结果如下表所示</p><p><img src="https://i0.hdslb.com/bfs/note/e03d9e9a52b6cae89f6486b233c68b19025cb8b5.png" alt=""></p><ul><li>上半部分是ImageNet-1K预训练的模型结果</li><li>下半部分是先用ImageNet-22K去预训练，然后又在ImageNet-1K上做微调，最后得到的结果</li><li>在表格的上半部分，作者先是跟之前最好的卷积神经网络做了一下对比，RegNet 是之前 facebook 用 NASA 搜出来的模型，EfficientNet 是 google 用NASA 搜出来的模型，这两个都算之前表现非常好的模型了，他们的性能最高会到 84.3</li><li>接下来作者就写了一下之前的 Vision Transformer 会达到什么效果，对于 ViT 来说，因为它没有用很好的数据增强，而且缺少偏置归纳，所以说它的结果是比较差的，只有70多</li><li>换上 DeiT 之后，因为用了更好的数据增强和模型蒸馏，所以说 DeiT Base 模型也能取得相当不错的结果，能到83.1</li><li>当然 Swin Transformer 能更高一些，Swin Base 最高能到84.5，稍微比之前最好的卷积神经网络高那么一点点，就比84.3高了0.2</li><li>虽然之前表现最好的 EfficientNet 的模型是在 600*600 的图片上做的，而 Swin Base 是在 384* 384 的图片上做的，所以说 EfficientNet 有一些优势，但是从模型的参数和计算的 FLOPs 上来说 EfficientNet 只有66M，而且只用了 37G 的 FLOPs，但是 Swin Transformer 用了 88M 的模型参数，而且用了 47G 的 FLOPs，所以总体而言是伯仲之间</li><li>表格的下半部分是用 ImageNet-22k 去做预训练，然后再在ImageNet-1k上微调最后得到的结果</li><li>这里可以看到，一旦使用了更大规模的数据集，原始标准的 ViT 的性能也就已经上来了，对于 ViT large 来说它已经能得到 85.2 的准确度了，已经相当高了</li><li>但是 Swin Large 更高，Swin Large 最后能到87.3，这个是在不使用JFT-300M，就是特别大规模数据集上得到的结果，所以还是相当高的</li></ul><h2 id="6-2-目标检测实验"><a href="#6-2-目标检测实验" class="headerlink" title="6.2 目标检测实验"></a>6.2 目标检测实验</h2><p>接下来是目标检测的结果，作者是在 COCO 数据集上训练并且进行测试的，结果如下图所示</p><p><img src="https://i0.hdslb.com/bfs/note/28a418d86e5392e975f92a514d9d99dd6d5a9d67.png" alt=""></p><ul><li>表2（a）中测试了在不同的算法框架下，Swin Transformer 到底比卷积神经网络要好多少，主要是想证明 Swin Transformer 是可以当做一个通用的骨干网络来使用的，所以用了 Mask R-CNN、ATSS、RepPointsV2 和SparseR-CNN，这些都是表现非常好的一些算法，在这些算法里，过去的骨干网络选用的都是 ResNet-50，现在替换成了 Swin Tiny</li><li>Swin Tiny 的参数量和 FLOPs 跟 ResNet-50 是比较一致的，从后面的对比里也可以看出来，所以他们之间的比较是相对比较公平的</li><li>可以看到，Swin Tiny 对 ResNet-50 是全方位的碾压，在四个算法上都超过了它，而且超过的幅度也是比较大的</li><li>接下来作者又换了一个方式做测试，现在是选定一个算法，选定了Cascade Mask R-CNN 这个算法，然后换更多的不同的骨干网络，比如 DeiT-S、ResNet-50 和 ResNet-101，也分了几组，结果如上图中表2（b）所示</li><li>可以看出，在相似的模型参数和相似的 Flops 之下，Swin Transformer 都是比之前的骨干网络要表现好的</li><li>接下来作者又做了第三种测试的方式，如上图中的表2（c）所示，就是系统层面的比较，这个层面的比较就比较狂野了，就是现在追求的不是公平比较，什么方法都可以上，可以使用更多的数据，可以使用更多的数据增强，甚至可以在测试的使用 test time augmentation（TTA）的方式</li><li>可以看到，之前最好的方法 Copy-paste 在 COCO Validation Set上的结果是55.9，在 Test Set 上的结果是56，而这里如果跟最大的 Swin Transformer—Swin Large 比，它的结果分别能达到58和58.7，这都比之前高了两到三个点</li></ul><h2 id="6-3-语义分割实验"><a href="#6-3-语义分割实验" class="headerlink" title="6.3 语义分割实验"></a>6.3 语义分割实验</h2><p>第三个实验作者选择了语义分割里的ADE20K数据集，结果如下图所示</p><p><img src="https://i0.hdslb.com/bfs/note/50f121284978f95ec6805cfe3c406109792054d4.png" alt=""></p><ul><li>上图表3里可以看到之前的方法，一直到 DeepLab V3、ResNet 其实都用的是卷积神经网络，之前的这些方法其实都在44、45左右徘徊</li><li>但是紧接着 Vision Transformer 就来了，那首先就是 SETR 这篇论文，他们用了 ViT Large，所以就取得了50.3的这个结果</li><li>Swin Transformer Large也取得了53.5的结果，就刷的更高了</li><li>其实作者这里也有标注，就是有两个“+”号的，意思是说这些模型是在ImageNet-22K 数据集上做预训练，所以结果才这么好</li></ul><h2 id="6-4-消融实验"><a href="#6-4-消融实验" class="headerlink" title="6.4 消融实验"></a>6.4 消融实验</h2><p>实验结果如下图所示，主要就是想说一下移动窗口以及相对位置编码到底对 Swin Transformer 有多有用</p><p><img src="https://i0.hdslb.com/bfs/note/f211abe5c107d0a044d2a8c95aa44ac31d0103fc.png" alt=""></p><ul><li>可以看到，如果光分类任务的话，其实不论是移动窗口，还是相对位置编码，它的提升相对于基线来说，也没有特别明显，当然在ImageNet的这个数据集上提升一个点也算是很显著了</li><li>但是他们更大的帮助，<strong>主要是出现在下游任务里</strong>，就是 COCO 和 ADE20K 这两个数据集上，也就是目标检测和语义分割这两个任务上</li><li>可以看到，用了移动窗口和相对位置编码以后，都会比之前大概高了3个点左右，提升是非常显著的，这也是合理的，因为如果现在去做这种密集型预测任务的话，就需要特征对位置信息更敏感，而且更需要周围的上下文关系，所以说通过移动窗口提供的窗口和窗口之间的互相通信，以及在每个 Transformer block都做更准确的相对位置编码，肯定是会对这类型的下游任务大有帮助的</li></ul><h1 id="7-点评"><a href="#7-点评" class="headerlink" title="7.点评"></a>7.<strong>点评</strong></h1><p>除了作者团队自己在过去半年中刷了那么多的任务，比如说最开始讲的自监督的Swin Transformer，还有 Video Swin Transformer 以及Swin MLP，同时 Swin Transformer 还被别的研究者用到了不同的领域。</p><p><img src="image/image_sQAstzNYbk.png" alt=""></p><p>所以说，Swin Transformer 真的是太火，真的是在视觉领域大杀四方，感觉以后每个任务都逃不了跟 Swin 比一比，而且因为 Swin 这么火，所以说其实很多开源包里都有 Swin 的实现</p><ul><li>百度的 PaddlePaddle</li><li>视觉里现在比较火的 pytorch-image-models，就是 Timm 这个代码库里面也是有 Swin 的实现的</li><li>同时 Hugging Face 估计也是有的</li></ul><p>虽然前面已经说了很多 Swin Transformer 的影响力啊已经这么巨大了，但其实他的影响力远远不止于此，<strong>论文里这种对卷积神经网络，对 Transformer，还有对 MLP 这几种架构深入的理解和分析是可以给更多的研究者带来思考的，从而不仅可以在视觉领域里激发出更好的工作，而且在多模态领域里，相信它也能激发出更多更好的工作。</strong></p>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> PaperReading </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 GPT、GPT-2、GPT-3</title>
      <link href="/paper_reading/2.5.GPT_GPT-2_GPT-3/"/>
      <url>/paper_reading/2.5.GPT_GPT-2_GPT-3/</url>
      
        <content type="html"><![CDATA[<h1 id="5-GPT，GPT-2，GPT-3"><a href="#5-GPT，GPT-2，GPT-3" class="headerlink" title="5.GPT，GPT-2，GPT-3"></a>5.GPT，GPT-2，GPT-3</h1><p>大力出奇迹模型</p><ul><li>李沐讲解：<a href="https://www.bilibili.com/video/BV1AF411b7xQ/?spm_id_from=333.999.0.0\&amp;vd_source=24ca68bef609ca7dbede1b048fc4a2f4" title="GPT，GPT-2，GPT-3 论文精读">GPT，GPT-2，GPT-3 论文精读</a></li></ul><p>最近以GPT系列为代表的大语言模型LLM掀起了一阵热潮，许多人惊叹LLM的震撼能力，因此紧跟时代潮流，学习GPT系列论文，加深自己对LLM的理解。总的来说，GPT整体的模型架构都是以Transformer的解码器为模块进行堆叠而成。主要的创新点集中在模型训练策略，还有就是讲究一个大力出奇迹。</p><h1 id="0-论文时间轴"><a href="#0-论文时间轴" class="headerlink" title="0.论文时间轴"></a>0.论文时间轴</h1><p><img src="image/image_HP6M_yvq-4.png" alt=""></p><p><strong>GPT</strong>：Transformer解码器，在没有标号的大量的文本数据上，训练一个语言模型，来获得预训练模型，后续在子任务上做微调，得到每一个任务所用的分类器。</p><p><strong>BERT</strong>：Transformer编码器，收集了一个更大的数据集，用来做预训练，效果比GPT好。BERT有两个模型，BERT-base模型与GPT模型参数相差不大，BERT-Large比BERT-base模型大。</p><p><strong>GPT-2</strong>：原作者吸取教训，收集更大的数据集，训练了一个更大的模型，GPT-2的模型比BERT-large要大。继续使用Transformer的解码器，发现非常适合做Zero Shot，步子跨的太大，效果上不是那么好。</p><p><strong>GPT-3</strong>：GPT-3对GPT-2的改进就是数据和模型都大了100倍。大力出奇迹，效果非常惊艳。几乎没有什么团队去复现结果。</p><p>Transformer和BERT来自Google，想要解决的问题都比较小；Transformer就想解决机器翻译这样的问题，从一个序列到另外一个序列；BERT想把计算机视觉中成熟的那一套预训练模型应用到NLP中。在同样大小的模型上，BERT的效果是要好于GPT的。所以，后续的工作，非常愿意使用BERT，而不是GPT。</p><h1 id="1-GPT"><a href="#1-GPT" class="headerlink" title="1.GPT"></a>1.GPT</h1><p>论文标题：“Improving Language Understanding by Generative Pre-Training”，2018.6.</p><p>论文链接：<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" title="language_understanding_paper.pdf">language_understanding_paper.pdf</a></p><h2 id="1-1-摘要"><a href="#1-1-摘要" class="headerlink" title="1.1 摘要"></a>1.1 摘要</h2><p>在自然语言处理领域，有很多各式各样的的任务，如问答，文本分类等。然而，现有的<strong>无标签文本非常多，而有标签的文本很少</strong>，这使得在这些有标签文本训练一个好的分辨模型很难，因为数据集太少。因此GPT第一个版本主要就是为了解决这个问题而提出的一套针对语言模型的预训练方法，<strong>使得大量的无标签数据能够被使用，并且对预训练好的模型加以微调使其适用于许多的下游任务</strong>。</p><p>在微调时，构造与子任务相关的输入，从而之只需要很少改变模型架构。</p><p>结果：12个任务中，有9个超过之前最好的成绩</p><h2 id="1-2-引言"><a href="#1-2-引言" class="headerlink" title="1.2 引言"></a>1.2 引言</h2><p>如何利用无标签的数据，当时最成功的还是词嵌入模型（Word Embeddings）。</p><p>使用无标记数据时，需要的两个困难：</p><ol><li><strong>损失函数设计困难</strong>：不清楚什么样的优化目标对文本有效</li><li><strong>特征迁移困难</strong>：怎么样把学习到的问题表示，传递到下游的子任务上；没有一种表示，能够迁移到所有子任务上</li></ol><p>提出一个半监督的方法（预训练+微调）：在没有标号的数据上，训练一个比较大的语言模型，然后再再子任务上微调。</p><ul><li>半监督学习：有一些标记的数据，但还有大量没有标记的相似数据，怎么把这些没有标记的数据用过来。</li><li>后续的BERT、GPT的方法，预训练 + 微调，不再叫做半监督学习，而叫做自监督学习。</li></ul><p>GPT架构使用Transformer块，相比于RNN，在做迁移学习时，Transformer块学到的特征更加稳健。作者猜测Transformer里面有更结构化的记忆，使得能够处理更长的问题信息，从而能偶抽取出句子层面、段落层面的语义信息。在迁移时，使用任务相关的输入表示。</p><h2 id="1-3-Framework"><a href="#1-3-Framework" class="headerlink" title="1.3 Framework"></a>1.3 Framework</h2><p>两个阶段：<strong>无监督预训练+有监督微调</strong></p><h3 id="（1）无监督预训练：预训练"><a href="#（1）无监督预训练：预训练" class="headerlink" title="（1）无监督预训练：预训练"></a>（1）无监督预训练：预训练</h3><p>给定一个未监督的语料信息 $\mathcal{U}=\left\{u_{1}, \ldots, u_{n}\right\}$，使用标准的语言模型，使下面这个似然函数最大化：</p><script type="math/tex; mode=display">L_{1}(\mathcal{U})=\sum \log P\left(u_{i} \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)</script><p>其中，k为上下文窗口大小，$\Theta$ 为模型参数。</p><p>简单来说就是<strong>根据上文的k个单词，预测下一个最大概率的单词</strong>$u_i$<strong>。</strong></p><p>使用多层Transformer decoder块作为语言模型（标准的语言模型，根据已有的词进行下一个词的预测，不能使它看到所有的词，所以只有解码器）。</p><p>模型输入输出如下所示：</p><script type="math/tex; mode=display">\begin{aligned} h_{0} & =U W_{e}+W_{p} \\ h_{l} & =\operatorname{transformer\_ block}\left(h_{l-1}\right) \forall i \in[1, n] \\ P(u) & =\operatorname{softmax}\left(h_{n} W_{e}^{T}\right)\end{aligned}</script><p>其中，$U=\left(u_{-k}, \ldots, u_{-1}\right)$为上下文tokens向量，$n$为transformer的层数，$W_e$为词嵌入矩阵维度，$W_p$为位置编码矩阵。</p><p>GPT和BERT区别：</p><ul><li>BERT：用的不是标准的语言模型，使用带掩码的语言模型，就是完形填空，预测时，既能看到之前的词，也能看到之后的词，对应使用Transformer的编码器。</li><li>区别：使用解码器和编码器不是主要的区别；主要时目标函数的选取不同，GPT选择更难的，给前面一句话，预测后面这个词，预测未来比完形填空更难。</li></ul><h3 id="（2）有监督微调"><a href="#（2）有监督微调" class="headerlink" title="（2）有监督微调"></a>（2）有监督微调</h3><p>在得到预训练模型后，就使用有标签的数据进行微调。具体来说每一次我给你一个长为m的一个词的序列，然后告诉你这个序列它对应的标号是y，也就是每次给定一个序列预测他这个y。微调优化目标是最小化分类目标函数。 &#x20;</p><p>预测y，将 $x^{1}, \ldots, x^{m}$序列放入之前训练好的GPT模型中，获得Transformer块的最后一层输出 $h_l^m$，然后乘以输出层 $W_y$，做softmax就得到y的概率</p><script type="math/tex; mode=display">P\left(y \mid x^{1}, \ldots, x^{m}\right)=\operatorname{softmax}\left(h_{l}^{m} W_{y}\right).</script><p>做目标函数最大化：</p><script type="math/tex; mode=display">L_{2}(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^{1}, \ldots, x^{m}\right)</script><p>然后总的损失除了考虑微调损失，还考虑了预训练部分的损失，并用λ加权。 &#x20;</p><script type="math/tex; mode=display">L_{3}(\mathcal{C})=L_{2}(\mathcal{C})+\lambda * L_{1}(\mathcal{C})</script><p>NLP领域四大常见的应用：</p><ol><li><strong>分类</strong>：给定一段话或文本，判断所属标号；如用户的评价是正面还是负面的。一段文本，在前面加入开始词元<code>[Start]</code>，在后面加入抽取词元<code>[Extract]</code>，做成一个序列，放入Transformer解码器中，模型对最后一个词抽取的特征<code>[Extract]</code>放入线性层进行分类。</li><li><strong>蕴含</strong>：给一段话，再给一个假设，判断前面一段话是否蕴含提出的假设。如文本：a给b送一朵玫瑰；假设：a喜欢b。判断前面文本是否支持假设。即两端文本做三分类问题，支持，不支持，既不支持也不反对。将两端问题串成一个序列，使用<code>开始符</code>，<code>分隔符</code>，<code>抽取符</code>。</li><li><strong>相似</strong>：判断两段文字是不是相似。如一个搜索词与文档是不是相似，或两个文档相似去重。相似是对称的，a和b相似，则b和a也相似。所以，做了两个序列，两个序列的文字顺序不同，再使用<code>开始符</code>，<code>分隔符</code>，<code>抽取符</code>。得到两段输出后，经过Transformer，在做加号，最后经过线性层输出，得到是相似还是不是相似的二分类问题。</li><li><strong>多选题</strong>：问一个问题，给几个答案，选择最可能的几个答案。如果有n个答案，构造n个序列，前面是问题，每一个答案作为第二个序列；每个序列进入Transformer块，最后经过线性层，输出答案的置信度；最后做一个softmax。</li></ol><p>复用预训练的Transformer的结构，加一个线性层，不同的任务需要不同的输入。</p><p><img src="image/image_QhDU1zKo9R.png" alt=""></p><h2 id="1-4-实验"><a href="#1-4-实验" class="headerlink" title="1.4 实验"></a>1.4 实验</h2><p>实验主要关注两点。</p><ul><li>第一点是GPT在一个叫做BooksCorpus 的一个数据集上训练出来的，这个地方有7,000篇没有被发表的书。</li><li>第二个是GPT整体模型，用了12层的一个Transformer 的解码器，每一层的维度是768。</li></ul><h1 id="2-GPT-2"><a href="#2-GPT-2" class="headerlink" title="2.GPT-2"></a>2.GPT-2</h1><p>论文标题：Language Models are Unsupervised Multitask Learners，2019</p><p>论文链接：<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></p><h2 id="2-1-摘要"><a href="#2-1-摘要" class="headerlink" title="2.1 摘要"></a>2.1 摘要</h2><p>自从GPT提出后，Google紧随其后提出了BERT预训练模型，采用更大的模型和更大的数据，在各方面效果都要优于GPT。作为竞争对手，GPT当然是要反击的，那怎么做呢？如果单纯加大模型于增加数据量，或许能击败BERT，但是却少了些新意，因此GPT2从另外一个角度，除了加大模型和数据量，还引入了zero-shot设定，就是在做下游任务是，不需要下游任务的任何标签信息，也不需要重新训练一个模型，即在更难的一个设定上体现他的一个新意度。</p><ul><li>构造 WebText 大数据集：百万级别文本</li><li>GPT-2：15亿参数，BERT-large参数3.4亿</li><li>zero-shot设置</li></ul><h2 id="2-2-引言"><a href="#2-2-引言" class="headerlink" title="2.2 引言"></a>2.2 引言</h2><p>现在主流的机器学习系统训练方式为：一个任务收集一个数据集，然后再上面做模型训练和预测，因现在模型泛化性能不是很好。</p><p>多任务学习：训练一个模型时，同时看多个数据集，通过多个损失函数，来达到一个模型能够在多个任务上都能用。但是NLP中使用的不多，主要使用的还是<strong>预训练+微调方式</strong>。但还是有两个问题：第一个是对每一个下游任务，需要重新训练模型；二是需要收集有标号的数据才行。这样导致，拓展到新的任务上，还是有成本的。</p><p>GPT-2还是在做语言模型，在到下游任务时，会使用<strong>zero-shot的设定</strong>（不需要下游任务的标注信息，不引入模型没有见过的特殊符号），这样训练一个模型，任何地方都可以用。</p><h2 id="2-3-模型"><a href="#2-3-模型" class="headerlink" title="2.3 模型"></a>2.3 模型</h2><h3 id="（1）GPT-1和GPT-2区别"><a href="#（1）GPT-1和GPT-2区别" class="headerlink" title="（1）GPT-1和GPT-2区别"></a>（1）GPT-1和GPT-2区别</h3><p>GPT-1时，根据不同的下游任务，会调整输入信息，会加入<code>开始符</code>、<code>分隔符</code>、<code>结束符</code>等信息，然后在使用有标记的数据进行微调，让模型去认识这些符号。</p><p>而GPT-2做下游任务时，不再加入<code>开始符</code>、<code>分隔符</code>、<code>结束符</code>等模型未见过的信息，而是采用zero-shot的设定。</p><p>GPT-2下游任务模型的输入，和预训练时，模型看到的输入是一样的。</p><p>例如：</p><ol><li>英语翻译为法语：<code>translate to french, english text, french text</code></li><li>做阅读理解：<code>answer the question, document, question, answer</code></li></ol><p>这个提示符后面叫做<strong>Prompt</strong>。</p><h3 id="（2）训练数据"><a href="#（2）训练数据" class="headerlink" title="（2）训练数据"></a>（2）训练数据</h3><p>BERT训练数据：Wikipedia</p><p>Common Crawl项目：一个公开的爬虫项目，不断抓取网页放在AWS上，是能下载到最大的文本数据集，TB级别的数据量。但作者认为这个不好用，因为信噪比比较低，抓取的网页，较多是没有信息的网页。</p><p>GPT-2使用Reddit里面的数据，选取最近3词评论以上的数据，得到4500万分链接，将数据抽取出来，得到一个数据集，约800万文本，40GB的文字。</p><h3 id="（3）GPT-2模型大小"><a href="#（3）GPT-2模型大小" class="headerlink" title="（3）GPT-2模型大小"></a>（3）GPT-2模型大小</h3><p>GPT2也是基于Transformer解码器的架构，作者设计了4种大小的模型，参数结构如下： &#x20;</p><p><img src="image/image_SjL3HobPNQ.png" alt=""></p><p>可以看到，最大模型的参数量已经去到了15个亿。还有一个细节就是GPT2调整了transformer解码器结构：将layer normalization放到每个sub-block之前，并在最后一个Self-attention后再增加一个layer normalization。</p><h2 id="2-4-训练范式"><a href="#2-4-训练范式" class="headerlink" title="2.4 训练范式"></a>2.4 训练范式</h2><p>采用预训练+zero-shot的训练范式。为实现zero-shot，GPT2在做下游任务时，输入就不能像GPT那样在构造输入时加入开始、中间和结束的特殊字符，因为这些特殊字符是模型在预训练时没有见过的。正确的输入应该和预训练模型看到的文本一样，更像一个自然语言。比如在做机器翻译时，直接可以输入“请将下面一段英文翻译成法语，英文文本”，由于在训练时可能已经存在很多这样的翻译文本样例，因此模型就可以实现输出一段法语。</p><h2 id="2-5-实验"><a href="#2-5-实验" class="headerlink" title="2.5 实验"></a>2.5 实验</h2><p>数据集：Webtext，包含4500w个链接的文本信息，总计800w的文本和40GB的文字。</p><p>训练：GPT-2去掉了fine-tuning训练，只保留无监督的预训练阶段，不再针对不同任务分别进行微调建模，而是不定义这个模型应该做什么任务，模型会自动识别出来需要做什么任务。</p><h1 id="3-GPT-3"><a href="#3-GPT-3" class="headerlink" title="3.GPT-3"></a>3.GPT-3</h1><p>论文题目：Language Models are Few-Shot Learners，2020</p><p>论文链接：<a href="https://arxiv.org/pdf/2005.14165.pdf" title="https://arxiv.org/pdf/2005.14165.pdf">https://arxiv.org/pdf/2005.14165.pdf</a></p><h2 id="3-1-摘要"><a href="#3-1-摘要" class="headerlink" title="3.1 摘要"></a>3.1 摘要</h2><p>GPT-3：自回归语言模型，有1750亿个可学习参数，比之前非稀疏模型在可学习参数上要大10倍。</p><p>GPT-3作用到子任务上，<strong>不做任何的梯度更新或是微调</strong>；</p><p>GPT-3可以生成一些人类难以区分的文章。</p><h2 id="3-2引言"><a href="#3-2引言" class="headerlink" title="3.2引言"></a>3.2引言</h2><p>GPT2实验采用了zero-shot设定，在新意度上很高，但是有效性却比较低。而GPT3则是尝试解决GPT2的有效性，因此回到了GPT提到的<code>Few-shot</code>设置，即模型在做下游任务时，可以看到一些任务的样例，而不是像GPT2那样啥样例都不给。此外，GPT3还是只采用无监督预训练方式，那么传统的二阶段训练方式（预训练+微调）有什么问题？二阶段训练方式在预训练好一个模型后，<strong>还需要一个与任务相关的数据集，以及跟任务相关的微调方式</strong>。去除这种方式是可取的，有以下几个原因：</p><ol><li><strong>微调需要一个较大的有标签数据</strong>，对于一些如问答型任务，做标签是很困难的；</li><li><strong>当一个样本没有出现在数据分布里是，微调模型的泛化能力不一定好</strong>，即尽管微调效果好，也不一定说明预训练的模型泛化能力好，因为极有可能微调是过拟合了预训练的训练数据，或者说预训练训练数据和下游任务数据有一定重合性，所以效果会好一点；</li><li>以人类角度来阐述为什么不用微调，就是说人<strong>类做任务不需要一个很大的数据集进行微调</strong>，比如一个人有一定的语言功底，让你去做一个别的事情，可能就是告诉你怎么做并提供几个样例就可以了，GPT3就是采用一样的思想。</li></ol><p>总的来说，GPT3就是一个参数特别大，效果也很好的一个模型。</p><p><strong>多任务预训练（上下文训练）</strong></p><p><img src="image/image_wLb_1C3_st.png" alt=""></p><p>模型评估方式（3种）：</p><ol><li><strong>few-shot learning</strong>：语境学习允许尽可能多的示例活动将适合模型的上下文窗口(通常10 - 100)</li><li><strong>one-shot learning</strong>：只提供一个示例，如英语翻译德语时，只提供第一个词怎么翻译，后续让模型自己翻译</li><li><strong>zero-shot</strong>：一个示例样本都没有</li></ol><p>在三个设定下，模型的学习区别，x轴为语言模型的大小，其中虚线是每个子任务，做平均变成了实线。</p><p><img src="image/image_dzIXrAcPvX.png" alt=""></p><h2 id="3-3-模型"><a href="#3-3-模型" class="headerlink" title="3.3 模型"></a>3.3 模型</h2><h3 id="（1）训练方式"><a href="#（1）训练方式" class="headerlink" title="（1）训练方式"></a>（1）训练方式</h3><p><img src="image/image_naR0Dd-aHQ.png" alt=""></p><ul><li><strong>Fine-tuning</strong>：训练完成预训练模型后，在每一个子任务上提供训练样本；微调对数据量的要求少于从0开始训练；</li><li><strong>Zero-shot</strong>：任务描述和prompt之间没有任何样本</li><li><strong>One-shot</strong>：任务描述和prompt之前，插入一个样本进来。样本只做预测，不做训练，模型在前向推理时，使用注意力机制，处理比较长的信息，从中间抽取出有用的信息。</li><li><strong>Few-shot</strong>：任务描述和prompt之前，插入多个样本进来。多个不一定有用，可能模型不能处理很长的数据。</li></ul><p>几种训练方式简单概括如下： &#x20;</p><ol><li>fine-tuning：预训练 + 微调计算loss更新梯度，然后预测。会更新模型参数 &#x20;</li><li>zero-shot：预训练 + task description + prompt，直接预测。不更新模型参数 &#x20;</li><li>one-shot：预训练 + task description + example + prompt，预测。不更新模型参数 &#x20;</li><li>few-shot：预训练 + task description + examples + prompt，预测。不更新模型参数</li></ol><h3 id="（2）模型架构"><a href="#（2）模型架构" class="headerlink" title="（2）模型架构"></a>（2）模型架构</h3><p>GPT-3的模型和GPT-2的模型是一样的，稍微有点改动，把transformer换成了Sparse Transformer中的结构，并设计8个不同大小的模型。</p><p><img src="image/image_GzqitUanm9.png" alt=""></p><ul><li>整体：GPT3模型偏扁</li><li>Batch Size：使用相对比较大的批量大小，计算性能更好，每台机器的并行度更高，通讯量变低，降低批量里的噪音分布式比较好，小的模型批量大小更容易过拟合—些。</li><li>过拟合：模型越来越大的时候过拟合没有那么的严重，搜索范围更广，可能存在一个比较简单的模型架构，SDG可以帮助找到那个模型，使泛化精度更好一些。</li><li>学习率模型批量大小增大学习率下降。</li></ul><h4 id="Sparse-Transformer-x20"><a href="#Sparse-Transformer-x20" class="headerlink" title="Sparse Transformer&#x20;"></a>Sparse Transformer&#x20;</h4><ul><li>Self-Attention：每个 token 之间两两计算 attention，复杂度$  O(n²) $</li><li>Sparse Attention：每个 token 只与其他 token 的一个子集计算 attention，复杂度 $O(n∗logn)$</li><li>sparseattention 除了相对距离不超过k以及相对距离为$k，2k，3k，…$ 的 token，其他所有token的注意力都设为0：</li></ul><p><img src="image/image_0_CF4-hsZ7.png" alt=""></p><h3 id="（3）训练数据"><a href="#（3）训练数据" class="headerlink" title="（3）训练数据"></a>（3）训练数据</h3><p>训练数据基于Common Crawl，做了三个步骤，是数据变得更干净：</p><ol><li><strong>过滤数据</strong>。GPT-2中使用的数据集为Raddit，质量较高。训练一个二分类网络，Raddit中数据为正例，Common Crawl部分数据为负例。使用这个二分类网络，对所有的Common Crawl中的数据进行分类，分类为正例留下，负例剔除。</li><li><strong>数据去重</strong>。LSH算法去重：很快的判断一个集合（一篇文章的单词）与另一个很大集合之间的相似度。</li><li><strong>增加高质量数据</strong>：增加一些已知的高质量数据集。</li></ol><p>最后，得到一个很大的一个数据集。</p><p><img src="image/image_opdt36hHBb.png" alt=""></p><p>虽然Common Crawl数据已经经过处理，但作者认为质量还是比较差，在采样的时候，使用了较低的权重。</p><h3 id="（4）模型训练"><a href="#（4）模型训练" class="headerlink" title="（4）模型训练"></a>（4）模型训练</h3><p>V100GPU训练</p><h3 id="（5）模型评估"><a href="#（5）模型评估" class="headerlink" title="（5）模型评估"></a>（5）模型评估</h3><p>使用上下文学习评估，在每个下游任务里面，在训练集中，采样k个样本作为条件，k可以等于0或其他数值；</p><p>prompt使用：”<code>Answer:</code> “ or “<code>A:</code>“，比如二分类问题中，使用True和False，而不是0或1，因为0和1在训练过程中，出现的概率比较低。</p><p>如果回答的答案是自由的，采用Beam Search，和机器翻译类似，生成一个序列，使用Beam Search找到一个比较好的答案</p><h2 id="3-4-实验"><a href="#3-4-实验" class="headerlink" title="3.4 实验"></a>3.4 实验</h2><h3 id="（1）模型大小与计算量"><a href="#（1）模型大小与计算量" class="headerlink" title="（1）模型大小与计算量"></a>（1）模型大小与计算量</h3><p>不同大小模型训练时与计算量的关系，x轴表示计算量，y轴表示验证损失，每根线表示一个参数的模型。黄色表示GPT-3参数最大的模型。</p><p>各个模型最好的点拉成一条线（黑色虚线），是一个power law的分布，即在最好的情况下，找到合适模型，随着计算量的指数级别的增加，损失是线性往下降的。</p><p><img src="image/image_1D8RJI4Se_.png" alt=""></p><h3 id="（2）不同训练方式精度"><a href="#（2）不同训练方式精度" class="headerlink" title="（2）不同训练方式精度"></a>（2）不同训练方式精度</h3><p>最好的Few-shot，还是与人类有差距。</p><p><img src="image/image_y9ZQXEXfDV.png" alt=""></p><h3 id="（3）开放区域QA任务"><a href="#（3）开放区域QA任务" class="headerlink" title="（3）开放区域QA任务"></a>（3）开放区域QA任务</h3><p><img src="image/image_-BvN4Mt5VZ.png" alt=""></p><h3 id="（4）机器翻译任务"><a href="#（4）机器翻译任务" class="headerlink" title="（4）机器翻译任务"></a>（4）机器翻译任务</h3><p>实线表示：其他语言翻译到英语</p><p>虚线表示：英语翻译到其他语言</p><p><img src="image/image_v7Tts4oWp1.png" alt=""></p><h2 id="3-5-局限性"><a href="#3-5-局限性" class="headerlink" title="3.5 局限性"></a>3.5 局限性</h2><ol><li>生成长文本依旧困难，比如写小说，可能还是会重复； &#x20;</li><li>语言模型只能看到前面的信息； &#x20;</li><li>语言模型只是根据前面的词均匀预测下一个词，而不知道前面哪个词权重大； &#x20;</li><li>只有文本信息，缺乏多模态； &#x20;</li><li>样本有效性不够； &#x20;</li><li>模型是从头开始学习到了知识，还是只是记住了一些相似任务，这一点不明确； &#x20;</li><li>可解释性弱，模型是怎么决策的，其中哪些权重起到决定作用都不好解释 &#x20;</li><li>负面影响：可能会生成假新闻；可能有一定的性别、地区及种族歧视。</li></ol><p><strong>偏见</strong>：</p><p><img src="image/image_2ZiIV97sKr.png" alt=""></p><p>种族偏见：</p><p><img src="image/image_JaeRTvr29Y.png" alt=""></p><h2 id="3-6-总结"><a href="#3-6-总结" class="headerlink" title="3.6 总结"></a>3.6 总结</h2><p>1750亿参数的语言模型，在NLP的任务中，做了zero-shot, one-shot, 和few-shot 的学习，在很多任务下，它能够媲美使用更多有标记数据微调的算法；能够生成一些高质量的文本，展示了不用微调的可能性。</p><h1 id="4-评论"><a href="#4-评论" class="headerlink" title="4.评论"></a>4.评论</h1><p>GPT首先把Transformer的部分拿过来做预训练，但是在二选一时，选择了一条不太好走的路，但作者没有放弃，因为有钱有人，所以一条路走到黑。</p><p>GPT-2把模型做的更大，尝试一个更难的问题，不再下游任务上做微调；</p><p>GPT-3，如果还打不过，加钱加人，终于扳回一局。</p>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> PaperReading </tag>
            
            <tag> GPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer中的位置编码</title>
      <link href="/llms/transformer/7.Transformer%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"/>
      <url>/llms/transformer/7.Transformer%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer中的位置编码"><a href="#Transformer中的位置编码" class="headerlink" title="Transformer中的位置编码"></a>Transformer中的位置编码</h1><p>原文链接：<a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#the-intuition" title="Transformer Architecture: The Positional Encoding">Transformer Architecture: The Positional Encoding</a></p><h2 id="1-位置编码"><a href="#1-位置编码" class="headerlink" title="1.位置编码"></a>1.位置编码</h2><p>对任何语言来说，句子中词汇的顺序和位置都是非常重要的。它们定义了语法，从而定义了句子的实际语义。RNN结构本身就涵盖了单词的顺序，RNN按顺序逐字分析句子，这就直接在处理的时候整合了文本的顺序信息。</p><p>但<strong>Transformer架构抛弃了循环机制</strong>，仅采用多头自注意机制。避免了RNN较大的时间成本。并且从理论上讲，它可以捕捉句子中较长的依赖关系。</p><p>由于句子中的单词同时流经Transformer的编码器、解码器堆栈，模型本身对每个单词没有任何位置信息的。因此，仍然需要一种方法将单词的顺序整合到模型中。</p><p>想给模型一些位置信息，一个方案是在每个单词中添加一条<strong>关于其在句子中位置</strong>的信息。我们称之为“信息片段”，即<strong>位置编码</strong>。</p><p><strong>第一个可能想到的方法是为每个时间步添加一个</strong>$[0,1]$<strong>范围内的数字</strong>，其中0表示第一个单词，1表示最后一个单词。</p><p>但这样会存在一个问题：无法计算出特定范围内有多少个单词。换句话说，时间步长在不同句子中的含义不一致。如下所示：</p><p><img src="image/image_7pnqUgEX5v.png" alt=""></p><p><strong>另一个想法是为每个时间步按一定步长线性分配一个数字</strong>**。** 也就是说，第一个单词是“1”，第二个单词是“2”，依此类推。这种方法的问题在于，随着句子变长，这些值可能会变得特别大，并且我们的模型可能会遇到比训练时更长的句子，此外，我们的模型可能会忽略某些长度的样本。这会损害模型的泛化。</p><p><strong>理想情况下，应满足以下标准：</strong></p><ul><li>每个时间部都有唯一的编码。</li><li>在不同长度的句子中，两个时间步之间的距离应该一致。</li><li>模型不受句子长短的影响，并且编码范围是有界的。（不会随着句子加长数字就无限增大）</li><li>必须是确定性的。</li></ul><h2 id="2-提出的方法"><a href="#2-提出的方法" class="headerlink" title="2.提出的方法"></a>2.提出的方法</h2><p>作者提出的编码是一种简单但是很精妙的方法，满足上述所有标准。</p><p>首先，它不是单独某个数字，它是一个$d$维向量，其中包含句子中特定位置的信息。其次，这种编码并没有集成到模型本身中，该向量用于为每个单词提供有关其在句子中位置的信息。</p><p>也就是说，其修改了模型的输入，添加了单词的顺序信息。</p><p>令：$t$是句子中某词汇的位置；$\overrightarrow{\operatorname{pt}}\in\mathbb{R^d}$是其encoding；d是encoding的维度（其中$d \equiv_2 0$）</p><p>f: $\mathbb{N}\to\mathbb{R}^{\mathrm{d}}$是将位置转化成位置向量$\overrightarrow{\operatorname{pt}}$的函数，其定义如下：</p><script type="math/tex; mode=display">\overrightarrow{p_t}^{(i)}=f(t)^{(i)}:=\begin{cases}\sin(\omega_k.t),&\text{if}i=2k\\ \cos(\omega_k.t),&\text{if}i=2k+1\end{cases}</script><p>其中：</p><script type="math/tex; mode=display">\omega_k=\frac{1}{10000^{2k/d}}</script><blockquote><p>$d \equiv_2 0$：意思就是定义一个 $d$，这个数除2之后余数为0。</p></blockquote><p>从函数定义可以推导出，频率沿向量维数递减。因此它在波长上形成了一个从 $2\pi$ 到 $10000 \cdot 2\pi$的几何级数。也可以把$ \overrightarrow{\operatorname{pt}}  $想象成一个sin和cos交替的向量（d可以被2整除）</p><script type="math/tex; mode=display">\overrightarrow{p_{t}}=\left[\begin{array}{c}\sin \left(\omega_{1} \cdot t\right) \\ \cos \left(\omega_{1} \cdot t\right) \\ \sin \left(\omega_{2} \cdot t\right) \\ \cos \left(\omega_{2} \cdot t\right) \\ \vdots \\ \sin \left(\omega_{d / 2} \cdot t\right) \\ \cos \left(\omega_{d / 2} \cdot t\right)\end{array}\right]_{d \times 1}</script><h1 id="3-图例"><a href="#3-图例" class="headerlink" title="3.图例"></a>3.图例</h1><p>那正余弦组合怎么能代表一个位置/顺序？其实很简单，假设你想用二进制格式表示一个数字：</p><p><img src="image/image_JDEUF8uEKZ.png" alt=""></p><p>你可以看到不同位置上的数字交替变化。最后一位数字每次都会0、1交替；倒数第二位置上两个交替一次，以此类推。（第$i$位置上$2^i$个数据交替一次）</p><p>但是在浮点数的世界中使用二进制值是对空间的浪费，所以我们可以用正弦函数代替。事实上，正弦函数也能表示出二进制那样的交替。此外随着正弦函数频率的降低，也可以达到上图红色位到橙色位交替频率的变化。</p><p>下图使用正弦函数编码，句子长度为50（纵坐标），编码向量维数128（横坐标）。可以看到交替频率从左到右逐渐减慢。</p><p><img src="image/image_F9jVnQ-nP2.png" alt=""></p><h2 id="4-其他细节"><a href="#4-其他细节" class="headerlink" title="4.其他细节"></a>4.其他细节</h2><p>在论文原文中是直接将词嵌入向量和位置编码进行相加，即对于句子$[w_1, …, w_n]$中的每个词$w_t$，最终的输入如下：</p><script type="math/tex; mode=display">\psi'\left(\mathbb{w_t}\right)=\psi\left(\mathbb{w_t}\right)+\overrightarrow{\mathbb{p_t}}</script><p>其中，$\overrightarrow{\mathbb{p_t}}$ 位置编码；$\psi\left(\mathbb{w_t}\right)$ 词嵌入</p><h2 id="5-相对位置"><a href="#5-相对位置" class="headerlink" title="5.相对位置"></a>5.相对位置</h2><p>正弦位置编码的另一个特点是，可以让模型获取相对位置。以下是原文中的一段话：</p><blockquote><p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.</p></blockquote><p>但为什么这一说法成立？可以参考如下文章：<a href="https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/" title="Linear Relationships in the Transformer’s Positional Encoding ">Linear Relationships in the Transformer’s Positional Encoding </a></p><p>对于对应频率$ω_k$的每个正余弦对，有一个线性变换 $\mathrm{M}\in\mathbb{R}^{2\times2}$（独立于t），使下列等式成立：</p><script type="math/tex; mode=display">M.\begin{bmatrix}\sin(\omega_k.t)\\ \cos(\omega_k.t)\end{bmatrix}=\begin{bmatrix}\sin(\omega_k.(t+\phi))\\ \cos(\omega_k.(t+\phi))\end{bmatrix}</script><p>类似的，<strong>我们可与为其他正余弦对找到 </strong>$M$<strong>，最终我们可以将</strong>$\overrightarrow{\mathrm{P_{t+\phi}}}$<strong>表示为任何固定偏移量</strong>$\phi$<strong>的</strong>$\overrightarrow{\mathrm{P_t}}$<strong>的线性函数</strong>。这一特性使模型很容易学到相对位置信息。</p><p>正弦位置编码的另一个特性是相邻时间步之间的距离是对称的，并随时间衰减。</p><p>下图是所有时间步位置编码的点乘积可视化：</p><p><img src="image/image_Ii2v5HIgHU.png" alt=""></p><h2 id="6-FAQ"><a href="#6-FAQ" class="headerlink" title="6.FAQ"></a>6.FAQ</h2><h3 id="6-1-为什么位置编码和词嵌入相加而不是拼接起来？"><a href="#6-1-为什么位置编码和词嵌入相加而不是拼接起来？" class="headerlink" title="6.1 为什么位置编码和词嵌入相加而不是拼接起来？"></a>6.1 为什么位置编码和词嵌入相加而不是拼接起来？</h3><p>找不到这个问题相关的理论依据。求和（与拼接相比）保存了模型的参数，现在我们将初始问题改为“在单词中添加位置嵌入有缺点吗?” 。这我会回答，不一定！</p><p>&#x20;首先，如果我们回想一下上边的第一张可视化图，我们会发现位置编码向量的前几个维度用于存储关于位置的信息（注意，虽然我们的示例很小只有128维，但论文中的输入维度是512）。由于Transformer中的嵌入是从头开始训练的，所以设置参数的时候，可能不会把单词的语义存储在前几个维度中，这样就避开了位置编码。</p><p>虽然没有进行直接concat，但是进行了隐式concat。位置编码前半段比较有用，所以在编码嵌入向量的时候，将其语义信息往后放：</p><p><img src="image/image_aT8I-t376t.png" alt=""></p><p>因此我认为最终的Transformer可以将单词的语义与其位置信息分开。此外，也没有理由支撑将二者分开拼接有什么好处。也许这样相加为模型提供比较好的特征。</p><p>更多相关信息可以看：</p><ol><li><a href="https://github.com/tensorflow/tensor2tensor/issues/1591" title="Why add positional embedding instead of concatenate? ">Why add positional embedding instead of concatenate? </a></li><li><a href="https://www.reddit.com/r/MachineLearning/comments/cttefo/comment/exs7d08/" title="Positional Encoding in Transformer ">Positional Encoding in Transformer </a></li></ol><h3 id="6-2-位置信息层层传递之后不会消失吗？"><a href="#6-2-位置信息层层传递之后不会消失吗？" class="headerlink" title="6.2 位置信息层层传递之后不会消失吗？"></a>6.2 位置信息层层传递之后不会消失吗？</h3><p>Transformer里加了残差连接，所以模型输入的信息可以有效地传播到其它层。</p><h3 id="6-3-为什么同时使用正弦和余弦？"><a href="#6-3-为什么同时使用正弦和余弦？" class="headerlink" title="6.3 为什么同时使用正弦和余弦？"></a>6.3 为什么同时使用正弦和余弦？</h3><p>只有同时使用正弦和余弦，我们才能将$sin(x+k)$和$cos(x+k)$表示为$sin(x)$和$cos(x)$的线性变换。好像不能只用正弦或者只用余弦就能达到这种效果。如果你能找到单个正余弦的线性变换，可以在评论区补充。</p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer架构细节</title>
      <link href="/llms/transformer/6.Transformer%E6%9E%B6%E6%9E%84%E7%BB%86%E8%8A%82/"/>
      <url>/llms/transformer/6.Transformer%E6%9E%B6%E6%9E%84%E7%BB%86%E8%8A%82/</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer架构细节"><a href="#Transformer架构细节" class="headerlink" title="Transformer架构细节"></a>Transformer架构细节</h1><p><img src="image/image_DJsgN20AKC.png" alt=""></p><h2 id="1-Transformer各个模块的作用"><a href="#1-Transformer各个模块的作用" class="headerlink" title="1.Transformer各个模块的作用"></a>1.Transformer各个模块的作用</h2><h3 id="（1）Encoder-模块-x20"><a href="#（1）Encoder-模块-x20" class="headerlink" title="（1）Encoder 模块&#x20;"></a>（1）Encoder 模块&#x20;</h3><ul><li>经典的Transformer架构中的Encoder模块包含6个Encoder Block. &#x20;</li><li>每个Encoder Block包含两个⼦模块, 分别是多头⾃注意⼒层, 和前馈全连接层. &#x20;<ul><li>多头⾃注意⼒层采⽤的是⼀种Scaled Dot-Product Attention的计算⽅式, 实验结果表  明, Multi-head可以在更细致的层⾯上提取不同head的特征, ⽐单⼀head提取特征的  效果更佳. &#x20;</li><li>前馈全连接层是由两个全连接层组成, 线性变换中间增添⼀个Relu激活函数, 具体的 维度采⽤4倍关系, 即多头⾃注意⼒的d_model=512, 则层内的变换维度d_ff=2048. &#x20;</li></ul></li></ul><h3 id="（2）Decoder-模块-x20"><a href="#（2）Decoder-模块-x20" class="headerlink" title="（2）Decoder 模块 &#x20;"></a>（2）Decoder 模块 &#x20;</h3><ul><li>经典的Transformer架构中的Decoder模块包含6个Decoder Block. &#x20;</li><li>每个Decoder Block包含3个⼦模块, 分别是多头⾃注意⼒层, Encoder-Decoder Attention  层, 和前馈全连接层. &#x20;<ul><li>多头⾃注意⼒层采⽤和Encoder模块⼀样的Scaled Dot-Product Attention的计算⽅  式, 最⼤的 区别在于<strong>需要添加look-ahead-mask,</strong> 即遮掩”未来的信息”. &#x20;</li><li>Encoder-Decoder Attention层和上⼀层多头⾃注意⼒层最主要的区别在于Q != K = V,  矩阵Q来源于上⼀层Decoder Block的输出, 同时K, V来源于Encoder端的输出. &#x20;</li><li>前馈全连接层和Encoder中完全⼀样. &#x20;</li></ul></li></ul><h3 id="（3）Add-amp-Norm模块-x20"><a href="#（3）Add-amp-Norm模块-x20" class="headerlink" title="（3）Add &amp; Norm模块 &#x20;"></a>（3）Add &amp; Norm模块 &#x20;</h3><ul><li>Add &amp; Norm模块接在每⼀个Encoder Block和Decoder Block中的每⼀个⼦层的后⾯. &#x20;</li><li>对于每⼀个Encoder Block, ⾥⾯的两个⼦层后⾯都有Add &amp; Norm. &#x20;</li><li>对于每⼀个Decoder Block, ⾥⾯的三个⼦层后⾯都有Add &amp; Norm. &#x20;</li><li>Add表示残差连接, 作⽤是为了将信息⽆损耗的传递的更深, 来增强模型的拟合能⼒. &#x20;</li><li>Norm表示LayerNorm, 层级别的数值标准化操作, 作⽤是防⽌参数过⼤过⼩导致的学习过程异常 , 模型收敛特别慢的问题. &#x20;</li></ul><h3 id="（4）位置编码器-Positional-Encoding-x20"><a href="#（4）位置编码器-Positional-Encoding-x20" class="headerlink" title="（4）位置编码器 Positional Encoding &#x20;"></a>（4）位置编码器 Positional Encoding &#x20;</h3><ul><li>Transformer中采⽤三⻆函数来计算位置编码. &#x20;</li><li>因为三⻆函数是周期性函数, 不受序列⻓度的限制, ⽽且这种计算⽅式可以对序列中不同  位置的编码的重要程度同等看待</li></ul><h2 id="2-Decoder端训练和预测的输入"><a href="#2-Decoder端训练和预测的输入" class="headerlink" title="2.Decoder端训练和预测的输入"></a>2.Decoder端训练和预测的输入</h2><ol><li>在Transformer结构中的Decoder模块的输⼊, 区分于不同的Block, 最底层的Block输⼊有其特殊的地⽅。第⼆层到第六层的输⼊⼀致, 都是上⼀层的输出和Encoder的输出。</li><li>最底层的Block在<strong>训练阶段</strong>, 每⼀个time step的输⼊是上⼀个time step的输⼊加上真实标  签序列向后移⼀位. 具体来看, 就是每⼀个time step的输⼊序列会越来越⻓, 不断的将之前的  输⼊融合进来. &#x20;<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">假设现在的真实标签序列等于&quot;How are you?&quot;, 当time step=1时, 输⼊张量为⼀个特殊的token, ⽐如&quot;SOS&quot;; 当time step=2时, 输⼊张量为&quot;SOS How&quot;; 当time step=3时, 输⼊张量为&quot;SOS How are&quot;, 以此类推...</span><br></pre></td></tr></table></figure></li><li>最底层的Block在<strong>训练阶段</strong>, 真实的代码实现中, 采⽤的是MASK机制来模拟输⼊序列不断添  加的过程. &#x20;</li><li>最底层的Block在<strong>预测阶段</strong>, 每⼀个time step的输⼊是从time step=0开始, ⼀直到上⼀个  time step的预测值的累积拼接张量. 具体来看, 也是随着每⼀个time step的输⼊序列会越来  越⻓. 相⽐于训练阶段最⼤的不同是这⾥不断拼接进来的token是每⼀个time step的预测值,  ⽽不是训练阶段每⼀个time step取得的groud truth值<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">当time step=1时, 输⼊的input_tensor=&quot;SOS&quot;, 预测出来的输出值是output_tensor=&quot;What&quot;;</span><br><span class="line">当time step=2时, 输⼊的input_tensor=&quot;SOS What&quot;, 预测出来的输出值是output_tensor=&quot;is&quot;;</span><br><span class="line">当time step=3时, 输⼊的input_tensor=&quot;SOS What is&quot;, 预测出来的输出值是output_tensor=&quot;the&quot;;</span><br><span class="line">当time step=4时, 输⼊的input_tensor=&quot;SOS What is the&quot;, 预测出来的输出值是output_tensor=&quot;matter&quot;;</span><br><span class="line">当time step=5时, 输⼊的input_tensor=&quot;SOS What is the matter&quot;, 预测出来的输出值是output_tensor=&quot;?&quot;;</span><br><span class="line">当time step=6时, 输⼊的input_tensor=&quot;SOS What is the matter ?&quot;, 预测出来的输出值是output_tensor=&quot;EOS&quot;, 代表句⼦的结束符, 说明解码结束, 预测结束.</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h2 id="3-Self-attention"><a href="#3-Self-attention" class="headerlink" title="3.Self-attention"></a>3.Self-attention</h2><blockquote><p>Transformer中⼀直强调的self-attention是什么? 为什么能  发挥如此⼤的作⽤? 计算的时候如果不使⽤三元组(Q, K, V), ⽽  仅仅使⽤(Q, V)或者(K, V)或者(V)⾏不⾏?</p></blockquote><h3 id="（1）self-attention的机制和原理"><a href="#（1）self-attention的机制和原理" class="headerlink" title="（1）self-attention的机制和原理"></a>（1）self-attention的机制和原理</h3><p>self-attention是⼀种通过⾃身和⾃身进⾏关联的attention机制, 从⽽得到更好的  representation来表达⾃身. &#x20;</p><p>self-attention是attention机制的⼀种特殊情况:  在self-attention中, Q=K=V, <strong>序列中的每个单词(token)都和该序列中的其他所有单词 (token)进⾏attention规则的计算.</strong> &#x20;</p><p>attention机制计算的特点在于, 可以<strong>直接跨越⼀句话中不同距离的token, 可以远距离的学习到序列的知识依赖和语序结构.</strong></p><p><img src="image/image_y4Ch3Hjyc9.png" alt=""></p><ul><li>从上图中可以看到, self-attention可以远距离的捕捉到语义层⾯的特征(it的指代对象是  animal). &#x20;</li><li>应⽤传统的RNN, LSTM, 在获取⻓距离语义特征和结构特征的时候, <strong>需要按照序列顺序依次  计算, 距离越远的联系信息的损耗越⼤, 有效提取和捕获的可能性越⼩.</strong> &#x20;</li><li>但是应⽤self-attention时, 计算过程中会直接将句⼦中任意两个token的联系通过⼀个计算  步骤直接联系起来,</li></ul><h3 id="（2）关于self-attention为什么要使⽤-Q-K-V-三元组⽽不是其他形式"><a href="#（2）关于self-attention为什么要使⽤-Q-K-V-三元组⽽不是其他形式" class="headerlink" title="（2）关于self-attention为什么要使⽤(Q, K, V)三元组⽽不是其他形式"></a>（2）关于self-attention为什么要使⽤(Q, K, V)三元组⽽不是其他形式</h3><p>⾸先⼀条就是从分析的⻆度看, 查询Query是⼀条独⽴的序列信息, 通过关键词Key的提示作⽤, 得到最终语义的真实值Value表达, 数学意义更充分, 完备. &#x20;</p><p>这⾥不使⽤(K, V)或者(V)没有什么必须的理由, 也没有相关的论⽂来严格阐述⽐较试验的结果差异, 所以可以作为开放性问题未来去探索, 只要明确在经典self-attention实现中⽤的是三元组就好</p><h2 id="4-Self-attention归一化和放缩"><a href="#4-Self-attention归一化和放缩" class="headerlink" title="4.Self-attention归一化和放缩"></a>4.Self-attention归一化和放缩</h2><h3 id="（1）self-attention中的归⼀化概述"><a href="#（1）self-attention中的归⼀化概述" class="headerlink" title="（1）self-attention中的归⼀化概述"></a>（1）self-attention中的归⼀化概述</h3><p><strong>训练上的意义</strong>：随着词嵌⼊维度d_k的增⼤, q * k 点积后的结果也会增⼤, 在训练时会将  softmax函数推⼊梯度⾮常⼩的区域, 可能出现梯度消失的现象, 造成模型收敛困难. &#x20;</p><p><strong>数学上的意义</strong>: 假设q和k的统计变量是满⾜标准正态分布的独⽴随机变量, 意味着q和k满⾜均  值为0, ⽅差为1。** 那么q和k的点积结果就是均值为0, ⽅差为**$d_k$<strong>, 为了抵消这种⽅差被放⼤</strong>$d_k$**  倍的影响, 在计算中主动将点积缩放**$\frac{1}{\sqrt(d_k)}$, 这样点积后的结果依然满⾜均值为0, ⽅差为  1。</p><h3 id="（2）softmax的梯度变化"><a href="#（2）softmax的梯度变化" class="headerlink" title="（2）softmax的梯度变化"></a>（2）softmax的梯度变化</h3><p>这⾥我们分3个步骤来解释softmax的梯度问题: &#x20;</p><h4 id="第⼀步-softmax函数的输⼊分布是如何影响输出的"><a href="#第⼀步-softmax函数的输⼊分布是如何影响输出的" class="headerlink" title="第⼀步: softmax函数的输⼊分布是如何影响输出的"></a>第⼀步: softmax函数的输⼊分布是如何影响输出的</h4><p>对于⼀个输⼊向量x, softmax函数将其做了⼀个归⼀化的映射, ⾸先通过⾃然底数e将输⼊元素之间的差距先”拉⼤”, 然后再归⼀化为⼀个新的分布。 <strong>在这个过程中假设某个输⼊x  中最⼤的元素下标是k, 如果输⼊的数量级变⼤(就是x中的每个分量绝对值都很⼤), 那么在数学上会造成y_k的值⾮常接近1。</strong> &#x20;</p><p>具体⽤⼀个例⼦来演示, 假设输⼊的向量$x = [a, a, 2a]$, 那么随便给⼏个不同数量级的值来看看对y3产⽣的影响</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = 1时,   y3 = 0.5761168847658291</span><br><span class="line">a = 10时,  y3 = 0.9999092083843412</span><br><span class="line">a = 100时, y3 = 1.0</span><br></pre></td></tr></table></figure><p>采⽤⼀段实例代码将a在不同取值下, 对应的y3全部画出来, 以曲线的形式展示:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> exp</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">f = <span class="keyword">lambda</span> x: exp(x * <span class="number">2</span>) / (exp(x) + exp(x) + exp(x * <span class="number">2</span>))</span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line">y_3 = [f(x_i) <span class="keyword">for</span> x_i <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x, y_3)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="image/image_k8mmMoDYKQ.png" alt=""></p><p>从上图可以很清楚的看到输⼊元素的数量级对softmax最终的分布影响⾮常之⼤。&#x20;</p><p><strong>结论</strong>： <strong>在输⼊元素的数量级较⼤时, softmax函数⼏乎将全部的概率分布都分配给了最⼤值分量所对应的标签</strong></p><h4 id="第⼆步-softmax函数在反向传播的过程中是如何梯度求导的"><a href="#第⼆步-softmax函数在反向传播的过程中是如何梯度求导的" class="headerlink" title="第⼆步: softmax函数在反向传播的过程中是如何梯度求导的"></a>第⼆步: softmax函数在反向传播的过程中是如何梯度求导的</h4><p>首先，定义神经网络的输入和输出</p><script type="math/tex; mode=display">设X=[x_1,x_2,..., x_n], Y=softmax(X)=[y_1, y_2,..., y_3] \\则~y_i=\frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}},~显然~  \sum_{j=1}^{n} e^{x_j}=1</script><p>反向传播就是输出端的损失函数对输⼊端求偏导的过程, 这⾥要分两种情况,&#x20;</p><p>**（1）当 **$i=j$<strong>时：</strong></p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial y_{i}}{\partial x_{j}}& =\frac{\partial y_i}{\partial x_i}  \\&=\frac{\partial}{\partial x_i}(\frac{e^{x_i}}{\sum_k e^{x_k}}) \\&=\frac{(e^{x_i})'(\sum_k e^{x_k})-e^{x_i}(\sum_k e^{x_k})'}{(\sum_k e^{x_k})^2} \\&=\frac{e^{x_i}\cdot(\sum_ke^{x_k})-e^{x_i}\cdot e^{x_i}}{(\sum_ke^{x_k})^2} \\&=\frac{e^{x_i}\cdot(\sum_k e^{x_k})}{(\sum_k e^{x_k})^2}-\frac{e^{x_i}\cdot e^{x_i}}{(\sum_k e^{x_k})^2} \\&=\frac{e^{x_i}}{\sum_k e^{x_k}}-\frac{e^{x_i}}{\sum_k e^{x_k}}\cdot\frac{e^{x_i}}{\sum_k e^{x_k}} \\&=y_i-y_i\cdot y_i \\&=y_i(1-y_i)\end{aligned}</script><p><strong>（2）当</strong>$  i ≠ j $<strong>时：</strong></p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial y_{i}}{\partial x_{j}} & =\frac{\partial}{\partial x_{j}}\left(\frac{e^{x_{i}}}{\sum_{k} e^{x_{k}}}\right) \\& =\frac{\left(e^{x_{i}}\right)^{\prime}\left(\sum_{k} e^{x_{k}}\right)-e^{x_{i}}\left(\sum_{k} e^{x_{k}}\right)^{\prime}}{\left(\sum_{k} e^{x_{k}}\right)^{2}} \\& =\frac{0 \cdot\left(\sum_{k} e^{x_{k}}\right)-e^{x_{i}} \cdot e^{x_{j}}}{\left(\sum_{k} e^{x_{k}}\right)^{2}} \\& =-\frac{e^{x_{i}} \cdot e^{x_{j}}}{\left(\sum_{k} e^{x_{k}}\right)^{2}} \\& =-\frac{e^{x_{i}}}{\sum_{k} e^{x_{k}}} \cdot \frac{e^{x_{i}}}{\sum_{k} e^{x_{k}}} \\& =-y_{i} \cdot y_{i}\end{aligned}</script><p>经过对两种情况分别的求导计算, 可以得出最终的结论如下:</p><script type="math/tex; mode=display">\begin{aligned}& 综上所述：\frac{\partial y_i}{\partial x_j}=\begin{cases}y_i-y_i\cdot y_i,&\text{i=j}\\ \\ 0-y_i\cdot y_i,&\text{i}\neq\text{j}\end{cases} \\& 所以：\frac{\partial Y}{\partial X}=diag(Y)-Y^T\cdot Y（当Y的shape为(1,n)时）\end{aligned}</script><h4 id="第三步-softmax函数出现梯度消失现象的原因"><a href="#第三步-softmax函数出现梯度消失现象的原因" class="headerlink" title="第三步: softmax函数出现梯度消失现象的原因"></a>第三步: softmax函数出现梯度消失现象的原因</h4><p>根据第二步中softmax函数的求导结果, 可以将最终的结果以矩阵形式展开如下:</p><script type="math/tex; mode=display">\frac{\partial g(X)}{\partial X} \approx\left[\begin{array}{cccc}\hat{y}_{1} & 0 & \cdots & 0 \\0 & \hat{y}_{2} & \cdots & 0 \\\vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & \hat{y}_{d}\end{array}\right]-\left[\begin{array}{cccc}\hat{y}_{1}^{2} & \hat{y}_{1} \hat{y}_{2} & \cdots & \hat{y}_{1} \hat{y}_{d} \\\hat{y}_{2} \hat{y}_{1} & \hat{y}_{2}^{2} & \cdots & \hat{y}_{2} \hat{y}_{d} \\\vdots & \vdots & \ddots & \vdots \\\hat{y}_{d} \hat{y}_{1} & \hat{y}_{d} \hat{y}_{2} & \cdots & \hat{y}_{d}^{2}\end{array}\right]</script><p>根据第一步中的讨论结果, 当输入x的分量值较大时, softmax函数会将大部分概率分配给最大的元素, 假设最大元素是x1, 那么softmax的输出分布将产生一个接近one-hot的结果张量y_ = [1, 0, 0,…, 0], 此时结果矩阵变为:</p><script type="math/tex; mode=display">\frac{\partial g(X)}{\partial X} \approx\left[\begin{array}{cccc}1 & 0 & \cdots & 0 \\ 0 & 0 & \cdots & 0 \\ & & & \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 0\end{array}\right]-\left[\begin{array}{cccc}1 & 0 & \cdots & 0 \\ 0 & 0 & \cdots & 0 \\ & & & \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 0\end{array}\right]=0</script><p>结论：综上可以得出,** 所有的梯度都消失为0(接近于0), 参数几乎无法更新, 模型收敛困难**.</p><h3 id="（3）维度与点积大小的关系"><a href="#（3）维度与点积大小的关系" class="headerlink" title="（3）维度与点积大小的关系"></a>（3）维度与点积大小的关系</h3><p>针对为什么维度会影响点积的大小, 原始论文中有这样的一点解释如下:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their doct product,q*k = (q1k1+q2k2+......+q(d_k)k(d_k)), has mean 0 and variance d_k.</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>分两步对其进行一个推导, 首先就是假设向量q和k的各个分量是相互独立的随机变量, $X = q_i$, $Y = k_i$, X和Y各自有d_k个分量, 也就是向量的维度等于d_k, 有$E(X) = E(Y) = 0$, 以及$D(X) = D(Y) = 1$.</p><p>可以得到$E(XY) = E(X)E(Y) = 0 * 0 = 0$</p><p>同理, 对于$D(XY)$推导如下:</p><script type="math/tex; mode=display">\begin{aligned}D(XY)& =E(X^2\cdot Y^2)-[E(XY)]^2  \\&=E(X^2)E(Y^2)-[E(X)E(Y)]^2 \\&=E(X^2-0^2)E(Y^2-0^2)-[E(X)E(Y)]^2 \\&=E(X^2-[E(X)]^2)E(Y^2-[E(Y)]^2)-[E(X)E(Y)]^2 \\&=D(X)D(Y)-[E(X)E(Y)]^2 \\&=1\times1-\left(0\times0\right)^2 \\&=1\end{aligned}</script><p>根据期望和方差的性质, 对于互相独立的变量满足下式：</p><script type="math/tex; mode=display">\begin{gathered}E(\sum_{i}Z_{i})=\sum_{i}E(Z_{i}), \\D(\sum_{i}Z_{i})=\sum_{i}D(Z_{i}) \end{gathered}</script><p>根据上面的公式, 可以很轻松的得出q*k的均值为$E(qk) = 0$, $D(qk) = d_k$。所以方差越大, <strong>对应的qk的点积就越大, 这样softmax的输出分布就会更偏向最大值所在的分量</strong>。一个技巧就是将点积除以$\sqrt{d_k}$ 将方差在数学上重新”拉回1”, 如下所示</p><script type="math/tex; mode=display">D(\frac{q\cdot k}{\sqrt{d_k}})=\frac{d_k}{(\sqrt{d_k})^2}=1</script><p>最终的结论：<strong>通过数学上的技巧将方差控制在1, 也就有效的控制了点积结果的发散, 也就控制了对应的梯度消失的问题</strong>!</p><h2 id="5-Multi-head-Attention"><a href="#5-Multi-head-Attention" class="headerlink" title="5.Multi-head Attention"></a>5.Multi-head Attention</h2><h3 id="（1）采⽤Multi-head-Attention的原因-x20"><a href="#（1）采⽤Multi-head-Attention的原因-x20" class="headerlink" title="（1）采⽤Multi-head Attention的原因 &#x20;"></a>（1）采⽤Multi-head Attention的原因 &#x20;</h3><ol><li>原始论⽂中提到进⾏Multi-head Attention的原因是将模型分为多个头, <strong>可以形成多个子空间间, 让模型去关注不同方面的信息</strong>, 最后再将各个⽅⾯的信息综合起来得到更好的效果. &#x20;</li><li>多个头进⾏attention计算最后再综合起来, 类似于CNN中采⽤多个卷积核的作⽤, 不同的卷  积核提取不同的特征, <strong>关注不同的部分, 最后再进行融合</strong>. &#x20;</li><li>直观上讲, <strong>多头注意力有助于神经⽹络捕捉到更丰富的特征信息</strong>.</li></ol><h4 id="（2）Multi-head-Attention的计算⽅式-x20"><a href="#（2）Multi-head-Attention的计算⽅式-x20" class="headerlink" title="（2）Multi-head Attention的计算⽅式 &#x20;"></a>（2）Multi-head Attention的计算⽅式 &#x20;</h4><ol><li>Multi-head Attention和单⼀head的Attention唯⼀的区别就在于,** 其对特征张量的最后⼀个维度进行了分割, ⼀般是对词嵌入的embedding_dim=512进⾏切割成head=8, **这样每⼀个head的嵌⼊维度就是512/8=64, 后续的Attention计算公式完全⼀致, 只不过是在64这个维度上进⾏⼀系列的矩阵运算⽽已. &#x20;</li><li>在head=8个头上分别进⾏注意⼒规则的运算后, 简单采⽤拼接<strong>concat</strong>的⽅式对结果张量进  ⾏融合就得到了Multi-head Attention的计算结果.</li></ol><h2 id="6-Transformer和RNN"><a href="#6-Transformer和RNN" class="headerlink" title="6.Transformer和RNN"></a>6.Transformer和RNN</h2><h3 id="（1）Transformer的并行计算-x20"><a href="#（1）Transformer的并行计算-x20" class="headerlink" title="（1）Transformer的并行计算 &#x20;"></a>（1）Transformer的并行计算 &#x20;</h3><p>对于Transformer⽐传统序列模型RNN/LSTM具备优势的第⼀⼤原因就是强⼤的并⾏计算能力.&#x20;</p><p>对于RNN来说, 任意时刻t的输⼊是时刻t的输⼊x(t)和上⼀时刻的隐藏层输出h(t-1), 经过运算后得到当前时刻隐藏层的输出h(t), 这个h(t)也即将作为下⼀时刻t+1的输⼊的⼀部分.  这个计算过程是RNN的本质特征, RNN的历史信息是需要通过这个时间步⼀步⼀步向后传递的. <strong>⽽这就意味着RNN序列后⾯的信息只能等到前⾯的计算结束后, 将历史信息通过hidden state传递给后⾯才能开始计算, 形成链式的序列依赖关系, 无法实现</strong>并行. &#x20;</p><p>对于Transformer结构来说, 在self-attention层, ⽆论序列的⻓度是多少, 都可以⼀次性计算所有单词之间的注意⼒关系, 这个attention的计算是同步的, 可以实现并⾏. &#x20;</p><h3 id="（2）Transformer的特征抽取能力-x20"><a href="#（2）Transformer的特征抽取能力-x20" class="headerlink" title="（2）Transformer的特征抽取能力&#x20;"></a>（2）Transformer的特征抽取能力&#x20;</h3><p>对于Transformer⽐传统序列模型RNN/LSTM具备优势的第⼆⼤原因就是强⼤的特征抽取能力 。&#x20;</p><p>Transformer因为采⽤了Multi-head Attention结构和计算机制, 拥有⽐RNN/LSTM更强⼤的特征抽取能⼒, 这⾥并不仅仅由理论分析得来, 而是⼤量的试验数据和对⽐结果, 清楚的展示了Transformer的特征抽取能⼒远远胜于RNN/LSTM. &#x20;</p><p><strong>注意</strong>: 不是越先进的模型就越无敌, 在很多具体的应⽤中RNN/LSTM依然⼤有⽤武之地, 要具体问题具体分析</p><h2 id="7-Transformer代替seq2seq？"><a href="#7-Transformer代替seq2seq？" class="headerlink" title="7.Transformer代替seq2seq？"></a>7.Transformer代替seq2seq？</h2><h3 id="（1）seq2seq的两大缺陷-x20"><a href="#（1）seq2seq的两大缺陷-x20" class="headerlink" title="（1）seq2seq的两大缺陷 &#x20;"></a>（1）seq2seq的两大缺陷 &#x20;</h3><ol><li>seq2seq架构的第⼀⼤缺陷是将Encoder端的所有信息<strong>压缩成⼀个固定⻓度的语义向量中,  ⽤这个固定的向量来代表编码器端的全部信息. 这样既会造成信息的损耗</strong>, 也⽆法让Decoder 端在解码的时候去⽤注意⼒聚焦哪些是更重要的信息. &#x20;</li><li>seq2seq架构的第二大缺陷是<strong>无法并行</strong>, 本质上和RNN/LSTM无法并行的原因⼀样. &#x20;</li></ol><h3 id="（2）Transformer的改进-x20"><a href="#（2）Transformer的改进-x20" class="headerlink" title="（2）Transformer的改进 &#x20;"></a>（2）Transformer的改进 &#x20;</h3><p>Transformer架构同时解决了seq2seq的两⼤缺陷, 既可以并⾏计算, ⼜应⽤Multi-head  Attention机制来解决Encoder固定编码的问题, 让Decoder在解码的每⼀步可以通过注意⼒去  关注编码器输出中最重要的那些部分.</p><h2 id="8-Transformer并行化"><a href="#8-Transformer并行化" class="headerlink" title="8.Transformer并行化"></a>8.Transformer并行化</h2><h3 id="（1）Encoder并行化"><a href="#（1）Encoder并行化" class="headerlink" title="（1）Encoder并行化"></a>（1）Encoder并行化</h3><p><img src="image/image_FXmXoczdDm.png" alt=""></p><ol><li>上图最底层绿⾊的部分, 整个序列所有的token可以并⾏的进⾏Embedding操作, 这⼀层的处理是没有依赖关系的. &#x20;</li><li>上图第⼆层⼟⻩⾊的部分, 也就是Transformer中最重要的self-attention部分, 这⾥对于任意⼀个单词⽐如x1, 要计算x1对于其他所有token的注意⼒分布, 得到z1. 这个过程是具有依赖性的, 必须等到序列中所有的单词完成Embedding才可以进⾏。因此这⼀步是不能并⾏处理的。 但是从另⼀个⻆度看, 我们真实计算注意⼒分布的时候, 采⽤的都是矩阵运算, 也就是可以⼀次性的计算出所有token的注意⼒张量, 从这个⻆度看也算是实现了并⾏, 只是矩 阵运算的”并⾏”和词嵌⼊的”并⾏”概念上不同⽽已. &#x20;</li><li>上图第三层蓝⾊的部分, 也就是前馈全连接层, 对于不同的向量z之间也是没有依赖关系的, 所以这⼀层是可以实现并⾏化处理的. 也就是所有的向量z输⼊Feed Forward⽹络的计算可以同步进⾏, 互不⼲扰</li></ol><h3 id="（2）Decoder的并行化"><a href="#（2）Decoder的并行化" class="headerlink" title="（2）Decoder的并行化"></a>（2）Decoder的并行化</h3><p><img src="image/image_ASVylibmq1.png" alt=""></p><ol><li>Decoder模块在训练阶段采用了并行化处理。 其中Self-Attention和Encoder-Decoder Attention两个子层的并行化也是在进行矩阵乘法, 和Encoder的理解是一致的. 在进行Embedding和Feed Forward的处理时, 因为各个token之间没有依赖关系, 所以也是可以完全并行化处理的, 这里和Encoder的理解也是一致的.</li><li>Decoder模块在预测阶段基本上不认为采用了并行化处理. 因为第一个time step的输入只是一个”SOS”, 后续每一个time step的输入也只是依次添加之前所有的预测token.</li><li><strong>注意:</strong> 最重要的区别是训练阶段目标文本如果有20个token, 在训练过程中是一次性的输入给Decoder端, 可以做到一些子层的并行化处理. 但是在预测阶段, 如果预测的结果语句总共有20个token, 则需要重复处理20次循环的过程, 每次的输入添加进去一个token, 每次的输入序列比上一次多一个token, 所以不认为是并行处理.</li></ol><h3 id="（3）总结"><a href="#（3）总结" class="headerlink" title="（3）总结"></a>（3）总结</h3><p><strong>Transformer架构中Encoder模块的并行化机制</strong></p><ul><li><strong>Encoder模块在训练阶段和测试阶段都可以实现完全相同的并行化.</strong></li><li>Encoder模块在Embedding层, Feed Forward层, Add &amp; Norm层都是可以并行化的.</li><li>Encoder模块在self-attention层, 因为各个token之间存在依赖关系, 无法独立计算, 不是真正意义上的并行化.</li><li>Encoder模块在self-attention层, 因为采用了矩阵运算的实现方式, 可以一次性的完成所有注意力张量的计算, 也是另一种”并行化”的体现.</li></ul><p><strong>Transformer架构中Decoder模块的并行化机制</strong></p><ul><li><strong>Decoder模块在训练阶段可以实现并行化</strong>.</li><li>Decoder模块在训练阶段的Embedding层, Feed Forward层, Add &amp; Norm层都是可以并行化的.</li><li>Decoder模块在self-attention层, 以及Encoder-Decoder Attention层, 因为各个token之间存在依赖关系, 无法独立计算, 不是真正意义上的并行化.</li><li>Decoder模块在self-attention层, 以及Encoder-Decoder Attention层, 因为采用了矩阵运算的实现方式, 可以一次性的完成所有注意力张量的计算, 也是另一种”并行化”的体现.</li><li><strong>Decoder模块在预测计算不能并行化处理.</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 ViT</title>
      <link href="/paper_reading/2.3.ViT/"/>
      <url>/paper_reading/2.3.ViT/</url>
      
        <content type="html"><![CDATA[<h1 id="3-ViT"><a href="#3-ViT" class="headerlink" title="3.ViT"></a>3.ViT</h1><p>论文链接：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2010.11929" title="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></p><p>源码链接：<a href="https://link.zhihu.com/?target=https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py" title="https://github.com/rwightman/py">https://github.com/rwightman/py</a></p><h1 id="1-标题-简介"><a href="#1-标题-简介" class="headerlink" title="1.标题 + 简介"></a>1.标题 + 简介</h1><p><strong>An image is worth 16*16 words</strong></p><p>每一个方格都是 16 * 16 大小，图片有很多 16 * 16 方格 patches —&gt; an image is worth 16 * 16 words</p><p>ViT：过去一年，CV 最有影响力的工作</p><ul><li>推翻了 2012 Alexnet 提出的 CNN 在 CV 的统治地位</li><li>有足够多的预训练数据，NLP 的 Transformer 搬运到 CV，效果很好</li><li>打破 CV 和 NLP 的壁垒，给 CV、多模态 <strong>挖坑</strong></li></ul><p>ViT效果有多好，CV 任务刷榜，paperwithcode网站&#x20;</p><ol><li>霸榜 ImageNet （基于 ViT）</li><li>&#x20;COCO ,目标检测（Swin Transformer ICCV 21 best paper：多尺度的 ViT ）的模型</li></ol><p>下图中的的四种情况 ViT 都能处理</p><p>遮挡、数据分布的偏移（纹理的去除）、鸟头部+对抗的patch、图片打散重新排列组合</p><p><img src="image/image_mzEpAZYXpH.png" alt=""></p><h1 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a><strong>2.摘要</strong></h1><p>Transformer 在 NLP 是基本操作，但 transformer 在 CV 的应用有限。</p><p>CV 里的 attention 是怎么用的呢？attention + CNN, or attention 替换 CNN components 但依然保持 CNN 整体结构。</p><p>本文怎么看 CV 里的 attention? attention 不用和 CNN 绑在一起，和 transformer 结合，在 CV 领域 大杀四方。</p><h1 id="3-引言"><a href="#3-引言" class="headerlink" title="3.引言"></a><strong>3.引言</strong></h1><h2 id="3-1-self-attention"><a href="#3-1-self-attention" class="headerlink" title="3.1 self attention"></a>3.1 self attention</h2><p>self-attention 架构， esp Transformers，是 NLP 必选模型。主流方式是 BERT 提出的，<strong>大规模数据集预训练，在特定领域的小数据集做微调</strong>。 Transformer 的 计算高效和可扩展性，1000亿参数都还没有性能饱和的现象。</p><h2 id="3-2-Transformer-应用在-CV-的难点"><a href="#3-2-Transformer-应用在-CV-的难点" class="headerlink" title="3.2 Transformer 应用在 CV 的难点"></a>3.2 Transformer 应用在 CV 的<strong>难点</strong></h2><p>计算像素的 self-attention，序列长，维度爆炸</p><p>Trnasformer 的计算复杂度是 序列长度 n 的 平方 $O(n^2)$</p><p>224 分辨率的图片，有 50176 个像素点，（2d 图片 flatten）序列长度是 BERT 的近 100 倍。</p><p><strong>CV 如何用 attention 呢？</strong></p><h3 id="（1）CNN-结构-self-attention-or-attention-替代卷积"><a href="#（1）CNN-结构-self-attention-or-attention-替代卷积" class="headerlink" title="（1）CNN 结构 + self-attention or attention 替代卷积"></a>（1）CNN 结构 + self-attention or attention 替代卷积</h3><p>降低序列长度的方式：CVPR Wang et al. 2018, Local Network, 网络中的特征图 输入 Transformer</p><p>ResNet 50 最后一个 stage, res4 的 feature map 14 * 14， 196</p><h3 id="（2）stand-alone-attention-孤立自注意力"><a href="#（2）stand-alone-attention-孤立自注意力" class="headerlink" title="（2）stand-alone attention 孤立自注意力"></a>（2）stand-alone attention 孤立自注意力</h3><p>用 local window 局部小窗口 控制 transformer 的计算复杂度，有点像卷积，卷积也有 locality，局部窗口卷积。</p><h3 id="（3）axial-attention-轴注意力"><a href="#（3）axial-attention-轴注意力" class="headerlink" title="（3）axial attention 轴注意力"></a>（3）axial attention 轴注意力</h3><p>2 个 1d 顺序操作，降低计算复杂度，图片的序列长度 n = H * W。</p><p>2d 矩阵 拆分为 2个1d 向量，先在 H 高度 dimension 做一次 self-attention，再 W 宽度 dimension 做一次 self-attention</p><p><strong>replacing the convolutions entirely 好不好呢？</strong></p><p>理论高效，但硬件无法加速 —\> 此类模型都还没有太大。</p><p>本段（第二段）总结：在大规模的图像识别上，ResNet 还是效果最好的。</p><h2 id="3-3-ViT"><a href="#3-3-ViT" class="headerlink" title="3.3 ViT"></a>3.3 ViT</h2><h3 id="（1）ViT做法"><a href="#（1）ViT做法" class="headerlink" title="（1）ViT做法"></a>（1）ViT做法</h3><p><strong>现状：</strong> attention 已经在 CV 领域有应用，甚至也有 attention 替代卷积的操作</p><p>标准 Transformer 直接应用于图片，做最少的修改，不做任何针对视觉任务的特定的改变。</p><p>把图片划分成很多 patches，每个 patch 元素 是 16 * 16，序列长度 14 * 14 = 196个元素</p><p>每一个 patch 经过一个 FC layer（fully connected layer）得到一个 linear embedding，patches 的 linear embeddings 是 Transformer 的输入。</p><p>一个 224 * 224 图片 变成一个 196 个的 16 * 16 图片块（words in NLP）。</p><p><img src="image/image_xD9SovxAY_.png" alt=""></p><h3 id="（2）为什么-transformer-的训练是-supervised-fashion？"><a href="#（2）为什么-transformer-的训练是-supervised-fashion？" class="headerlink" title="（2）为什么 transformer 的训练是 supervised fashion？"></a>（2）<strong>为什么 transformer 的训练是 supervised fashion？</strong></h3><p>NLP 的 Transformer 无监督训练 by language model LM or mask language model MLM；</p><p>CV 任务的 benchmark 使用有监督训练。</p><p><strong>ViT 把 CV 任务当成 NLP 任务，模型使用的是 BERT, Transformer encoder 简洁框架</strong>。</p><h3 id="（3）Transformer-in-CV，之前有人做吗？"><a href="#（3）Transformer-in-CV，之前有人做吗？" class="headerlink" title="（3）Transformer in CV，之前有人做吗？"></a>（3）<strong>Transformer in CV，之前有人做吗？</strong></h3><p>ICLR 2020 从输入图片里抽取 2 * 2 patches。 2 * 2 size enough：CIFAR-10 32 * 32 图片，16 * 16 会过大。 抽好 patch 之后，在 patches 上 做 self-attention。 —&gt; 技术上的 Vision Transformer</p><p><strong>ViT 和 ICLR 2 * 2 patches 的区别？</strong></p><ul><li>ViT证明了 大规模数据集预训练 （NLP 常用）之后的 Transformer，不需要做 针对视觉任务的 修改，比最好的 CNNs 效果差不多 or 甚至更好。</li><li>2 * 2 patches applicable only to small-resolution images, ViT handles medium-resolution images as well.</li><li>ViT 告诉大家，Transformer 在 vision 领域能拓展到有多好。large 数据集 + large 模型，transformer 能否取代 CNN 地位？</li></ul><h3 id="（4）ViT-任何情况比CNN都强吗？"><a href="#（4）ViT-任何情况比CNN都强吗？" class="headerlink" title="（4）ViT 任何情况比CNN都强吗？"></a>（4）ViT 任何情况比CNN都强吗？</h3><p>mid-sized datasets ImageNet without strong regularization，ViT 比 ResNet of comparable size 弱几个点。</p><p><strong>Transformer 比 CNN 少 inductive biases 归纳偏置</strong></p><p>CNN 有 locality 和 translation equivariance 归纳偏置，—&gt; CNN 有 很多先验信息 —&gt; 需要较少的数据去学好一个模型。</p><p>Transformer 没有这些先验信息，只能 从图片数据里，自己学习对 视觉世界 的感知。</p><h4 id="inductive-biases-归纳偏置：先验知识-or-提前的假设"><a href="#inductive-biases-归纳偏置：先验知识-or-提前的假设" class="headerlink" title="inductive biases 归纳偏置：先验知识 or 提前的假设"></a>inductive biases 归纳偏置：先验知识 or 提前的假设</h4><p>CNN 的 inductive biases 是 locality（相似特征） 和  translation equaivariance（平移不变性 ）。</p><ul><li>locality: CNN用滑动窗口在图片上做卷积。假设是图片相邻的区域有相似的特征。i.e., 桌椅在一起的概率大，距离近的物品 相关性越强。</li><li>translation equaivariance：$f (g(x)) = g( f(x) )$，f 和 g 函数的顺序不影响结果。</li><li>f：卷积 g：平移; 无论先做平移 g 还是先做卷积 f , 最后结果一样。</li></ul><h4 id="怎么验证-Transformer-无-inductive-bias-的假设？"><a href="#怎么验证-Transformer-无-inductive-bias-的假设？" class="headerlink" title="怎么验证 Transformer 无 inductive bias 的假设？"></a><strong>怎么验证 Transformer 无 inductive bias 的假设？</strong></h4><p>在 1400万(ImageNet-21K) - 3000 万(JFT-300)得到图片数据集上预训练 trumps inductive bias, ViT +足够训练数据，CV SOTA。</p><p>VTAB 融合了 19 个数据集，检测模型的稳健性，ViT的 robustness 也很好。</p><h2 id="3-4引言总结"><a href="#3-4引言总结" class="headerlink" title="3.4引言总结"></a>3.4引言总结</h2><p>第一段：Transformer 在 NLP 扩展的很好，没有因为大模型和大数据集而饱和，performance 一直有提升，Transformer 在 CV 里能不能也有大幅度的提升呢？</p><p>第二段：前人工作。这么好的 idea 有哪些人做过呢？<strong>要讲清楚自己的工作和 related works 的区别</strong>。之前的工作是 CNN + attention 或者 attention 替代 convolutions，没有工作将 transformer 用到 CV 领域，没有得到很好的扩展效果。</p><p>第三段：Vision Transformer 是 standard Transformer with the fewest possible modifications。图片变成 16 * 16 的像素块 patches，经过 一个 fc layer 得到的 linear embeddings 输入 transformer。<strong>ViT 融合了 CV 和 NLP 领域</strong>。</p><p>第四+五段：show 结果，足够多的数据集，ViT 能 SOTA</p><h1 id="4-结论"><a href="#4-结论" class="headerlink" title="4.结论"></a><strong>4.结论</strong></h1><p>直接 用 NLP 的 Transformer 来处理图片，<strong>和其它 self-attention in CV 的工作不同</strong>：除了 将图片转成 16 * 16 patches + 位置编码 之外，没有额外引入 图像特有的 inductive bias</p><p><strong>没有图片的 inductive bias 的好处是什么？</strong></p><p>不需要对 vision 领域的了解，不需要 domain knowledge，直接把 图片理解成 a sequence of patches, i.e., 一个句子里的很多单词。</p><p><strong>新问题</strong> —— CV 除了 image classfication 其他的任务，行不行呢？分割、检测</p><ul><li>DETR (Carion et al. 2020) 目标检测的力作，改变了目标检测 出框的方式。ViT 做其它 CV 任务应该效果也很好。</li><li>ViT-FRCNN 检测 detection</li><li>SETR 分割 segmentation</li><li>（3个月后）Swin Transformer 融合 Transformer 和多尺度设计</li></ul><p>Transformer 是 CV 领域的一个通用的骨干网络 backbone</p><p>另外一个未来工作方向，<strong>自监督的预训练方式</strong>。NLP 大的 transformer 模型使用 自监督 预训练，ViT有 initial experiments 证明 自监督预训练也可以，但和有监督的训练有差距 still large gap。</p><p><strong>ViT 挖坑</strong>：</p><ul><li>视觉领域 CV</li><li>多模态，一个 transformer 处理 CV 和 NLP问题</li></ul><h1 id="5-相关工作"><a href="#5-相关工作" class="headerlink" title="5.相关工作"></a><strong>5.相关工作</strong></h1><h2 id="5-1-Transformer"><a href="#5-1-Transformer" class="headerlink" title="5.1 Transformer"></a>5.1 Transformer</h2><p>Transformer 在 NLP 领域的应用：BERT, GPT</p><ul><li>Transformer 先在大规模语料库上做预训练，再根据具体的任务数据集进行微调。</li><li>BERT： denosing mask挖词、完形填空，把masked的词预测出来</li><li>GPT：language modelling, 预测下一个词 next word prediction</li><li>自监督的训练方式：完形填空 or 预测下一个词，人为设定。语料句子是完整的，去掉某些词（完形填空） or 最后词（预测下一个词）</li></ul><h2 id="5-2-self-attention-在视觉领域的应用"><a href="#5-2-self-attention-在视觉领域的应用" class="headerlink" title="5.2 self-attention 在视觉领域的应用"></a>5.2 self-attention 在视觉领域的应用</h2><p>用图片的所有像素做Transformer会产生复杂度爆炸的问题，Transformer的复杂度是输入维度的平方（$O(n^2 )$）</p><p>self-attention to each image with approximations：</p><ul><li>不用整张图，只用 local neighborhoods，降低序列长度</li></ul><p>sparse transformer</p><ul><li>全局注意力的近似</li><li>只对 稀疏的点 做注意力</li></ul><p>scale attention by applying attention in blocks of varying size</p><ul><li>把自注意力用到不同大小的 blocks</li><li>in the extreme case only along individual axes 极端情况，只关心轴， axial self-attention，横轴 + 纵轴</li></ul><p>小结：以上 self-attention + CV 效果不错，但工程实现加速很难。可在 cpu gpu跑，但大规模训练不行。</p><h2 id="5-3-和-ViT-最相似的工作：-x20"><a href="#5-3-和-ViT-最相似的工作：-x20" class="headerlink" title="5.3 和 ViT 最相似的工作：&#x20;"></a>5.3 <strong>和 ViT 最相似的工作：</strong>&#x20;</h2><h3 id="（1）ICLR-2020-2-2-patches-for-CIFAR-10-32-32-图片"><a href="#（1）ICLR-2020-2-2-patches-for-CIFAR-10-32-32-图片" class="headerlink" title="（1）ICLR 2020 2 * 2 patches for CIFAR-10 32 * 32 图片"></a>（1）ICLR 2020 2 * 2 patches for CIFAR-10 32 * 32 图片</h3><p><strong>ViT 胜在哪里:</strong> 更大的 patches 16 *16 + 更大的训练数据集</p><p>CV 中 检测、分类、视频处理、多模态 self-attention with CNNs</p><h3 id="（2）image-GPT"><a href="#（2）image-GPT" class="headerlink" title="（2）image GPT"></a><strong>（2）image GPT</strong></h3><p>GPT 是 NLP 的生成模型，image GPT 无监督预训练，生成模型。</p><p>image GPT 也用了 transformer 图片（降低分辨率和 color space）。用训练好的 image GPT or 直接把 image GPT 当成特征提取器， ImageNet 准确率 72%；ViT ImageNet 准确率 88.5%</p><h3 id="（3）最近爆火的-MAE"><a href="#（3）最近爆火的-MAE" class="headerlink" title="（3）最近爆火的 MAE"></a><strong>（3）最近爆火的 MAE</strong></h3><p>在 BEiT 或 MAE 论文之前，生成式网络 在 CV 比 判别式网络 弱很多。</p><p>MAE 生成式模型 在 ImageNet-1k 做训练，比判别式模型好。分类，目标检测  （transfer learning）都有不错的效果。</p><h1 id="6-ViT模型"><a href="#6-ViT模型" class="headerlink" title="6.ViT模型"></a><strong>6.ViT模型</strong></h1><p>ViT 尽可能使用 original Transformer，享受 Transformer 的高效实现。</p><h2 id="6-1-Vision-Transformer"><a href="#6-1-Vision-Transformer" class="headerlink" title="6.1 Vision Transformer"></a><strong>6.1 Vision Transformer</strong></h2><h3 id="（1）Model"><a href="#（1）Model" class="headerlink" title="（1）Model"></a>（1）Model</h3><p><img src="image/image_0cloNkz9xq.png" alt=""></p><p><img src="image/image_Un5thXW-QB.png" alt=""></p><p><strong>ViT 对 图片的操作</strong>： 划分 patches，flatten patches 的线性投影 + patches 的位置信息，得到输入 transformer 的 tokens</p><p><strong>如何分类</strong>：借鉴 BERT，插入一个特殊的字符 <code>* [CLS]</code>，用于分类。<code>* [CLS]</code>也有 position embedding, 0(永远是0)。<code>* [CLS]</code> 输入 一个通用的 MLP Head，得到 Class，cross-entropy 损失函数训练模型。</p><p>图像示例（维度变化），以224*224输入图像为例：</p><ul><li>**图片 **$X$： <code>224 * 224 * 3</code>(RGB, 3 channels)</li><li>$ patches  $**数 **$N$： <code>224 ^ 2 / 16 ^ 2 = 14 ^ 2 = 196</code></li><li><strong>每一个 patch 的维度</strong>：<code>16 * 16 * 3 (RGB, 3 channels) = 768</code></li><li>Linear Projection 全连接层 $E$输入：<code>768( 不变，patch 计算而来 ) * D(embedding_dim， 768 或 更大)</code></li><li>**Linear Projection 全连接层 **$E$<strong>输出</strong>：<code>X * E = patches (196 patches 个数 * 768 每个 patch 的维度) * E ( 768 * D ) = 196 * D (768)</code></li><li>position embedding向量维度： <code>1 * 768</code>，通过sum加到输入信息中；</li><li>进入Transformer Encoder维度：196 * 768(图片对应的 tokens) 拼接 concatenate [CLS] token (1 * 768) = <code>197 * 768</code></li><li>ViT base MLA： <code>12 heads</code>；</li><li><strong>MLP</strong>：放大 <code>4</code> 倍，再缩小到原维度大小</li><li>Transfomer encoder 输入输出维度一致，可以直接叠加 <code>L</code> 个</li></ul><p><img src="image/image_HTcwHJy9-L.png" alt=""></p><h3 id="（2）ViT和CNN"><a href="#（2）ViT和CNN" class="headerlink" title="（2）ViT和CNN"></a>（2）ViT和CNN</h3><p><strong>class token：证明 标准的 transformer 做视觉，没问题！</strong></p><p>ViT 除了标准的 transformer，关键部分是 怎么对图片进行预处理 和 怎么对图片最后的输出进行后处理。</p><p>控制和 NLP 的差异：使用 BERT 的 CLS，CLS 在 NLP 理解为 一个全局的对句子理解的特征；ViT 的 CLS 理解为 一个图像的整体特征。</p><ul><li>CLS token + MLP (tanh acitvation) == 分类</li><li>CV 通常的 全局特征：(i.e., Res50)  feature map (14 * 14) —&gt; GAP globally average-pooling 全局平均池化 —&gt; a flatten vector 全局的图片特征向量 —&gt; MLP 分类</li><li>CLS 可用 GAP global average pooling 替换</li><li><strong>CV 的 CLS GAP 和 NLP 的 CLS 效果差异不大</strong>**，<strong>为了和原始Transformer保持统一</strong>。** 但CLS-Token 和 GAP 的 适用参数 不一样。</li></ul><p><strong>位置编码：</strong> 1d、2d、 relative编码都相差不大，为了和原始Transformer保持一致，使用1D编码。</p><ul><li>猜测相差不大原因：ViT 直接作用于 14 * 14 patches，而不是 224 * 224 像素。较少数量的 patches 之间的相对位置信息，容易学到。</li></ul><h3 id="（3）Inductive-bias"><a href="#（3）Inductive-bias" class="headerlink" title="（3）Inductive bias"></a><strong>（3）Inductive bias</strong></h3><p><strong>CNN 的 inductive bias</strong>： locality 局部性、 translation equivalence 平移等变性。在 CNN 模型每一层都有所体现，模型的先验知识从头到尾，贯穿整个模型。</p><p><strong>ViT 比 CNN 的 inductive bias 少, only MLP</strong></p><p><strong>ViT 的 inductive bias in images</strong>：图片 切成 patches；+ position embedding（随机初始化，没有携带 2d 位置信息）</p><p>ViT 的 patches 块的 2d 位置信息 + spatial relations 图像块之间的场景信息，都需要重新学。所以 <strong>ViT 没有很多 inductive bias</strong>，在中小型数据集训练 ViT 效果不如 CNN。</p><h3 id="（4）Hybrid-architecture"><a href="#（4）Hybrid-architecture" class="headerlink" title="（4）Hybrid architecture"></a>（4）<strong>Hybrid architecture</strong></h3><p>Transformer：全局建模能力强</p><p>CNN： data-efficient 不用那么多训练数据</p><p>前 CNN + 后 Transformer —&gt; Hybrid archtecture</p><p><strong>不同的图片预处理方式</strong>：不划分 patches，采用 CNN (Res50 的 feature map 14 * 14 = 196)，过全连接层 <strong>E</strong> Linear projections 得到图片的 embedding</p><p><strong>ViT 的图片预处理方式</strong>**：** 把一张图划分成 patches，直接过全连接层 FC</p><p><img src="image/image_E2mWR1qp4h.png" alt=""></p><h2 id="6-2-Fine-tuning-and-higher-resolution"><a href="#6-2-Fine-tuning-and-higher-resolution" class="headerlink" title="6.2 Fine-tuning and higher resolution"></a><strong>6.2 Fine-tuning and higher resolution</strong></h2><h3 id="（1）预训练好的-ViT-可以在更大尺寸的图片上微调吗？"><a href="#（1）预训练好的-ViT-可以在更大尺寸的图片上微调吗？" class="headerlink" title="（1）预训练好的 ViT 可以在更大尺寸的图片上微调吗？"></a><strong>（1）预训练好的 ViT 可以在更大尺寸的图片上微调吗？</strong></h3><p>if patch size 不变 16 * 16，更大尺寸的图片 —&gt; 序列长度的增加；Transformer 理论上，可以处理任意长度。* <em>But，</em> *<strong>提前训练好的 position embedding 可能失效</strong>。</p><h3 id="2-patches-数增多，如何使用已预训练好的位置编码吗？-​"><a href="#2-patches-数增多，如何使用已预训练好的位置编码吗？-​" class="headerlink" title="** (2) patches 数增多，如何使用已预训练好的位置编码吗？** ​"></a>** (2) patches 数增多，如何使用已预训练好的位置编码吗？** ​</h3><p>不可以。但可以使用2d 插值，增加位置编码的长度，但效果会掉点，是临时解决方案，ViT 微调时的一个局限。</p><ul><li>2d 插值，torch 的 interpolate 函数实现；但也不是任意长度增加都能保持效果。</li></ul><p>ViT 用了图片 2d 结构 的 inductive bias 地方：resolution adjustment 尺寸改变 和 patch extraction 抽 patches</p><h1 id="7-实验"><a href="#7-实验" class="headerlink" title="7.实验"></a><strong>7.实验</strong></h1><p>对比 <code>ResNet</code>, <code>ViT</code>, <code>Hybrid ViT</code> (CNN 特征图，不是图片直接 patch 化) 的 representation learning capabilities 表征学习能力。</p><p>为了了解每个模型预训练好 到底需要多少数据，在不同大小的数据集上预训练，然后在很多 benchmark tasks 做测试。</p><p>考虑模型预训练的计算成本时，ViT performs very favourably 表现很好， SOTA + fewer resource 训练时间更少；</p><p>ViT 的自监督训练，可行，效果也还不错，有潜力；一年之后，MAE 用自监督训练 ViT 效果很好。</p><p>数据集：</p><ul><li><code>ImageNet-1K</code>: 1000 classes, 1.3M images</li><li><code>ImageNet-21K</code>: 21000 classes, 14M images</li><li><code>JFG-300</code>: 303M images Google 不开源</li><li>下游任务：分类 CFIAR etc.</li></ul><h2 id="7-1-ViT-model-variants"><a href="#7-1-ViT-model-variants" class="headerlink" title="7.1 ViT model variants"></a>7.1 ViT model variants</h2><p><img src="image/image_96dem1nNZT.png" alt=""></p><p>模型变体 = (Base, Large, Hugh) + (patch size 表示)</p><p>ViT-L/16 使用 Large 参数 和 patch 16 * 16 输入</p><h2 id="7-2-结果"><a href="#7-2-结果" class="headerlink" title="7.2 结果"></a>7.2 结果</h2><p>和 CNN 的工作 BiT-L, Noisy Student 做对比</p><ul><li><strong>BiT-L:</strong> CNN比较大的模型，ViT论文作者团队自己的工作</li><li><strong>Noisy Student:</strong> ImageNet 当时表现最好的方法。用 伪标签 pseudo-label 去 self-training</li></ul><p><img src="image/image_aM_NJtUFRn.png" alt=""></p><p>ViT-H/14 训练比 ViT-H/16 贵，效果和 BiT-L 差不多，优势不明显。<strong>怎么突出 ViT 的好呢？</strong></p><p><strong>ViT 训练更便宜</strong>。TPUv3 天数：ViT-H/14 2.5K, BiT-L 9.9K, Noisy Student 12.3K</p><p><strong>ViT 优点：效果好 + 训练快</strong></p><h2 id="7-3-结果分析"><a href="#7-3-结果分析" class="headerlink" title="7.3 结果分析"></a>7.3 <strong>结果分析</strong></h2><h3 id="（1）Vision-Transformer-到底需要多少数据才能训练好？"><a href="#（1）Vision-Transformer-到底需要多少数据才能训练好？" class="headerlink" title="（1）Vision Transformer 到底需要多少数据才能训练好？"></a>（1）Vision Transformer 到底需要多少数据才能训练好？</h3><p>图中灰色区域 ResNet 的效果，圆圈 ViT 的效果</p><p><img src="image/image_o0i2Wm3-gE.png" alt=""></p><p>如果想用 ViT，至少需要 ImageNet-21K 14M 大小的数据集</p><ul><li>小于整个数据量，CNN 更合适，更好的利用 inductive bias，ViT 没有特别多 inductive bias 需要更多数据训练。</li></ul><p>数据集规模比 ImageNet-21K 更大时，Vision Transformer 效果更好，因为可扩展性 scaling 更好。</p><h3 id="（2）Linear-few-shot-evaluation"><a href="#（2）Linear-few-shot-evaluation" class="headerlink" title="（2）Linear few-shot evaluation"></a><strong>（2）Linear few-shot evaluation</strong></h3><p><img src="image/image_pv4zY639Ju.png" alt=""></p><p><strong>linear evalution</strong>：把 ViT 预训练好的模型 直接作为 特征提取器，不 fine-tune，+ 一个 logistic regression 得到分类结果。</p><p><strong>Few-shot</strong>：5-shot，在 ImageNet 做 linear evaluation 时，每类图片随机选取 5 个 samples，evaluation 很快，做 消融实验。</p><p>linear few-shot evaluation 采用 JFT 数据集 10M, 30M, 100M, 300M。来自同一个数据集，数据没有 distribution gap，模型的效果更能体现 Vision Transformer 本身特质。</p><p>ViT 图中 效果 和 上图差不多。</p><p><strong>如何用 ViT 做小样本学习，未来研究方向之一。</strong>&#x20;</p><h3 id="（3）用-ViT-比-CNNs-便宜"><a href="#（3）用-ViT-比-CNNs-便宜" class="headerlink" title="（3）用 ViT 比 CNNs 便宜"></a>（3）<strong>用 ViT 比 CNNs 便宜</strong></h3><p>大家的印象：Transformer 又大又贵，很难训练</p><p><img src="image/image_XOqmsgtP7e.png" alt=""></p><ul><li><strong>average-5</strong>：ImageNet-real, Pets, Flower, CIFAR10, CIFAR100 平均</li><li>ImageNet 单独的对比</li></ul><p><strong>同等计算复杂度：ViT 比 ResNet 效果好，印证了 ViT 训练更便宜</strong></p><p><strong>Q：Hybrid 模型，CNN 抽取出来的特征，能不能帮助 Transformer 更好的学习呢？</strong></p><ul><li>小模型，Hybrid 模型吸收 CNN 和 Transformer 的优点，效果好。不需要很多的数据预训练，达到 Transformer 的效果</li><li>大模型，Hybrid 模型 和 Transformer 差不多，甚至不如 Transformer 模型。</li></ul><h2 id="7-4-Inspecting-Vision-Transformer"><a href="#7-4-Inspecting-Vision-Transformer" class="headerlink" title="7.4 Inspecting Vision Transformer"></a><strong>7.4 Inspecting Vision Transformer</strong></h2><p>可视化分析 ViT 内部表征 internal representations: <strong>Patch embedding, position embedding</strong></p><h3 id="（1）-Linear-projection-E"><a href="#（1）-Linear-projection-E" class="headerlink" title="（1）** Linear projection E **"></a>（1）** Linear projection E **</h3><p><strong>ViT 第一层 Linear projection E 学到了什么？</strong></p><p>embed RGB value 前 28 个主成分</p><p><img src="image/image_P_ocxXsixV.png" alt=""></p><p>Vision Transformer 和 CNN 学到的很像，类似 gabor filter 有颜色、纹理， 可以做 plausible basis functions，可以描述每个图像块的底层信息 a low-dimensional representation of the fine structure within each patch.</p><h3 id="（2）Position-embedding"><a href="#（2）Position-embedding" class="headerlink" title="（2）Position embedding"></a>（2）<strong>Position embedding</strong></h3><p><strong>Position embedding 能学到一些表示位置距离的信息</strong></p><p><img src="image/image_1XvQ8mrrd2.png" alt=""></p><ul><li>patch 自己本身 相似度高 黄色 1</li><li>学到了距离的概念</li><li>(4, 4) 黄色中心点，越边缘，相似度越低，颜色越蓝</li><li>学到了 行 和 列 的距离规则</li><li>同行同列，颜色条 的表示</li></ul><p>虽然是 1d 的 position embedding，但已经学到了 2d 的图像位置概念；所以换成 2d position 提升不多。</p><h3 id="（3）Self-attention-有没有起作用？"><a href="#（3）Self-attention-有没有起作用？" class="headerlink" title="（3）Self-attention 有没有起作用？"></a>（3）<strong>Self-attention 有没有起作用？</strong></h3><p>用 Transformer 的原因：自注意力 能模拟长距离的关系。</p><ul><li>NLP 一个很长的句子里，开头的一个词和结尾的一个词 可能互相有关联。</li><li>CV 里 很远的两个像素点之间 也能做自注意力。</li></ul><p><strong>ViT 的 self-attention 是不是 很远的像素点也能有交互？</strong></p><p>ViT-L/16 有 24 层（横坐标值），五颜六色的点：transformer 每层 multi-head 的heads，ViT-L 16 heads, 每一列有 16 个点</p><p><img src="image/image_RBoM5A-PX8.png" alt=""></p><p>纵轴是 mean attention distance</p><p>$d_{ab} = l_{ab} <em> A_{ab} = ab 两点 pixel 之间的距离差 </em> ab 两点之间的attention ~weights$</p><p>d_ab 的大小，反映模型能不能注意到很远的 2 个 pixels</p><ul><li>self-attention 刚开始能注意到 10 - 110 pixels</li><li>self-attention 刚开始就注意到全局的信息；CNN 刚开始第一层的感受野 receptive filed 很小，只能看到附近的 pixel</li></ul><p>网络加深，模型学到的特征越来越 high level，越来越有语义信息，像素的自注意力距离 越来越远，不是靠邻近的像素点做判断。</p><p><strong>证明 自注意力 有学到 很远距离的 pixel 信息，</strong> ViT 最后一层 output 的 token 的 self-attention 折射（逆向映射）回 原来的输入图片。ViT 真的学到了一些概念：狗、飞机</p><p><img src="image/image_gAoexe0iVB.png" alt=""></p><p>Globally 全局来说，输出的 token 是融合全局的特征信息，ViT 模型可以关注到 和 classfication 分类相关的图像区域。</p><h2 id="7-5-self-supervision"><a href="#7-5-self-supervision" class="headerlink" title="7.5 self-supervision"></a><strong>7.5 self-supervision</strong></h2><p>如何用 自监督 的方式 训练一个 vision transformer？</p><p>因为 NLP 的 transformer 都是用 large scale self-supervised pre-training <strong>大规模、自监督</strong> 的方式预训练的。</p><p><strong>NLP 的 自监督</strong>：BERT <strong>完形填空</strong> Mask language model，GPT 生成，<strong>预测下一个词</strong> by language model</p><p>ViT 借鉴 BERT，创建一个专属于 vision 的目标函数，<strong>masked patch prediction</strong>。一张图片的某些 patches 随机抹掉，ViT 重建缺失的patches</p><p><strong>Note：从 模型、目标函数上，CV 和 NLP 的大一统。</strong></p><p>但是，ViT-B/16 with masked patch prediction 在 ImageNet ~80% 准确率。~80% 比 从头训练 ViT 好 2%，比 supervised pre-training 低 4%。</p><h1 id="8-评论"><a href="#8-评论" class="headerlink" title="8.评论"></a><strong>8.评论</strong></h1><h2 id="8-1-写作"><a href="#8-1-写作" class="headerlink" title="8.1 写作"></a>8.1 写作</h2><p>写作：简洁明了、有轻有重（重要结果放正文），图表清晰。</p><p>内容：Vision Transformer 挖了一个大坑：各个角度的分析，提升 or 推广</p><p>task 任务角度：ViT 只做了分类，检测、分割、其它领域的任务 future work</p><p>ViT 结构的角度：</p><ul><li>改刚开始的 tokenization</li><li>改 transformer block, i.e., self-attention 换成 MLP works</li><li>MetaFormer 认为 transformer work 的原因是 transformer 的架构，不是 transformer 某些特殊的算子</li><li>MetaFormer，self-attention 改成 （不能学习的）pooling 池化操作；甚至改成 Identity，不用注意力</li><li>改 目标函数：有监督、or 不同的自监督训练方式</li></ul><h2 id="8-2-ViT"><a href="#8-2-ViT" class="headerlink" title="8.2 ViT"></a>8.2 ViT</h2><p>ViT 的大坑：</p><ul><li><strong>打通了 CV 和 LP 之间的鸿沟</strong></li><li>挖了一个更大的<strong>多模态</strong>的坑</li><li>视频、音频、基于 touch 的信号</li><li>各种 modality 的信号都可以拿来用</li></ul><p><strong>CNN, self-attention, MLP 鹿死谁手？</strong> 犹未可知，期待下一个改进的 vision transformer</p><ul><li>一个简洁、高效、通用的视觉骨干网络 CV backbone，甚至完全不用任何标注信息</li></ul>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> PaperReading </tag>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer中的注意力</title>
      <link href="/llms/transformer/5.Transformer%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
      <url>/llms/transformer/5.Transformer%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B/</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer中的注意力"><a href="#Transformer中的注意力" class="headerlink" title="Transformer中的注意力"></a>Transformer中的注意力</h1><p>本文主要来自：<a href="https://jalammar.github.io/illustrated-transformer/" title="The Illustrated Transformer">The Illustrated Transformer</a></p><h1 id="1-自注意力"><a href="#1-自注意力" class="headerlink" title="1.自注意力"></a>1.自注意力</h1><p>假设我们要翻译下边这句话：“The animal didn’t cross the street because it was too tired”。这里<code>it</code>指的是什么？是<code>street</code>还是<code>animal</code>？人理解起来很容易，但是对算法来讲就不那么容易了。</p><p><strong>当模型处理it这个词的时候，自注意力会让**</strong><code>it</code><strong><strong>和</strong></strong><code>animal</code><strong>**关联起来</strong>。</p><p>当模型编码每个位置上的单词的时候，自注意力的作用就是：看一看输入句子中其他位置的单词，试图寻找一种对当前单词更好的编码方式。</p><p>如果你熟悉RNNs模型，回想一下RNN如何处理当前时间步的隐藏状态：将之前的隐藏状态与当前位置的输入结合起来。</p><p>在Transformer中，自注意力机制也可以将其他相关单词的“理解”融入到我们当前处理的单词中。</p><p><img src="image/image_tFpf1GBUq8.png" alt=""></p><p>可以去<a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb" title="Tensor2Tensor">Tensor2Tensor</a> ，自己体验一下上图的可视化。动图如下所示：</p><p><img src="image/attention_35nwxhUMX3.gif" alt=""></p><h2 id="2-图解注意力计算"><a href="#2-图解注意力计算" class="headerlink" title="2.图解注意力计算"></a>2.图解注意力计算</h2><p>先画图用向量解释一下自注意力是怎么算的，之后再看一下实际实现中是怎么用矩阵算的。</p><h4 id="第一步：计算query、key、value向量"><a href="#第一步：计算query、key、value向量" class="headerlink" title="第一步：计算query、key、value向量"></a><strong>第一步</strong>：计算query、key、value向量</h4><p>对编码器的每个输入向量都计算三个向量，就是对每个输入向量都算一个query、key、value向量。</p><p>把输入的词嵌入向量与三个权重矩阵相乘。权重矩阵是模型训练阶段训练出来的。</p><p><strong>注意</strong>：这三个向量维度是64，比嵌入向量的维度小，嵌入向量、编码器的输入输出维度都是512。这三个向量<strong>不是必须</strong>比编码器输入输出的维数小，这样做主要是为了让多头注意力的计算更稳定。</p><p><img src="image/image_fvOLMgXzfg.png" alt=""></p><p>将$x_1$和$W^Q$权重矩阵相乘得到$q_1$，就得到与该单词（$x_1$）相关的查询（query）。按照这样的方法，最终我们给输入的每个单词都计算出一个”query”，一个”key” 和一个”value”。</p><h4 id="第二步：计算注意力得分"><a href="#第二步：计算注意力得分" class="headerlink" title="第二步：计算注意力得分"></a>第二步：计算注意力得分</h4><p>假设我们现在在计算输入中第一个单词<code>Thinking</code>的自注意力。我们需要使用自注意力给输入句子中的每个单词打分，这个分数决定当我们编码某个位置的单词的时候，应该对其他位置上的单词给予多少关注度。</p><p>这个得分是query和key的点乘积得出来的。</p><p>举个栗子，我们要算第一个位置的注意力得分的时候就要将第一个单词的query和其他的key依次相乘，在这里就是 $q_1 \cdot k_1$，$q_1 \cdot k_2$</p><p><img src="image/image_vNyb--4Pct.png" alt=""></p><h4 id="第三步：将计算获得注意力分数除以-sqrt-d-8"><a href="#第三步：将计算获得注意力分数除以-sqrt-d-8" class="headerlink" title="第三步：将计算获得注意力分数除以 $\sqrt{d}=8$"></a>第三步：将计算获得注意力分数除以 $\sqrt{d}=8$</h4><p>为什么选8？是因为key向量的维度是64，取其平方根，这样让梯度计算的时候更稳定。</p><h4 id="第四步：归一化softmax"><a href="#第四步：归一化softmax" class="headerlink" title="第四步：归一化softmax"></a>第四步：归一化softmax</h4><p>&#x20;除8之后将结果扔进softmax计算，使结果归一化，softmax之后注意力分数相加等于1，并且都是正数。</p><p><img src="image/image_2DzVZj8Etr.png" alt=""></p><p>这个softmax之后的注意力分数表示 在计算当前位置的时候，其他单词受到的关注度的大小。显然在当前位置的单词肯定有一个高分，但是有时候也会注意到与当前单词相关的其他词汇。</p><h4 id="第五步：计算注意力分数"><a href="#第五步：计算注意力分数" class="headerlink" title="第五步：计算注意力分数"></a>第五步：计算注意力分数</h4><p>将每个value向量乘以注意力分数。这是为了留下我们想要关注的单词的value，并把其他不相关的单词丢掉。</p><p>在第一个单词位置得到新的$v_1$</p><h4 id="第六步：计算注意力结果"><a href="#第六步：计算注意力结果" class="headerlink" title="第六步：计算注意力结果"></a>第六步：计算注意力结果</h4><p>将上一步的结果相加，输出本位置的注意力结果。第一个单词的注意力结果就是 $z_1$。</p><p><img src="image/image_AV_bivY4tR.png" alt=""></p><p>这就是自注意力的计算。计算得到的向量直接传递给前馈神经网络。但是为了处理的更迅速，实际是用矩阵进行计算的。接下来我们看一下怎么用矩阵计算。</p><h2 id="3-矩阵计算self-attention"><a href="#3-矩阵计算self-attention" class="headerlink" title="3.矩阵计算self-attention"></a>3.矩阵计算self-attention</h2><p>计算Query, Key, Value矩阵。直接把输入的向量打包成一个矩阵$X$，再把它乘以训练好的$\color{purple}{W^Q}$, $\color{yellow}{W^K}$,$  \color{cyan}{W^V} $.</p><p>X矩阵每一行都代表输入句子中的一个单词，整个矩阵代表输入的句子。</p><p><img src="image/image_DG4O6x-sHq.png" alt=""></p><blockquote><p>论文中词嵌入矩阵维度维512，q、k、v矩阵的长度为64，这里分别使用4个格子和3个格子表示。</p></blockquote><p>因为我们现在用矩阵处理，所以可以直接将之前的第二步到第六步压缩到一个公式中一步到位获得最终的注意力结果 $\color{pink}{Z}$</p><p><img src="image/image_5Zhltf8IU3.png" alt=""></p><h2 id="4-多头自注意力"><a href="#4-多头自注意力" class="headerlink" title="4.多头自注意力"></a>4.多头自注意力</h2><p>论文进一步改进了自注意力层，增加了一个机制，也就是多头注意力机制。这样做有两个好处：</p><ol><li>扩展了模型专注于不同位置的能力</li><li>给了注意力层多个“表示子空间”</li></ol><h3 id="4-1-扩展了模型专注于不同位置的能力"><a href="#4-1-扩展了模型专注于不同位置的能力" class="headerlink" title="4.1 扩展了模型专注于不同位置的能力"></a>4.1 扩展了模型专注于不同位置的能力</h3><p>在上面例子里只计算一个自注意力的的例子中，编码“Thinking”的时候，虽然最后 $Z_1$或多或少包含了其他位置单词的信息，但是它实际编码中还是被“Thinking”单词本身所支配。</p><p>如果我们翻译一个句子，比如“The animal didn’t cross the street because it was too tired”，我们会想知道“it”指的是哪个词，这时模型的“多头”注意力机制会起到作用。</p><h3 id="4-2-给了注意力层多个“表示子空间”"><a href="#4-2-给了注意力层多个“表示子空间”" class="headerlink" title="4.2 给了注意力层多个“表示子空间”"></a>4.2 给了注意力层多个“表示子空间”</h3><p>就是在多头注意力中同时用多个不同的$\color{purple}{W^Q}$, $\color{yellow}{W^K}$,$  \color{cyan}{W^V} $权重矩阵(Transformer使用8个头部，因此我们最终会得到8个计算结果)，每个权重都是随机初始化的。经过训练每个$\color{purple}{W^Q}$, $\color{yellow}{W^K}$,$  \color{cyan}{W^V} $都能将输入的矩阵投影到不同的表示子空间。</p><p><img src="image/image_4TDT9YHSs_.png" alt=""></p><p>Transformer中的一个多头注意力（有8个head）的计算，就相当于用自注意力做8次不同的计算，并得到8个不同的结果$\color{pink}{Z}$</p><p><img src="image/image_nWN7f8tXSd.png" alt=""></p><p>但是这会存在一点问题，多头注意力出来的结果会进入一个前馈神经网络，这个前馈神经网络可不能一下接收8个注意力矩阵，它的输入需要是单个矩阵（矩阵中每个行向量对应一个单词），所以我们需要一种方法把这8个压缩成一个矩阵。</p><p><img src="image/image_PxWLZypxsk.png" alt=""></p><p>以上就是多头自注意力的全部内容。让我们把多头注意力上述内容 放到一张图里看一下子：</p><p><img src="image/image_kcyI07MfCJ.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为何Transformer在计算机视觉中如此受欢迎？</title>
      <link href="/llms/transformer/4.Transformer%E4%B8%BA%E5%95%A5%E8%BF%99%E4%B9%88%E6%AC%A2%E8%BF%8E/"/>
      <url>/llms/transformer/4.Transformer%E4%B8%BA%E5%95%A5%E8%BF%99%E4%B9%88%E6%AC%A2%E8%BF%8E/</url>
      
        <content type="html"><![CDATA[<p>转自：<a href="https://www.msra.cn/zh-cn/news/features/cv-transformer">为何Transformer在计算机视觉中如此受欢迎？ (msra.cn)</a></p><p>2021-09-24 | 作者：胡瀚</p><p>编者按：近一年来，Transformer 在计算机视觉领域所带来的革命性提升，引起了学术界的广泛关注，有越来越多的研究人员投入其中。Transformer 的特点和优势是什么？为什么在计算机领域中 Transformer 可以频频出圈？让我们通过今天的文章来一探究竟吧！</p><hr><p>“统一性”是很多学科共同追求的目标，例如在物理学领域，科学家们追求的大统一，就是希望用单独一种理论来解释力与力之间的相互作用。人工智能领域自然也存在着关于“统一性”的目标。在深度学习的浪潮中，人工智能领域已经朝着统一性的目标前进了一大步。比如，一个新的任务基本都会遵循同样的流程对新数据进行预测：收集数据，做标注，定义网络结构，训练网络参数。</p><p>但是，在人工智能的不同子领域中，基本建模的方式各种各样，并不统一，例如：在自然语言处理（NLP）领域目前的主导建模网络是 Transformer；计算机视觉（CV）领域很长一段时间的主导网络是卷积神经网络（CNN）；社交网络领域目前的主导网络则是图网络等。</p><p>尽管如此，从2020年年底开始，Transformer 还是在 CV 领域中展现了革命性的性能提升。这就表明 CV 和 NLP 有望统一在 Transformer 结构之下。这一趋势对于两个领域的发展来说有很多好处：<strong>1）使视觉和语言的联合建模更容易；2）两个领域的建模和学习经验可以深度共享，从而加快各自领域的进展</strong>。</p><h2 id="1-Transformer在视觉任务中的优异性能"><a href="#1-Transformer在视觉任务中的优异性能" class="headerlink" title="1.Transformer在视觉任务中的优异性能"></a>1.Transformer在视觉任务中的优异性能</h2><p>视觉 Transformer 的先驱工作是谷歌在 ICLR 2021 上发表的 ViT [1]，该工作把图像分成多个图像块（例如16x16像素大小），并把这些图像块比作 NLP 中的 token。然后直接将 NLP 中的标准 Transformer 编码器应用于这些 “token”，并据此进行图像分类。该工作结合了海量的预训练数据（如谷歌内部3亿图片分类训练库 JFT-300M），在 ImageNet-1K 的 validation 评测集上取得了88.55%的准确率，刷新了该榜单上的纪录。</p><p>ViT 应用 Transformer 比较简单直接，因为其<strong>没有仔细考虑视觉信号本身的特点</strong>，所以它主要适应于图像分类任务，对于区域级别和像素级别的任务并不是很友好，例如物体检测和语义分割等。为此，学术界展开了大量的改进工作。其中，Swin Transformer 骨干网络 [2] 在物体检测和语义分割任务中大幅刷新了此前的纪录，让学术界更加确信 Transformer 结构将会成为视觉建模的新主流。</p><p>具体而言，在物体检测的重要评测集 COCO 上，Swin Transformer 取得了单模型58.7的 box mAP 和51.1的 mask mAP，分别比此前最好的、没有扩充数据的单模型方法高出了+2.7个点和+2.6个点。此后，通过改进检测框架以及更好地利用数据，基于 Swin Transformer 网络的方法性能进一步取得了61.3的 box mAP 和53.0的 mask mAP，累计提升达+5.3 box mAP 和+5.5 mask mAP。在语义分割的重要评测数据集 ADE20K 上，Swin Transformer 也取得了显著的性能提升，达到了53.5 mIoU，比此前最好的方法高出+3.2 mIoU，此后随着分割框架和训练方法的进一步改进，目前已达到57.0 mIoU 的性能。</p><p><img src="images/1686120321197.png" alt="1686120321197" title="*图1 历年COCO物体检测评测集上的记录*"></p><center><p>图1 历年COCO物体检测评测集上的记录</p></center><p>除了在物体检测和语义分割任务上表现亮眼外，基于 Swin Transformer 骨干网络的方法在众多视觉任务中也取得了优异的成绩，如视频动作识别 [3]、视觉自监督学习 [4] [5]、图像复原 [6]、行人 Re-ID [7]、医疗图像分割 [8]等。</p><p>Swin Transformer 的主要思想是<strong>将具有很强建模能力的 Transformer 结构和重要的视觉信号先验结合起来</strong>。这些先验具有<strong>层次性（Hierarchy）</strong>、<strong>局部性（locality）</strong>以及<strong>平移不变性的特点（translation invariance）</strong>。Swin Transformer 的一个重要设计是移位的不重叠窗口（shifted windows），不同于传统的滑动窗，不重叠窗口的设计对硬件实现更加友好，从而具有更快的实际运行速度。如图2（左）所示，在滑动窗口设计中，不同的点采用了不同的邻域窗口来计算相互关系，这种计算对硬件并不友好。而如图2（右）所示，Swin Transformer 使用的不重叠窗口中，统一窗口内的点将采用相同的邻域来进行计算，对速度更友好。实际测试表明，非重叠窗口方法的速度比滑动窗口方法快了2倍左右。在两个连续的层中还做了移位的操作。在 L 层中，窗口分区从图像的左上角开始；在 L+1 层中，窗口划分则往右下移动了半个窗口。这样的设计保证了不重叠的窗口间可以有信息的交换。</p><p><img src="images/1686128650551.png" alt="1686128650551" title="*图2：传统的滑动窗口方法（左），由于不同的查询所用到的关键字集合不同，其对存储的访问不太友好，实际运行速度较慢。移位的不重叠窗口方法（右），由于不同的查询共享关键字集合，所以实际运行速度更快，从而更实用。*"></p><center><p>图2：传统的滑动窗口方法（左），由于不同的查询所用到的关键字集合不同，其对存储的访问不太友好，实际运行速度较慢。移位的不重叠窗口方法（右），由于不同的查询共享关键字集合，所以实际运行速度更快，从而更实用。</p></center><p>在过去的大半年中，学术界视觉 Transformer 还涌现了大量变种，包括 DeiT [9]，LocalViT [10]，Twins [11]，PvT [12]，T2T-ViT [13], ViL [14]，CvT [15]，CSwin [16]，Focal Transformer [17]，Shuffle Transformer [18] 等。</p><h2 id="2-拥抱Transformer的五个理由"><a href="#2-拥抱Transformer的五个理由" class="headerlink" title="2.拥抱Transformer的五个理由"></a>2.拥抱Transformer的五个理由</h2><p>除了刷新很多视觉任务的性能纪录以外，视觉 Transformer 还拥有诸多好处。事实上，过去4年间学术界不断挖掘出了 Transformer 建模的各种优点，可以总结为图3所示的五个方面。</p><p><img src="images/1686128721835.png" alt="1686128721835" title="*图3：过去4年学术界不断挖掘出的 Transformer 建模的五个优点*"></p><center><p>图3：过去4年学术界不断挖掘出的 Transformer 建模的五个优点</p></center><h3 id="理由1：通用的建模能力"><a href="#理由1：通用的建模能力" class="headerlink" title="理由1：通用的建模能力"></a>理由1：通用的建模能力</h3><p>Transformer 的通用建模能力来自于两个方面：一方面 <strong>Transformer 可以看作是一种图建模方法</strong>。图是全连接的，节点之间的关系通过数据驱动的方式来学习得到。由于任意概念（无论具体或抽象）都可以用图中的节点来表示，且概念之间的关系可以用图上的边来刻画，因此 Transformer 建模具有很强的通用性。</p><p>另一方面，<strong>Transformer 通过验证的哲学来建立图节点之间的关系</strong>，具有较好的通用性：无论节点多么异构，<strong>它们之间的关系都可以通过投影到一个可以比较的空间里计算相似度来建立</strong>。如图4（右）所示，节点可以是不同尺度的图像块，也可以是“运动员”的文本输入，Transformer 均可以刻画这些异构节点之间的关系。</p><p><img src="images/1686130053895.png" alt="1686130053895" title="*图4：促成 Transformer 通用建模能力的两大原因：图建模（左）和验证哲学（右）*"></p><center><p>图4：促成 Transformer 通用建模能力的两大原因：图建模（左）和验证哲学（右）</p></center><p>正是因为具备这样的通用建模能力，Transformer 中的注意力单元可以被应用到各种各样的视觉任务中。具体而言，计算机视觉处理的对象主要涉及两个层次的基本元素：像素和物体。而计算机视觉所涉及到的任务主要就囊括了这些基本元素之间的关系，包括像素-像素，物体-像素和物体-物体的关系建模。此前，前两种关系建模主要是分别由卷积和 RoIAlign 来实现的，最后一种关系通常没有很好的建模方法。但是，Transformer 中的注意力单元因其通用的建模能力，可以被应用到所有这些基本关系的建模中。</p><p>近些年，在这个领域中已经出现了很多代表性的工作，例如：1） 非局部网络 [19]。王小龙等人将注意力单元用于建模像素-像素的关系，证明了 Transformer 可以帮助视频动作分类和物体检测等任务。元玉慧等人将其应用于语义分割问题，也取得了显著的性能提升[20]。2）物体关系网络 [21]。注意力单元用于物体检测中的物体关系建模，这一模块也被广泛应用于视频物体分析中 [22, 23, 24]。3）物体和像素的关系建模，典型的工作包括 DETR [25]，LearnRegionFeat [26]，以及 RelationNet++ [27]等。</p><p><img src="images/1686130290248.png" alt="1686130290248" title="*图5：Transformer 能被应用于各种视觉基本元素之间的关系建模，包括像素-像素（左），物体-像素（中），物体-物体（右）*"></p><center><p>图5：Transformer 能被应用于各种视觉基本元素之间的关系建模，包括像素-像素（左），物体-像素（中），物体-物体（右）</p></center><h3 id="理由2：和卷积形成互补"><a href="#理由2：和卷积形成互补" class="headerlink" title="理由2：和卷积形成互补"></a>理由2：和卷积形成互补</h3><p><strong>卷积是一种局部操作，一个卷积层通常只会建模邻域像素之间的关系。Transformer 则是全局操作，一个 Transformer 层能建模所有像素之间的关系，双方可以很好地进行互补</strong>。最早将这种互补性联系起来的是非局部网络 [19]，在这个工作中，少量 Transformer 自注意单元被插入到了原始网络的几个地方，作为卷积网络的补充，并被证明其在物体检测、语义分割和视频动作识别等问题中广泛有效。</p><p>此后，也有工作发现非局部网络在视觉中很难真正学到像素和像素之间的二阶关系 [28]，为此，有研究员们也提出了一些针对这一模型的改进，例如解耦非局部网络 [29]。</p><h3 id="理由3：更强的建模能力"><a href="#理由3：更强的建模能力" class="headerlink" title="理由3：更强的建模能力"></a>理由3：更强的建模能力</h3><p>卷积可以看作是一种模板匹配，图像中不同位置采用相同的模板进行滤波。<strong>而 Transformer 中的注意力单元则是一种自适应滤波，模板权重由两个像素的可组合性来决定，这种自适应计算模块具有更强的建模能力</strong>。</p><p>最早将 Transformer 这样一种自适应计算模块应用于视觉骨干网络建模的方法是局部关系网络 LR-Net [30] 和 SASA [31]，它们都将自注意的计算限制在一个局部的滑动窗口内，在相同理论计算复杂度的情况下取得了相比于 ResNet 更好的性能。然而，虽然理论上与 ResNet 的计算复杂度相同，但在实际使用中它们却要慢得多。一个主要原因是不同的查询（query）使用不同的关键字（key）集合，如图2（左）所示，对内存访问不太友好。</p><p>Swin Transformer 提出了一种新的局部窗口设计——移位窗口（shifted windows）。这一局部窗口方法将图像划分成不重叠的窗口，这样在同一个窗口内部，不同查询使用的关键字集合将是相同的，进而可以拥有更好的实际计算速度。在下一层中，窗口的配置会往右下移动半个窗口，从而构造了前一层中不同窗口像素间的联系。</p><h3 id="理由4：对大模型和大数据的可扩展性"><a href="#理由4：对大模型和大数据的可扩展性" class="headerlink" title="理由4：对大模型和大数据的可扩展性"></a>理由4：对大模型和大数据的可扩展性</h3><p>在 NLP 领域，<strong>Transformer 模型在大模型和大数据方面展示了强大的可扩展性</strong>。图6中，蓝色曲线显示近年来 NLP 的模型大小迅速增加。大家都见证了大模型的惊人能力，例如微软的 Turing 模型、谷歌的 T5 模型以及 OpenAI 的 GPT-3 模型。</p><p>视觉 Transformer 的出现为视觉模型的扩大提供了重要的基础，目前最大的视觉模型是谷歌的150亿参数 ViT-MoE 模型 [32]，这些大模型在 ImageNet-1K 分类上刷新了新的纪录。</p><p><img src="images/1686130501173.png" alt="1686130501173" title="*图6：NLP 领域和计算机视觉领域模型大小的变迁*"></p><center><p>图6：NLP 领域和计算机视觉领域模型大小的变迁</p></center><h3 id="理由5：更好地连接视觉和语言"><a href="#理由5：更好地连接视觉和语言" class="headerlink" title="理由5：更好地连接视觉和语言"></a>理由5：更好地连接视觉和语言</h3><p>在以前的视觉问题中，科研人员通常只会处理几十类或几百类物体类别。例如 COCO 检测任务中包含了80个物体类别，而 ADE20K 语义分割任务包含了150个类别。<strong>视觉 Transformer 模型的发明和发展，使视觉领域和 NLP 领域的模型趋同，有利于联合视觉和 NLP 建模，从而将视觉任务与其所有概念联系起来</strong>。这方面的先驱性工作主要有 OpenAI 的 CLIP [33] 和 DALL-E 模型 [34]。</p><p>考虑到上述的诸多优点，相信视觉 Transformer 将开启计算机视觉建模的新时代，我们也期待学术界和产业界共同努力，进一步挖掘和探索这一新的建模方法给视觉领域带来的全新机遇和挑战。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021</p><p>[2] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. ICCV 2021</p><p>[3] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han Hu. Video Swin Transformer. Tech report 2021</p><p>[4] Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, Han Hu. Self-Supervised Learning with Swin Transformers. Tech report 2021</p><p>[5] Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, Jianfeng Gao. Efficient Self-supervised Vision Transformers for Representation Learning. Tech report 2021</p><p>[6] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte. SwinIR: Image Restoration Using Swin Transformer. Tech report 2021</p><p>[7] <a href="https://github.com/layumi/Person_reID_baseline_pytorch">https://github.com/layumi/Person_reID_baseline_pytorch</a></p><p>[8] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, Manning Wang. Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation. Tech report 2021</p><p>[9] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou. Training data-efficient image transformers &amp; distillation through attention. Tech report 2021</p><p>[10] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, Luc Van Gool. LocalViT: Bringing Locality to Vision Transformers. Tech report 2021</p><p>[11] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, Chunhua Shen. Twins: Revisiting the Design of Spatial Attention in Vision Transformers. Tech report 2021</p><p>[12] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. ICCV 2021</p><p>[13] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, Shuicheng Yan. Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. Tech report 2021</p><p>[14] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, Jianfeng Gao. Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding. Tech report 2021</p><p>[15] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang. CvT: Introducing Convolutions to Vision Transformers. ICCV 2021</p><p>[16] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, Baining Guo. CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. Tech report 2021</p><p>[17] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, Jianfeng Gao. Focal Self-attention for Local-Global Interactions in Vision Transformers. Tech report 2021</p><p>[18] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, Bin Fu. Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer. Tech report 2021</p><p>[19] Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He. Non-local Neural Networks. CVPR 2018</p><p>[20] Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, Jingdong Wang. OCNet: Object Context for Semantic Segmentation. IJCV 2021</p><p>[21] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei. Relation Networks for Object Detection. CVPR 2018</p><p>[22] Jiarui Xu, Yue Cao, Zheng Zhang, Han Hu. Spatial-Temporal Relation Networks for Multi-Object Tracking. ICCV 2019</p><p>[23] Yihong Chen, Yue Cao, Han Hu, Liwei Wang. Memory Enhanced Global-Local Aggregation for Video Object Detection. CVPR 2020</p><p>[24] Jiajun Deng, Yingwei Pan, Ting Yao, Wengang Zhou, Houqiang Li, and Tao Mei. Relation distillation networks for video object detection. ICCV 2019</p><p>[25] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko. End-to-End Object Detection with Transformers. ECCV 2020</p><p>[26] Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, Jifeng Dai. Learning Region Features for Object Detection. ECCV 2018</p><p>[27] Cheng Chi, Fangyun Wei, Han Hu. RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder. NeurIPS 2020</p><p>[28] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, Han Hu. GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond. ICCV workshop 2019</p><p>[29] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, Han Hu. Disentangled Non-Local Neural Networks. ECCV 2020</p><p>[30] Han Hu, Zheng Zhang, Zhenda Xie, Stephen Lin. Local Relation Networks for Image Recognition. ICCV 2019</p><p>[31] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jonathon Shlens. Stand-Alone Self-Attention in Vision Models. NeurIPS 2019</p><p>[32] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, Neil Houlsby. Scaling Vision with Sparse Mixture of Experts. Tech report 2021</p><p>[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. Learning Transferable Visual Models from Natural Language Supervision. Tech report 2021</p><p>[34] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever. Zero-Shot Text-to-Image Generation. Tech report 2021</p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 BERT</title>
      <link href="/paper_reading/2.2.BERT/"/>
      <url>/paper_reading/2.2.BERT/</url>
      
        <content type="html"><![CDATA[<h1 id="2-BERT"><a href="#2-BERT" class="headerlink" title="2.BERT"></a>2.BERT</h1><p>论文链接：<a href="https://arxiv.org/pdf/1810.04805.pdf" title="1810.04805.pdf (arxiv.org)">1810.04805.pdf (arxiv.org)</a></p><p>解读视频：<a href="https://www.bilibili.com/video/BV1PL411M7eQ/?spm_id_from=333.337.search-card.all.click\&amp;vd_source=6bc8f793c75740c7bcfb8e281f986a8e" title="BERT 论文逐段精读【论文精读】_哔哩哔哩_bilibili">BERT 论文逐段精读【论文精读】_哔哩哔哩_bilibili</a></p><p>参考代码：<a href="https://github.com/wdndev/personal/tree/main/bert_torch" title="personal/bert_torch at main · wdndev/personal · GitHub">personal/bert_torch at main · wdndev/personal · GitHub</a></p><h1 id="1-题目-作者"><a href="#1-题目-作者" class="headerlink" title="1.题目+作者"></a>1.题目+作者</h1><p>“BERT” (Devlin 等, 2018, p. 1) 自然语言中中近三年最重要的文章；在计算机视觉里面很早就能够在一个大的数据集（比如说ImageNet）上训练出一个CNN模型，用这个模型可以用来处理一大片的机器视觉任务，来提升他们的性能；但是在自然语言处理里面，在BERT之前一直没有一个深的神经网络使得它训练好之后能够帮处理一大片的NLP任务，在NLP中很多时候还是对每个任务构造自己的神经网路，然后再做训练；<strong>BERT的出现使得我们能够在一个大的数据集上面训练好一个比较深的神经网络，然后应用在很多的NLP任务上面，这样既简化了NLP任务的训练，又提升了它的性能，所以BERT和它之后的一系列工作使得自然语言处理在过去三年中有了质的飞跃</strong>；</p><ul><li><strong>pre-training</strong>：在一个数据集上训练好一个模型，这个模型主要的目的是用在另外一个任务上面，所以如果另外一个任务叫training的话，那么在大的数据集上训练的这个任务（模型）就叫做pre-training，即training之前的任务</li><li><strong>deep</strong>：更深的神经网络</li><li><strong>bidirectional</strong>：双向的</li><li><strong>transformers</strong>：</li><li><strong>language understanding</strong>：transformer主要是用在机器翻译这个小任务上，这里使用的是一个更加广义的词，就是对语言的理解</li></ul><h1 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a>2.摘要</h1><p>BERT是一个新的语言表示模型，BERT的名字来自于：</p><ul><li>Bidirectional</li><li>Encoder</li><li>Representation</li><li>Transformer</li></ul><p>它的意思是transformer这个模型双向的编码器表示，这四个词跟标题是不一样的</p><p><strong>它的想法是基于ELMo</strong></p><ul><li>ELMo来自于芝麻街中人物的名字，芝麻街是美国的一个少儿英语学习节目</li><li>BERT是芝麻街中另外一个主人公的名字</li><li>这篇文章和之前的ELMo开创了NLP的芝麻街系列文章</li></ul><p><strong>BERT和最近的一些语言的表示模型有所不同</strong></p><ul><li>Peters 引用的是ELMo</li><li>Radfor 引用的是GPT</li></ul><p><strong>BERT和ELMo、GPT的区别：</strong></p><ul><li>BERT是设计用来训练深的双向表示，<strong>使用的是没有标号的数据，再联合左右的上下文信息，</strong>因为这样的设计导致训练好的BERT只用加一个额外的输出层，就可以在很多NLP的任务（比如问答、语言推理）上面得到一个不错的结果，而且不需要对任务做很多特别的架构上的改动</li><li>GPT考虑的是单向（用左边的上下文信息去预测未来），BERT同时使用了左侧和右侧的信息，它是双向的（Bidirectional）</li><li>ELMO用的是一个基于RNN的架构，BERT用的是transformer，所以ELMo在用到一些下游任务的时候需要对架构做一点点调整，但是BERT相对比较简单，和GPT一样只需要改最上层就可以了</li></ul><h1 id="3-Introduction"><a href="#3-Introduction" class="headerlink" title="3.Introduction"></a>3.Introduction</h1><h2 id="3-1自然语言任务和预训练"><a href="#3-1自然语言任务和预训练" class="headerlink" title="3.1自然语言任务和预训练"></a>3.1自然语言任务和预训练</h2><p>在语言模型中，预训练可以用来提升很多自然语言的任务</p><p>自然语言任务包括两类</p><ul><li><strong>句子层面的任务（sentence-level）</strong>：主要是用来建模句子之间的关系，比如说对句子的情绪识别或者两个句子之间的关系</li><li><strong>词元层面的任务（token-level）</strong>：包括实体命名的识别（对每个词识别是不是实体命名，比如说人名、街道名），这些任务需要输出一些细腻度的词元层面上的输出</li></ul><p>预训练在NLP中已经流行了有一阵子了，在计算机视觉里面已经用了很多年了，同样的方法用到自然语言上面也不会很新，但是在介绍BERT的时候，很有可能会把NLP做预训练归功于BERT，BERT不是第一个提出来的而是BERT让这个方法出圈了让后面的研究者跟着做自然语言的任务</p><h2 id="3-2-预训练任务的策略"><a href="#3-2-预训练任务的策略" class="headerlink" title="3.2 预训练任务的策略"></a>3.2 预训练任务的策略</h2><p>在使用预训练模型做特征表示的时候，一般有两类策略</p><ul><li>一个策略是<strong>基于特征</strong>的，代表作是ELMo，对每一个下游的任务构造一个跟这个任务相关的神经网络，它<strong>使用的RNN的架构</strong>，然后将预训练好的这些表示（比如说词嵌入也好，别的东西也好）作为一个额外的特征和输入一起输入进模型中，希望这些特征已经有了比较好的表示，所以导致模型训练起来相对来说比较容易，这也是NLP中使用预训练模型最常用的做法（把学到的特征和输入一起放进去作为一个很好的特征表达）</li><li>另一个策略是<strong>基于微调</strong>的，这里举的是GPT的例子，就是把预训练好的模型放在下游任务的时候不需要改变太多，只需要改动一点就可以了。这个模型预训练好的参数会在下游的数据上再进行微调（所有的权重再根据新的数据集进行微调）</li></ul><p>上述两个途径在预训练的时候都是使用一个相同的目标函数，都是使用一个单向的语言模型（给定一些词去预测下一个词是什么东西，说一句话然后预测这句话下面的词是什么东西，属于一个预测模型，用来预测未来，所以是单向的）</p><h2 id="3-3-现有技术的限制"><a href="#3-3-现有技术的限制" class="headerlink" title="3.3 现有技术的限制"></a>3.3 现有技术的限制</h2><p>本文的主要想法：现在这些技术会有局限性，特别是做预训练的表征的时候，主要的问题是标准的语言模型是单向的，这样就导致在选架构的时候会有局限性</p><ul><li>在GPT中使用的是一个从左到右的架构（在看句子的时候只能从左看到右），这样的坏处在于如果要做句子层面的分析的话，比如说要判断一个句子层面的情绪是不是对的话，从左看到右和从右看到左都是合法的，另外，就算是词元层面上的一些任务，比如QA的时候也是看完整个句子再去选答案，而不是一个一个往下走</li></ul><p>因此<strong>如果将两个方向的信息都放进去的话，应该是能够提升这些任务的性能的</strong></p><h2 id="3-4-BERT"><a href="#3-4-BERT" class="headerlink" title="3.4 BERT"></a>3.4 BERT</h2><p>提出了BERT，BERT是用来减轻之前提到的语言模型是一个单向的限制，<strong>使用的是一个带掩码的语言模型（masked language model）</strong>，这个语言模型是受Cloze任务的启发（引用了一篇1953年的论文）</p><ul><li>这个带掩码的语言模型<strong>每一次随机地选一些资源，然后将它们盖住，目标函数就是预测被盖住的字</strong>，等价于将一个句子挖一些空完后进行完形填空；</li><li>跟标准的语言模型从左看到右的不同之处在于：<strong>带掩码的语言模型是允许看到左右的信息的</strong>（相当于看完形填空的时候不能只看完形填空的左边，也需要看完形填空的右边），这样的话它允许训练深的双向的transformer模型；</li><li>在带掩码的语言模型之外还训练了一个任务，<strong>预测下一个句子</strong>，核心思想是给定两个句子，然后判断这两个句子在原文里面是相邻的，还是随机采样得来的，这样就让模型学习了句子层面的信息；</li></ul><h2 id="3-5-文章的贡献"><a href="#3-5-文章的贡献" class="headerlink" title="3.5 文章的贡献"></a>3.5 文章的贡献</h2><ol><li>展示了双向信息的重要性，GPT只用了单向，之前有的工作只是很简单地把一个从左看到右的语言模型和一个从右看到左的语言模型简单地合并到一起，类似于双向的RNN模型（contact到一起），这个模型在双向信息的应用上更好</li><li>假设有一个比较好的预训练模型就不需要对特定任务做特定的模型改动。BERT是第一个在一系列的NLP任务上（包括在句子层面上和词元层面上的任务）都取得了最好的成绩的基于微调的模型</li><li>代码和模型全部放在：<a href="https://github.com/google-research/bert" title="https://github.com/google-research/bert">https://github.com/google-research/bert</a></li></ol><h1 id="4-结论"><a href="#4-结论" class="headerlink" title="4.结论"></a>4.结论</h1><p>最近一些实验表明，使用无监督的预训练是非常好的，这样使得资源不多（训练样本比较少的任务也能够享受深度神经网络）,本文主要的工作就是把前人的工作扩展到深的双向的架构上，使得同样的预训练模型能够处理大量的不同的自然语言任务</p><p>简单概括一下：本文之前的两个工作一个叫<strong>ELMo</strong>，它使用了双向的信息但是它网络架构比较老，用的是RNN，另外一个工作是<strong>GPT</strong>，它用的是transformer的架构，但是它只能处理单向的信息，因此本文将ELMo双向的想法和GPT的transformer架构结合起来就成为了BERT</p><ul><li><strong>具体的改动是在做语言模型的时候不是预测未来，而是变成完形填空</strong></li></ul><h1 id="5-相关工作"><a href="#5-相关工作" class="headerlink" title="5.相关工作"></a>5.相关工作</h1><h1 id="6-BERT"><a href="#6-BERT" class="headerlink" title="6.BERT"></a>6.BERT</h1><h2 id="6-1-pre-training-and-fine-tuning"><a href="#6-1-pre-training-and-fine-tuning" class="headerlink" title="6.1 pre-training and fine-tuning"></a>6.1 pre-training and fine-tuning</h2><p>BERT中有两个步骤：</p><ul><li><strong>预训练</strong>：在预训练中，这个模型是在一个没有标号的数据集上训练的</li><li><strong>微调</strong>：在微调的时候同样是用一个BERT模型，但是它的权重被初始化成在预训练中得到的权重，所有的权重在微调的时候都会参与训练，用的是有标号的数据</li></ul><p>每一个下游的任务都会创建一个新的BERT模型，虽然它们都是用最早预训练好的BERT模型作为初始化，但是每个下游任务都会根据自己的数据训练好自己的模型。</p><p>虽然预训练和微调不是BERT独创的，在计算机视觉中用的比较多，但是作者还是做了一个简单的介绍（在写论文的时候遇到一些技术需要使用的时候，而且可能应该所有人都知道，最好不要一笔带过，论文是需要自洽的，后面的人读过来可能不太了解这些技术，但是这些技术又是论文中方法不可缺少的一部分的话，最好还是能够做一个简单的说明）</p><p><img src="image/image_tuUrpi62qn.png" alt=""></p><ul><li>预训练的时候输入是一些没有标号的句子对</li><li>这里是在一个没有标号的数据上训练出一个BERT模型，把他的权重训练好，对下游的任务来说，对每个任务创建一个同样的BERT模型，但是它的权重的初始化值来自于前面预训练训练好的权重，对于每一个任务都会有自己的有标号的数据，然后对BERT继续进行训练，这样就得到了对于某一任务的BERT版本。</li></ul><blockquote><p>[CLS]是在每个输入示例前添加的特殊符号，[SEP]是特殊的分隔符标记（例如，分隔问题/答案）</p></blockquote><h2 id="6-2-模型架构"><a href="#6-2-模型架构" class="headerlink" title="6.2 模型架构"></a>6.2 模型架构</h2><p>BERT模型就是一个多层的双向transformer编码器，而且它是直接基于原始的论文和它原始的代码，没有做改动</p><h3 id="（1）三个参数"><a href="#（1）三个参数" class="headerlink" title="（1）三个参数"></a>（1）三个参数</h3><ul><li>L：transformer块的个数</li><li>H：隐藏层的大小</li><li>A：自注意力机制中多头的头的个数</li></ul><h3 id="（2）两个模型"><a href="#（2）两个模型" class="headerlink" title="（2）两个模型"></a>（2）两个模型</h3><ul><li>BERT base：它的选取是使得跟GPT模型的参数差不多，来做一个比较公平的比较</li><li>BERT large：用来刷榜</li><li><strong>BERT中的模型复杂度和层数是一个线性关系，和宽度是一个平方的关系</strong></li></ul><h3 id="（3）怎样把超参数换算成可学习参数的大小"><a href="#（3）怎样把超参数换算成可学习参数的大小" class="headerlink" title="（3）怎样把超参数换算成可学习参数的大小"></a>（3）<strong>怎样把超参数换算成可学习参数的大小</strong></h3><p><img src="image/image_yZqmWRzAGX.png" alt=""></p><p>模型中可学习参数主要来自两块</p><ul><li><strong>嵌入层</strong>：就是一个矩阵，输入是字典的大小（假设是30k），输出等于隐藏单元的个数（假设是H）</li><li><strong>transformer块</strong>：transformer中有两部分：一个是自注意力机制（它本身是没有可学习参数的，但是对多头注意力的话，他会把所有进入的K、V、Q分别做一次投影，每一次投影的维度是等于64的，因为有多个头，头的个数A乘以64得到H，所以进入的话有key、value、q，他们都有自己的投影矩阵，这些投影矩阵在每个头之间合并起来其实就是H<em>H的矩阵了，同样道理拿到输出之后也会做一次投影，他也是一个H</em>H的矩阵，所以对于一个transformer块，他的自注意力可学习的参数是H的平方乘以4），一个是后面的MLP（MLP里面需要两个全连接层，第一个层的输入是H，但是它的输出是4<em>H，另外一个全连接层的输入是4</em>H，输出是H，所以每一个矩阵的大小是H*4H，两个矩阵就是H的平方乘以8），这两部分加起来就是一个transformer块中的参数，还要乘以L（transformer块的个数）</li></ul><p>所以总参数的个数就是30k乘以H（这部分就是嵌入层总共可以学习的参数个数）再加上L层乘以H的平方再乘以12</p><h2 id="6-3-输入和输出"><a href="#6-3-输入和输出" class="headerlink" title="6.3 输入和输出"></a>6.3 输入和输出</h2><h3 id="（1）输入形式"><a href="#（1）输入形式" class="headerlink" title="（1）输入形式"></a>（1）输入形式</h3><p>对于下游任务的话，有些任务是处理一个句子，有些任务是处理两个句子，所以为了使BERT模型能够处理所有的任务，它的输入既可以是一个句子，也可以是一个句子对</p><ul><li>这里的一个句子是指一段连续的文字，不一定是真正的语义上的一段句子</li><li>输入叫做一个序列，可以是一个句子，也可以是两个句子</li><li>这和之前文章里的transformer是不一样的：transformer在训练的时候，他的输入是一个序列对，因为它的编码器和解码器分别会输入一个序列，但是BERT只有一个编码器，所以为了使它能够处理两个句子，就需要把两个句子变成一个序列</li></ul><h3 id="（2）序列的构成"><a href="#（2）序列的构成" class="headerlink" title="（2）序列的构成"></a><strong>（2）序列的构成</strong></h3><p>这里使用的切词的方法是WordPiece，核心思想是：</p><ul><li>假设按照空格切词的话，一个词作为一个token，因为数据量相对比较大，所以会导致词典大小特别大，可能是百万级别的，那么根据之前算模型参数的方法，如果是百万级别的话，就导致整个可学习参数都在嵌入层上面</li><li>WordPiece是说假设一个词在整个里面出现的概率不大的话，那么应该把它切开看它的一个子序列，它的某一个子序列很有可能是一个词根，这个词很有可能出现的概率比较大话，那么就只保留这个子序列就行了。这样的话，可以把一个相对来说比较长的词切成很多一段一段的片段，而且这些片段是经常出现的，这样的话就可以用一个相对来说比较小的词典就能够表示一个比较大的文本了</li></ul><p>切好词之后如何将两个句子放在一起</p><ul><li><strong>序列的第一个词永远是一个特殊的记号**</strong><code>[CLS]</code>**，CLS表示classification，这个词的作用是BERT希望最后的输出代表的是整个序列的信息（比如说整个句子层面的信息），因为BERT使用的是transformer的编码器，所以它的自注意力层中每一个词都会去看输出入中所有词的关系，就算是词放在第一的位置，它也是有办法能够看到之后的所有词</li></ul><p>把两个句子合在一起，但是因为要做句子层面的分类，所以需要区分开来这两个句子，这里有两个办法：</p><ol><li>在每一个句子后面放一个特殊的词：<code>[SEP]</code>表示separate</li><li>学一个嵌入层来表示这个句子到底是第一个句子还是第二个句子</li></ol><p><code>[CLS]</code>是第一个特殊的记号表示分类，中间用一个特殊的记号<code>[SEP]</code>分隔开，每一个token进入BERT得到这个token的embedding表示（对BERT来讲，就是输入一个序列，然后得到另外一个序列），最后transformer块的输出就表示这个词元的BERT表示，最后再添加额外的输出层来得到想要的结果。</p><h3 id="（3）embedding"><a href="#（3）embedding" class="headerlink" title="（3）embedding"></a>（3）embedding</h3><p>对于每一个词元进入BERT的向量表示，它是这个词元本身的embedding加上它在哪一个句子的embedding再加上位置的embedding，如下图所示：</p><p><img src="image/image_oJD6S2AS3W.png" alt=""></p><ul><li>上图演示的是BERT的嵌入层的做法，即由一个词元的序列得到一个向量的序列，这个向量的序列会进入transformer块</li><li>上图中每一个方块是一个词元</li><li><strong>token embedding</strong>：这是一个正常的embedding层，对每一个词元输出它对应的向量</li><li><strong>segment embedding</strong>：表示是第一句话还是第二句话</li><li><strong>position embedding</strong>：输入的大小是这个序列的最大长度，它的输入就是每个词元这个序列中的位置信息（从零开始），由此得到对应的位置的向量</li><li>最终就是每个词元本身的嵌入加上在第几个句子的嵌入再加上在句子中间的位置嵌入</li><li>在transformer中，位置信息是手动构造出来的一个矩阵，但是在BERT中不管是属于哪个句子，还是具体的位置，它对应的向量表示都是通过学习得来的</li></ul><h2 id="6-4-预训练"><a href="#6-4-预训练" class="headerlink" title="6.4 预训练"></a>6.4 预训练</h2><p>在预训练的时候，主要有两个东西比较关键</p><ul><li>目标函数</li><li>用来做预训练的数据</li></ul><h3 id="（1）TASK-1：Masked-LM"><a href="#（1）TASK-1：Masked-LM" class="headerlink" title="（1）TASK #1：Masked LM"></a>（1）TASK #1：Masked LM</h3><p>对于输入的词元序列，如果词元序列是由WordPiece生成的话，那么它有15%的概率会随机替换成掩码，但是对于特殊的词元（第一个词元和中间的分割词元不做替换），如果输入序列长度是1000的话，那么就要预测150个词。</p><p><img src="image/image_cn80GZGzOT.png" alt=""></p><p>这里也会存在问题：因为<strong>在做掩码的时候会把词元替换成一个特殊的token（[MASK]），在训练的时候大概会看到15%的词元，但是在微调的时候是没有的</strong>，因为在微调的时候不用这个目标函数，所以没有mask这个东西，导致在预训练和微调的时候所看到的数据会有多不同。</p><p><strong>解决方法</strong>：对这15%的被选中作为掩码的词有80%的概率是真的将它替换成这个特殊的掩码符号（[MASK]），还有10%的概率将它替换成一个随机的词元（其实是加入了一些噪音），最后有10%的概率什么都不干，就把它存在那里用来做预测（附录中有例子）。</p><h3 id="（2）TASK-2：Next-Sentence-Prediction（NSP）"><a href="#（2）TASK-2：Next-Sentence-Prediction（NSP）" class="headerlink" title="（2）TASK #2：Next Sentence Prediction（NSP）"></a>（2）TASK #2：Next Sentence Prediction（NSP）</h3><p>在问答（QA）和自然语言推理（NLI）中都是句子对，如果让它学习一些句子层面的信息也不错，具体来说，一个输入序列里面有两个句子：a和b，有50的概率b在原文中间真的是在a的后面，还有50%的概率b就是随机从别的地方选取出来的句子，这就意味着有50%的样本是正例（两个句子是相邻的关系），50%的样本是负例（两个句子没有太大的关系），加入这个目标函数能够极大地提升在QA和自然语言推理的效果（附录中有例子）</p><p><img src="image/image_u5BdxsXmyY.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line">Input = [CLS] the man went to [MASK] store [SEP]</span><br><span class="line">he bought a gallon [MASK] milk [SEP]</span><br><span class="line">Label = IsNext</span><br><span class="line">Input = [CLS] the man [MASK] to the store [SEP]</span><br><span class="line">penguin [MASK] are flight <span class="comment">##less birds [SEP] # 两个## 表示后面这个词less跟在flight后，二者是同一个词被切成两半</span></span><br><span class="line">Label = NotNext</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>预训练过程很大程度上遵循现存文献的语言模型预训练，使用BooksCorpus（800M个单词）和English Wikipedia（25亿个单词）。为了获取长的连续序列，使用文档级的语料（文章）比使用像Billion Word Benchmark这样无序的句子级语料更为重要。</p><h3 id="（3）预训练数据"><a href="#（3）预训练数据" class="headerlink" title="（3）预训练数据"></a>（3）预训练数据</h3><p>使用了两个数据集</p><ul><li>BooksCorpus</li><li>English Wikipedia</li></ul><p>应该使用文本层面的数据集，即数据集里面是一篇一篇的文章而不是一些随机打乱的句子，因为transformer确实能够处理比较长的序列，所以对于整个文本序列作为数据集效果会更好一些。</p><h2 id="6-5-微调"><a href="#6-5-微调" class="headerlink" title="6.5 微调"></a>6.5 微调</h2><p>BERT和一些基于编码器解码器的架构有什么不同</p><ul><li>transformer是编码器解码器架构</li><li>因为把整个句子对都放在一起放进去了，所以自注意力能够在两端之间相互能够看到，但是在编码器解码器这个架构中，编码器一般是看不到解码器的东西的，所以BERT在这一块会更好一点，但是实际上也付出了一定的代价（不能像transformer一样能够做机器翻译）</li></ul><p>在做下游任务的时候会根据任务设计任务相关的输入和输出，好处是模型其实不需要做大的改动，主要是怎么样把输入改成所要的那个句子对</p><ul><li>如果真的有两个句子的话就是句子a和b</li><li>如果只有一个句子的话，比如说要做一个句子的分类，b就没有了</li></ul><p>然后根据下游的任务要求，要么是拿到第一个词元对应的输出做分类或者是拿到对应的词元的输出做所想要的输出，不管怎么样都是在最后加一个输出层，然后用一个softnax得到想要的标号</p><p>跟预训练比微调相对来说比较便宜，所有的结果都可以使用一个TPU跑一个小时就可以了，使用GPU的话多跑几个小时也行</p><h1 id="7-实验"><a href="#7-实验" class="headerlink" title="7.实验"></a>7.实验</h1><h2 id="7-1-GLUE"><a href="#7-1-GLUE" class="headerlink" title="7.1 GLUE"></a><strong>7.1 GLUE</strong></h2><p>它里面包含了多个数据集，是一个句子层面的任务</p><p>BERT就是把第一个特殊词元[CLS]的最后的向量拿出来，然后学习一个输出层w，放进去之后用softmax就能得到标号，这就变成了一个很正常的多分类问题了</p><p>下图表示了在这个分类任务上的结果</p><p><img src="image/image_MwJx_plxkW.png" alt=""></p><ul><li>average表示在所有数据集上的平均值，它表示精度，越高越好</li><li>可以发现就算是BERT就算是在base跟GPT可学习参数差不多的情况下，也还是能够有比较大的提升</li></ul><h2 id="7-2-SQuAD-v1-1"><a href="#7-2-SQuAD-v1-1" class="headerlink" title="7.2 SQuAD v1.1"></a><strong>7.2 SQuAD v1.1</strong></h2><p>斯坦福的一个QA数据集</p><p>QA任务是说给定一段话，然后问一个问题，需要在这段话中找出问题的答案（类似于阅读理解），答案在给定的那段话中，只需要把答案对应的小的片段找出来就可以了（找到这个片段的开始和结尾）</p><ul><li>就是对每个词元进行判断，看是不是答案的开头或者答案的结尾</li></ul><p>具体来说就是学两个向量S和E，分别对应这个词元是答案开始的概率和答案最后的概率，它对每个词元（也就是第二句话中每个词元）的S和Ti相乘，然后再做softmax，就会得到这个段中每一个词元是答案开始的概率，公式如下图所示，同理也可以得出是答案末尾的概率:</p><script type="math/tex; mode=display">P_i = \frac{e^{S\cdot T_i}}{\sum_j e^{S\cdot T_j}}</script><ul><li>Ti表示第 i 个输入词元对应的最后一个隐藏向量</li></ul><p><strong>注意：在做微调的时候的参数设置</strong></p><ul><li>使用了3个epoch，扫描了3遍数据</li><li>学习率是5e-5</li><li>batchsize是32</li></ul><p>用BERT做微调的时候结果非常不稳定，同样的参数、同样的数据集，训练十遍，可能会得到不同的结果。最后发现3其实是不够的，可能多学习几遍会好一点</p><p>BERT用的优化器是adam的不完全版，当BERT要训练很长时间的时候是没有影响的，但是如果BERT只训练一小段时间的话，它可能会带来影响（将这个优化器换成adam的正常版就可以解决这个问题了）</p><h1 id="8-Ablation-Studies"><a href="#8-Ablation-Studies" class="headerlink" title="8.Ablation Studies"></a>8.Ablation Studies</h1><p>介绍了BERT中每一块最后对结果的贡献</p><h2 id="8-1-Effect-of-Pre-training-Tasks"><a href="#8-1-Effect-of-Pre-training-Tasks" class="headerlink" title="8.1 Effect of Pre-training Tasks"></a>8.1 Effect of Pre-training Tasks</h2><p><img src="image/image_Bk66lmk49Z.png" alt=""></p><ul><li><strong>No NSP</strong>：假设去掉对下一个句子的预测</li><li>L<strong>TR &amp; No NSP</strong>：使用一个从左看到右的单向的语言模型（而不是用带掩码的语言模型），然后去掉对下一个句子的预测</li><li><strong>+ BiLSTM</strong>：在上面加一个双向的LSTM</li></ul><p>从结果来看，去掉任何一部分，结果都会打折扣</p><h2 id="8-2-Effect-of-Model-Size"><a href="#8-2-Effect-of-Model-Size" class="headerlink" title="8.2 Effect of Model Size"></a>8.2 Effect of Model Size</h2><p>BERT base中有1亿的可学习参数，BERT large中有3亿可学习的参数，相对于之前的transformer，可学习参数数量的提升还是比较大的。</p><p>当模型变得越来越大的时候，效果会越来越好，这是第一个展示将模型变得特别大的时候对语言模型有较大提升的工作。</p><p>虽然现在GPT3已经做到1000亿甚至在向万亿级别发展，但是在三年前，BERT确实是开创性地将一个模型推到如此之大，引发了之后的模型大战</p><h2 id="8-3-Feature-based-Approach-with-BERT"><a href="#8-3-Feature-based-Approach-with-BERT" class="headerlink" title="8.3 Feature-based Approach with BERT"></a>8.3 Feature-based Approach with BERT</h2><p>假设不用BERT做微调而是把BERT的特征作为一个静态特征输进去会怎样</p><p>结论是效果确实没有微调好，所有用BERT的话应该用微调</p><h1 id="9-评论"><a href="#9-评论" class="headerlink" title="9.评论"></a><strong>9.评论</strong></h1><p><strong>写作</strong></p><ul><li>先写了BERT和GPT的区别</li><li>然后介绍了BERT模型</li><li>接下来是在各个实验上的设置</li><li>最后对比结果，结果非常好</li></ul><p>这篇文章认为本文的最大贡献就是<strong>双向性</strong>（写文章最好能有一个明确的卖点，有得有失，都应该写出来）</p><ul><li>但是今天来看，这篇文章的贡献不仅仅只有双向性，还有其它东西</li><li>从写作上来说，至少要说选择双向性所带来的不好的地方是什么，做一个选择，会得到一些东西，也会失去一些东西：和GPT比，BERT用的是编码器，GPT用的是解码器，得到了一些好处，但是也有坏处（比如做机器翻译和文本摘要比较困难，做生成类的东西就没那么方便了）</li><li>分类问题在NLP中更加常见，所以NLP的研究者更喜欢用BERT，会更容易一些</li></ul><p>BERT所提供的是一个完整的解决问题的思路，符合了大家对于深度学习模型的期望：在一个很大的数据集上训练好一个很深很宽的模型，这个模型拿出来之后可以用在很多小问题上，通过微调可以全面提升这些小数据上的性能</p>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> PaperReading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLMs 目录</title>
      <link href="/llms/llms_idx/"/>
      <url>/llms/llms_idx/</url>
      
        <content type="html"><![CDATA[<h2 id="0-LLM八股"><a href="#0-LLM八股" class="headerlink" title="0.LLM八股"></a>0.<a href="/note/llm/llm_concept/llm八股.html">LLM八股</a></h2><h2 id="1-LLMs相关文章"><a href="#1-LLMs相关文章" class="headerlink" title="1.LLMs相关文章"></a>1.LLMs相关文章</h2><ol><li><a href="/llms/llms_article/1.llm推理优化技术">LLMs推理优化技术</a></li><li><a href="/llms/llms_article/2.主流大语言模型的技术原理细节">主流大语言模型的技术原理细节</a></li><li><a href="/llms/llms_article/3.llama系列模型">LLaMA系列模型架构</a></li><li><a href="/llms/llms_article/4.chatglm系列模型">ChatGLM系列模型架构</a></li><li><a href="/llms/llms_article/5.RAG技术">RAG（检索增强生成）技术</a></li><li><a href="/llms/llms_article/6.大模型Agent技术">大模型Agent技术</a></li><li><a href="/llms/llms_article/7.大语言模型方法与实践">大语言模型方法与实践</a></li><li><a href="/llms/llms_article/8.LLM 推理常见参数">LLM 推理常见参数</a></li><li><a href="/llms/llms_article/9.检索增强LLM">检索增强LLM</a></li></ol><h2 id="2-LLMs相关论文"><a href="#2-LLMs相关论文" class="headerlink" title="2.LLMs相关论文"></a>2.LLMs相关论文</h2><ol><li><a href="/paper_reading/4.1.ZeRO">ZeRO</a></li><li><a href="/paper_reading/2.5.GPT_GPT-2_GPT-3">GPT_GPT-2_GPT-3</a></li><li><a href="/paper_reading/2.6.InstructGPT">InstructGPT</a></li><li><a href="/paper_reading/2.7.GPT-4">GPT-4</a></li><li><a href="/paper_reading/255.1.ChatGPT 相关核心算法">ChatGPT 相关核心算法</a></li></ol><h2 id="3-Transformer相关文章"><a href="#3-Transformer相关文章" class="headerlink" title="3.Transformer相关文章"></a>3.Transformer相关文章</h2><ol><li><a href="/llms/transformer/0.Transformer综述">Transformer综述</a></li><li><a href="/llms/transformer/1.The_Annotated_Transformer最新翻译">The Annotated Transformer最新翻译</a></li><li><a href="/llms/transformer/2.Transformer架构解析">Transformer架构解析</a></li><li><a href="/llms/transformer/3.Transformer构建语言模型">Transformer构建语言模型</a></li><li><a href="/llms/transformer/4.Transformer为啥这么欢迎">Transformer为啥这么欢迎</a></li><li><a href="/llms/transformer/5.Transformer中的注意力">Transformer中的注意力</a></li><li><a href="/llms/transformer/6.Transformer架构细节">Transformer架构细节</a></li><li><a href="/llms/transformer/7.Transformer中的位置编码">Transformer中的位置编码</a></li></ol><h2 id="4-Transformer相关论文"><a href="#4-Transformer相关论文" class="headerlink" title="4.Transformer相关论文"></a>4.Transformer相关论文</h2><ol><li><a href="/paper_reading/2.1.Transformer">Transformer</a></li><li><a href="/paper_reading/2.2.BERT">BERT</a></li><li><a href="/paper_reading/2.3.ViT">ViT</a></li><li><a href="/paper_reading/2.4.Swin Transformer">Swim Transformer</a></li><li><a href="/paper_reading/2.8.MAE">MAE</a></li></ol><h2 id="5-清华大模型公开课"><a href="#5-清华大模型公开课" class="headerlink" title="5.清华大模型公开课"></a>5.清华大模型公开课</h2><ul><li>视频连接：<a href="https://www.bilibili.com/video/BV1UG411p7zv">https://www.bilibili.com/video/BV1UG411p7zv</a></li><li>文档资料：<a href="https://www.openbmb.org/community/course">OpenBMB - 让大模型飞入千家万户</a></li></ul><h4 id="大模型基础知识"><a href="#大模型基础知识" class="headerlink" title="大模型基础知识"></a>大模型基础知识</h4><ol><li><a href="/llms/llms_course/1.NLP_大模型基础">NLP 大模型基础</a></li><li><a href="/llms/llms_course/2.神经网络基础">神经网络基础</a></li><li><a href="/llms/llms_course/3.Transformer基础">Transformer基础</a></li></ol><h4 id="大模型关键技术"><a href="#大模型关键技术" class="headerlink" title="大模型关键技术"></a>大模型关键技术</h4><ol><li><a href="/llms/llms_course/4.Prompt_Tuning_Delta_Tuning">Prompt Tuning &amp; Delta Tuning</a></li><li><a href="/llms/llms_course/5.高效训练_模型压缩">高效训练 &amp; 模型压缩</a></li><li><a href="/llms/llms_course/6.文本理解和生成大模型">文本理解和生成大模型</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> LLMs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> LLMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读目录</title>
      <link href="/paper_reading/pr_content/"/>
      <url>/paper_reading/pr_content/</url>
      
        <content type="html"><![CDATA[<h2 id="0-如何阅读论文"><a href="#0-如何阅读论文" class="headerlink" title="0.如何阅读论文"></a>0.如何阅读论文</h2><ol><li><a href="/paper_reading/0.如何阅读论文">如何阅读论文</a></li></ol><h2 id="1-Deep-Learning"><a href="#1-Deep-Learning" class="headerlink" title="1.Deep Learning"></a>1.Deep Learning</h2><ol><li><a href="/paper_reading/1.1.GNN">GNN</a></li><li><a href="/paper_reading/1.2.GAN">GAN</a></li><li><a href="/paper_reading/1.3.MoCo">MoCo</a></li><li><a href="/paper_reading/1.4.对比学习论文综述">对比学习论文综述</a></li><li><a href="/paper_reading/1.5.ELMo">ELMo</a></li><li><a href="/paper_reading/1.6.MoE">MoE</a></li><li><a href="/paper_reading/1.7.MoE经典论文简牍">MoE经典论文简牍</a></li></ol><h2 id="2-Transformer"><a href="#2-Transformer" class="headerlink" title="2.Transformer"></a>2.Transformer</h2><ol><li><a href="/paper_reading/2.1.Transformer">Transformer</a></li><li><a href="/paper_reading/2.2.BERT">BERT</a></li><li><a href="/paper_reading/2.3.ViT">ViT</a></li><li><a href="/paper_reading/2.4.Swin Transformer">Swim Transformer</a></li><li><a href="/paper_reading/2.5.GPT_GPT-2_GPT-3">GPT_GPT-2_GPT-3</a></li><li><a href="/paper_reading/2.6.InstructGPT">InstructGPT</a></li><li><a href="/paper_reading/2.7.GPT-4">GPT-4</a></li><li><a href="/paper_reading/2.8.MAE">MAE</a></li></ol><h2 id="3-Reinforcement-Learning"><a href="#3-Reinforcement-Learning" class="headerlink" title="3.Reinforcement Learning"></a>3.Reinforcement Learning</h2><ol><li><a href="/paper_reading/3.1.WU_UCT">WU-UCT</a></li><li><a href="/paper_reading/3.2.Rainbow">Rainbow</a></li></ol><h2 id="4-LLMs"><a href="#4-LLMs" class="headerlink" title="4.LLMs"></a>4.LLMs</h2><ol><li><a href="/paper_reading/4.1.ZeRO">ZeRO</a></li></ol><h2 id="5-Others"><a href="#5-Others" class="headerlink" title="5.Others"></a>5.Others</h2><ol><li><a href="/paper_reading/255.1.ChatGPT 相关核心算法">ChatGPT 相关核心算法</a></li><li><a href="/paper_reading/255.2.大模型时代下做科研的四个思路">大模型时代下做科研的四个思路</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PaperReading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 Transformer</title>
      <link href="/paper_reading/2.1.Transformer/"/>
      <url>/paper_reading/2.1.Transformer/</url>
      
        <content type="html"><![CDATA[<p>论文地址: <a href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></p><p>解析视频：<a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&amp;vd_source=6bc8f793c75740c7bcfb8e281f986a8e">Transformer论文逐段精读【论文精读</a></p><p>Transformer Model：<a href="/Transformer/2.Transformer架构解析">Transformer架构解析</a></p><p>代码链接：<a href="https://github.com/wdndev/personal/tree/main/transformer">personal/transformer · GitHub</a></p><p>1.Transformer</p><h1 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1.摘要"></a>1.摘要</h1><p>1、transduction models”：序列转录模型：给以序列生成另一个序列</p><h1 id="2-结论"><a href="#2-结论" class="headerlink" title="2.结论"></a>2.结论</h1><h1 id="3-导言"><a href="#3-导言" class="headerlink" title="3.导言"></a>3.导言</h1><h1 id="4-相关工作"><a href="#4-相关工作" class="headerlink" title="4.相关工作"></a>4.相关工作</h1><h1 id="5-模型"><a href="#5-模型" class="headerlink" title="5.模型"></a>5.模型</h1><p><img src="image/image_algQelDfMK.png" alt=""></p><h2 id="5-1-LayerNorm-和-BatchNorm"><a href="#5-1-LayerNorm-和-BatchNorm" class="headerlink" title="5.1 LayerNorm 和 BatchNorm"></a>5.1 LayerNorm 和 BatchNorm</h2><h3 id="（1）BatchNorm-简单的-2-维-情况（蓝色）"><a href="#（1）BatchNorm-简单的-2-维-情况（蓝色）" class="headerlink" title="（1）BatchNorm 简单的 2 维 情况（蓝色）"></a>（1）BatchNorm 简单的 2 维 情况（蓝色）</h3><p>每一行是一个样本 X，每一列是 一个 feature</p><p>BatchNorm：每次把一列（1 个 feature）放在一个 mini-batch 里，均值变成 0， 方差变成 1 的标准化。</p><p>How：（该列向量 - mini-batch 该列向量的均值）/（mini - batch 该列向量的方差）</p><p>训练时：mini-batch 计算均值；</p><p>测试时：使用 全局 均值、方差。</p><p>BatchNorm 还会学 lambda beta，BatchNorm 可以通过学习将向量 放缩成 任意均值、任意方差 的一个向量。</p><h3 id="（2）Layernorm-（黄色）"><a href="#（2）Layernorm-（黄色）" class="headerlink" title="（2）Layernorm （黄色）"></a>（2）Layernorm （黄色）</h3><p>LayerNorm 跟 BatchNorm 在很多时候几乎是一样的，除了实现的方法有点不一样之外。</p><p>LayerNorm：对每个样本做 Normalization（把每一行变成 均值为 0、方差为 1），不是对每个特征做 normalization。</p><p><img src="image/image_GxT-ELqyUq.png" alt=""></p><h3 id="（3）LayerNorm-在操作上-和-BatchNorm-二维输入-的关系"><a href="#（3）LayerNorm-在操作上-和-BatchNorm-二维输入-的关系" class="headerlink" title="（3）LayerNorm 在操作上 和 BatchNorm (二维输入) 的关系"></a>（3）LayerNorm 在操作上 和 BatchNorm (二维输入) 的关系</h3><p>LayerNorm 整个把数据转置一次，放到 BatchNorm 里面出来的结果，再转置回去，基本上可以得到LayerNorm的结果。</p><h3 id="（4）三维输入"><a href="#（4）三维输入" class="headerlink" title="（4）三维输入"></a>（4）三维输入</h3><p>Transformer 和 RNN 里面：3 维输入。</p><ul><li>输入的是一个序列的样本，每个样本中有很多元素，是一个序列。</li><li>一个句子里面有 n 个词，每个词对应一个向量，+ 一个 batch —&gt; 3 维</li></ul><p>列 是 seq 序列长度 n；第 3 维 feature 是每个词额外的向量，d = 512 in transformer  </p><p><strong>BatchNorm</strong> （蓝色线）：每次取一个特征，切一块，拉成一个向量，均值为 0 、方差为 1 的标准化。</p><p><strong>LayerNorm (橙色)</strong>：横着切</p><p><img src="image/image_5mmUINSJAV.png" alt=""></p><h3 id="（5）举例"><a href="#（5）举例" class="headerlink" title="（5）举例"></a>（5）举例</h3><p>时序数据中 样本长度可能不一样。</p><p>举例分析：4个长度不一样的样本，0 填充到 max_len</p><p><img src="image/image_n3Y7nnXBBE.png" alt=""></p><p><strong>BatchNorm 切出来的结果</strong>，BatchNorm 计算均值和方差，有效的是阴影部分，其余是 0</p><p>Mini-batch 的均值和方差：如果样本长度变化比较大的时候，每次计算小批量的均值和方差，均值和方差的抖动大。</p><p>全局的均值和方差：测试时遇到一个特别长的全新样本 （最上方蓝色阴影块），训练时未见过，训练时计算的均值和方差可能不好用。</p><p><img src="image/image_rFLVX2KB_N.png" alt=""></p><p>LayerNorm 切出来的结果</p><p>LayerNorm 每个样本自己算均值和方差，不需要存全局的均值和方差。</p><p>LayerNorm 更稳定，不管样本长还是短，均值和方差是在每个样本内计算。</p><p><img src="image/image_ldiJhNEGuF.png" alt=""></p><h3 id="（6）LayerNorm-和-BatchNorm-的例子理解：n-本书"><a href="#（6）LayerNorm-和-BatchNorm-的例子理解：n-本书" class="headerlink" title="（6）LayerNorm 和 BatchNorm 的例子理解：n 本书"></a>（6）LayerNorm 和 BatchNorm 的例子理解：n 本书</h3><p><strong>BatchNorm</strong>：n本书，每本书的第一页拿出来，根据 n 本书的第一页的字数均值 做 Norm</p><p><strong>LayerNorm</strong>：针对某一本书，这本书的每一页拿出来，根据次数每页的字数均值，自己做 Norm</p><h2 id="5-2-注意力"><a href="#5-2-注意力" class="headerlink" title="5.2 注意力"></a>5.2 注意力</h2><h3 id="（1）注意力机制"><a href="#（1）注意力机制" class="headerlink" title="（1）注意力机制"></a>（1）注意力机制</h3><p>注意力函数是 一个将一个 query 和一些 key - value 对 映射成一个输出的函数，其中所有的 query、key、value 和 output 都是一些向量。</p><p>具体来说，output 是 value 的一个加权和 —&gt; 输出的维度 ==  value 的维度。</p><p>output 中 value 的权重 = 查询 query 和对应的 key 的相似度 or compatibility function</p><p>权重等价于 query 和对应的 key 的相似度</p><p><img src="image/image_CXnkw4YUSw.png" alt=""></p><p>图中，红色表示value，蓝色表示key：</p><ul><li>给定q为黄色，靠近key的第一第二个，所以output更多偏向与value的第一和第二个；</li><li>给定q为绿色，靠近key的第二第三个，所以output更多偏向于value的第二和第三个。</li></ul><p>虽然 key-value 并没有变，但是随着 query 的改变，因为权重的分配不一样，导致 输出会有不一样，这就是注意力机制。</p><h3 id="（2）实际使用"><a href="#（2）实际使用" class="headerlink" title="（2）实际使用"></a>（2）实际使用</h3><p>实际计算：不会一个 query 一个 query 的计算，因为运算比较慢。把多个 query 写成 一个矩阵，并行化运算。</p><p>每一行蓝色的线：一个 query 对所有 key 的内积值，然后再除以sqrt(dk)， 再做 softmax。 softmax 是对每一行的值做 softmax，然后每一行之间是独立的，会得到权重。</p><p>绿色的每一行它就是 attention。</p><p>attention 的计算：2次矩阵乘法、并行计算</p><p><img src="image/image_ns_bMcDUGb.png" alt=""></p><h3 id="（3）Scaled-Dot-Product-Attention-和-别的注意力机制的区别"><a href="#（3）Scaled-Dot-Product-Attention-和-别的注意力机制的区别" class="headerlink" title="（3）Scaled Dot-Product Attention 和 别的注意力机制的区别"></a>（3）Scaled Dot-Product Attention 和 别的注意力机制的区别</h3><p>2 种常见的注意力机制：加性的注意力机制（它可以处理你的 query 和 key 不等长的情况，点积 dot-product 的注意力机制 （本文采用 scaled，➗ sqrt(dk) ），所以你可以看到它的名字它叫做 scale 的。</p><p>选用 dot-product 原因：两种注意力机制其实都差不多， 点乘实现 简单、高效，两次矩阵乘法计算。</p><p>scale dot-product 原因 ➗ sqrt(dk) ：防止softmax函数的梯度消失。</p><p>dk不是很大的时候，➗ 不➗ 都 ok。dk 比较大时 （2 个向量的长度比较长的时候），点积的值会比较大，or 会比较小。</p><p>当你的值比较大的时候，相对的差距会变大，导致最大值 softmax会更加靠近于1，剩下那些值就会更加靠近于0。值就会更加向两端靠拢，算梯度的时候，梯度比较小。</p><p>softmax会让大的数据更大，小的更小</p><p>因为 softmax 最后的结果是希望 softmax 的预测值，置信的地方尽量靠近，不置信的地方尽量靠近零，以保证收敛差不多了。这时候梯度就会变得比较小，那就会跑不动。</p><p>在 trasformer 里面一般用的 dk 比较大 (本文 512)，所以➗ sqrt(dk) 是一个不错的选择。</p><h3 id="（4）怎么做mask？"><a href="#（4）怎么做mask？" class="headerlink" title="（4）怎么做mask？"></a>（4）怎么做mask？</h3><p>避免在 t 时刻，看到 t 时刻以后的输入。</p><p>在计算权重的时候，t 时刻只用了 v1, …, vt-1 的结果，不要用到 t 时刻以后的内容。</p><p>把 t 时刻以后 Qt 和 Kt 的值换成一个很大的负数，如 1 ^ (-10)，进入 softmax 后，权重为0。 —&gt; 和 V 矩阵做矩阵乘法时，没看到 t 时刻以后的内容，只看 t 时刻之前的 key - value pair。</p><p><strong>理解</strong>：mask是个 0 1矩阵，和attention（scale QK）size一样，t 时刻以后 mask 为 0。</p><h3 id="（5）多头注意力机制"><a href="#（5）多头注意力机制" class="headerlink" title="（5）多头注意力机制"></a>（5）多头注意力机制</h3><p>与其做一个单个的注意力函数，不如说把整个 query、key、value 整个投影 project 到 1个低维，投影 h 次。然后再做 h 次的注意力函数，把每一个函数的输出 拼接在一起，然后 again projected，会得到最终的输出。</p><p>输入是：原始的 value、key、query</p><p>进入一个线形层，线形层把 value、key、query 投影到比较低的维度。然后再做一个 scaled dot product 。</p><p>执行 h 次会得到 h 个输出，再把 h 个 输出向量全部合并 concat 在一起，最后做一次线性的投影 Linear，会回到我们的 multi-head attention。</p><p><strong>为什么要做多头注意力机制呢</strong>？一个 dot product 的注意力里面，没有什么可以学的参数。具体函数就是内积，为了识别不一样的模式，希望有不一样的计算相似度的办法。加性 attention 有一个权重可学，也许能学到一些内容。</p><p>本文的 dot-product attention，先投影到低维，投影的 w 是可以学习的。</p><p>multi-head attention 给 h 次机会去学习 不一样的投影的方法，使得在投影进去的度量空间里面能够去匹配不同模式需要的一些相似函数，然后把 h 个 heads 拼接起来，最后再做一次投影。</p><p>有点像 CNN 多个输出通道的感觉。</p><p>multi-head attention 具体公式</p><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat(head_1, ..., head_n)W^O \\where ~ head_i = Attention(QW_i^Q, KW_i^K, VW_i^v)</script><p>Multi-head 的输入还是Q,K,V</p><p>但是输出是 不同的头的输出的 concat 起来，再投影到一个 WO 里面。</p><p>每一个头 hi 是把 Q,K,V 通过 可以学习的 Wq, Wk, Wv 投影到 dv 上，再通过注意力函数，得到 headi。  </p><h2 id="5-3-FFN"><a href="#5-3-FFN" class="headerlink" title="5.3 FFN"></a>5.3 FFN</h2><p>作用在最后一个维度的 MLP</p><p>MLP: applied to each position separtely and identically.</p><p>Point-wise: 把一个 MLP 对每一个词 （position）作用一次，对每个词作用的是同样的 MLP</p><p>FFN： Linear + ReLU + Linear</p><p>单隐藏层的 MLP，中间 W1 扩维到4倍 2048，最后 W2 投影回到 512 维度大小，便于残差连接。</p><p>pytorch实现：2个线性层。pytorch在输入是3d的时候，默认在最后一个维度做计算。</p><h3 id="（1）Transformer和RNN"><a href="#（1）Transformer和RNN" class="headerlink" title="（1）Transformer和RNN"></a>（1）Transformer和RNN</h3><p>最简单情况：没有残差连接、没有 layernorm、 attention 单头、没有投影。看和 RNN 区别</p><p>attention 对输入做一个加权和，加权和 进入 point-wise MLP。（画了多个红色方块 MLP， 是一个权重相同的 MLP）</p><p>point-wise MLP 对 每个输入的点 做计算，得到输出。</p><p>attention 作用：把整个序列里面的信息抓取出来，做一次汇聚 aggregation</p><p><img src="image/image_ajhIlcE_-z.png" alt=""></p><p>RNN 跟 transformer <strong>异：如何传递序列的信</strong>息</p><p>RNN 是把上一个时刻的信息输出传入下一个时候做输入。Transformer 通过一个 attention 层，去全局的拿到整个序列里面信息，再用 MLP 做语义的转换。</p><p>RNN 跟 transformer <strong>同：语义空间的转换 + 关注点</strong></p><p>用一个线性层 or 一个 MLP 来做语义空间的转换。</p><p>关注点：怎么有效的去使用序列的信息。</p><h2 id="5-4-Embedding"><a href="#5-4-Embedding" class="headerlink" title="5.4 Embedding"></a>5.4 Embedding</h2><p>embedding：将输入的一个词、词语 token 映射成 为一个长为 d 的向量。学习到的长为 d 的向量 来表示整个词、词语 token。</p><h2 id="5-5-Position-Encoding"><a href="#5-5-Position-Encoding" class="headerlink" title="5.5 Position Encoding"></a>5.5 Position Encoding</h2><p>How：RNN 把上一时刻的输出 作为下一个时刻的输入，来传递时序信息。</p><p>How：attention 在输入里面加入时序信息 —&gt; positional encoding。一个在位置 i 的词，会把 i 位置信息加入到输入里面。</p><h1 id="6-实验"><a href="#6-实验" class="headerlink" title="6.实验"></a>6.实验</h1><h1 id="7-评论"><a href="#7-评论" class="headerlink" title="7.评论"></a>7.评论</h1><p><strong>Transformer 模型出圈 —&gt; 多模态</strong>：像 CNN 对 CV 的作用，不仅仅应用在NLP，在 CV、Video上也有很好的应用。</p><p>启示：一个新的模型可以在 DL 上 通用。人的感知是多模态的、使得 Transformer 在文本、语音、视频抽取多维特征。</p><p><strong>对 Transformer 中 attention 的理解</strong>：attention只是起到 把整个序列的信息聚合起来 的作用，后面的 MLP 和 残差连接 是缺一不可的。去掉 MLP 和 残差连接，只有 attention，也什么都训练不出来。</p><p>attention 用了更广泛的 inductive bias 归置偏置，使得 attention 没有用空间上的假设，取得和 CNN 一样、 甚至更好的结果。</p><p>代价：假设更加一般、对数据的抓取能力差，需要使用更多的数据、更大的模型 才能训练出一样的效果</p><h1 id="8-Transformer常见问题与回答总结"><a href="#8-Transformer常见问题与回答总结" class="headerlink" title="8.Transformer常见问题与回答总结"></a>8.Transformer常见问题与回答总结</h1><p><strong>Transformer为何使用多头注意力机制？</strong>（为什么不使用一个头）</p><ul><li>多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。可以类比CNN中同时使用<strong>多个滤波器</strong>的作用，直观上讲，多头的注意力<strong>有助于网络捕捉到更丰富的特征/信息。</strong></li><li>参考：<a href="https://www.zhihu.com/question/341222779" title="https://www.zhihu.com/question/341222779">https://www.zhihu.com/question/341222779</a></li></ul><p><strong>Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</strong> （注意和第一个问题的区别）</p><ul><li>使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。</li><li>同时，由softmax函数的性质决定，实质做的是一个soft版本的arg max操作，得到的向量接近一个one-hot向量（接近程度根据这组数的数量级有所不同）。如果令Q=K，那么得到的模型大概率会得到一个类似单位矩阵的attention矩阵，<strong>这样self-attention就退化成一个point-wise线性映射</strong>。这样至少是违反了设计的初衷。</li><li>参考：<a href="https://www.zhihu.com/question/319339652" title="https://www.zhihu.com/question/319339652">https://www.zhihu.com/question/319339652</a></li></ul><p><strong>Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</strong></p><ul><li>K和Q的点乘是为了得到一个attention score 矩阵，用来对V进行提纯。K和Q使用了不同的W_k, W_Q来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。</li><li>为了计算更快。矩阵加法在加法这一块的计算量确实简单，但是作为一个整体计算attention的时候相当于一个隐层，整体计算量和点积相似。在效果上来说，从实验分析，两者的效果和dk相关，dk越大，加法的效果越显著。</li></ul><p><strong>为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根）</strong>，并使用公式推导进行讲解</p><ul><li>这取决于softmax函数的特性，如果softmax内计算的数数量级太大，会输出近似one-hot编码的形式，导致梯度消失的问题，所以需要scale</li><li>那么至于为什么需要用维度开根号，假设向量q，k满足各分量独立同分布，均值为0，方差为1，那么qk点积均值为0，方差为dk，从统计学计算，若果让qk点积的方差控制在1，需要将其除以dk的平方根，是的softmax更加平滑</li><li>参考：<a href="https://www.zhihu.com/question/339723385/answer/782509914" title="https://www.zhihu.com/question/339723385/answer/782509914">https://www.zhihu.com/question/339723385/answer/782509914</a></li></ul><p><strong>在计算attention score的时候如何对padding做mask操作？</strong></p><ul><li>padding位置置为负无穷(一般来说-1000就可以)，再对attention score进行相加。对于这一点，涉及到batch_size之类的，具体的大家可以看一下抱抱脸实现的源代码，位置在这里：<a href="https://link.zhihu.com/?target=https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720" title="https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720">https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720</a></li></ul><p><strong>为什么在进行多头注意力的时候需要对每个head进行降维？</strong>（可以参考上面一个问题）</p><ul><li>将原有的<strong>高维空间转化为多个低维空间</strong>并再最后进行拼接，形成同样维度的输出，借此丰富特性信息<ul><li>基本结构：Embedding + Position Embedding，Self-Attention，Add + LN，FN，Add + LN</li></ul></li></ul><p><strong>为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？</strong></p><ul><li>embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</li></ul><p><strong>简单介绍一下Transformer的位置编码？有什么意义和优缺点？</strong></p><ul><li>因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden embedding都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个token的位置信息，transformer使用了固定的positional encoding来表示token在句子中的绝对位置信息。<ul><li><a href="https://link.zhihu.com/?target=https://mp.weixin.qq.com/s/QxaZTVOUrzKfO7B78EM5Uw" title="一文读懂Transformer模型的位置编码">一文读懂Transformer模型的位置编码</a></li><li><a href="https://link.zhihu.com/?target=https://mp.weixin.qq.com/s/vXYJKF9AViKnd0tbuhMWgQ" title="浅谈Transformer模型中的位置表示">浅谈Transformer模型中的位置表示</a></li><li><a href="https://link.zhihu.com/?target=https://mp.weixin.qq.com/s/NPM3w7sIYVLuMYxQ_R6PrA" title="Transformer改进之相对位置编码RPE">Transformer改进之相对位置编码RPE</a></li><li><a href="https://link.zhihu.com/?target=https://mp.weixin.qq.com/s/ENpXBYQ4hfdTLSXBIoF00Q" title="如何优雅地编码文本中的位置信息？三种positioanl encoding方法简述">如何优雅地编码文本中的位置信息？三种positioanl encoding方法简述</a></li><li><a href="https://link.zhihu.com/?target=https://www.cnblogs.com/shiyublog/p/11185625.html" title="相对位置编码一)Relative Position Representatitons RPR - Transformer">相对位置编码一)Relative Position Representatitons RPR - Transformer</a></li><li><a href="https://link.zhihu.com/?target=https://www.cnblogs.com/shiyublog/p/11236212.html" title="相对位置编码(二) Relative Positional Encodings - Transformer-XL">相对位置编码(二) Relative Positional Encodings - Transformer-XL</a></li></ul></li></ul><p><strong>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</strong>（参考上一题）</p><ul><li>相对位置编码（RPE）1.在计算attention score和weighted value时各加入一个可训练的表示相对位置的参数。2.在生成多头注意力时，把对key来说将绝对位置转换为相对query的位置3.复数域函数，已知一个词在某个位置的词向量表示，可以计算出它在任何位置的词向量表示。前两个方法是词向量+位置编码，属于亡羊补牢，复数域是生成词向量的时候即生成对应的位置信息。</li></ul><p><strong>简单讲一下Transformer中的残差结构以及意义。</strong></p><ul><li>就是ResNet的优点，解决梯度消失</li></ul><p><strong>为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</strong></p><ul><li>LN：针对每个样本序列进行Norm，没有样本间的依赖。对一个序列的不同特征维度进行Norm</li><li>CV使用BN是认为channel维度的信息对cv方面有重要意义，如果对channel维度也归一化会造成不同通道信息一定的损失。而同理nlp领域认为句子长度不一致，并且各个batch的信息没什么关系，因此只考虑句子内信息的归一化，也就是LN。</li></ul><p><img src="image/image_qKWpmJyP38.png" alt=""></p><p><strong>简答讲一下BatchNorm技术，以及它的优缺点。</strong></p><ul><li>优点：<ul><li>第一个就是可以解决内部协变量偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN缓解了这个问题。当然后来也有论文证明BN有作用和这个没关系，而是可以使<strong>损失平面更加的平滑</strong>，从而加快的收敛速度。</li><li>第二个优点就是缓解了<strong>梯度饱和问题</strong>（如果使用sigmoid激活函数的话），加快收敛。</li></ul></li><li>缺点：<ul><li>第一个，batch_size较小的时候，效果差。这一点很容易理解。BN的过程，使用 整个batch中样本的均值和方差来模拟全部数据的均值和方差，在batch_size 较小的时候，效果肯定不好。</li><li>第二个缺点就是 BN 在RNN中效果比较差。</li></ul></li></ul><p><strong>简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</strong></p><ul><li>ReLU</li></ul><script type="math/tex; mode=display">FFN(x)=max(0, xW_1+b_1)W_2+b_2</script><p><strong>Encoder端和Decoder端是如何进行交互的？</strong>（在这里可以问一下关于seq2seq的attention知识）</p><ul><li>Cross Self-Attention，Decoder提供Q，Encoder提供K，V</li></ul><p><strong>Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？</strong>（为什么需要decoder自注意力需要进行 sequence mask)</p><ul><li>让输入序列只看到过去的信息，不能让他看到未来的信息</li></ul><p><strong>Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？</strong></p><ul><li>Encoder侧：模块之间是串行的，一个模块计算的结果做为下一个模块的输入，互相之前有依赖关系。从每个模块的角度来说，注意力层和前馈神经层这两个子模块单独来看都是可以并行的，不同单词之间是没有依赖关系的。</li><li>Decode引入sequence mask就是为了并行化训练，Decoder推理过程没有并行，只能一个一个的解码，很类似于RNN，这个时刻的输入依赖于上一个时刻的输出。</li></ul><p><strong>简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？</strong></p><ul><li>传统词表示方法无法很好的处理未知或罕见的词汇（OOV问题），传统词tokenization方法不利于模型学习词缀之间的关系”</li><li>BPE（字节对编码）或二元编码是一种简单的数据压缩形式，其中最常见的一对连续字节数据被替换为该数据中不存在的字节。后期使用时需要一个替换表来重建原始数据。</li><li>优点：可以有效地平衡词汇表大小和步数（编码句子所需的token次数）。</li><li>缺点：基于贪婪和确定的符号替换，不能提供带概率的多个分片结果。</li></ul><p><strong>Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</strong></p><ul><li>Dropout测试的时候记得对输入整体呈上dropout的比率</li></ul><p><strong>引申一个关于bert问题，bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</strong></p><ul><li>BERT和transformer的目标不一致，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。</li></ul>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> PaperReading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer构建语言模型</title>
      <link href="/llms/transformer/3.Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
      <url>/llms/transformer/3.Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer构建语言模型"><a href="#Transformer构建语言模型" class="headerlink" title="Transformer构建语言模型"></a>Transformer构建语言模型</h1><p>本文主要来自：<a href="http://nlp.seas.harvard.edu/annotated-transformer/#hardware-and-schedule">The Annotated Transformer</a></p><p>Transformer Model：<a href="/Transformer/2.Transformer架构解析">Transformer架构解析</a></p><p>论文地址: <a href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></p><p>代码链接：<a href="https://github.com/wdndev/personal/tree/main/transformer">personal/transformer · GitHub</a></p><p><strong>什么是语言模型</strong>:</p><p>以一个符合语言规律的序列为输入，模型将利用序列间关系等特征，输出一个在所有词汇上的概率分布.这样的模型称为语言模型.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语言模型的训练语料一般来自于文章，对应的源文本和目标文本形如:</span></span><br><span class="line">src1 = <span class="string">&quot;I can do&quot;</span> tgt1 = <span class="string">&quot;can do it&quot;</span></span><br><span class="line">src2 = <span class="string">&quot;can do it&quot;</span>, tgt2 = <span class="string">&quot;do it &lt;eos&gt;&quot;</span></span><br></pre></td></tr></table></figure><hr><p><strong>语言模型能解决哪些问题</strong></p><ol><li>根据语言模型的定义，可以在它的基础上完成机器翻译，文本生成等任务，因为我们通过最后输出的概率分布来预测下一个词汇是什么.</li><li>语言模型可以判断输入的序列是否为一句完整的话，因为我们可以根据输出的概率分布查看最大概率是否落在句子结束符上，来判断完整性.</li><li>语言模型本身的训练目标是预测下一个词，因为它的特征提取部分会抽象很多语言序列之间的关系，这些关系可能同样对其他语言类任务有效果。因此可以作为预训练模型进行迁移学习.</li></ol><p><strong>整个案例的实现可分为以下五个步骤</strong></p><ul><li>第一步: 导入必备的工具包</li><li>第二步: 导入wikiText-2数据集并作基本处理</li><li>第三步: 构建用于模型输入的批次化数据</li><li>第四步: 构建训练和评估函数</li><li>第五步: 进行训练和评估(包括验证以及测试)</li></ul><h2 id="1-导入必备的工具包"><a href="#1-导入必备的工具包" class="headerlink" title="1.导入必备的工具包"></a>1.导入必备的工具包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数学计算工具包math</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch以及torch.nn, torch.nn.functional</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch中经典文本数据集有关的工具包</span></span><br><span class="line"><span class="comment"># 具体详情参考下方torchtext介绍</span></span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"></span><br><span class="line"><span class="comment"># torchtext中的数据处理工具, get_tokenizer用于英文分词</span></span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 已经构建完成的TransformerModel</span></span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer <span class="keyword">import</span> TransformerModel</span><br></pre></td></tr></table></figure><p><strong>torchtext介绍</strong>：它是torch工具中处理NLP问题的常用数据处理包.</p><p><strong>torchtext的重要功能：</strong></p><ul><li>对文本数据进行处理, 比如文本语料加载, 文本迭代器构建等.</li><li>包含很多经典文本语料的预加载方法. 其中包括的语料有：用于情感分析的SST和IMDB, 用于问题分类的TREC, 用于及其翻译的 WMT14， IWSLT，以及用于语言模型任务wikiText-2, WikiText103, PennTreebank.</li></ul><p>我们这里使用wikiText-2来训练语言模型, 下面有关该数据集的相关详情:</p><p><img src="image/image_R1ZgEz5vwJ.png" alt=""></p><p>wikiText-2数据集的体量中等, 训练集共有600篇短文, 共208万左右的词汇, 33278个不重复词汇, OoV（有多少正常英文词汇不在该数据集中的占比）为2.6%，数据集中的短文都是维基百科中对一些概念的介绍和描述.</p><h2 id="2-导入wikiText-2数据集并作基本处理"><a href="#2-导入wikiText-2数据集并作基本处理" class="headerlink" title="2.导入wikiText-2数据集并作基本处理"></a>2.导入wikiText-2数据集并作基本处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="comment"># 英文分词工具包</span></span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line"><span class="keyword">from</span> torchtext.legacy.data <span class="keyword">import</span> Field</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_wiki_text2_data</span>():</span><br><span class="line">    <span class="comment"># 创建语料域, 语料域是存放语料的数据结构, </span></span><br><span class="line">    <span class="comment"># 它的四个参数代表给存放语料（或称作文本）施加的作用. </span></span><br><span class="line">    <span class="comment"># 分别为 tokenize,使用get_tokenizer(&quot;basic_english&quot;)获得一个分割器对象,</span></span><br><span class="line">    <span class="comment"># 分割方式按照文本为基础英文进行分割. </span></span><br><span class="line">    <span class="comment"># init_token为给文本施加的起始符 &lt;sos&gt;给文本施加的终止符&lt;eos&gt;, </span></span><br><span class="line">    <span class="comment"># 最后一个lower为True, 存放的文本字母全部小写.</span></span><br><span class="line">    text_field = Field(tokenize=get_tokenizer(<span class="string">&quot;basic_english&quot;</span>),</span><br><span class="line">                    init_token=<span class="string">&#x27;&lt;sos&gt;&#x27;</span>,</span><br><span class="line">                    eos_token=<span class="string">&#x27;&lt;eos&gt;&#x27;</span>,</span><br><span class="line">                    lower=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 最终获得一个Field对象.</span></span><br><span class="line">    <span class="comment"># &lt;torchtext.data.field.Field object at 0x7fc42a02e7f0&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 然后使用torchtext的数据集方法导入WikiText2数据, </span></span><br><span class="line">    <span class="comment"># 并切分为对应训练文本, 验证文本，测试文本, 并对这些文本施加刚刚创建的语料域.</span></span><br><span class="line">    train_txt, val_txt, test_txt = torchtext.legacy.datasets.WikiText2.splits(text_field)</span><br><span class="line">    <span class="comment"># print(test_txt.examples[0].text[:10])</span></span><br><span class="line">    <span class="comment"># 我们可以通过examples[0].text取出文本对象进行查看.</span></span><br><span class="line">    <span class="comment"># &gt;&gt;&gt; test_txt.examples[0].text[:10]</span></span><br><span class="line">    <span class="comment"># [&#x27;&lt;eos&gt;&#x27;, &#x27;=&#x27;, &#x27;robert&#x27;, &#x27;&lt;unk&gt;&#x27;, &#x27;=&#x27;, &#x27;&lt;eos&gt;&#x27;, &#x27;&lt;eos&gt;&#x27;, &#x27;robert&#x27;, &#x27;&lt;unk&gt;&#x27;, &#x27;is&#x27;]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将训练集文本数据构建一个vocab对象, </span></span><br><span class="line">    <span class="comment"># 这样可以使用vocab对象的stoi方法统计文本共包含的不重复词汇总数.</span></span><br><span class="line">    text_field.build_vocab(train_txt)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text_field, train_txt, val_txt, test_txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后选择设备cuda或者cpu</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="3-构建用于模型输入的批次化数据"><a href="#3-构建用于模型输入的批次化数据" class="headerlink" title="3.构建用于模型输入的批次化数据"></a>3.构建用于模型输入的批次化数据</h2><h3 id="3-1-文本数据映射函数"><a href="#3-1-文本数据映射函数" class="headerlink" title="3.1 文本数据映射函数"></a>3.1 文本数据映射函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batchify</span>(<span class="params">text_field:Field, data, batch_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 构建批次数据, 将文本数据映射成连续数字, 并转换成指定的样式</span></span><br><span class="line"><span class="string">        - data : 代表之前得到的文本数据（train_txt, val_txt, test_txt）</span></span><br><span class="line"><span class="string">        - batch_size: 每次模型更新参数的数据量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 使用TEXT的numericalize方法将单词映射成对应的连续数字.</span></span><br><span class="line">    data = text_field.numericalize([data.examples[<span class="number">0</span>].text])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着用数据词汇总数除以batch_size,</span></span><br><span class="line">    <span class="comment"># 取整数得到一个nbatch代表需要多少次batch后能够遍历完所有数据</span></span><br><span class="line">    nbatch = data.size(<span class="number">0</span>) // batch_size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 之后使用narrow方法对不规整的剩余数据进行删除,</span></span><br><span class="line">    <span class="comment"># 第一个参数是代表横轴删除还是纵轴删除, 0为横轴，1为纵轴</span></span><br><span class="line">    <span class="comment"># 第二个和第三个参数代表保留开始轴到结束轴的数值.类似于切片</span></span><br><span class="line">    data = data.narrow(<span class="number">0</span>, <span class="number">0</span>, nbatch * batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着我们使用view方法对data进行矩阵变换, 使其成为如下样式:</span></span><br><span class="line">    <span class="comment"># tensor([[    3,    25,  1849,  ...,     5,    65,    30],</span></span><br><span class="line">    <span class="comment">#    [   12,    66,    13,  ...,    35,  2438,  4064],</span></span><br><span class="line">    <span class="comment">#    [ 3852, 13667,  2962,  ...,   902,    33,    20],</span></span><br><span class="line">    <span class="comment">#    ...,</span></span><br><span class="line">    <span class="comment">#    [  154,     7,    10,  ...,     5,  1076,    78],</span></span><br><span class="line">    <span class="comment">#    [   25,     4,  4135,  ...,     4,    56,   299],</span></span><br><span class="line">    <span class="comment">#    [    6,    57,   385,  ...,  3168,   737,    36]])</span></span><br><span class="line">    <span class="comment"># 因为会做转置操作, 因此这个矩阵的形状是[None, bsz],</span></span><br><span class="line">    <span class="comment"># 如果输入是训练数据的话，形状为[104335, 20], 可以通过打印data.shape获得.</span></span><br><span class="line">    <span class="comment"># 也就是data的列数是等于bsz的值的.</span></span><br><span class="line">    data = data.view(batch_size, -<span class="number">1</span>).t().contiguous()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data.to(device)</span><br></pre></td></tr></table></figure><p><strong><code>batchify</code>**</strong>的样式转化图**：大写字母A，B，C … 代表句子中的每个单词.</p><p><img src="image/image_4Y3uomKplq.png" alt=""></p><p>使用<code>batchify</code>来处理训练数据，验证数据以及测试数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练数据的batch size</span></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证和测试数据（统称为评估数据）的batch size</span></span><br><span class="line">eval_batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得train_data, val_data, test_data</span></span><br><span class="line">train_data = batchify(train_txt, batch_size)</span><br><span class="line">val_data = batchify(val_txt, eval_batch_size)</span><br><span class="line">test_data = batchify(test_txt, eval_batch_size)</span><br></pre></td></tr></table></figure><p>上面的分割批次并没有进行源数据与目标数据的处理, 接下来将根据语言模型训练的语料规定来构建源数据与目标数据.</p><p><strong>语言模型训练的语料规定</strong>：</p><ul><li>如果源数据为句子ABCD, ABCD代表句子中的词汇或符号, 则它的目标数据为BCDE, BCDE分别代表ABCD的下一个词汇.</li></ul><p><img src="image/image_bw8T9TKMkV.png" alt=""></p><ul><li>如图所示，我们这里的句子序列是竖着的, 而且发现如果用一个批次处理完所有数据, 以训练数据为例, 每个句子长度高达104335, 这明显是不科学的, 因此我们在这里要限定每个批次中的句子长度允许的最大值<code>bptt</code>.</li></ul><h3 id="3-2-分割源数据和目标数据"><a href="#3-2-分割源数据和目标数据" class="headerlink" title="3.2 分割源数据和目标数据"></a>3.2 分割源数据和目标数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_batch</span>(<span class="params">src, i, bptt=<span class="number">35</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;  用于获得每个批次合理大小的源数据和目标数据</span></span><br><span class="line"><span class="string">        - src: 通过batchify得到的train_data/val_data/test_data.</span></span><br><span class="line"><span class="string">        - i: 批次数</span></span><br><span class="line"><span class="string">        - bptt: 句子的最大长度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 首先我们确定句子长度, 它将是在bptt和len(source) - 1 - i中最小值</span></span><br><span class="line">    <span class="comment"># 实质上, 前面的批次中都会是bptt的值, 只不过最后一个批次中, 句子长度</span></span><br><span class="line">    <span class="comment"># 可能不够bptt的35个, 因此会变为len(source) - 1 - i的值.</span></span><br><span class="line">    seq_len = <span class="built_in">min</span>(bptt, <span class="built_in">len</span>(src) - <span class="number">1</span> - i)</span><br><span class="line">    <span class="comment"># 语言模型训练的源数据的第i批数据将是batchify的结果的切片[i:i+seq_len]</span></span><br><span class="line">    data = src[i : i + seq_len]</span><br><span class="line">    <span class="comment"># 根据语言模型训练的语料规定, 它的目标数据是源数据向后移动一位</span></span><br><span class="line">    <span class="comment"># 因为最后目标数据的切片会越界, 因此使用view(-1)来保证形状正常.</span></span><br><span class="line">    target = src[i + <span class="number">1</span> : i + <span class="number">1</span> + seq_len].view(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data, target</span><br></pre></td></tr></table></figure><p>测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">text_field, train_txt, val_txt, test_txt = get_wiki_text2_data()</span><br><span class="line"><span class="comment"># 训练数据的batch size</span></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line"><span class="comment"># 验证和测试数据（统称为评估数据）的batch size</span></span><br><span class="line">eval_batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得train_data, val_data, test_data</span></span><br><span class="line">train_data = batchify(text_field, train_txt, batch_size)</span><br><span class="line">val_data = batchify(text_field, val_txt, eval_batch_size)</span><br><span class="line">test_data = batchify(text_field, test_txt, eval_batch_size)</span><br><span class="line"></span><br><span class="line">x, y = get_batch(test_data, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">tensor([[   12,  1053,   355,   134,    37,     7,     4,     0,   835,  9834],</span><br><span class="line">        [  635,     8,     5,     5,   421,     4,    88,     8,   573,  2511],</span><br><span class="line">        [    0,    58,     8,     8,     6,   692,   544,     0,   212,     5],</span><br><span class="line">        [   12,     0,   105,    26,     3,     5,     6,     0,     4,    56],</span><br><span class="line">        [    3, 16074, 21254,   320,     3,   262,    16,     6,  1087,    89],</span><br><span class="line">        [    3,   751,  3866,    10,    12,    31,   246,   238,    79,    49],</span><br><span class="line">        [  635,   943,    78,    36,    12,   475,    66,    10,     4,   924],</span><br><span class="line">        [    0,  2358,    52,     4,    12,     4,     5,     0, 19831,    21],</span><br><span class="line">        [   26,    38,    54,    40,  1589,  3729,  1014,     5,     8,     4],</span><br><span class="line">        [   33, 17597,    33,  1661,    15,     7,     5,     0,     4,   170],</span><br><span class="line">        [  335,   268,   117,     0,     0,     4,  3144,  1557,     0,   160],</span><br><span class="line">        [  106,     4,  4706,  2245,    12,  1074,    13,  2105,     5,    29],</span><br><span class="line">        [    5, 16074,    10,  1087,    12,   137,   251, 13238,     8,     4],</span><br><span class="line">        [  394,   746,     4,     9,    12,  6032,     4,  2190,   303, 12651],</span><br><span class="line">        [    8,   616,  2107,     4,     3,     4,   425,     0,    10,   510],</span><br><span class="line">        [ 1339,   112,    23,   335,     3, 22251,  1162,     9,    11,     9],</span><br><span class="line">        [ 1212,   468,     6,   820,     9,     7,  1231,  4202,  2866,   382],</span><br><span class="line">        [    6,    24,   104,     6,     4,     4,     7,    10,     9,   588],</span><br><span class="line">        [   31,   190,     0,     0,   230,   267,     4,   273,   278,     6],</span><br><span class="line">        [   34,    25,    47,    26,  1864,     6,   694,     0,  2112,     3],</span><br><span class="line">        [   11,     6,    52,   798,     8,    69,    20,    31,    63,     9],</span><br><span class="line">        [ 1800,    25,  2141,  2442,   117,    31,   196,  7290,     4,   298],</span><br><span class="line">        [   15,   171,    15,    17,  1712,    13,   217,    59,   736,     5],</span><br><span class="line">        [ 4210,   191,   142,    14,  5251,   939,    59,    38, 10055, 25132],</span><br><span class="line">        [  302,    23, 11718,    11,    11,   599,   382,   317,     8,    13],</span><br><span class="line">        [   16,  1564,     9,  4808,     6,     0,     6,     6,     4,     4],</span><br><span class="line">        [    4,     7,    39,     7,  3934,     5,     9,     3,  8047,   557],</span><br><span class="line">        [  394,     0, 10715,  3580,  8682,    31,   242,     0, 10055,   170],</span><br><span class="line">        [   96,     6,   144,  3403,     4,    13,  1014,    14,     6,  2395],</span><br><span class="line">        [    4,     3, 13729,    14,    40,     0,     5,    18,   676,  3267],</span><br><span class="line">        [ 1031,     3,     0,   628,  1589,    22, 10916, 10969,     5, 22548],</span><br><span class="line">        [    9,    12,     6,    84,    15,    49,  3144,     7,   102,    15],</span><br><span class="line">        [  916,    12,     4,   203,     0,   273,   303,   333,  4318,     0],</span><br><span class="line">        [    6,    12,     0,  4842,     5,    17,     4,    47,  4138,  2072],</span><br><span class="line">        [   38,   237,     5,    50,    35,    27, 18530,   244,    20,     6]],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor([  635,     8,     5,     5,   421,     4,    88,     8,   573,  2511,</span><br><span class="line">            0,    58,     8,     8,     6,   692,   544,     0,   212,     5,</span><br><span class="line">           12,     0,   105,    26,     3,     5,     6,     0,     4,    56,</span><br><span class="line">            3, 16074, 21254,   320,     3,   262,    16,     6,  1087,    89,</span><br><span class="line">            3,   751,  3866,    10,    12,    31,   246,   238,    79,    49,</span><br><span class="line">          635,   943,    78,    36,    12,   475,    66,    10,     4,   924,</span><br><span class="line">            0,  2358,    52,     4,    12,     4,     5,     0, 19831,    21,</span><br><span class="line">           26,    38,    54,    40,  1589,  3729,  1014,     5,     8,     4,</span><br><span class="line">           33, 17597,    33,  1661,    15,     7,     5,     0,     4,   170,</span><br><span class="line">          335,   268,   117,     0,     0,     4,  3144,  1557,     0,   160,</span><br><span class="line">          106,     4,  4706,  2245,    12,  1074,    13,  2105,     5,    29,</span><br><span class="line">            5, 16074,    10,  1087,    12,   137,   251, 13238,     8,     4,</span><br><span class="line">          394,   746,     4,     9,    12,  6032,     4,  2190,   303, 12651,</span><br><span class="line">            8,   616,  2107,     4,     3,     4,   425,     0,    10,   510,</span><br><span class="line">         1339,   112,    23,   335,     3, 22251,  1162,     9,    11,     9,</span><br><span class="line">         1212,   468,     6,   820,     9,     7,  1231,  4202,  2866,   382,</span><br><span class="line">            6,    24,   104,     6,     4,     4,     7,    10,     9,   588,</span><br><span class="line">           31,   190,     0,     0,   230,   267,     4,   273,   278,     6,</span><br><span class="line">           34,    25,    47,    26,  1864,     6,   694,     0,  2112,     3,</span><br><span class="line">           11,     6,    52,   798,     8,    69,    20,    31,    63,     9,</span><br><span class="line">         1800,    25,  2141,  2442,   117,    31,   196,  7290,     4,   298,</span><br><span class="line">           15,   171,    15,    17,  1712,    13,   217,    59,   736,     5,</span><br><span class="line">         4210,   191,   142,    14,  5251,   939,    59,    38, 10055, 25132,</span><br><span class="line">          302,    23, 11718,    11,    11,   599,   382,   317,     8,    13,</span><br><span class="line">           16,  1564,     9,  4808,     6,     0,     6,     6,     4,     4,</span><br><span class="line">            4,     7,    39,     7,  3934,     5,     9,     3,  8047,   557,</span><br><span class="line">          394,     0, 10715,  3580,  8682,    31,   242,     0, 10055,   170,</span><br><span class="line">           96,     6,   144,  3403,     4,    13,  1014,    14,     6,  2395,</span><br><span class="line">            4,     3, 13729,    14,    40,     0,     5,    18,   676,  3267,</span><br><span class="line">         1031,     3,     0,   628,  1589,    22, 10916, 10969,     5, 22548,</span><br><span class="line">            9,    12,     6,    84,    15,    49,  3144,     7,   102,    15,</span><br><span class="line">          916,    12,     4,   203,     0,   273,   303,   333,  4318,     0,</span><br><span class="line">            6,    12,     0,  4842,     5,    17,     4,    47,  4138,  2072,</span><br><span class="line">           38,   237,     5,    50,    35,    27, 18530,   244,    20,     6,</span><br><span class="line">           13,  1083,    35,  1990,   653,    13,    10,    11,  1538,    56],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="4-构建训练和评估函数"><a href="#4-构建训练和评估函数" class="headerlink" title="4.构建训练和评估函数"></a>4.构建训练和评估函数</h2><p>模型训练代码分析:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ones</span>(<span class="params">model, epoch, train_data, criterion, optimizer, scheduler, ntokens, bptt=<span class="number">35</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 训练函数</span></span><br><span class="line"><span class="string">        - model: 训练模型</span></span><br><span class="line"><span class="string">        - train_data: 训练数据</span></span><br><span class="line"><span class="string">        - criterion: 损失函数</span></span><br><span class="line"><span class="string">        - optimizer: 优化器</span></span><br><span class="line"><span class="string">        - scheduler: 学习率下降函数</span></span><br><span class="line"><span class="string">        - ntokens: 不重复词汇总数</span></span><br><span class="line"><span class="string">        - bptt=35 : 句子的最大长度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 模型开启训练模式</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment"># 定义初始损失为0</span></span><br><span class="line">    total_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 日志打印间隔定为200</span></span><br><span class="line">    log_interval = <span class="number">200</span></span><br><span class="line">    <span class="comment"># 获得当前时间</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="comment"># 开始遍历批次数据</span></span><br><span class="line">    <span class="keyword">for</span> batch, i <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">range</span>(<span class="number">0</span>, train_data.size(<span class="number">0</span>) - <span class="number">1</span>, bptt)):</span><br><span class="line">        <span class="comment"># 通过get_batch获得源数据和目标数据</span></span><br><span class="line">        data, targets = get_batch(train_data, i, bptt)</span><br><span class="line">        <span class="comment"># 设置优化器初始梯度为0梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 将数据装入model得到输出</span></span><br><span class="line">        output = model(data)</span><br><span class="line">        <span class="comment"># 将输出和目标数据传入损失函数对象</span></span><br><span class="line">        loss = criterion(output.view(-<span class="number">1</span>, ntokens), targets)</span><br><span class="line">        <span class="comment"># 损失进行反向传播以获得总的损失</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 使用nn自带的clip_grad_norm_方法进行梯度规范化, 防止出现梯度消失或爆炸</span></span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">        <span class="comment"># 模型参数进行更新</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># 将每层的损失相加获得总的损失</span></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        <span class="comment"># 如果batch是200的倍数且大于0，则打印相关日志</span></span><br><span class="line">        <span class="keyword">if</span> batch % log_interval == <span class="number">0</span> <span class="keyword">and</span> batch &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 平均损失为总损失除以log_interval</span></span><br><span class="line">            cur_loss = total_loss / log_interval</span><br><span class="line">            <span class="comment"># 需要的时间为当前时间减去开始时间</span></span><br><span class="line">            elapsed = time.time() - start_time</span><br><span class="line">            <span class="comment"># 打印轮数, 当前批次和总批次, 当前学习率, 训练速度(每豪秒处理多少批次),</span></span><br><span class="line">            <span class="comment"># 平均损失, 以及困惑度, 困惑度是衡量语言模型的重要指标, 它的计算方法就是</span></span><br><span class="line">            <span class="comment"># 对交叉熵平均损失取自然对数的底数.</span></span><br><span class="line">            <span class="comment"># print(&#x27;| epoch &#123;:3d&#125; | &#123;:5d&#125;/&#123;:5d&#125; batches | &#x27;</span></span><br><span class="line">            <span class="comment">#       &#x27;lr &#123;:02.2f&#125; | ms/batch &#123;:5.2f&#125; | &#x27;</span></span><br><span class="line">            <span class="comment">#       &#x27;loss &#123;:5.2f&#125; | ppl &#123;:8.2f&#125;&#x27;.format(</span></span><br><span class="line">            <span class="comment">#         epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],</span></span><br><span class="line">            <span class="comment">#         elapsed * 1000 / log_interval,</span></span><br><span class="line">            <span class="comment">#         cur_loss, math.exp(cur_loss)))</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;| epoch &#123;:3d&#125; | &#123;:5d&#125;/&#123;:5d&#125; batches | &#x27;</span></span><br><span class="line">                  <span class="string">&#x27;lr &#123;:02.2f&#125; | ms/batch &#123;:5.2f&#125; | &#x27;</span></span><br><span class="line">                  <span class="string">&#x27;loss &#123;:5.2f&#125; | ppl &#123;:8.2f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    epoch, batch, <span class="built_in">len</span>(train_data) // bptt, scheduler.get_lr()[<span class="number">0</span>],</span><br><span class="line">                    elapsed * <span class="number">1000</span> / log_interval,</span><br><span class="line">                    cur_loss, <span class="number">00</span>))</span><br><span class="line">            <span class="comment"># 每个批次结束后, 总损失归0</span></span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            <span class="comment"># 开始时间取当前时间</span></span><br><span class="line">            start_time = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>模型评估代码分析:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">eval_model, datasets, criterion, ntokens, bptt=<span class="number">35</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 评估函数, 评估阶段包括验证和测试</span></span><br><span class="line"><span class="string">        - eval_model: 每轮训练产生的模型</span></span><br><span class="line"><span class="string">        - datasets: 验证或测试数据集</span></span><br><span class="line"><span class="string">        - criterion: 损失函数</span></span><br><span class="line"><span class="string">        - ntokens: 不重复词汇总数</span></span><br><span class="line"><span class="string">        - bptt=35: 句子的最大长度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 模型开启评估模式</span></span><br><span class="line">    eval_model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 总损失归0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 因为评估模式模型参数不变, 因此反向传播不需要求导, 以加快计算</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 与训练过程相同, 但是因为过程不需要打印信息, 因此不需要batch数</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, datasets.size(<span class="number">0</span>) - <span class="number">1</span>, bptt):</span><br><span class="line">            <span class="comment"># 首先还是通过通过get_batch获得验证数据集的源数据和目标数据</span></span><br><span class="line">            data, targets = get_batch(datasets, i, bptt)</span><br><span class="line">            <span class="comment"># 通过eval_model获得输出</span></span><br><span class="line">            output = eval_model(data)</span><br><span class="line">            <span class="comment"># 对输出形状扁平化, 变为全部词汇的概率分布</span></span><br><span class="line">            output_flat = output.view(-<span class="number">1</span>, ntokens)</span><br><span class="line">            <span class="comment"># 获得评估过程的总损失</span></span><br><span class="line">            total_loss += criterion(output_flat, targets)</span><br><span class="line">            <span class="comment"># 计算平均损失</span></span><br><span class="line">            cur_loss = total_loss / ((datasets.size(<span class="number">0</span>) - <span class="number">1</span>) / bptt)     </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cur_loss</span><br></pre></td></tr></table></figure><h2 id="5-模型训练和评估"><a href="#5-模型训练和评估" class="headerlink" title="5.模型训练和评估"></a>5.模型训练和评估</h2><p>训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    text_field, train_txt, val_txt, test_txt = get_wiki_text2_data()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练数据的batch size</span></span><br><span class="line">    batch_size = <span class="number">20</span></span><br><span class="line">    <span class="comment"># 验证和测试数据（统称为评估数据）的batch size</span></span><br><span class="line">    eval_batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得train_data, val_data, test_data</span></span><br><span class="line">    train_data = batchify(text_field, train_txt, batch_size).to(device)</span><br><span class="line">    val_data = batchify(text_field, val_txt, eval_batch_size).to(device)</span><br><span class="line">    test_data = batchify(text_field, test_txt, eval_batch_size).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x, y = get_batch(test_data, 1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(x)</span></span><br><span class="line">    <span class="comment"># print(y)</span></span><br><span class="line">    <span class="comment"># 通过TEXT.vocab.stoi方法获得不重复词汇总数</span></span><br><span class="line">    ntokens = <span class="built_in">len</span>(text_field.vocab.stoi)</span><br><span class="line">    <span class="comment"># 词嵌入大小为200</span></span><br><span class="line">    emsize = <span class="number">200</span></span><br><span class="line">    <span class="comment"># 前馈全连接层的节点数</span></span><br><span class="line">    nhid = <span class="number">200</span></span><br><span class="line">    <span class="comment"># 编码器层的数量</span></span><br><span class="line">    nlayers = <span class="number">2</span></span><br><span class="line">    <span class="comment"># 多头注意力机制的头数</span></span><br><span class="line">    nhead = <span class="number">2</span></span><br><span class="line">    <span class="comment"># 置0比率</span></span><br><span class="line">    dropout = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">    bptt = <span class="number">35</span></span><br><span class="line">    <span class="comment"># 将参数输入到TransformerModel中</span></span><br><span class="line">    model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)</span><br><span class="line">    <span class="comment"># 模型初始化后, 接下来进行损失函数和优化方法的选择.</span></span><br><span class="line">    <span class="comment"># 关于损失函数, 我们使用nn自带的交叉熵损失</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 学习率初始值定为5.0</span></span><br><span class="line">    lr = <span class="number">5.0</span></span><br><span class="line">    <span class="comment"># 优化器选择torch自带的SGD随机梯度下降方法, 并把lr传入其中</span></span><br><span class="line">    optimizer = torch.optim.SGD(model.parameters(), lr=lr)</span><br><span class="line">    <span class="comment"># 定义学习率调整方法, 使用torch自带的lr_scheduler, 将优化器传入其中.</span></span><br><span class="line">    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1.0</span>, gamma=<span class="number">0.95</span>)</span><br><span class="line"></span><br><span class="line">    epochs = <span class="number">3</span></span><br><span class="line">    best_val_loss = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line"></span><br><span class="line">    best_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">        epoch_start_time = time.time()</span><br><span class="line"></span><br><span class="line">        model = train_ones(model, epoch, train_data, criterion, optimizer, scheduler, ntokens, bptt)</span><br><span class="line"></span><br><span class="line">        val_loss = evaluate(model, val_data, criterion, ntokens, bptt)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">89</span>)</span><br><span class="line">        <span class="comment"># print(&#x27;| end of epoch &#123;:3d&#125; | time: &#123;:5.2f&#125;s | valid loss &#123;:5.2f&#125; | &#x27;</span></span><br><span class="line">        <span class="comment">#     &#x27;valid ppl &#123;:8.2f&#125;&#x27;.format(epoch, (time.time() - epoch_start_time),</span></span><br><span class="line">        <span class="comment">#                                 val_loss, math.exp(val_loss)))</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;| end of epoch &#123;:3d&#125; | time: &#123;:5.2f&#125;s | valid loss &#123;:5.2f&#125; | &#x27;</span></span><br><span class="line">            <span class="string">&#x27;valid ppl &#123;:8.2f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, (time.time() - epoch_start_time),</span><br><span class="line">                                        val_loss, <span class="number">00</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">89</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 并取该损失下的模型为best_model</span></span><br><span class="line">        <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">            best_val_loss = val_loss</span><br><span class="line">            <span class="comment"># 使用深拷贝，拷贝最优模型</span></span><br><span class="line">            best_model = copy.deepcopy(model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每轮都会对优化方法的学习率做调整</span></span><br><span class="line">        scheduler.step()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出效果:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 30.03 | loss  7.68 | ppl  2158.52</span><br><span class="line">| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 28.90 | loss  5.26 | ppl   193.39</span><br><span class="line">| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 28.90 | loss  4.07 | ppl    58.44</span><br><span class="line">| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 28.88 | loss  3.41 | ppl    30.26</span><br><span class="line">| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 28.89 | loss  2.98 | ppl    19.72</span><br><span class="line">| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 28.90 | loss  2.79 | ppl    16.30</span><br><span class="line">| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 28.91 | loss  2.67 | ppl    14.38</span><br><span class="line">| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 28.92 | loss  2.58 | ppl    13.19</span><br><span class="line">| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 28.91 | loss  2.43 | ppl    11.32</span><br><span class="line">| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 28.92 | loss  2.39 | ppl    10.93</span><br><span class="line">| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 28.91 | loss  2.33 | ppl    10.24</span><br><span class="line">| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 28.91 | loss  2.36 | ppl    10.59</span><br><span class="line">| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 28.90 | loss  2.33 | ppl    10.31</span><br><span class="line">| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 28.92 | loss  2.26 | ppl     9.54</span><br><span class="line">-----------------------------------------------------------------------------------------</span><br><span class="line">| end of epoch   1 | time: 90.01s | valid loss  1.32 | valid ppl     3.73</span><br><span class="line">-----------------------------------------------------------------------------------------</span><br><span class="line">| epoch   2 |   200/ 2981 batches | lr 4.75 | ms/batch 29.08 | loss  2.18 | ppl     8.83</span><br><span class="line">| epoch   2 |   400/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss  2.11 | ppl     8.24</span><br><span class="line">| epoch   2 |   600/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss  1.98 | ppl     7.23</span><br><span class="line">| epoch   2 |   800/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss  2.00 | ppl     7.39</span><br><span class="line">| epoch   2 |  1000/ 2981 batches | lr 4.75 | ms/batch 28.94 | loss  1.94 | ppl     6.96</span><br><span class="line">| epoch   2 |  1200/ 2981 batches | lr 4.75 | ms/batch 28.92 | loss  1.97 | ppl     7.15</span><br><span class="line">| epoch   2 |  1400/ 2981 batches | lr 4.75 | ms/batch 28.94 | loss  1.98 | ppl     7.28</span><br><span class="line">| epoch   2 |  1600/ 2981 batches | lr 4.75 | ms/batch 28.92 | loss  1.97 | ppl     7.16</span><br><span class="line">| epoch   2 |  1800/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss  1.92 | ppl     6.84</span><br><span class="line">| epoch   2 |  2000/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss  1.96 | ppl     7.11</span><br><span class="line">| epoch   2 |  2200/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss  1.92 | ppl     6.80</span><br><span class="line">| epoch   2 |  2400/ 2981 batches | lr 4.75 | ms/batch 28.94 | loss  1.94 | ppl     6.93</span><br><span class="line">| epoch   2 |  2600/ 2981 batches | lr 4.75 | ms/batch 28.76 | loss  1.91 | ppl     6.76</span><br><span class="line">| epoch   2 |  2800/ 2981 batches | lr 4.75 | ms/batch 28.75 | loss  1.89 | ppl     6.64</span><br><span class="line">-----------------------------------------------------------------------------------------</span><br><span class="line">| end of epoch   2 | time: 89.71s | valid loss  1.01 | valid ppl     2.74</span><br><span class="line">-----------------------------------------------------------------------------------------</span><br><span class="line">| epoch   3 |   200/ 2981 batches | lr 4.51 | ms/batch 28.88 | loss  1.78 | ppl     5.96</span><br><span class="line">| epoch   3 |   400/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss  1.89 | ppl     6.59</span><br><span class="line">| epoch   3 |   600/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss  1.72 | ppl     5.58</span><br><span class="line">| epoch   3 |   800/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss  1.73 | ppl     5.63</span><br><span class="line">| epoch   3 |  1000/ 2981 batches | lr 4.51 | ms/batch 28.73 | loss  1.65 | ppl     5.22</span><br><span class="line">| epoch   3 |  1200/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss  1.69 | ppl     5.40</span><br><span class="line">| epoch   3 |  1400/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss  1.73 | ppl     5.66</span><br><span class="line">| epoch   3 |  1600/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss  1.75 | ppl     5.73</span><br><span class="line">| epoch   3 |  1800/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss  1.67 | ppl     5.33</span><br><span class="line">| epoch   3 |  2000/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss  1.69 | ppl     5.41</span><br><span class="line">| epoch   3 |  2200/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss  1.66 | ppl     5.26</span><br><span class="line">| epoch   3 |  2400/ 2981 batches | lr 4.51 | ms/batch 28.76 | loss  1.69 | ppl     5.43</span><br><span class="line">| epoch   3 |  2600/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss  1.71 | ppl     5.55</span><br><span class="line">| epoch   3 |  2800/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss  1.72 | ppl     5.58</span><br><span class="line">-----------------------------------------------------------------------------------------</span><br><span class="line">| end of epoch   3 | time: 89.26s | valid loss  0.85 | valid ppl     2.33</span><br><span class="line">-----------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>模型测试代码分析:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们仍然使用evaluate函数，这次它的参数是best_model以及测试数据</span></span><br><span class="line">test_loss = evaluate(best_model, test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印测试日志，包括测试损失和测试困惑度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span> * <span class="number">89</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;| End of training | test loss &#123;:5.2f&#125; | test ppl &#123;:8.2f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">    test_loss, math.exp(test_loss)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span> * <span class="number">89</span>)</span><br></pre></td></tr></table></figure><p>输出结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">=========================================================================================</span><br><span class="line">| End of training | <span class="built_in">test</span> loss  0.83 | <span class="built_in">test</span> ppl     2.30</span><br><span class="line">=========================================================================================</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer架构解析</title>
      <link href="/llms/transformer/2.Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/"/>
      <url>/llms/transformer/2.Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer架构解析"><a href="#Transformer架构解析" class="headerlink" title="Transformer架构解析"></a>Transformer架构解析</h1><p>本文主要来自：<a href="http://nlp.seas.harvard.edu/annotated-transformer/#hardware-and-schedule">The Annotated Transformer</a></p><p>论文地址: <a href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></p><p>代码链接：<a href="https://github.com/wdndev/personal/tree/main/transformer">personal/transformer · GitHub</a></p><h1 id="1-Transformer架构图"><a href="#1-Transformer架构图" class="headerlink" title="1.Transformer架构图"></a>1.Transformer架构图</h1><h2 id="1-1-Transformer模型的作用"><a href="#1-1-Transformer模型的作用" class="headerlink" title="1.1 Transformer模型的作用"></a>1.1 Transformer模型的作用</h2><p>基于seq2seq架构的transformer模型可以完成NLP领域研究的典型任务, 如机器翻译, 文本生成等. 同时又可以构建预训练语言模型，用于不同任务的迁移学习.</p><p><a href="https://www.bilibili.com/video/BV1qh4y1o7UU">https://www.bilibili.com/video/BV1qh4y1o7UU</a></p><h2 id="1-2-Transformer总体架构"><a href="#1-2-Transformer总体架构" class="headerlink" title="1.2 Transformer总体架构"></a>1.2 Transformer总体架构</h2><p><img src="image/image_V6s3Iu8dFP.png" alt=""></p><h3 id="（1）输入部分"><a href="#（1）输入部分" class="headerlink" title="（1）输入部分"></a>（1）输入部分</h3><ul><li>源文本嵌入层及其位置编码器</li><li>目标文本嵌入层及其位置编码器</li></ul><p><img src="image/image_g26cBonBG7.png" alt=""></p><h3 id="（2）输出部分"><a href="#（2）输出部分" class="headerlink" title="（2）输出部分"></a>（2）输出部分</h3><ul><li>线性层</li><li>softmax层</li></ul><p><img src="image/image_NV2DvH8m8o.png" alt=""></p><h3 id="（3）编码器"><a href="#（3）编码器" class="headerlink" title="（3）编码器"></a>（3）编码器</h3><ul><li>由N个编码器层堆叠而成</li><li>每个编码器层由两个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul><p><img src="image/image_6RlUw8Ldsk.png" alt=""></p><h3 id="（4）解码器部分"><a href="#（4）解码器部分" class="headerlink" title="（4）解码器部分:"></a>（4）解码器部分:</h3><ul><li>由N个解码器层堆叠而成</li><li>每个解码器层由三个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li><li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul><p><img src="image/image_IlV0Rf2hVU.png" alt=""></p><h1 id="2-输入部分Embeddings"><a href="#2-输入部分Embeddings" class="headerlink" title="2.输入部分Embeddings"></a>2.输入部分Embeddings</h1><h2 id="2-1文本嵌入层"><a href="#2-1文本嵌入层" class="headerlink" title="2.1文本嵌入层"></a>2.1文本嵌入层</h2><p>无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.</p><h3 id="（1）实现"><a href="#（1）实现" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul><li>初始化函数以<code>d_model</code>， 词嵌入维度, 和<code>vocab</code>， 词汇总数为参数, 内部主要使用了nn中的预定层Embedding进行词嵌入.</li><li>在forward函数中, 将输入x传入到Embedding的实例化对象中, 然后乘以一个根号下<code>d_model</code>进行缩放, 控制数值大小。</li><li>它的输出是文本嵌入后的结果。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch中变量封装函数Variable.</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="comment"># 定义Embeddings类来实现文本嵌入层，这里s说明代表两个一模一样的嵌入层, 他们共享参数.</span></span><br><span class="line"><span class="comment"># 该类继承nn.Module, 这样就有标准层的一些功能, 这里我们也可以理解为一种模式, 我们自己实现的所有层都会这样去写.</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;类的初始化函数, </span></span><br><span class="line"><span class="string">         d_model: 指词嵌入的维度, </span></span><br><span class="line"><span class="string">         vocab: 指词表的大小.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 接着就是使用super的方式指明继承nn.Module的初始化函数, 我们自己实现的所有层都会这样去写.</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        <span class="comment"># 之后就是调用nn中的预定义层Embedding, 获得一个词嵌入对象self.lut</span></span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        <span class="comment"># 最后就是将d_model传入类中</span></span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; x: 因为Embedding层是首层, 所以代表输入给模型的文本通过词汇映射后的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将x传给self.lut并与根号下self.d_model相乘作为结果返回</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><h3 id="（2）测试"><a href="#（2）测试" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 词嵌入维度是512维</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line"><span class="comment"># 词表大小是1000</span></span><br><span class="line">vocab = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 输入x是一个使用Variable封装的长整型张量, 形状是2 x 4</span></span><br><span class="line">x = Variable(torch.LongTensor([[<span class="number">100</span>,<span class="number">2</span>,<span class="number">421</span>,<span class="number">508</span>],[<span class="number">491</span>,<span class="number">998</span>,<span class="number">1</span>,<span class="number">221</span>]]))</span><br><span class="line">emb = Embeddings(d_model, vocab)</span><br><span class="line">embr = emb(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;embr:&quot;</span>, embr)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">embr: Variable containing:</span><br><span class="line">( 0 ,.,.) = </span><br><span class="line">  35.9321   3.2582 -17.7301  ...    3.4109  13.8832  39.0272</span><br><span class="line">   8.5410  -3.5790 -12.0460  ...   40.1880  36.6009  34.7141</span><br><span class="line"> -17.0650  -1.8705 -20.1807  ...  -12.5556 -34.0739  35.6536</span><br><span class="line">  20.6105   4.4314  14.9912  ...   -0.1342  -9.9270  28.6771</span><br><span class="line"></span><br><span class="line">( 1 ,.,.) = </span><br><span class="line">  27.7016  16.7183  46.6900  ...   17.9840  17.2525  -3.9709</span><br><span class="line">   3.0645  -5.5105  10.8802  ...  -13.0069  30.8834 -38.3209</span><br><span class="line">  33.1378 -32.1435  -3.9369  ...   15.6094 -29.7063  40.1361</span><br><span class="line"> -31.5056   3.3648   1.4726  ...    2.8047  -9.6514 -23.4909</span><br><span class="line">[torch.FloatTensor of size 2x4x512]</span><br></pre></td></tr></table></figure><h2 id="2-2-位置编码器PositionalEncoding"><a href="#2-2-位置编码器PositionalEncoding" class="headerlink" title="2.2 位置编码器PositionalEncoding"></a>2.2 位置编码器PositionalEncoding</h2><p>因为在Transformer的编码器结构中, 并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，<strong>将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失</strong>.</p><p>使用不同频率的正弦和余弦函数：</p><script type="math/tex; mode=display">PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})</script><script type="math/tex; mode=display">PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})</script><p>其中$pos$是位置，$i$ 是维度。也就是说，位置编码的每个维度对应于一个正弦曲线。 这些波长形成一个从 $2\pi$ 到 $10000 \cdot 2\pi$的集合级数。我们选择这个函数是因为我们假设它会让模型很容易学习对相对位置的关注，因为对任意确定的偏移k , $PE_{pos+k}$可以表示为 $PE_{pos}$的线性函数。</p><h3 id="（1）实现-1"><a href="#（1）实现-1" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul><li>初始化函数以<code>d_model</code>, <code>dropout</code>, <code>max_len</code>为参数, 分别代表<code>d_model</code>: 词嵌入维度, <code>dropout</code>: 置0比率, <code>max_len</code>: 每个句子的最大长度.</li><li>forward函数中的输入参数为x, 是Embedding层的输出.</li><li>最终输出一个加入了位置编码信息的词嵌入张量.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义位置编码器类, 我们同样把它看做一个层, 因此会继承nn.Module    </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;位置编码器类的初始化函数, 共有三个参数, 分别是</span></span><br><span class="line"><span class="string">          d_model: 词嵌入维度, </span></span><br><span class="line"><span class="string">          dropout: 置0比率, </span></span><br><span class="line"><span class="string">          max_len: 每个句子的最大长度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实例化nn中预定义的Dropout层, 并将dropout传入其中, 获得对象self.dropout</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        <span class="comment"># 初始化一个位置编码矩阵, 它是一个0阵，矩阵的大小是max_len x d_model.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        <span class="comment"># 初始化一个绝对位置矩阵, 在我们这里，词汇的绝对位置就是用它的索引去表示. </span></span><br><span class="line">        <span class="comment"># 所以我们首先使用arange方法获得一个连续自然数向量，然后再使用unsqueeze方法拓展向量维度使其成为矩阵， </span></span><br><span class="line">        <span class="comment"># 又因为参数传的是1，代表矩阵拓展的位置，会使向量变成一个max_len x 1 的矩阵， </span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 绝对位置矩阵初始化之后，接下来就是考虑如何将这些位置信息加入到位置编码矩阵中，</span></span><br><span class="line">        <span class="comment"># 最简单思路就是先将max_len x 1的绝对位置矩阵， 变换成max_len x d_model形状，然后覆盖原来的初始位置编码矩阵即可， </span></span><br><span class="line">        <span class="comment"># 要做这种矩阵变换，就需要一个1xd_model形状的变换矩阵div_term，我们对这个变换矩阵的要求除了形状外，</span></span><br><span class="line">        <span class="comment"># 还希望它能够将自然数的绝对位置编码缩放成足够小的数字，有助于在之后的梯度下降过程中更快的收敛.  这样我们就可以开始初始化这个变换矩阵了.</span></span><br><span class="line">        <span class="comment"># 首先使用arange获得一个自然数矩阵， 但是细心的同学们会发现， 我们这里并没有按照预计的一样初始化一个1xd_model的矩阵， </span></span><br><span class="line">        <span class="comment"># 而是有了一个跳跃，只初始化了一半即1xd_model/2 的矩阵。 为什么是一半呢，其实这里并不是真正意义上的初始化了一半的矩阵，</span></span><br><span class="line">        <span class="comment"># 我们可以把它看作是初始化了两次，而每次初始化的变换矩阵会做不同的处理，第一次初始化的变换矩阵分布在正弦波上， 第二次初始化的变换矩阵分布在余弦波上， </span></span><br><span class="line">        <span class="comment"># 并把这两个矩阵分别填充在位置编码矩阵的偶数和奇数位置上，组成最终的位置编码矩阵.</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 这样我们就得到了位置编码矩阵pe, pe现在还只是一个二维矩阵，要想和embedding的输出（一个三维张量）相加，</span></span><br><span class="line">        <span class="comment"># 就必须拓展一个维度，所以这里使用unsqueeze拓展维度.</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 最后把pe位置编码矩阵注册成模型的buffer，什么是buffer呢，</span></span><br><span class="line">        <span class="comment"># 我们把它认为是对模型效果有帮助的，但是却不是模型结构中超参数或者参数，不需要随着优化步骤进行更新的增益对象. </span></span><br><span class="line">        <span class="comment"># 注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载.</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; x, 表示文本序列的词嵌入表示 &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 在相加之前我们对pe做一些适配工作， 将这个三维张量的第二维也就是句子最大长度的那一维将切片到与输入的x的第二维相同即x.size(1)，</span></span><br><span class="line">        <span class="comment"># 因为我们默认max_len为5000一般来讲实在太大了，很难有一条句子包含5000个词汇，所以要进行与输入张量的适配. </span></span><br><span class="line">        <span class="comment"># 最后使用Variable进行封装，使其与x的样式相同，但是它是不需要进行梯度求解的，因此把requires_grad设置成false.</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 最后使用self.dropout对象进行&#x27;丢弃&#x27;操作, 并返回结果.</span></span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><h3 id="（2）测试-1"><a href="#（2）测试-1" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 词嵌入维度是512维</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line"><span class="comment"># 置0比率为0.1</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 句子最大长度</span></span><br><span class="line">max_len=<span class="number">60</span></span><br><span class="line"></span><br><span class="line">x = embr</span><br><span class="line">pe = PositionalEncoding(d_model, dropout, max_len)</span><br><span class="line">pe_result = pe(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;pe_result:&quot;</span>, pe_result)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pe_result: Variable containing:</span><br><span class="line">( 0 ,.,.) = </span><br><span class="line"> -19.7050   0.0000   0.0000  ...  -11.7557  -0.0000  23.4553</span><br><span class="line">  -1.4668 -62.2510  -2.4012  ...   66.5860 -24.4578 -37.7469</span><br><span class="line">   9.8642 -41.6497 -11.4968  ...  -21.1293 -42.0945  50.7943</span><br><span class="line">   0.0000  34.1785 -33.0712  ...   48.5520   3.2540  54.1348</span><br><span class="line">( 1 ,.,.) = </span><br><span class="line">   7.7598 -21.0359  15.0595  ...  -35.6061  -0.0000   4.1772</span><br><span class="line"> -38.7230   8.6578  34.2935  ...  -43.3556  26.6052   4.3084</span><br><span class="line">  24.6962  37.3626 -26.9271  ...   49.8989   0.0000  44.9158</span><br><span class="line"> -28.8435 -48.5963  -0.9892  ...  -52.5447  -4.1475  -3.0450</span><br><span class="line">[torch.FloatTensor of size 2x4x512]</span><br></pre></td></tr></table></figure><h3 id="（3）绘制词汇向量中特征的分布曲线"><a href="#（3）绘制词汇向量中特征的分布曲线" class="headerlink" title="（3）绘制词汇向量中特征的分布曲线"></a>（3）绘制词汇向量中特征的分布曲线</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一张15 x 5大小的画布</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 实例化PositionalEncoding类得到pe对象, 输入参数是20和0</span></span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 然后向pe传入被Variable封装的tensor, 这样pe会直接执行forward函数, </span></span><br><span class="line"><span class="comment"># 且这个tensor里的数值都是0, 被处理后相当于位置编码张量</span></span><br><span class="line">y = pe(Variable(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>)))</span><br><span class="line"><span class="comment"># 然后定义画布的横纵坐标, 横坐标到100的长度, 纵坐标是某一个词汇中的某维特征在不同长度下对应的值</span></span><br><span class="line"><span class="comment"># 因为总共有20维之多, 我们这里只查看4，5，6，7维的值.</span></span><br><span class="line">plt.plot(np.arange(<span class="number">100</span>), y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].data.numpy())</span><br><span class="line"><span class="comment"># 在画布上填写维度提示信息</span></span><br><span class="line">plt.legend([<span class="string">&quot;dim %d&quot;</span>%p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br></pre></td></tr></table></figure><p><img src="image/image_ZclAi-4PTQ.png" alt=""></p><p>效果分析：</p><ul><li>每条颜色的曲线代表某一个词汇中的特征在不同位置的含义.</li><li>保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化。</li><li>正弦波和余弦波的值域范围都是1到-1这又很好的控制了嵌入数值的大小, 有助于梯度的快速计算。</li></ul><h1 id="3-编码器"><a href="#3-编码器" class="headerlink" title="3.编码器"></a>3.编码器</h1><ul><li>由N个编码器层堆叠而成</li><li>每个编码器层由两个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul><p><img src="image/image_-NDHADCxJy.png" alt=""></p><h2 id="3-1-掩码张量subsequent-mask"><a href="#3-1-掩码张量subsequent-mask" class="headerlink" title="3.1 掩码张量subsequent_mask"></a>3.1 掩码张量subsequent_mask</h2><p>掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有1和0的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是让另外一个张量中的一些数值被遮掩，也可以说被替换, 它的表现形式是一个张量</p><p><strong>掩码张量的作用</strong>：在transformer中, 掩码张量的主要作用在应用attention时，有一些生成的attention张量中的值计算有可能已知了未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用。</p><h3 id="（1）实现-2"><a href="#（1）实现-2" class="headerlink" title="（1）实现"></a>（1）实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">subsequent_mask</span>(<span class="params">size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成向后遮掩的掩码张量, </span></span><br><span class="line"><span class="string">      参数size是掩码张量最后两个维度的大小, 它的最后两维形成一个方阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在函数中, 首先定义掩码张量的形状</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 然后使用np.ones方法向这个形状中添加1元素,形成上三角阵, 最后为了节约空间, </span></span><br><span class="line">    <span class="comment"># 再使其中的数据类型变为无符号8位整形unit8 </span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后将numpy类型转化为torch中的tensor, 内部做一个1 - 的操作, </span></span><br><span class="line">    <span class="comment"># 在这个其实是做了一个三角阵的反转, subsequent_mask中的每个元素都会被1减, </span></span><br><span class="line">    <span class="comment"># 如果是0, subsequent_mask中的该位置由0变成1</span></span><br><span class="line">    <span class="comment"># 如果是1, subsequent_mask中的该位置由1变成0 </span></span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(<span class="number">1</span> - subsequent_mask)</span><br></pre></td></tr></table></figure><h3 id="（2）测试-2"><a href="#（2）测试-2" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成的掩码张量的最后两维的大小</span></span><br><span class="line">size = <span class="number">5</span></span><br><span class="line">sm = subsequent_mask(size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;sm:&quot;</span>, sm)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[1, 0, 0, 0, 0],</span><br><span class="line">         [1, 1, 0, 0, 0],</span><br><span class="line">         [1, 1, 1, 0, 0],</span><br><span class="line">         [1, 1, 1, 1, 0],</span><br><span class="line">         [1, 1, 1, 1, 1]]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure><h3 id="（3）掩码张量的可视化"><a href="#（3）掩码张量的可视化" class="headerlink" title="（3）掩码张量的可视化"></a>（3）掩码张量的可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.imshow(subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p><img src="image/image_bHivDZ57nK.png" alt=""></p><p>效果分析:</p><ul><li>通过观察可视化方阵, 黄色是1的部分, 这里代表被遮掩, 紫色代表没有被遮掩的信息, 横坐标代表目标词汇的位置, 纵坐标代表可查看的位置;</li><li>我们看到, 在0的位置我们一看望过去都是黄色的, 都被遮住了，1的位置一眼望过去还是黄色, 说明第一次词还没有产生, 从第二个位置看过去, 就能看到位置1的词, 其他位置看不到, 以此类推.</li></ul><h2 id="3-2-注意力机制Attention"><a href="#3-2-注意力机制Attention" class="headerlink" title="3.2 注意力机制Attention"></a>3.2 注意力机制Attention</h2><h3 id="（1）什么是注意力"><a href="#（1）什么是注意力" class="headerlink" title="（1）什么是注意力"></a>（1）什么是注意力</h3><p>我们观察事物时，之所以能够快速判断一种事物（当然允许判断是错误的）, 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果。正是基于这样的理论，就产生了注意力机制。</p><p>Attention功能可以描述为将query和一组key-value对映射到输出，其中query、key、value和输出都是向量。输出为value的加权和，其中每个value的权重通过query与相应key的兼容函数来计算。</p><p>具体来说，output 是 value 的一个加权和 —&gt; 输出的维度 ==  value 的维度。output 中 value 的权重 = 查询 query 和对应的 key 的相似度 ；权重等价于 query 和对应的 key 的相似度</p><p><img src="image/image_mo1QFm0-QD.png" alt=""></p><p>图中，红色表示value，蓝色表示key：</p><ul><li>给定q为黄色，靠近key的第一第二个，所以output更多偏向与value的第一和第二个；</li><li>给定q为绿色，靠近key的第二第三个，所以output更多偏向于value的第二和第三个。</li></ul><p>虽然 key-value 并没有变，但是随着 query 的改变，因为权重的分配不一样，导致 输出会有不一样，这就是注意力机制。</p><h3 id="（2）注意力计算规则"><a href="#（2）注意力计算规则" class="headerlink" title="（2）注意力计算规则"></a>（2）注意力计算规则</h3><p>需要三个指定的输入Q(query), K(key), V(value)， 然后通过公式得到注意力的计算结果, 这个结果代表<strong>query在key和value作用下的表示</strong>。而这个具体的计算规则有很多种，我这里只介绍我们用到的这一种.</p><script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><h3 id="（3）Q-K-V的比喻解释"><a href="#（3）Q-K-V的比喻解释" class="headerlink" title="（3）Q, K, V的比喻解释"></a>（3）Q, K, V的比喻解释</h3><p>假如我们有一个问题： 给出一段文本，使用一些关键词对它进行描述!</p><p>为了方便统一正确答案，这道题可能预先已经给大家写出了一些关键词作为提示.其中这些给出的提示就可以看作是key， 而整个的文本信息就相当于是query，value的含义则更抽象，可以比作是你看到这段文本信息后，脑子里浮现的答案信息，这里我们又假设大家最开始都不是很聪明，第一次看到这段文本后脑子里基本上浮现的信息就只有提示这些信息，因此key与value基本是相同的，但是随着我们对这个问题的深入理解，通过我们的思考脑子里想起来的东西原来越多，并且能够开始对我们query也就是这段文本，提取关键信息进行表示.  这就是注意力作用的过程， 通过这个过程，我们最终脑子里的value发生了变化，根据提示key生成了query的关键词表示方法，也就是另外一种特征表示方法.</p><p>刚刚说到key和value一般情况下默认是相同，与query是不同的，这种是我们一般的注意力输入形式，但有一种特殊情况，就是我们query与key和value相同，这种情况我们称为<strong>自注意力机制</strong>，就如同我们的刚刚的例子， 使用一般注意力机制，是使用不同于给定文本的关键词表示它. 而自注意力机制，需要用给定文本自身来表达自己，也就是说你需要从给定文本中抽取关键词来表述它, 相当于对文本自身的一次特征提取.</p><h3 id="（4）注意力机制"><a href="#（4）注意力机制" class="headerlink" title="（4）注意力机制"></a>（4）注意力机制</h3><p>注意力机制是注意力计算规则能够应用的深度学习网络的载体, 除了注意力计算规则外, 还包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体。使用自注意力计算规则的注意力机制称为自注意力机制。</p><p>将particular attention称之为“缩放的点积Attention”(Scaled Dot-Product Attention”)。其输入为query、key(维度是$d_k$)以及values(维度是$d_v$)。我们计算query和所有key的点积，然后对每个除以 $\sqrt{d_k}$ , 最后用softmax函数获得value的权重。</p><p><img src="image/image_oiPuFb7z05.png" alt=""></p><p>两个最常用的attention函数是加法attention(cite)和点积（乘法）attention。除了缩放因子 $\frac{1}{\sqrt{d_k}}$ ，点积Attention跟我们的平时的算法一样。加法attention使用具有单个隐层的前馈网络计算兼容函数。虽然理论上点积attention和加法attention复杂度相似，但在实践中，点积attention可以使用高度优化的矩阵乘法来实现，因此点积attention计算更快、更节省空间。</p><p>当 $d_k$ 的值比较小的时候，这两个机制的性能相近。当 $d_k$ 比较大时，加法attention比不带缩放的点积attention性能好 (cite)。我们怀疑，对于很大的 $d_k$ 值, 点积大幅度增长，将softmax函数推向具有极小梯度的区域。(为了说明为什么点积变大，假设$q$和$k$是独立的随机变量，均值为0，方差为1。那么它们的点积 $q \cdot k = \sum_{i=1}^{d_k} q_ik_i$ , 均值为0方差为$d_k$ )。为了抵消这种影响，我们将点积缩小 $\frac{1}{\sqrt{d_k}}$ 倍。</p><p><strong>为什么Attention中除以</strong>$\sqrt{d}$<strong> 这么重要？</strong></p><p>Attention的计算是在内积之后进行softmax，主要涉及的运算是$e^{q \cdot k}$，可以大致认为内积之后、softmax之前的数值在$-3\sqrt{d}$到$3\sqrt{d}$这个范围内，由于d通常都至少是64，所以$e^{3\sqrt{d}}$比较大而 $e^{-3\sqrt{d}}$比较小，因此经过softmax之后，Attention的分布非常接近一个one hot分布了，这带来严重的梯度消失问题，导致训练效果差。（例如y=softmax(x)在|x|较大时进入了饱和区，x继续变化y值也几乎不变，即饱和区梯度消失）</p><p>相应地，解决方法就有两个:</p><ol><li><p>像NTK参数化那样，在内积之后除以 $\sqrt{d}$，使q⋅k的方差变为1，对应$e^3,e^{−3}$都不至于过大过小，这样softmax之后也不至于变成one hot而梯度消失了，这也是常规的Transformer如BERT里边的Self Attention的做法</p></li><li><p>另外就是不除以 $\sqrt{d}$，但是初始化q,k的全连接层的时候，其初始化方差要多除以一个d，这同样能使得使q⋅k的初始方差变为1，T5采用了这样的做法。</p></li></ol><h3 id="（5）代码实现"><a href="#（5）代码实现" class="headerlink" title="（5）代码实现"></a>（5）代码实现</h3><ul><li>输入就是Q，K，V以及mask和dropout, mask用于掩码, dropout用于随机置0.</li><li>输出有两个, query的注意力表示以及注意力张量.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;注意力机制的实现, </span></span><br><span class="line"><span class="string">        输入分别是query, key, value, mask: 掩码张量, </span></span><br><span class="line"><span class="string">       dropout是nn.Dropout层的实例化对象, 默认为None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将query的最后一个维度提取出来，代表的是词嵌入的维度</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按照注意力公式, 将query与key的转置相乘, 这里面key是将最后两个维度进行转置, </span></span><br><span class="line">    <span class="comment"># 再除以缩放系数根号下d_k, 这种计算方法也称为缩放点积注意力计算.</span></span><br><span class="line">    <span class="comment"># 得到注意力得分张量scores</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着判断是否使用掩码张量</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 使用tensor的masked_fill方法, 将掩码张量和scores张量每个位置一一比较, </span></span><br><span class="line">        <span class="comment"># 如果掩码张量处为0，则对应的scores张量用-1e9这个值来替换, 如下演示</span></span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对scores的最后一维进行softmax操作, 使用F.softmax方法, 第一个参数是softmax对象, 第二个是目标维度.</span></span><br><span class="line">    <span class="comment"># 这样获得最终的注意力张量</span></span><br><span class="line">    p_attn = scores.softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 之后判断是否使用dropout进行随机置0</span></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 将p_attn传入dropout对象中进行&#x27;丢弃&#x27;处理</span></span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后, 根据公式将p_attn与value张量相乘获得最终的query注意力表示, 同时返回注意力张量</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><h3 id="（6）测试"><a href="#（6）测试" class="headerlink" title="（6）测试"></a>（6）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> embedding <span class="keyword">import</span> Embeddings</span><br><span class="line"><span class="keyword">from</span> positional_encoding <span class="keyword">import</span> PositionalEncoding</span><br><span class="line"></span><br><span class="line">  d_model = <span class="number">512</span></span><br><span class="line">  dropout = <span class="number">0.1</span></span><br><span class="line">  max_len = <span class="number">60</span></span><br><span class="line"></span><br><span class="line">  vocab = <span class="number">1000</span></span><br><span class="line">  emb = Embeddings(d_model, vocab)</span><br><span class="line">  <span class="built_in">input</span> = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>], [<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">9</span>]]))</span><br><span class="line">  embr = emb(<span class="built_in">input</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;embr: &quot;</span>, embr)</span><br><span class="line">  <span class="built_in">print</span>(embr.shape)</span><br><span class="line"></span><br><span class="line">  pe = PositionalEncoding(d_model, dropout, max_len)</span><br><span class="line">  pe_res = pe(embr)</span><br><span class="line"></span><br><span class="line">  query = key = value = pe_res</span><br><span class="line">  attn, p_attn = attention(query, key, value)</span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;attn:&quot;</span>, attn)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;attn shape:&quot;</span>, attn.shape)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;p_attn:&quot;</span>, p_attn)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;p_attn shape:&quot;</span>, p_attn.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">attn: tensor([[[ -6.9986, -22.1325,  44.3268,  ...,   0.0000,  -2.9953,  -3.8844],</span><br><span class="line">         [ 57.2318,   5.9384,   0.0000,  ...,  38.5518,   0.5860,  12.5283],</span><br><span class="line">         [-56.2970,   0.0000,  -4.3592,  ...,  26.0355,  -2.3129,   0.0000],</span><br><span class="line">         [ -2.1557,   3.4803, -36.6878,  ...,  15.8174, -21.3978, -30.9041]],</span><br><span class="line"></span><br><span class="line">        [[-57.3073, -25.4691,  -5.3997,  ...,  26.0355,  -2.3131,  14.3969],</span><br><span class="line">         [  6.9673, -32.6722,  39.3464,  ...,  -5.4699, -10.4042,  -4.4331],</span><br><span class="line">         [ 57.3072,   4.8757,  -5.6530,  ...,  38.5518,   0.5862,   0.0000],</span><br><span class="line">         [ 18.3028,  -9.1978,  59.9258,  ...,  -9.5552, -45.4553,   0.0000]]],</span><br><span class="line">       grad_fn=&lt;UnsafeViewBackward&gt;)</span><br><span class="line">attn shape: torch.Size([2, 4, 512])</span><br><span class="line">p_attn: tensor([[[1., 0., 0., 0.],</span><br><span class="line">         [0., 1., 0., 0.],</span><br><span class="line">         [0., 0., 1., 0.],</span><br><span class="line">         [0., 0., 0., 1.]],</span><br><span class="line"></span><br><span class="line">        [[1., 0., 0., 0.],</span><br><span class="line">         [0., 1., 0., 0.],</span><br><span class="line">         [0., 0., 1., 0.],</span><br><span class="line">         [0., 0., 0., 1.]]], grad_fn=&lt;SoftmaxBackward&gt;)</span><br><span class="line">p_attn shape: torch.Size([2, 4, 4])</span><br></pre></td></tr></table></figure><p>待用mask的输出效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">query = key = value = pe_result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 令mask为一个2x4x4的零张量</span></span><br><span class="line">mask = Variable(torch.zeros(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">attn, p_attn = attention(query, key, value, mask=mask)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;attn:&quot;</span>, attn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;p_attn:&quot;</span>, p_attn)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># query的注意力表示:</span></span><br><span class="line">attn: Variable containing:</span><br><span class="line">( 0 ,.,.) = </span><br><span class="line">   0.4284  -7.4741   8.8839  ...    1.5618   0.5063   0.5770</span><br><span class="line">   0.4284  -7.4741   8.8839  ...    1.5618   0.5063   0.5770</span><br><span class="line">   0.4284  -7.4741   8.8839  ...    1.5618   0.5063   0.5770</span><br><span class="line">   0.4284  -7.4741   8.8839  ...    1.5618   0.5063   0.5770</span><br><span class="line"></span><br><span class="line">( 1 ,.,.) = </span><br><span class="line">  -2.8890   9.9972 -12.9505  ...    9.1657  -4.6164  -0.5491</span><br><span class="line">  -2.8890   9.9972 -12.9505  ...    9.1657  -4.6164  -0.5491</span><br><span class="line">  -2.8890   9.9972 -12.9505  ...    9.1657  -4.6164  -0.5491</span><br><span class="line">  -2.8890   9.9972 -12.9505  ...    9.1657  -4.6164  -0.5491</span><br><span class="line">[torch.FloatTensor of size 2x4x512]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意力张量:</span></span><br><span class="line">p_attn: Variable containing:</span><br><span class="line">(0 ,.,.) = </span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line"></span><br><span class="line">(1 ,.,.) = </span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">[torch.FloatTensor of size 2x4x4]</span><br></pre></td></tr></table></figure><h2 id="3-3-多头注意力机制MultiHeadAttention"><a href="#3-3-多头注意力机制MultiHeadAttention" class="headerlink" title="3.3 多头注意力机制MultiHeadAttention"></a>3.3 多头注意力机制MultiHeadAttention</h2><h3 id="（1）什么是多头注意力机制"><a href="#（1）什么是多头注意力机制" class="headerlink" title="（1）什么是多头注意力机制"></a>（1）什么是多头注意力机制</h3><p>从多头注意力的结构图中，貌似这个所谓的多个头就是指多组线性变换层，其实并不是，我只有使用了一组线性变化层，即三个变换张量对Q，K，V分别进行线性变换，<strong>这些变换不会改变原有张量的尺寸</strong>，因此每个变换矩阵都是方阵，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量。这就是所谓的多头，将每个头的获得的输入送到注意力机制中, 就形成多头注意力机制.</p><p>Multi-head attention允许模型共同关注来自不同位置的不同表示子空间的信息，如果只有一个attention head，它的平均值会削弱这个信息。</p><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O  \\where ~ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</script><p>其中映射由权重矩阵完成：$ W^Q_i \in \mathbb{R}^{d_ \times d_k}<br>  $ , $W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$ , $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$ 和 $W^O_i \in \mathbb{R}^{hd_v \times d_{\text{model}} }$ 。</p><p><img src="image/image_saBcwnCQZW.png" alt=""></p><h3 id="（2）多头注意力作用"><a href="#（2）多头注意力作用" class="headerlink" title="（2）多头注意力作用"></a>（2）多头注意力作用</h3><p>这种结构设计能<strong>让每个注意力机制去优化每个词汇的不同特征部分</strong>，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</p><p><strong>为什么要做多头注意力机制呢</strong>？</p><ul><li>一个 dot product 的注意力里面，没有什么可以学的参数。具体函数就是内积，为了识别不一样的模式，希望有不一样的计算相似度的办法。加性 attention 有一个权重可学，也许能学到一些内容。</li><li>multi-head attention 给 h 次机会去学习 不一样的投影的方法，使得在投影进去的度量空间里面能够去匹配不同模式需要的一些相似函数，然后把 h 个 heads 拼接起来，最后再做一次投影。</li><li>每一个头 hi 是把 Q,K,V 通过 可以学习的 Wq, Wk, Wv 投影到 dv 上，再通过注意力函数，得到 headi。&#x20;</li></ul><h3 id="（3）实现"><a href="#（3）实现" class="headerlink" title="（3）实现"></a>（3）实现</h3><ul><li>因为多头注意力机制中需要使用多个相同的线性层, 首先实现了克隆函数clones.</li><li>clones函数的输入是module，N，分别代表克隆的目标层，和克隆个数.</li><li>clones函数的输出是装有N个克隆层的Module列表.</li><li>接着实现MultiHeadedAttention类, 它的初始化函数输入是h, d_model, dropout分别代表头数，词嵌入维度和置零比率.</li><li>它的实例化对象输入是Q, K, V以及掩码张量mask.</li><li>它的实例化对象输出是通过多头注意力机制处理的Q的注意力表示.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">model, N</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    用于生成相同网络层的克隆函数, </span></span><br><span class="line"><span class="string">        - module表示要克隆的目标网络层, </span></span><br><span class="line"><span class="string">        - N代表需要克隆的数量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在函数中, 我们通过for循环对module进行N次深度拷贝, 使其每个module成为独立的层,</span></span><br><span class="line">    <span class="comment"># 然后将其放在nn.ModuleList类型的列表中存放.</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(model) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, head, embedding_dim, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 多头注意力机制</span></span><br><span class="line"><span class="string">            - head代表头数</span></span><br><span class="line"><span class="string">            - embedding_dim代表词嵌入的维度， </span></span><br><span class="line"><span class="string">           - dropout代表进行dropout操作时置0比率，默认是0.1</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，</span></span><br><span class="line">        <span class="comment"># 这是因为我们之后要给每个头分配等量的词特征.也就是embedding_dim/head个.</span></span><br><span class="line">        <span class="keyword">assert</span> embedding_dim % head == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到每个头获得的分割词向量维度d_k</span></span><br><span class="line">        self.d_k = embedding_dim // head</span><br><span class="line">        <span class="comment"># 传入头数head</span></span><br><span class="line">        self.head = head</span><br><span class="line"></span><br><span class="line">         <span class="comment"># 然后获得线性层对象，通过nn的Linear实例化，</span></span><br><span class="line">         <span class="comment"># 它的内部变换矩阵是embedding_dim x embedding_dim，</span></span><br><span class="line">         <span class="comment"># 然后使用clones函数克隆四个，为什么是四个呢，</span></span><br><span class="line">         <span class="comment"># 这是因为在多头注意力中，Q，K，V各需要一个，</span></span><br><span class="line">         <span class="comment"># 最后拼接的矩阵还需要一个，因此一共是四个.</span></span><br><span class="line">        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None.</span></span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 输入参数有四个，前三个就是注意力机制需要的Q, K, V，</span></span><br><span class="line"><span class="string">            最后一个是注意力机制中可能需要的mask掩码张量，默认是None.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 如果存在掩码张量mask</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 使用unsqueeze拓展维度</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本.</span></span><br><span class="line">        batch_size = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 之后就进入多头处理环节</span></span><br><span class="line">        <span class="comment"># 首先利用zip将输入QKV与三个线性层组到一起，然后使用for循环，</span></span><br><span class="line">        <span class="comment"># 将输入QKV分别传到线性层中，做完线性变换后，</span></span><br><span class="line">        <span class="comment"># 开始为每个头分割输入，这里使用view方法对线性变换的结果进行维度重塑，多加了一个维度h，代表头数，</span></span><br><span class="line">        <span class="comment"># 这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，</span></span><br><span class="line">        <span class="comment"># 计算机会根据这种变换自动计算这里的值.然后对第二维和第三维进行转置操作，</span></span><br><span class="line">        <span class="comment"># 为了让代表句子长度维度和词向量维度能够相邻，</span></span><br><span class="line">        <span class="comment"># 这样注意力机制才能找到词义与句子位置的关系，</span></span><br><span class="line">        <span class="comment"># 从attention函数中可以看到，利用的是原始输入的倒数第一和第二维.</span></span><br><span class="line">        <span class="comment"># 这样我们就得到了每个头的输入.</span></span><br><span class="line">        query, key, value = [</span><br><span class="line">            model(x).view(batch_size, -<span class="number">1</span>, self.head, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">for</span> model, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到每个头的输入后，接下来就是将他们传入到attention中，</span></span><br><span class="line">        <span class="comment"># 这里直接调用我们之前实现的attention函数.同时也将mask和dropout传入其中.</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，</span></span><br><span class="line">        <span class="comment"># 我们需要将其转换为输入的形状以方便后续的计算，</span></span><br><span class="line">        <span class="comment"># 因此这里开始进行第一步处理环节的逆操作，</span></span><br><span class="line">        <span class="comment"># 先对第二和第三维进行转置，然后使用contiguous方法，</span></span><br><span class="line">        <span class="comment"># 这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，</span></span><br><span class="line">        <span class="comment"># 所以，下一步就是使用view重塑形状，变成和输入形状相同.</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.head * self.d_k)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 最后使用线性层列表中的最后一个线性层对输入进行线性变换得到最终的多头注意力结构的输出.</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure><h3 id="（4）测试"><a href="#（4）测试" class="headerlink" title="（4）测试"></a>（4）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">d_model = <span class="number">512</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line">max_len = <span class="number">60</span></span><br><span class="line">vocab = <span class="number">1000</span></span><br><span class="line">emb = Embeddings(d_model, vocab)</span><br><span class="line"><span class="built_in">input</span> = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>], [<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">9</span>]]))</span><br><span class="line">embr = emb(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">pe = PositionalEncoding(d_model, dropout, max_len)</span><br><span class="line">pe_res = pe(embr)</span><br><span class="line"></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">embedding_dim = <span class="number">512</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">query = key = value = pe_res</span><br><span class="line"></span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">mha = MultiHeadAttention(head, embedding_dim, dropout)</span><br><span class="line">mha_res = mha(query, key, value, mask)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[-2.3411, -0.8430, -4.1038,  ...,  1.4731, -0.7992,  0.9026],</span><br><span class="line">         [-5.1657, -2.4703, -7.4543,  ...,  0.8810,  0.3061,  0.6387],</span><br><span class="line">         [-4.2553, -0.1940, -6.1963,  ..., -3.4095,  0.6791, -0.6660],</span><br><span class="line">         [-4.8889, -4.0475, -5.9836,  ..., -3.4044,  0.5312,  0.7642]],</span><br><span class="line"></span><br><span class="line">        [[-4.8633,  2.5490, -6.3160,  ..., -1.7124, -2.2730,  0.7630],</span><br><span class="line">         [-5.1141,  2.4704, -4.4557,  ...,  2.4667, -0.3286,  0.8127],</span><br><span class="line">         [-8.8165,  1.9820, -6.3692,  ..., -1.9055,  2.4552, -6.4086],</span><br><span class="line">         [-6.2969,  2.9008, -1.2483,  ...,  0.1594, -4.0804,  0.0228]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure><h2 id="3-4-前馈全连接层PositionwiseFeedForward"><a href="#3-4-前馈全连接层PositionwiseFeedForward" class="headerlink" title="3.4 前馈全连接层PositionwiseFeedForward"></a>3.4 前馈全连接层PositionwiseFeedForward</h2><p>在Transformer中前馈全连接层就是具有两层线性层的全连接网络.</p><h3 id="（1）前馈全连接层作用"><a href="#（1）前馈全连接层作用" class="headerlink" title="（1）前馈全连接层作用"></a>（1）前馈全连接层作用</h3><p>考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.</p><script type="math/tex; mode=display">FFN(x)=max(0, ~ xW_1+b_1)W_2+b_2</script><blockquote><p>Position就是序列中每个token，<code>Position-wise</code> 就是把MLP对每个token作用一次，且作用的是同一个MLP。</p></blockquote><h3 id="（2）代码实现"><a href="#（2）代码实现" class="headerlink" title="（2）代码实现"></a>（2）代码实现</h3><ul><li>实例化参数为<code>d_model</code>, <code>d_ff</code>, <code>dropout</code>, 分别代表词嵌入维度, 线性变换维度, 和置零比率.</li><li>输入参数x, 表示上层的输出.</li><li>输出是经过2层线性网络变换的特征表示.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 实现前馈全连接层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            - d_model: 线性层的输入维度也是第二个线性层的输出维度，</span></span><br><span class="line"><span class="string">                    因为我们希望输入通过前馈全连接层后输入和输出的维度不变. </span></span><br><span class="line"><span class="string">           - d_ff: 第二个线性层的输入维度和第一个线性层的输出维度. </span></span><br><span class="line"><span class="string">           - dropout: 置0比率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.w1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 首先经过第一个线性层，然后使用Funtional中relu函数进行激活,</span></span><br><span class="line">        <span class="comment"># 之后再使用dropout进行随机置0，最后通过第二个线性层w2，返回最终结果.</span></span><br><span class="line">        <span class="keyword">return</span> self.w2(self.dropout(self.w1(x).relu()))</span><br></pre></td></tr></table></figure><h3 id="（3）测试"><a href="#（3）测试" class="headerlink" title="（3）测试"></a>（3）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">d_model = <span class="number">512</span></span><br><span class="line"><span class="comment"># 线性变化的维度</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mha_res.shape)</span><br><span class="line">ff_result = ff(mha_res)</span><br><span class="line"><span class="built_in">print</span>(ff_result)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[-2.8747, -0.1289, -0.4966,  ..., -1.2763, -2.0888,  0.3344],</span><br><span class="line">         [-1.2404,  0.3891,  1.3854,  ..., -1.2675, -1.8324,  0.2271],</span><br><span class="line">         [-1.6913,  1.2393,  0.1528,  ..., -0.7420, -2.5605,  0.9924],</span><br><span class="line">         [-3.4989,  1.4898, -0.7094,  ..., -1.1352, -1.9817,  0.4473]],</span><br><span class="line"></span><br><span class="line">        [[-2.0806,  0.1014,  1.4044,  ...,  0.1496, -2.4822, -1.5388],</span><br><span class="line">         [-3.5828,  0.3326,  1.2598,  ...,  0.8470, -2.5095,  0.1296],</span><br><span class="line">         [-2.7594, -0.2307,  1.4870,  ...,  1.1056, -2.3847, -1.6484],</span><br><span class="line">         [-1.1717, -1.2086,  0.6444,  ..., -0.5858, -3.3344, -0.9535]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure><h2 id="3-5-规范化层LayerNorm"><a href="#3-5-规范化层LayerNorm" class="headerlink" title="3.5 规范化层LayerNorm"></a>3.5 规范化层LayerNorm</h2><h3 id="（1）规范化层作用"><a href="#（1）规范化层作用" class="headerlink" title="（1）规范化层作用"></a>（1）规范化层作用</h3><p>它是所有深层网络模型都需要的标准网络层，因为<strong>随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况</strong>，这样可能会导致学习过程出现异常，模型可能收敛非常的慢。 因此都<strong>会在一定层数后接规范化层进行数值的规范化</strong>，使其特征数值在合理范围内.</p><h3 id="（2）代码实现-1"><a href="#（2）代码实现-1" class="headerlink" title="（2）代码实现"></a>（2）代码实现</h3><ul><li>实例化参数有两个, <code>features</code>和<code>eps</code>，分别表示词嵌入特征大小，和一个足够小的数.</li><li>输入参数x代表来自上一层的输出.</li><li>输出就是经过规范化的特征表示.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 实现规范化层的类</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化参数</span></span><br><span class="line"><span class="string">            - features: 表示词嵌入的维度,</span></span><br><span class="line"><span class="string">            - eps: 一个足够小的数, 在规范化公式的分母中出现,防止分母为0.默认是1e-6.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据features的形状初始化两个参数张量a2，和b2，第一个初始化为1张量，</span></span><br><span class="line">        <span class="comment"># 也就是里面的元素都是1，第二个初始化为0张量，也就是里面的元素都是0，</span></span><br><span class="line">        <span class="comment"># 这两个张量就是规范化层的参数，因为直接对上一层得到的结果做规范化公式计算，</span></span><br><span class="line">        <span class="comment"># 将改变结果的正常表征，因此就需要有参数作为调节因子，使其即能满足规范化要求，</span></span><br><span class="line">        <span class="comment"># 又能不改变针对目标的表征.最后使用nn.parameter封装，代表他们是模型的参数。</span></span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line"></span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 在函数中，首先对输入变量x求其最后一个维度的均值，并保持输出维度与输入维度一致.</span></span><br><span class="line">        <span class="comment"># 接着再求最后一个维度的标准差，然后就是根据规范化公式，</span></span><br><span class="line">        <span class="comment"># 用x减去均值除以标准差获得规范化的结果，</span></span><br><span class="line">        <span class="comment"># 最后对结果乘以我们的缩放参数，即a2，*号代表同型点乘，即对应位置进行乘法操作，</span></span><br><span class="line">        <span class="comment"># 加上位移参数b2.返回即可.</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure><h3 id="（3）测试-1"><a href="#（3）测试-1" class="headerlink" title="（3）测试"></a>（3）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">features = d_model = <span class="number">512</span></span><br><span class="line">eps = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line">x = ff_result</span><br><span class="line">ln = LayerNorm(features, eps)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mha_res.shape)</span><br><span class="line">ln_result = ln(x)</span><br><span class="line"><span class="built_in">print</span>(ln_result)</span><br><span class="line"><span class="built_in">print</span>(ln_result.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[-0.6481,  0.6222, -0.2731,  ...,  0.2868,  1.0175,  0.6720],</span><br><span class="line">         [-0.3056,  0.6657,  0.1862,  ...,  0.3972,  0.9435,  0.5340],</span><br><span class="line">         [-0.1928,  1.0572,  0.3111,  ..., -0.0410,  0.5555,  0.5671],</span><br><span class="line">         [-0.4334, -0.2361, -0.1477,  ...,  0.0923,  2.0700,  0.7032]],</span><br><span class="line"></span><br><span class="line">        [[ 1.0477, -0.7183,  0.0449,  ...,  1.6828,  0.3927,  0.5616],</span><br><span class="line">         [ 1.5125, -1.1870,  0.5266,  ...,  1.4665,  1.8670,  0.2973],</span><br><span class="line">         [ 0.8196, -2.3064, -0.2661,  ...,  1.0591,  1.1476, -0.2259],</span><br><span class="line">         [ 0.3523, -0.5912,  0.5318,  ...,  1.0312,  0.6859, -0.6222]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure><h3 id="（3）LayerNorm-和-BatchNorm"><a href="#（3）LayerNorm-和-BatchNorm" class="headerlink" title="（3）LayerNorm 和 BatchNorm"></a>（3）LayerNorm 和 BatchNorm</h3><h4 id="BatchNorm-简单的-2-维-情况（蓝色）"><a href="#BatchNorm-简单的-2-维-情况（蓝色）" class="headerlink" title="BatchNorm 简单的 2 维 情况（蓝色）"></a>BatchNorm 简单的 2 维 情况（蓝色）</h4><ul><li>每一行是一个样本 X，每一列是 一个 feature</li><li><strong>BatchNorm</strong>：每次把一列（1 个 feature）放在一个 mini-batch 里，均值变成 0， 方差变成 1 的标准化。</li><li><strong>How</strong>：（该列向量 - mini-batch 该列向量的均值）/（mini - batch 该列向量的方差）</li><li><strong>训练时</strong>：mini-batch 计算均值；</li><li><strong>测试时</strong>：使用全局均值、方差。</li><li>BatchNorm 还会学 $\lambda$$  \beta $，BatchNorm 可以通过学习将向量放缩成任意均值、任意方差 的一个向量。</li></ul><h4 id="Layernorm-（黄色）"><a href="#Layernorm-（黄色）" class="headerlink" title="Layernorm （黄色）"></a>Layernorm （黄色）</h4><ul><li>LayerNorm 跟 BatchNorm 在很多时候几乎是一样的，除了实现的方法有点不一样之外。</li><li><strong>LayerNorm</strong>：对每个样本做 Normalization（把每一行变成 均值为 0、方差为 1），不是对每个特征做 normalization。</li></ul><p><img src="image/image_jeoTTRYHi-.png" alt=""></p><h4 id="LayerNorm-在操作上-和-BatchNorm-二维输入-的关系"><a href="#LayerNorm-在操作上-和-BatchNorm-二维输入-的关系" class="headerlink" title="LayerNorm 在操作上 和 BatchNorm (二维输入) 的关系"></a>LayerNorm 在操作上 和 BatchNorm (二维输入) 的关系</h4><p>LayerNorm 整个把数据转置一次，放到 BatchNorm 里面出来的结果，再转置回去，基本上可以得到LayerNorm的结果。</p><h4 id="三维输入"><a href="#三维输入" class="headerlink" title="三维输入"></a>三维输入</h4><p>Transformer 和 RNN 里面：3 维输入。</p><ul><li>输入的是一个序列的样本，每个样本中有很多元素，是一个序列。</li><li>一个句子里面有 n 个词，每个词对应一个向量，+ 一个 batch —&gt; 3 维&#x20;</li><li>列 是 seq 序列长度 n；第 3 维 feature 是每个词额外的向量，d = 512 in transformer &#x20;</li></ul><p><strong>BatchNorm</strong> （蓝色线）：每次取一个特征，切一块，拉成一个向量，均值为 0 、方差为 1 的标准化。</p><p><strong>LayerNorm (橙色)</strong>：横着切</p><p><img src="image/image_nQvpI0YdcK.png" alt=""></p><h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>时序数据中 样本长度可能不一样。</p><p>举例分析：4个长度不一样的样本，0 填充到 max_len</p><p><strong>BatchNorm 切出来的结果</strong>（蓝色)</p><ul><li>BatchNorm 计算均值和方差，有效的是阴影部分，其余是 0</li><li>Mini-batch 的均值和方差：如果样本长度变化比较大的时候，每次计算小批量的均值和方差，均值和方差的抖动大。</li><li>局的均值和方差：测试时遇到一个特别长的全新样本 （最上方蓝色阴影块），训练时未见过，训练时计算的均值和方差可能不好用。</li></ul><p><strong>LayerNorm 切出来的结果</strong>（黄色）</p><ul><li>ayerNorm 每个样本自己算均值和方差，不需要存全局的均值和方差。</li><li>ayerNorm 更稳定，不管样本长还是短，均值和方差是在每个样本内计算。</li></ul><p><img src="image/image_P2kdW23lox.png" alt=""></p><h4 id="LayerNorm-和-BatchNorm-的例子理解：n-本书"><a href="#LayerNorm-和-BatchNorm-的例子理解：n-本书" class="headerlink" title="LayerNorm 和 BatchNorm 的例子理解：n 本书"></a>LayerNorm 和 BatchNorm 的例子理解：n 本书</h4><ul><li><strong>BatchNorm</strong>：n本书，每本书的第一页拿出来，根据 n 本书的第一页的字数均值 做 Norm</li><li><strong>LayerNorm</strong>：针对某一本书，这本书的每一页拿出来，根据次数每页的字数均值，自己做 Norm</li></ul><h2 id="3-6-子层连接结构SublayerConnection"><a href="#3-6-子层连接结构SublayerConnection" class="headerlink" title="3.6 子层连接结构SublayerConnection"></a>3.6 子层连接结构SublayerConnection</h2><p>如图所示，输入到每个子层以及规范化层的过程中，还使用了<strong>残差链接</strong>（跳跃连接），因此我们把这一部分结构整体叫做<strong>子层连接</strong>（代表子层及其链接结构），在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构.</p><p><img src="image/image_-2lYYJLnn7.png" alt=""></p><h3 id="（1）代码实现"><a href="#（1）代码实现" class="headerlink" title="（1）代码实现"></a>（1）代码实现</h3><ul><li>类的初始化函数输入参数是<code>size</code>, <code>dropout</code>, 分别代表词嵌入大小和置零比率.</li><li>它的实例化对象输入参数是<code>x</code>, <code>sublayer</code>, 分别代表上一层输出以及子层的函数表示.</li><li>它的输出就是通过子层连接结构处理的输出.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 子层连接结构的类</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化</span></span><br><span class="line"><span class="string">            - size: 词嵌入维度的大小， </span></span><br><span class="line"><span class="string">            - dropout: 是对模型结构中的节点数进行随机抑制的比率， </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        <span class="comment"># 实例化了规范化对象self.norm</span></span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 前向逻辑函数中, 接收上一个层或者子层的输入作为第一个参数，</span></span><br><span class="line"><span class="string">           将该子层连接中的子层函数作为第二个参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 我们首先对输出进行规范化，然后将结果传给子层处理，之后再对子层进行dropout操作，</span></span><br><span class="line">        <span class="comment"># 随机停止一些网络中神经元的作用，来防止过拟合. 最后还有一个add操作， </span></span><br><span class="line">        <span class="comment"># 因为存在跳跃连接，所以是将输入x与dropout后的子层输出结果相加作为最终的子层连接输出.</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><h3 id="（2）测试-3"><a href="#（2）测试-3" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = pe_result</span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">mha = MultiHeadAttention(head, embedding_dim, dropout)</span><br><span class="line"></span><br><span class="line">sublayer = <span class="keyword">lambda</span> x : mha(x, x, x, mask)</span><br><span class="line">size = <span class="number">512</span></span><br><span class="line"></span><br><span class="line">sc = SublayerConnection(size, dropout)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">sc_result = sc(x, sublayer)</span><br><span class="line"><span class="built_in">print</span>(sc_result)</span><br><span class="line"><span class="built_in">print</span>(sc_result.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[ -8.1109,   0.2217,   1.0194,  ...,  22.8768,  40.2057,  14.8928],</span><br><span class="line">         [ 31.5267,  33.8321,  29.1483,  ...,  17.2134, -16.1430,  -4.6771],</span><br><span class="line">         [  7.9553, -20.3707, -41.5156,  ..., -10.6798, -23.8106,   4.3206],</span><br><span class="line">         [-59.5128,  31.6802,   1.1462,  ..., -15.4225,  -2.6904,  45.0427]],</span><br><span class="line"></span><br><span class="line">        [[  6.9450, -18.4468, -42.5561,  ..., -10.3473, -23.6293,   4.3888],</span><br><span class="line">         [-38.2155,  46.7522, -22.8546,  ...,  28.8744,  -0.0767,  13.4702],</span><br><span class="line">         [ 31.4326,  33.3512,  29.2566,  ...,  17.6481, -16.0407,  -4.4314],</span><br><span class="line">         [-16.7070, -15.7774,  16.0646,  ..., -65.1326,   0.0000,   0.1910]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure><h2 id="3-7-编码器层EncoderLayer"><a href="#3-7-编码器层EncoderLayer" class="headerlink" title="3.7 编码器层EncoderLayer"></a>3.7 编码器层EncoderLayer</h2><p>作为编码器的组成单元,<strong> 每个编码器层完成一次对输入的特征提取过程</strong>, 即编码过程.</p><p><img src="image/image_WHGcn6ZmRg.png" alt=""></p><h3 id="（1）代码实现-1"><a href="#（1）代码实现-1" class="headerlink" title="（1）代码实现"></a>（1）代码实现</h3><ul><li>类的初始化函数共有4个, 第一个是<code>size</code>，其实就是我们词嵌入维度的大小. 第二个<code>self_attn</code>，之后我们将传入多头自注意力子层实例化对象, 并且是自注意力机制. 第三个是<code>feed_froward</code>, 之后我们将传入前馈全连接层实例化对象. 最后一个是置0比率<code>dropout</code>.</li><li>实例化对象的输入参数有2个，x代表来自上一层的输出, mask代表掩码张量.</li><li>它的输出代表经过整个编码层的特征表示.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 编码器层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化</span></span><br><span class="line"><span class="string">            - size: 词嵌入维度的大小，它也将作为我们编码器层的大小, </span></span><br><span class="line"><span class="string">            - self_attn: 传入多头自注意力子层实例化对象, 并且是自注意力机制, </span></span><br><span class="line"><span class="string">            - eed_froward: 传入前馈全连接层实例化对象,</span></span><br><span class="line"><span class="string">            - dropout: 置0比率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.size = size</span><br><span class="line">        <span class="comment"># 如图所示, 编码器层中有两个子层连接结构, 所以使用clones函数进行克隆</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; x和mask，分别代表上一层的输出，和掩码张量mask</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#  首先通过第一个子层连接结构，其中包含多头自注意力子层，</span></span><br><span class="line">        <span class="comment"># 然后通过第二个子层连接结构，其中包含前馈全连接子层. 最后返回结果.</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x : self.self_attn(x, x, x, mask))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><h3 id="（2）测试-4"><a href="#（2）测试-4" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">size = d_model = <span class="number">512</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">x = pe_result</span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">self_attn = MultiHeadAttention(head, d_model)</span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">el = EncoderLayer(size, self_attn, ff, dropout)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">el_result = el(x, mask)</span><br><span class="line"><span class="built_in">print</span>(el_result)</span><br><span class="line"><span class="built_in">print</span>(el_result.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[-29.5621,  -2.0977,   2.6601,  ...,  33.7814, -41.1742, -10.7692],</span><br><span class="line">         [ 36.1439, -27.8296, -23.2643,  ...,  21.5115,  -4.6657,  14.0641],</span><br><span class="line">         [ -8.1926, -29.1385,  -2.2535,  ...,  -5.4215,   2.7747,  -0.4909],</span><br><span class="line">         [-25.4288, -14.2624, -22.5432,  ...,  -5.3338,   9.2610,   4.8978]],</span><br><span class="line"></span><br><span class="line">        [[ -9.4071, -27.6196,  -2.8486,  ...,  -5.1319,   2.3754,  14.1391],</span><br><span class="line">         [-15.6512,  -1.9466, -36.3869,  ...,  19.8941,  24.4394,  40.9649],</span><br><span class="line">         [ 36.5416, -28.4673, -22.8311,  ...,  21.5283,  -4.6554,  14.3312],</span><br><span class="line">         [-30.7237,  38.2961,  -8.4991,  ...,  57.8437,  11.2464,  -5.4290]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure><h2 id="3-8-编码器Encoder"><a href="#3-8-编码器Encoder" class="headerlink" title="3.8 编码器Encoder"></a>3.8 编码器Encoder</h2><p>编码器用于对输入进行指定的特征提取过程, 也称为编码, 由N个编码器层堆叠而成.</p><p><img src="image/image_hCQZA6gkw_.png" alt=""></p><h3 id="（1）实现-3"><a href="#（1）实现-3" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul><li>类的初始化函数参数有两个，分别是<code>layer</code>和<code>N</code>，代表编码器层和编码器层的个数.</li><li>forward函数的输入参数也有两个, 和编码器层的forward相同, x代表上一层的输出, mask代码掩码张量.</li><li>编码器类的输出就是Transformer中编码器的特征提取表示, 它将成为解码器的输入的一部分.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 实现编码器</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化</span></span><br><span class="line"><span class="string">            - layer: 编码器层</span></span><br><span class="line"><span class="string">            - N: 编码器层的个数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        <span class="comment"># 使用clones函数克隆N个编码器层放在self.layers中</span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;forward函数的输入和编码器层相同, </span></span><br><span class="line"><span class="string">            - x: 上一层的输出, </span></span><br><span class="line"><span class="string">            - mask: 掩码张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 首先就是对我们克隆的编码器层进行循环，每次都会得到一个新的x，</span></span><br><span class="line">        <span class="comment"># 这个循环的过程，就相当于输出的x经过了N个编码器层的处理. </span></span><br><span class="line">        <span class="comment"># 最后再通过规范化层的对象self.norm进行处理，最后返回结果. </span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><h3 id="（2）测试-5"><a href="#（2）测试-5" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">size = d_model = <span class="number">512</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">x = pe_result</span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">N = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">self_attn = MultiHeadAttention(head, d_model)</span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">layer = EncoderLayer(size, copy.deepcopy(self_attn), copy.deepcopy(ff), dropout)</span><br><span class="line"></span><br><span class="line">encoder = Encoder(layer, N)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">en_result = encoder(x, mask)</span><br><span class="line"><span class="built_in">print</span>(en_result)</span><br><span class="line"><span class="built_in">print</span>(en_result.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[-0.7567, -1.2521, -0.2055,  ...,  0.8205, -1.2941, -2.0247],</span><br><span class="line">         [-0.0359,  0.9469,  0.0691,  ..., -0.6150,  0.4005, -0.1147],</span><br><span class="line">         [-1.3874,  0.9941,  0.1449,  ..., -0.3395,  1.3993, -2.0148],</span><br><span class="line">         [-0.5812, -0.6430,  2.1250,  ...,  1.8703, -0.1342,  0.6250]],</span><br><span class="line"></span><br><span class="line">        [[-1.4746,  1.0971, -0.0154,  ..., -0.3533,  1.4110, -1.8592],</span><br><span class="line">         [-0.5287, -1.6246,  0.7500,  ...,  0.4196,  0.8892,  0.2809],</span><br><span class="line">         [-0.1306,  0.8462,  0.0411,  ..., -0.5721,  0.4040, -0.1732],</span><br><span class="line">         [-0.8179, -1.3323, -0.7204,  ..., -0.4005,  0.5500, -0.0986]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure><h1 id="4-解码器"><a href="#4-解码器" class="headerlink" title="4.解码器"></a>4.解码器</h1><ul><li>由N个解码器层堆叠而成</li><li>每个解码器层由<strong>三个子层</strong>连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li><li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul><p><img src="image/image_0fQfOONvyp.png" alt=""></p><h2 id="4-1-解码器层DecoderLayer"><a href="#4-1-解码器层DecoderLayer" class="headerlink" title="4.1 解码器层DecoderLayer"></a>4.1 解码器层DecoderLayer</h2><p>作为解码器的组成单元, <strong>每个解码器层根据给定的输入向目标方向进行特征提取操作</strong>，即解码过程。</p><h3 id="（1）实现-4"><a href="#（1）实现-4" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul><li>类的初始化函数的参数有5个, 分别是<code>size</code>，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，第二个是<code>self_attn</code>，多头自注意力对象，也就是说这个注意力机制需要<code>Q=K=V</code>，第三个是<code>src_attn</code>，多头注意力对象，这里<code>Q!=K=V</code>， 第四个是前馈全连接层对象，最后就是<code>droupout</code>置0比率.</li><li>forward函数的参数有4个，分别是来自上一层的输入x，来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.</li><li>最终输出了由编码器输入和目标数据一同作用的特征提取结果.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 解码器层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化</span></span><br><span class="line"><span class="string">            - size：代表词嵌入的维度大小, 同时也代表解码器层的尺寸，</span></span><br><span class="line"><span class="string">            - self_attn： 多头自注意力对象，也就是说这个注意力机制需要Q=K=V， </span></span><br><span class="line"><span class="string">            - src_attn：多头注意力对象，这里Q!=K=V， </span></span><br><span class="line"><span class="string">            - feed_forward： 前馈全连接层对象，</span></span><br><span class="line"><span class="string">            - droupout：置0比率.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        <span class="comment"># 按照结构图使用clones函数克隆三个子层连接对象.</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;forward函数中的参数有4个，</span></span><br><span class="line"><span class="string">            - x: 来自上一层的输入x，</span></span><br><span class="line"><span class="string">            - mermory: 来自编码器层的语义存储变量 </span></span><br><span class="line"><span class="string">            - src_mask: 源数据掩码张量</span></span><br><span class="line"><span class="string">            - tgt_mask: 目标数据掩码张量.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 将memory表示成m方便之后使用</span></span><br><span class="line">        m = memory</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，</span></span><br><span class="line">        <span class="comment"># 因为是自注意力机制，所以Q,K,V都是x，最后一个参数是目标数据掩码张量，</span></span><br><span class="line">        <span class="comment"># 这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据，</span></span><br><span class="line">        <span class="comment"># 比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，</span></span><br><span class="line">        <span class="comment"># 但是我们不希望在生成第一个字符时模型能利用这个信息，因此我们会将其遮掩，</span></span><br><span class="line">        <span class="comment"># 同样生成第二个字符或词汇时，</span></span><br><span class="line">        <span class="comment"># 模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用.</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x : self.self_attn(x, x, x, tgt_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 接着进入第二个子层，这个子层中常规的注意力机制，q是输入x; k，v是编码层输出memory， </span></span><br><span class="line">        <span class="comment"># 同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄漏，</span></span><br><span class="line">        <span class="comment"># 而是遮蔽掉对结果没有意义的字符而产生的注意力值，</span></span><br><span class="line">        <span class="comment"># 以此提升模型效果和训练速度. 这样就完成了第二个子层的处理.</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x : self.src_attn(x, m, m, src_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><h3 id="（2）测试-6"><a href="#（2）测试-6" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">size = d_model = <span class="number">512</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">self_attn = src_attn = MultiHeadAttention(head, d_model, dropout)</span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line"><span class="comment"># x是来自目标数据的词嵌入表示, 但形式和源数据的词嵌入表示相同, 这里使用per充当.</span></span><br><span class="line">x = pe_result</span><br><span class="line"><span class="comment"># memory是来自编码器的输出</span></span><br><span class="line">memory = en_result</span><br><span class="line"><span class="comment"># 实际中source_mask和target_mask并不相同, 这里为了方便计算使他们都为mask</span></span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">src_mask = tgt_mask = mask</span><br><span class="line"></span><br><span class="line">dl = DecoderLayer(size, self_attn, src_attn, ff, dropout)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">dl_result = dl(x, memory, src_mask, tgt_mask)</span><br><span class="line"><span class="built_in">print</span>(dl_result)</span><br><span class="line"><span class="built_in">print</span>(dl_result.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[  0.3128, -28.9028,  12.6505,  ..., -25.0090,   0.0671,  43.7430],</span><br><span class="line">         [-18.6592, -20.2816,  -0.2946,  ...,  30.0085, -15.1012, -14.5942],</span><br><span class="line">         [-19.6110,   0.1298,  -0.3384,  ..., -14.7287, -22.1352,  12.7321],</span><br><span class="line">         [-33.9689,   0.9680, -61.3009,  ..., -51.5810,  -8.8205,  -6.2392]],</span><br><span class="line"></span><br><span class="line">        [[-21.1268,  10.4200,   3.9523,  ..., -15.5449,  -0.2314,  12.6887],</span><br><span class="line">         [  6.3277,  27.3815,  43.6648,  ..., -21.2202, -48.8453, -20.5100],</span><br><span class="line">         [-18.5797, -21.5331, -38.0592,  ...,  29.7248, -15.0411,   0.4119],</span><br><span class="line">         [ 65.4318,  15.5895, -23.6869,  ..., -25.7464,  42.8896,  14.5587]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure><h2 id="4-2-解码器Decoder"><a href="#4-2-解码器Decoder" class="headerlink" title="4.2 解码器Decoder"></a>4.2 解码器Decoder</h2><p><strong>根据编码器的结果以及上一次预测的结果, 对下一次可能出现的’值’进行特征表示</strong>.</p><h3 id="（1）实现-5"><a href="#（1）实现-5" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul><li>类的初始化函数的参数有两个，第一个就是解码器层<code>layer</code>，第二个是解码器层的个数<code>N</code>.</li><li>forward函数中的参数有4个，<code>x</code>代表目标数据的嵌入表示，<code>memory</code>是编码器层的输出，<code>src_mask</code>, <code>tgt_mask</code>代表源数据和目标数据的掩码张量.</li><li>输出解码过程的最终特征表示.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 实现解码器</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化</span></span><br><span class="line"><span class="string">            - layer： 解码器层</span></span><br><span class="line"><span class="string">            - N：解码器层个数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        <span class="comment"># 首先使用clones方法克隆了N个layer，然后实例化了一个规范化层. </span></span><br><span class="line">        <span class="comment"># 因为数据走过了所有的解码器层后最后要做规范化处理. </span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line"></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            - x: 来自上一层的输入x，</span></span><br><span class="line"><span class="string">            - mermory: 来自编码器层的语义存储变量 </span></span><br><span class="line"><span class="string">            - src_mask: 源数据掩码张量</span></span><br><span class="line"><span class="string">            - tgt_mask: 目标数据掩码张量.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 然后就是对每个层进行循环，当然这个循环就是变量x通过每一个层的处理，</span></span><br><span class="line">        <span class="comment"># 得出最后的结果，再进行一次规范化返回即可.</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><h3 id="（2）测试-7"><a href="#（2）测试-7" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">size = d_model = <span class="number">512</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">attn = MultiHeadAttention(head, d_model)</span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">layer = DecoderLayer(d_model, copy.deepcopy(attn), copy.deepcopy(attn), copy.deepcopy(ff), dropout)</span><br><span class="line"></span><br><span class="line">N = <span class="number">8</span></span><br><span class="line"><span class="comment"># 输入参数与解码器层的输入参数相同</span></span><br><span class="line">x = pe_result</span><br><span class="line">memory = en_result</span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">src_mask = tgt_mask = mask</span><br><span class="line"></span><br><span class="line">de = Decoder(layer, N)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">de_result = de(x, memory, src_mask, tgt_mask)</span><br><span class="line"><span class="built_in">print</span>(de_result)</span><br><span class="line"><span class="built_in">print</span>(de_result.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[ 6.2605e-01, -1.6188e+00, -2.0886e+00,  ...,  1.0329e-01,</span><br><span class="line">          -9.3746e-01, -2.6656e-01],</span><br><span class="line">         [ 1.0500e-01, -2.6750e+00,  2.6044e+00,  ..., -5.4699e-01,</span><br><span class="line">          -7.5199e-02, -2.8667e-01],</span><br><span class="line">         [-1.6483e-02, -3.6539e-01, -3.1693e-01,  ..., -2.1838e-01,</span><br><span class="line">           5.6952e-01,  1.4017e+00],</span><br><span class="line">         [-4.9258e-01, -9.2657e-01, -1.3348e-01,  ..., -2.1710e-01,</span><br><span class="line">           1.3200e+00,  1.3176e+00]],</span><br><span class="line"></span><br><span class="line">        [[-2.6396e-03, -2.1476e-01, -2.9699e-01,  ..., -2.0103e-02,</span><br><span class="line">           5.1760e-01,  1.5096e+00],</span><br><span class="line">         [-4.1995e-01, -2.5207e+00, -1.1587e-01,  ..., -4.2679e-01,</span><br><span class="line">           7.7861e-01,  1.7993e-02],</span><br><span class="line">         [ 2.2358e-02, -2.7497e+00,  2.5735e+00,  ..., -4.2572e-01,</span><br><span class="line">          -4.1822e-01, -1.9397e-01],</span><br><span class="line">         [ 2.8288e-01,  4.5199e-01,  3.6352e-01,  ...,  2.1069e+00,</span><br><span class="line">          -8.3942e-01,  3.1137e-02]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure><h1 id="5-输出部分"><a href="#5-输出部分" class="headerlink" title="5.输出部分"></a>5.输出部分</h1><ul><li>线性层</li><li>softmax层</li></ul><p><img src="image/image_GROtcgUedV.png" alt=""></p><h2 id="5-1-线性层"><a href="#5-1-线性层" class="headerlink" title="5.1 线性层"></a>5.1 线性层</h2><p>通过对上一步的线性变化得到指定维度的输出, 也就是<strong>转换维度</strong>的作用.</p><h2 id="5-2-softmax层"><a href="#5-2-softmax层" class="headerlink" title="5.2 softmax层"></a>5.2 softmax层</h2><p>使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.</p><h2 id="5-3-线性层和softmax层的类-Generator"><a href="#5-3-线性层和softmax层的类-Generator" class="headerlink" title="5.3 线性层和softmax层的类 Generator"></a>5.3 线性层和softmax层的类 Generator</h2><h3 id="（1）实现-6"><a href="#（1）实现-6" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul><li>初始化函数的输入参数有两个, <code>d_model</code>代表词嵌入维度, <code>vocab_size</code>代表词表大小.</li><li>forward函数接受上一层的输出.</li><li>最终获得经过线性层和softmax层处理的结果.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 生成器类</span></span><br><span class="line"><span class="string">        将线性层和softmax计算层一起实现, 因为二者的共同目标是生成最后的结构</span></span><br><span class="line"><span class="string">        因此把类的名字叫做Generator, 生成器类</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            - d_model: 词嵌入维度</span></span><br><span class="line"><span class="string">            - vocab_size: 词表的总大小</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.project = nn.Linear(d_model, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            - x: 上一层的输出张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 在函数中, 首先使用上一步得到的self.project对x进行线性变化, </span></span><br><span class="line">        <span class="comment"># 然后使用F中已经实现的log_softmax进行的softmax处理.</span></span><br><span class="line">        <span class="comment"># 在这里之所以使用log_softmax是因为和我们这个pytorch版本的损失函数实现有关, 在其他版本中将修复.</span></span><br><span class="line">        <span class="comment"># log_softmax就是对softmax的结果又取了对数, 因为对数函数是单调递增函数, </span></span><br><span class="line">        <span class="comment"># 因此对最终我们取最大的概率值没有影响. 最后返回结果即可.</span></span><br><span class="line">        <span class="keyword">return</span> log_softmax(self.project(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="（2）测试-8"><a href="#（2）测试-8" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">d_model = <span class="number">512</span></span><br><span class="line">vocab_size = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 输入x是上一层网络的输出, 我们使用来自解码器层的输出</span></span><br><span class="line">x = de_result</span><br><span class="line"></span><br><span class="line">gen = Generator(d_model, vocab_size)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">gen_result = gen(x)</span><br><span class="line"><span class="built_in">print</span>(gen_result)</span><br><span class="line"><span class="built_in">print</span>(gen_result.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[-7.6141, -7.1042, -6.0669,  ..., -6.8734, -7.0652, -7.3009],</span><br><span class="line">         [-7.1238, -6.8357, -7.2459,  ..., -7.1225, -8.0530, -6.9380],</span><br><span class="line">         [-7.5266, -7.6293, -8.1937,  ..., -7.6398, -7.8350, -7.6071],</span><br><span class="line">         [-7.4972, -7.2781, -7.3025,  ..., -6.6653, -6.4995, -6.9529]],</span><br><span class="line"></span><br><span class="line">        [[-7.5171, -7.0331, -8.0956,  ..., -7.4018, -7.6130, -7.9539],</span><br><span class="line">         [-6.4713, -7.4932, -6.8351,  ..., -6.6046, -8.0713, -7.4401],</span><br><span class="line">         [-7.3482, -6.6409, -7.5268,  ..., -7.1031, -8.2056, -7.2852],</span><br><span class="line">         [-8.1393, -7.1066, -7.4460,  ..., -6.9347, -6.3511, -6.9577]]],</span><br><span class="line">       grad_fn=&lt;LogSoftmaxBackward&gt;)</span><br><span class="line">torch.Size([2, 4, 1000])</span><br></pre></td></tr></table></figure><h1 id="6-模型构建"><a href="#6-模型构建" class="headerlink" title="6.模型构建"></a>6.模型构建</h1><h2 id="6-1-编码器-解码器EncoderDecoder"><a href="#6-1-编码器-解码器EncoderDecoder" class="headerlink" title="6.1 编码器-解码器EncoderDecoder"></a>6.1 编码器-解码器EncoderDecoder</h2><p>大部分神经序列转换模型都有一个编码器-解码器结构。编码器把一个输入序列$(x_1, …, x_n)$映射到一个连续的表示 $z=(z_1, .., z_n)$中。解码器对z中的每个元素，生成输出序列$(y_1, …, y_m)$，一个时间步生成一个元素。在每一步中，模型都是自回归的，在生成下一个结果时，会将先前生成的结构加入输入序列来一起预测。（自回归模型的特点）</p><h3 id="（1）实现-7"><a href="#（1）实现-7" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul><li>类的初始化函数传入5个参数, 分别是编码器对象, 解码器对象, 源数据嵌入函数, 目标数据嵌入函数, 以及输出部分的类别生成器对象.</li><li>类中共实现三个函数, <code>forward</code>, <code>encode</code>, <code>decode</code></li><li>forward是主要逻辑函数, 有四个参数, source代表源数据, target代表目标数据, source_mask和target_mask代表对应的掩码张量.</li><li>encode是编码函数, 以source和source_mask为参数.</li><li>decode是解码函数, 以memory即编码器的输出, source_mask, target, target_mask为参数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 实现编码器-解码器结构</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化</span></span><br><span class="line"><span class="string">            - encoder： 编码器对象</span></span><br><span class="line"><span class="string">            - decoder： 解码器对象</span></span><br><span class="line"><span class="string">            - src_embed：源数据嵌入函数</span></span><br><span class="line"><span class="string">            - tgt_embed： 目标数据嵌入函数</span></span><br><span class="line"><span class="string">            - generator：输出部分类别生成器</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            - src：源数据</span></span><br><span class="line"><span class="string">            - tgt：目标数据</span></span><br><span class="line"><span class="string">            - src_mask：源数据的掩码张量</span></span><br><span class="line"><span class="string">            - tgt_mask：目标数据的掩码张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 在函数中, 将source, source_mask传入编码函数, 得到结果后,</span></span><br><span class="line">        <span class="comment"># 与source_mask，target，和target_mask一同传给解码函数.</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 编码函数</span></span><br><span class="line"><span class="string">            - src：源数据</span></span><br><span class="line"><span class="string">            - src_mask：源数据的掩码张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 解码函数</span></span><br><span class="line"><span class="string">            - memory：经历编码器编码后的输出张量</span></span><br><span class="line"><span class="string">            - src_mask：源数据的掩码张量</span></span><br><span class="line"><span class="string">            - tgt：目标数据</span></span><br><span class="line"><span class="string">            - tgt_mask：目标数据的掩码张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure><h3 id="（2）测试-9"><a href="#（2）测试-9" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = <span class="number">1000</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">encoder = en</span><br><span class="line">decoder = de</span><br><span class="line">src_embed = nn.Embedding(vocab_size, d_model)</span><br><span class="line">tgt_embed = nn.Embedding(vocab_size, d_model)</span><br><span class="line">generator = Generator(d_model, vocab_size)</span><br><span class="line"></span><br><span class="line">src = tgt = Variable(torch.LongTensor([[<span class="number">100</span>, <span class="number">2</span>, <span class="number">421</span>, <span class="number">508</span>], [<span class="number">491</span>, <span class="number">998</span>, <span class="number">1</span>, <span class="number">221</span>]]))</span><br><span class="line">src_mask = tgt_mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">ed = EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)</span><br><span class="line"></span><br><span class="line">ed_result = ed(src, tgt, src_mask, tgt_mask)</span><br><span class="line"><span class="built_in">print</span>(ed_result)</span><br><span class="line"><span class="built_in">print</span>(ed_result.shape)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 0.3879,  0.1344, -0.5700,  ..., -0.2206, -0.7505, -0.3314],</span><br><span class="line">         [-0.7957,  0.8759, -0.5033,  ..., -1.3409, -1.4451, -0.3243],</span><br><span class="line">         [ 0.3513,  0.8083,  0.1246,  ..., -0.4443, -1.3551, -1.3547],</span><br><span class="line">         [ 0.0050,  0.6573, -1.1390,  ...,  0.1529, -1.5487, -0.8990]],</span><br><span class="line"></span><br><span class="line">        [[-0.1515,  1.9247, -0.0315,  ..., -0.5945, -2.7363, -1.2481],</span><br><span class="line">         [-0.6422,  1.5250,  0.7561,  ..., -1.4778, -1.2162, -2.2946],</span><br><span class="line">         [ 0.0163,  1.8034,  0.1408,  ..., -0.4170, -1.7017, -1.6474],</span><br><span class="line">         [ 0.2880, -0.0269, -0.1636,  ..., -0.7687, -1.3453, -0.8909]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure><h2 id="6-2-Transformer模型make-model"><a href="#6-2-Transformer模型make-model" class="headerlink" title="6.2 Transformer模型make_model"></a>6.2 Transformer模型make_model</h2><h3 id="（1）实现-8"><a href="#（1）实现-8" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul><li>有7个参数，分别是源数据特征(词汇)总数，目标数据特征(词汇)总数，编码器和解码器堆叠数，词向量映射维度，前馈全连接网络中变换矩阵的维度，多头注意力结构中的多头数，以及置零比率dropout.</li><li>该函数最后返回一个构建好的模型对象.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_model</span>(<span class="params">src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 构建transformer模型</span></span><br><span class="line"><span class="string">        - src_vocab: 源数据特征(词汇)总数</span></span><br><span class="line"><span class="string">        - tgt_vocab: 目标数据特征(词汇)总数</span></span><br><span class="line"><span class="string">        - N: 编码器和解码器堆叠数</span></span><br><span class="line"><span class="string">        - d_model: 词向量映射维度</span></span><br><span class="line"><span class="string">        - d_ff: 前馈全连接网络中变换矩阵的维度</span></span><br><span class="line"><span class="string">        - h : 多头注意力结构中的多头数</span></span><br><span class="line"><span class="string">        - dropout: 置零比率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 首先得到一个深度拷贝命令，接下来很多结构都需要进行深度拷贝，</span></span><br><span class="line">    <span class="comment"># 来保证他们彼此之间相互独立，不受干扰.</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    <span class="comment"># 实例化了多头注意力类，得到对象attn</span></span><br><span class="line">    attn = MultiHeadAttention(h, d_model)</span><br><span class="line">    <span class="comment"># 然后实例化前馈全连接类，得到对象ff </span></span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    <span class="comment"># 实例化位置编码类，得到对象position</span></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    <span class="comment"># 根据结构图, 最外层是EncoderDecoder，在EncoderDecoder中，</span></span><br><span class="line">    <span class="comment"># 分别是编码器层，解码器层，源数据Embedding层和位置编码组成的有序结构，</span></span><br><span class="line">    <span class="comment"># 目标数据Embedding层和位置编码组成的有序结构，以及类别生成器层. </span></span><br><span class="line">    <span class="comment"># 在编码器层中有attention子层以及前馈全连接子层，</span></span><br><span class="line">    <span class="comment"># 在解码器层中有两个attention子层以及前馈全连接层.</span></span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 模型结构完成后，接下来就是初始化模型中的参数，比如线性层中的变换矩阵</span></span><br><span class="line">    <span class="comment"># 这里一但判断参数的维度大于1，则会将其初始化成一个服从均匀分布的矩阵，</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h3 id="（2）测试-10"><a href="#（2）测试-10" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">src_vocab = <span class="number">11</span></span><br><span class="line">tgt_vocab = <span class="number">11</span></span><br><span class="line"></span><br><span class="line">model= make_model(src_vocab, tgt_vocab)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br></pre></td><td class="code"><pre><span class="line">EncoderDecoder(</span><br><span class="line">  (encoder): Encoder(</span><br><span class="line">    (layers): ModuleList(</span><br><span class="line">      (0): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (1): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (2): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (3): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (4): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (5): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): LayerNorm()</span><br><span class="line">  )</span><br><span class="line">  (decoder): Decoder(</span><br><span class="line">    (layers): ModuleList(</span><br><span class="line">      (0): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (2): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (1): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (2): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (2): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (2): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (3): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (2): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (4): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (2): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (5): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">  (src_embed): Sequential(</span><br><span class="line">    (0): Embeddings(</span><br><span class="line">      (lut): Embedding(8316, 512)</span><br><span class="line">    )</span><br><span class="line">    (1): PositionalEncoding(</span><br><span class="line">      (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (tgt_embed): Sequential(</span><br><span class="line">    (0): Embeddings(</span><br><span class="line">      (lut): Embedding(6385, 512)</span><br><span class="line">    )</span><br><span class="line">    (1): PositionalEncoding(</span><br><span class="line">      (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (generator): Generator(</span><br><span class="line">    (project): Linear(in_features=512, out_features=6385, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="6-3-Inference"><a href="#6-3-Inference" class="headerlink" title="6.3 Inference"></a>6.3 Inference</h2><p>在这里，我们用生成模型的预测。 我们尝试使用我们的transformer 来记住输入。 正如您将看到的那样，由于模型尚未训练，输出是随机生成的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inference_test</span>():</span><br><span class="line">    test_model = make_model(<span class="number">11</span>, <span class="number">11</span>, <span class="number">2</span>)</span><br><span class="line">    test_model.<span class="built_in">eval</span>()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    memory = test_model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">        out = test_model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = test_model.generator(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.empty(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Example Untrained Model Prediction:&quot;</span>, ys)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_tests</span>():</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        inference_test()</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Example Untrained Model Prediction: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[0, 3, 4, 4, 4, 4, 4, 4, 4, 4]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0, 10, 10, 10,  3,  2,  5,  7,  9,  6]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  4,  3,  6, 10, 10,  2,  6,  2,  2]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  9,  0,  1,  5, 10,  1,  5, 10,  6]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  1,  5,  1, 10,  1, 10, 10, 10, 10]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  1, 10,  9,  9,  9,  9,  9,  1,  5]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  3,  1,  5, 10, 10, 10, 10, 10, 10]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  3,  5, 10,  5, 10,  4,  2,  4,  2]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[0, 5, 6, 2, 5, 6, 2, 6, 2, 2]])</span><br></pre></td></tr></table></figure><h1 id="7-测试运行"><a href="#7-测试运行" class="headerlink" title="7.测试运行"></a>7.测试运行</h1><h2 id="7-1-copy任务简介"><a href="#7-1-copy任务简介" class="headerlink" title="7.1 copy任务简介"></a>7.1 copy任务简介</h2><p><strong>任务描述</strong>：针对数字序列进行学习, 学习的最终目标是使输出与输入的序列相同. 如输入[1, 5, 8, 9, 3], 输出也是[1, 5, 8, 9, 3].</p><p>任务意义：copy任务在模型基础测试中具有重要意义，因为copy操作对于模型来讲是一条明显规律, 因此模型能否在短时间内，小数据集中学会它，可以帮助我们断定模型所有过程是否正常，是否已具备基本学习能力。</p><h2 id="7-2-模型基本测试"><a href="#7-2-模型基本测试" class="headerlink" title="7.2 模型基本测试"></a>7.2 模型基本测试</h2><h3 id="（1）构建数据集生成器"><a href="#（1）构建数据集生成器" class="headerlink" title="（1）构建数据集生成器"></a>（1）构建数据集生成器</h3><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_generator</span>(<span class="params">V, batch_size, num_batch</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 该函数用于随机生成copy任务的数据</span></span><br><span class="line"><span class="string">        - V: 随机生成数字的最大值+1</span></span><br><span class="line"><span class="string">        - batch: 每次输送给模型更新一次参数的数据量</span></span><br><span class="line"><span class="string">        - num_batch: 一共输送num_batch次完成一轮</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_batch):</span><br><span class="line">        <span class="comment"># 在循环中使用randint方法随机生成[1, V)的整数, </span></span><br><span class="line">        <span class="comment"># 分布在(batch, 10)形状的矩阵中, </span></span><br><span class="line">        data = torch.randint(<span class="number">1</span>, V, size=(batch_size, <span class="number">10</span>))</span><br><span class="line">        <span class="comment"># 接着使数据矩阵中的第一列数字都为1, 这一列也就成为了起始标志列, </span></span><br><span class="line">        <span class="comment"># 当解码器进行第一次解码的时候, 会使用起始标志列作为输入.</span></span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># 因为是copy任务, 所有source与target是完全相同的, 且数据样本作用变量不需要求梯度</span></span><br><span class="line">        <span class="comment"># 因此requires_grad设置为False</span></span><br><span class="line">        src = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        tgt = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        <span class="comment"># 使用Batch对source和target进行对应批次的掩码张量生成, 最后使用yield返回</span></span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">V = <span class="number">11</span></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">num_batch = <span class="number">30</span></span><br><span class="line">res = data_generator(V, batch_size, num_batch)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;generator object data_generator at 0x00000245A1AD4BA0&gt;</span><br></pre></td></tr></table></figure><h3 id="（2）获得Transformer模型及优化器和损失函数"><a href="#（2）获得Transformer模型及优化器和损失函数" class="headerlink" title="（2）获得Transformer模型及优化器和损失函数"></a>（2）获得Transformer模型及优化器和损失函数</h3><h4 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h4><p><strong>损失函数计算</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleLossCompute</span>:</span><br><span class="line">    <span class="string">&quot;损失函数计算&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, generator, criterion</span>):</span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = criterion</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x, y, norm</span>):</span><br><span class="line">        x = self.generator(x)</span><br><span class="line">        sloss = self.criterion(</span><br><span class="line">                x.contiguous().view(-<span class="number">1</span>, x.size(-<span class="number">1</span>)), y.contiguous().view(-<span class="number">1</span>)</span><br><span class="line">            ) / norm</span><br><span class="line">        <span class="keyword">return</span> sloss.data * norm, sloss</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>标签平滑</strong></p><p>在训练过程中，我们使用的label平滑的值为\epsilon_{ls}=0.1 (cite)。这让模型不易理解，因为模型学得更加不确定，但提高了准确性和BLEU得分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LabelSmoothing</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implement label smoothing.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, padding_idx, smoothing=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(reduction=<span class="string">&quot;sum&quot;</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, target</span>):</span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>))</span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, true_dist.clone().detach())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">V = <span class="number">11</span></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">num_batch = <span class="number">30</span></span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 获得模型优化器</span></span><br><span class="line">optimizer = torch.optim.Adam(</span><br><span class="line">    model.parameters(), lr=<span class="number">0.5</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 使用LabelSmoothing获得标签平滑对象</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line"><span class="comment"># 使用SimpleLossCompute获得利用标签平滑结果的损失计算方法</span></span><br><span class="line">loss = SimpleLossCompute(model.generator, criterion)</span><br></pre></td></tr></table></figure><h4 id="标签平滑示例"><a href="#标签平滑示例" class="headerlink" title="标签平滑示例"></a>标签平滑示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用LabelSmoothing实例化一个crit对象.</span></span><br><span class="line"><span class="comment"># 第一个参数size代表目标数据的词汇总数, 也是模型最后一层得到张量的最后一维大小</span></span><br><span class="line"><span class="comment"># 这里是5说明目标词汇总数是5个. 第二个参数padding_idx表示要将那些tensor中的数字</span></span><br><span class="line"><span class="comment"># 替换成0, 一般padding_idx=0表示不进行替换. 第三个参数smoothing, 表示标签的平滑程度</span></span><br><span class="line"><span class="comment"># 如原来标签的表示值为1, 则平滑后它的值域变为[1-smoothing, 1+smoothing].</span></span><br><span class="line">crit = LabelSmoothing(size=<span class="number">5</span>, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定一个任意的模型最后输出预测结果和真实结果</span></span><br><span class="line">predict = Variable(torch.FloatTensor([[<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">                            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>], </span><br><span class="line">                            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标签的表示值是0，1，2</span></span><br><span class="line">target = Variable(torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将predict, target传入到对象中</span></span><br><span class="line">crit(predict, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制标签平滑图像</span></span><br><span class="line">plt.imshow(crit.true_dist)</span><br><span class="line">plt.waitforbuttonpress()</span><br></pre></td></tr></table></figure><p><img src="image/image_imustA3jpO.png" alt=""></p><p>标签平滑图像分析:</p><ul><li>我们目光集中在黄色小方块上, 它相对于横坐标横跨的值域就是标签平滑后的正向平滑值域, 我们可以看到大致是从0.5到2.5.</li><li>它相对于纵坐标横跨的值域就是标签平滑后的负向平滑值域, 我们可以看到大致是从-0.5到1.5, 总的值域空间由原来的[0, 2]变成了[-0.5, 2.5].</li></ul><h3 id="（3）运行模型进行训练和评估"><a href="#（3）运行模型进行训练和评估" class="headerlink" title="（3）运行模型进行训练和评估"></a>（3）运行模型进行训练和评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">V = <span class="number">11</span></span><br><span class="line">batch_size = <span class="number">80</span></span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 获得模型优化器</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), </span><br><span class="line">                             lr=<span class="number">0.5</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>)</span><br><span class="line"><span class="comment"># 使用LabelSmoothing获得标签平滑对象</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line"><span class="comment"># 使用SimpleLossCompute获得利用标签平滑结果的损失计算方法</span></span><br><span class="line">loss = SimpleLossCompute(model.generator, criterion)</span><br><span class="line">lr_scheduler = LambdaLR(optimizer=optimizer,</span><br><span class="line">    lr_lambda=<span class="keyword">lambda</span> step: rate(step, model_size=model.src_embed[<span class="number">0</span>].d_model, </span><br><span class="line">                                factor=<span class="number">1.0</span>, warmup=<span class="number">400</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">     <span class="comment"># 模型使用训练模式, 所有参数将被更新</span></span><br><span class="line">    model.train()</span><br><span class="line">    run_epoch(data_generator(V, batch_size, <span class="number">20</span>), model, loss,</span><br><span class="line">        optimizer, lr_scheduler, mode=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 模型使用评估模式, 参数将不会变化 </span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    run_epoch(data_generator(V, batch_size, <span class="number">5</span>), model, loss,</span><br><span class="line">        DummyOptimizer(), DummyScheduler(), mode=<span class="string">&quot;eval&quot;</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="（4）使用模型进行贪婪解码"><a href="#（4）使用模型进行贪婪解码" class="headerlink" title="（4）使用模型进行贪婪解码"></a>（4）使用模型进行贪婪解码</h3><p>为简单起见，此代码使用贪婪解码预测翻译。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decode</span>(<span class="params">model, src, src_mask, max_len, start_symbol</span>):</span><br><span class="line">    memory = model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_len - <span class="number">1</span>):</span><br><span class="line">        out = model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = model.generator(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> ys</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">V = <span class="number">11</span></span><br><span class="line">batch_size = <span class="number">80</span></span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>, d_model=<span class="number">256</span>, d_ff=<span class="number">512</span>, h=<span class="number">4</span>, dropout=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 获得模型优化器</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), </span><br><span class="line">                             lr=<span class="number">0.5</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>)</span><br><span class="line"><span class="comment"># 使用LabelSmoothing获得标签平滑对象</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line"><span class="comment"># 使用SimpleLossCompute获得利用标签平滑结果的损失计算方法</span></span><br><span class="line">loss = SimpleLossCompute(model.generator, criterion)</span><br><span class="line">lr_scheduler = LambdaLR(optimizer=optimizer,</span><br><span class="line">    lr_lambda=<span class="keyword">lambda</span> step: rate(step, model_size=model.src_embed[<span class="number">0</span>].d_model, </span><br><span class="line">                                factor=<span class="number">1.0</span>, warmup=<span class="number">400</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">     <span class="comment"># 模型使用训练模式, 所有参数将被更新</span></span><br><span class="line">    model.train()</span><br><span class="line">    run_epoch(data_generator(V, batch_size, <span class="number">20</span>), model, loss,</span><br><span class="line">        optimizer, lr_scheduler, mode=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 模型使用评估模式, 参数将不会变化 </span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    run_epoch(data_generator(V, batch_size, <span class="number">5</span>), model, loss,</span><br><span class="line">        DummyOptimizer(), DummyScheduler(), mode=<span class="string">&quot;eval&quot;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">src = torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">max_len = src.shape[<span class="number">1</span>]</span><br><span class="line">src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, max_len)</span><br><span class="line">result = greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])</span><br></pre></td></tr></table></figure><h1 id="8-Transformer常见问题"><a href="#8-Transformer常见问题" class="headerlink" title="8.Transformer常见问题"></a>8.Transformer常见问题</h1><h2 id="8-1-Transformer和RNN"><a href="#8-1-Transformer和RNN" class="headerlink" title="8.1 Transformer和RNN"></a>8.1 Transformer和RNN</h2><p>最简单情况：没有残差连接、没有 layernorm、 attention 单头、没有投影。看和 RNN 区别</p><ul><li>attention 对输入做一个加权和，加权和 进入 point-wise MLP。（画了多个红色方块 MLP， 是一个权重相同的 MLP）</li><li>point-wise MLP 对 每个输入的点 做计算，得到输出。</li><li>attention 作用：把整个序列里面的信息抓取出来，做一次汇聚 aggregation</li></ul><p><img src="image/image_MkcmTq99sT.png" alt=""></p><p>RNN 跟 transformer <strong>异：如何传递序列的信</strong>息</p><p>RNN 是把上一个时刻的信息输出传入下一个时候做输入。Transformer 通过一个 attention 层，去全局的拿到整个序列里面信息，再用 MLP 做语义的转换。</p><p>RNN 跟 transformer <strong>同：语义空间的转换 + 关注点</strong></p><p>用一个线性层 or 一个 MLP 来做语义空间的转换。</p><p><strong>关注点</strong>：怎么有效的去使用序列的信息。</p><h2 id="8-2-一些细节"><a href="#8-2-一些细节" class="headerlink" title="8.2 一些细节"></a>8.2 一些细节</h2><p><strong>Transformer为何使用多头注意力机制？</strong>（为什么不使用一个头）</p><ul><li>多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。可以类比CNN中同时使用<strong>多个滤波器</strong>的作用，直观上讲，多头的注意力<strong>有助于网络捕捉到更丰富的特征/信息。</strong></li></ul><p><strong>Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</strong> （注意和第一个问题的区别）</p><ul><li>使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。</li><li>同时，由softmax函数的性质决定，实质做的是一个soft版本的arg max操作，得到的向量接近一个one-hot向量（接近程度根据这组数的数量级有所不同）。如果令Q=K，那么得到的模型大概率会得到一个类似单位矩阵的attention矩阵，<strong>这样self-attention就退化成一个point-wise线性映射</strong>。这样至少是违反了设计的初衷。</li></ul><p><strong>Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</strong></p><ul><li>K和Q的点乘是为了得到一个attention score 矩阵，用来对V进行提纯。K和Q使用了不同的W_k, W_Q来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。</li><li>为了计算更快。矩阵加法在加法这一块的计算量确实简单，但是作为一个整体计算attention的时候相当于一个隐层，整体计算量和点积相似。在效果上来说，从实验分析，两者的效果和dk相关，dk越大，加法的效果越显著。</li></ul><p><strong>为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根）</strong>，并使用公式推导进行讲解</p><ul><li>这取决于softmax函数的特性，如果softmax内计算的数数量级太大，会输出近似one-hot编码的形式，导致梯度消失的问题，所以需要scale</li><li>那么至于为什么需要用维度开根号，假设向量q，k满足各分量独立同分布，均值为0，方差为1，那么qk点积均值为0，方差为dk，从统计学计算，若果让qk点积的方差控制在1，需要将其除以dk的平方根，是的softmax更加平滑</li></ul><p><strong>在计算attention score的时候如何对padding做mask操作？</strong></p><ul><li>padding位置置为负无穷(一般来说-1000就可以)，再对attention score进行相加。对于这一点，涉及到batch_size之类的，具体的大家可以看一下实现的源代码，位置在这里：<a href="https://link.zhihu.com/?target=https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720" title="https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720">https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720</a></li><li>padding位置置为负无穷而不是0，是因为后续在softmax时，$e^0=1$，不是0，计算会出现错误；而$e^{-\infty} = 0$，所以取负无穷</li></ul><p><strong>为什么在进行多头注意力的时候需要对每个head进行降维？</strong>（可以参考上面一个问题）</p><ul><li>将原有的<strong>高维空间转化为多个低维空间</strong>并再最后进行拼接，形成同样维度的输出，借此丰富特性信息<ul><li>基本结构：Embedding + Position Embedding，Self-Attention，Add + LN，FN，Add + LN</li></ul></li></ul><p><strong>为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？</strong></p><ul><li>embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</li></ul><p><strong>简单介绍一下Transformer的位置编码？有什么意义和优缺点？</strong></p><ul><li>因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden embedding都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个token的位置信息，transformer使用了固定的positional encoding来表示token在句子中的绝对位置信息。</li></ul><p><strong>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</strong>（参考上一题）</p><ul><li>相对位置编码（RPE）1.在计算attention score和weighted value时各加入一个可训练的表示相对位置的参数。2.在生成多头注意力时，把对key来说将绝对位置转换为相对query的位置3.复数域函数，已知一个词在某个位置的词向量表示，可以计算出它在任何位置的词向量表示。前两个方法是词向量+位置编码，属于亡羊补牢，复数域是生成词向量的时候即生成对应的位置信息。</li></ul><p><strong>简单讲一下Transformer中的残差结构以及意义。</strong></p><ul><li>就是ResNet的优点，解决梯度消失</li></ul><p><strong>为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</strong></p><ul><li>LN：针对每个样本序列进行Norm，没有样本间的依赖。对一个序列的不同特征维度进行Norm</li><li>CV使用BN是认为channel维度的信息对cv方面有重要意义，如果对channel维度也归一化会造成不同通道信息一定的损失。而同理nlp领域认为句子长度不一致，并且各个batch的信息没什么关系，因此只考虑句子内信息的归一化，也就是LN。</li></ul><p><strong>简答讲一下BatchNorm技术，以及它的优缺点。</strong></p><ul><li>优点：<ul><li>第一个就是可以解决内部协变量偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN缓解了这个问题。当然后来也有论文证明BN有作用和这个没关系，而是可以使<strong>损失平面更加的平滑</strong>，从而加快的收敛速度。</li><li>第二个优点就是缓解了<strong>梯度饱和问题</strong>（如果使用sigmoid激活函数的话），加快收敛。</li></ul></li><li>缺点：<ul><li>第一个，batch_size较小的时候，效果差。这一点很容易理解。BN的过程，使用 整个batch中样本的均值和方差来模拟全部数据的均值和方差，在batch_size 较小的时候，效果肯定不好。</li><li>第二个缺点就是 BN 在RNN中效果比较差。</li></ul></li></ul><p><strong>简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</strong></p><ul><li>ReLU</li></ul><script type="math/tex; mode=display">FFN(x)=max(0,~ xW_1+b_1)W_2+b_2</script><p><strong>Encoder端和Decoder端是如何进行交互的？</strong>（在这里可以问一下关于seq2seq的attention知识）</p><ul><li>Cross Self-Attention，Decoder提供Q，Encoder提供K，V</li></ul><p><strong>Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？</strong>（为什么需要decoder自注意力需要进行 sequence mask)</p><ul><li>让输入序列只看到过去的信息，不能让他看到未来的信息</li></ul><p><strong>Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？</strong></p><ul><li>Encoder侧：模块之间是串行的，一个模块计算的结果做为下一个模块的输入，互相之前有依赖关系。从每个模块的角度来说，注意力层和前馈神经层这两个子模块单独来看都是可以并行的，不同单词之间是没有依赖关系的。</li><li>Decode引入sequence mask就是为了并行化训练，Decoder推理过程没有并行，只能一个一个的解码，很类似于RNN，这个时刻的输入依赖于上一个时刻的输出。</li></ul><p><strong>简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？</strong></p><ul><li>传统词表示方法无法很好的处理未知或罕见的词汇（OOV问题），传统词tokenization方法不利于模型学习词缀之间的关系”</li><li>BPE（字节对编码）或二元编码是一种简单的数据压缩形式，其中最常见的一对连续字节数据被替换为该数据中不存在的字节。后期使用时需要一个替换表来重建原始数据。</li><li>优点：可以有效地平衡词汇表大小和步数（编码句子所需的token次数）。</li><li>缺点：基于贪婪和确定的符号替换，不能提供带概率的多个分片结果。</li></ul><p><strong>Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</strong></p><ul><li>Dropout测试的时候记得对输入整体呈上dropout的比率</li></ul><p><strong>引申一个关于bert问题，bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</strong></p><ul><li>BERT和transformer的目标不一致，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Annotated Transformer最新翻译</title>
      <link href="/llms/transformer/1.The_Annotated_Transformer%E6%9C%80%E6%96%B0%E7%BF%BB%E8%AF%91/"/>
      <url>/llms/transformer/1.The_Annotated_Transformer%E6%9C%80%E6%96%B0%E7%BF%BB%E8%AF%91/</url>
      
        <content type="html"><![CDATA[<p>The Annotated Transformer最新翻译</p><p>2023版最新<a href="http://nlp.seas.harvard.edu/annotated-transformer/#hardware-and-schedule">The Annotated Transformer</a>翻译</p><p>原文地址：<a href="http://nlp.seas.harvard.edu/annotated-transformer/#hardware-and-schedule" title="http://nlp.seas.harvard.edu/annotated-transformer/#hardware-and-schedule">http://nlp.seas.harvard.edu/annotated-transformer/#hardware-and-schedule</a></p><h1 id="0-Prelims"><a href="#0-Prelims" class="headerlink" title="0.Prelims"></a>0.Prelims</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> exists</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> log_softmax, pad</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> LambdaLR</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> altair <span class="keyword">as</span> alt</span><br><span class="line"><span class="keyword">from</span> torchtext.data.functional <span class="keyword">import</span> to_map_style_dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="keyword">import</span> torchtext.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> GPUtil</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.distributed <span class="keyword">import</span> DistributedSampler</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set to False to skip notebook execution (e.g. for debugging)</span></span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line">RUN_EXAMPLES = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>一些公用辅助函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">is_interactive_notebook</span>():</span><br><span class="line">    <span class="keyword">return</span> __name__ == <span class="string">&quot;__main__&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_example</span>(<span class="params">fn, args=[]</span>):</span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span> <span class="keyword">and</span> RUN_EXAMPLES:</span><br><span class="line">        <span class="keyword">return</span> fn(*args)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">execute_example</span>(<span class="params">fn, args=[]</span>):</span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span> <span class="keyword">and</span> RUN_EXAMPLES:</span><br><span class="line">        fn(*args)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DummyOptimizer</span>(torch.optim.Optimizer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.param_groups = [&#123;<span class="string">&quot;lr&quot;</span>: <span class="number">0</span>&#125;]</span><br><span class="line">        <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">zero_grad</span>(<span class="params">self, set_to_none=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DummyScheduler</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="literal">None</span></span><br></pre></td></tr></table></figure><h1 id="1-Background"><a href="#1-Background" class="headerlink" title="1.Background"></a>1.Background</h1><p>Extended Neural GPU、ByteNet和ConvS2S的出现是为了减少序列计算量，他们都使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的隐藏表示。在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数量随着位置之间的距离而增长，对于ConvS2S呈线性增长，对于ByteNet则呈对数增长。 这使得学习远距离位置之间的依赖关系变得更加困难。在Transformer中，这被减少到恒定数量的操作，尽管由于平均注意力加权位置而降低了效果，我们用多头注意力来抵消这种影响。</p><p>自注意力（Self-attention），有时称为内部注意力，是一种将单个序列的不同位置相关联以计算序列表示的注意力机制。 自注意力已成功用于各种任务，包括阅读理解、抽象摘要、文本蕴涵和学习与任务无关的句子表示。 端到端记忆网络基于循环注意力机制而不是序列对齐循环，并且已被证明在简单语言问答和语言建模任务上表现良好。</p><p>然而，据我们所知，Transformer 是第一个完全依赖自注意力来计算其输入和输出表示，而不是使用序列对齐 RNN 或卷积的模型。</p><h1 id="2-Model-Architecture"><a href="#2-Model-Architecture" class="headerlink" title="2.Model Architecture"></a>2.Model Architecture</h1><p>大部分神经序列转换模型都有一个<a href="https://arxiv.org/abs/1409.0473" title="编码器-解码器结构">编码器-解码器结构</a>。编码器把一个输入序列$(x_1, …, x_n)$映射到一个连续的表示$  z=(z_1, .., z_n) $中。解码器对z中的每个元素，生成输出序列$(y_1, …, y_m)$，一个时间步生成一个元素。在每一步中，模型都是<a href="https://arxiv.org/abs/1308.0850" title="自回归">自回归</a>的，在生成下一个结果时，会将先前生成的结构加入输入序列来一起预测。（自回归模型的特点）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many</span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Take in and process masked src and target sequences.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Define standard linear + softmax generation step.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Transformer的编码器和解码器都使用self-attention堆叠和point-wise、全连接层。如图1的左、右两边所示。</p><p><img src="image/image_jR19AIL-yM.png" alt=""></p><h2 id="2-1-Encoder-and-Decoder"><a href="#2-1-Encoder-and-Decoder" class="headerlink" title="2.1 Encoder and Decoder"></a>2.1 Encoder and Decoder</h2><p>编码器由N = 6 个完全相同的层组成。</p><h3 id="（1）Encoder"><a href="#（1）Encoder" class="headerlink" title="（1）Encoder"></a>（1）Encoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Core encoder is a stack of N layers&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>编码器的每个子层（Self Attention 层和 FFNN）都再接一个残差连接<a href="https://arxiv.org/abs/1512.03385" title="(cite)">(cite)</a>。然后是层标准化（layer-normalization） <a href="https://arxiv.org/abs/1607.06450" title="(cite)">(cite)</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Construct a layernorm module (See citation for details).&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>每个子层的输出是 $LayerNorm(x + Sublayer(x))$，其中  $ Sublayer(x)  $是子层本身实现的函数。 我们将dropout <a href="http://jmlr.org/papers/v15/srivastava14a.html" title="(cite)">(cite)</a> 应用于每个子层的输出，然后再将其添加到子层输入中并进行归一化。</p><p>为了便于进行残差连接，模型中的所有子层以及embedding层产生的输出的维度都为 $d_{model}=512$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;Apply residual connection to any sublayer with the same size.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>每一层都有两个子层。 第一层是一个multi-head self-attention机制（的层），第二层是一个简单的、全连接的前馈网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Encoder is made up of self-attn and feed forward (defined below)&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (left) for connections.&quot;</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><h3 id="（2）Decoder"><a href="#（2）Decoder" class="headerlink" title="（2）Decoder"></a>（2）Decoder</h3><p>解码器也是由N = 6 个完全相同的层组成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>除了每个decoder层中的两个子层之外，decoder还有第三个子层，该层对encoder的输出执行multi-head attention。（即encoder-decoder-attention层，q向量来自上一层的输入，k和v向量是encoder最后层的输出向量memory）与encoder类似，我们在每个子层再采用残差连接，然后进行层标准化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Decoder is made of self-attn, src-attn, and feed forward (defined below)&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (right) for connections.&quot;</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们还修改decoder层中的self-attention子层，以防止在当前位置关注到后面的位置。这种掩码结合将输出embedding偏移一个位置，确保对位置$i$的预测只依赖位置$i$之前的已知输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">subsequent_mask</span>(<span class="params">size</span>):</span><br><span class="line">    <span class="string">&quot;Mask out subsequent positions.&quot;</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=<span class="number">1</span>).<span class="built_in">type</span>(torch.uint8)</span><br><span class="line">    <span class="keyword">return</span> subsequent_mask == <span class="number">0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>下面的attention mask显示了每个tgt单词（行）允许查看（列）的位置。在训练时将当前单词的未来信息屏蔽掉，阻止此单词关注到后面的单词。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">example_mask</span>():</span><br><span class="line">    LS_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;Subsequent Mask&quot;</span>: subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>][x, y].flatten(),</span><br><span class="line">                    <span class="string">&quot;Window&quot;</span>: y,</span><br><span class="line">                    <span class="string">&quot;Masking&quot;</span>: x,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(LS_data)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .properties(height=<span class="number">250</span>, width=<span class="number">250</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            alt.X(<span class="string">&quot;Window:O&quot;</span>),</span><br><span class="line">            alt.Y(<span class="string">&quot;Masking:O&quot;</span>),</span><br><span class="line">            alt.Color(<span class="string">&quot;Subsequent Mask:Q&quot;</span>, scale=alt.Scale(scheme=<span class="string">&quot;viridis&quot;</span>)),</span><br><span class="line">        )</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">show_example(example_mask)</span><br></pre></td></tr></table></figure><p><img src="image/image_NYko3ybWur.png" alt=""></p><h3 id="（3）Attention"><a href="#（3）Attention" class="headerlink" title="（3）Attention"></a>（3）Attention</h3><p>Attention功能可以描述为将query和一组key-value对映射到输出，其中query、key、value和输出都是向量。输出为value的加权和，其中每个value的权重通过query与相应key的兼容函数来计算。</p><p>我们将particular attention称之为“缩放的点积Attention”(Scaled Dot-Product Attention”)。其输入为query、key(维度是$d_k$)以及values(维度是$d_v$)。我们计算query和所有key的点积，然后对每个除以 $\sqrt{d_k}$ , 最后用softmax函数获得value的权重。</p><p><img src="image/image_2KY-X_ewcz.png" alt=""></p><p>在实践中，我们同时计算一组query的attention函数，并将它们组合成一个矩阵$ Q  $。key和value也一起组成矩阵$K$和$V$。 我们计算的输出矩阵为：</p><script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = scores.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>两个最常用的attention函数是加法attention(cite)和点积（乘法）attention。除了缩放因子 $\frac{1}{\sqrt{d_k}}$ ，点积Attention跟我们的平时的算法一样。加法attention使用具有单个隐层的前馈网络计算兼容函数。虽然理论上点积attention和加法attention复杂度相似，但在实践中，点积attention可以使用高度优化的矩阵乘法来实现，因此点积attention计算更快、更节省空间。</p><p>当$d_k$的值比较小的时候，这两个机制的性能相近。当$d_k$比较大时，加法attention比不带缩放的点积attention性能好 (cite)。我们怀疑，对于很大的 $d_k$值, 点积大幅度增长，将softmax函数推向具有极小梯度的区域。(为了说明为什么点积变大，假设$q$和$k$是独立的随机变量，均值为0，方差为1。那么它们的点积 $q \cdot k = \sum_{i=1}^{d_k} q_ik_i$, 均值为0方差为$d_k$)。为了抵消这种影响，我们将点积缩小 $\frac{1}{\sqrt{d_k}}$ 倍。</p><blockquote><p><strong>为什么Attention中除以</strong>$\sqrt{d}$<strong> 这么重要？</strong><br>Attention的计算是在内积之后进行softmax，主要涉及的运算是$e^{q \cdot k}$，我们可以大致认为内积之后、softmax之前的数值在$-3\sqrt{d}$到$3\sqrt{d}$这个范围内，由于d通常都至少是64，所以$e^{3\sqrt{d}}$比较大而 $e^{-3\sqrt{d}}$比较小，因此经过softmax之后，Attention的分布非常接近一个one hot分布了，这带来严重的梯度消失问题，导致训练效果差。（例如y=softmax(x)在|x|较大时进入了饱和区，x继续变化y值也几乎不变，即饱和区梯度消失）<br>相应地，解决方法就有两个:<br>1.像NTK参数化那样，在内积之后除以 $\sqrt{d}$，使q⋅k的方差变为1，对应$e^3$,$e^{−3}$都不至于过大过小，这样softmax之后也不至于变成one hot而梯度消失了，这也是常规的Transformer如BERT里边的Self Attention的做法<br>2.另外就是不除以 $\sqrt{d}$，但是初始化q,k的全连接层的时候，其初始化方差要多除以一个d，这同样能使得使q⋅k的初始方差变为1，T5采用了这样的做法。</p></blockquote><p><img src="image/image_ym_MiTTSCN.png" alt=""></p><p>Multi-head attention允许模型共同关注来自不同位置的不同表示子空间的信息，如果只有一个attention head，它的平均值会削弱这个信息。</p><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O \\where ~ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</script><p>其中映射由权重矩阵完成：$ W^Q_i \in \mathbb{R}^{d_ \times d_k}<br>  $, $W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$和$W^O_i \in \mathbb{R}^{hd_v \times d_{\text{model}} }$。</p><p>在这项工作中，我们采用h=8个平行attention层或者叫head。对于这些head中的每一个，我们使用 $ d_k=d_v=d_{\text{model}}/h=64  $,由于每个head的维度减小，总计算成本与具有全部维度的单个head attention相似。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&quot;Take in model size and number of heads.&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;Implements Figure 2&quot;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span></span><br><span class="line">        query, key, value = [</span><br><span class="line">            lin(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">for</span> lin, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        x, self.attn = attention(</span><br><span class="line">            query, key, value, mask=mask, dropout=self.dropout</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear.</span></span><br><span class="line">        x = (</span><br><span class="line">            x.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            .contiguous()</span><br><span class="line">            .view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">del</span> query</span><br><span class="line">        <span class="keyword">del</span> key</span><br><span class="line">        <span class="keyword">del</span> value</span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure><h3 id="（4）模型中Attention的应用"><a href="#（4）模型中Attention的应用" class="headerlink" title="（4）模型中Attention的应用"></a>（4）模型中Attention的应用</h3><p>multi-head attention在Transformer中有三种不同的使用方式：</p><ul><li>在encoder-decoder attention层中，queries来自前面的decoder层，而keys和values来自encoder的输出。这使得decoder中的每个位置都能关注到输入序列中的所有位置。这是模仿序列到序列模型中典型的编码器—解码器的attention机制，例如 (<a href="https://arxiv.org/abs/1609.08144" title="cite">cite</a>).</li><li>encoder包含self-attention层。在self-attention层中，所有key，value和query来自同一个地方，即encoder中前一层的输出。在这种情况下，encoder中的每个位置都可以关注到encoder上一层的所有位置。</li><li>类似地，decoder中的self-attention层允许decoder中的每个位置都关注decoder层中当前位置之前的所有位置（包括当前位置）。 为了保持解码器的自回归特性，需要防止解码器中的信息向左流动。我们在缩放点积attention的内部，通过屏蔽softmax输入中所有的非法连接值（设置为$-\infty$）实现了这一点。</li></ul><h2 id="2-2-基于位置的前馈网络"><a href="#2-2-基于位置的前馈网络" class="headerlink" title="2.2 基于位置的前馈网络"></a>2.2 基于位置的前馈网络</h2><p>除了attention子层之外，我们的编码器和解码器中的每个层都包含一个全连接的前馈网络，该前馈网络分别且相同地应用于每个位置。网络包括两个线性变换，并在两个线性变换中间有一个ReLU激活函数。</p><script type="math/tex; mode=display">FFN(x)=max(0, xW_1+b_1)W_2+b_2</script><blockquote><p>Position就是序列中每个token，<code>Position-wise</code> 就是把MLP对每个token作用一次，且作用的是同一个MLP。</p></blockquote><p>尽管两层都是线性变换，但它们在层与层之间使用不同的参数。另一种描述方式是两个内核大小为1的卷积。 输入和输出的维度都是 $ d_{\text{model}}=512  $内层维度是$d_{ff}=2048$.（也就是第一层输入512维,输出2048维；第二层输入2048维，输出512维）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implements FFN equation.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(self.w_1(x).relu()))</span><br></pre></td></tr></table></figure><h2 id="2-3-Embeddings-and-Softmax"><a href="#2-3-Embeddings-and-Softmax" class="headerlink" title="2.3 Embeddings and Softmax"></a>2.3 Embeddings and Softmax</h2><p>与其他序列转换模型类似，我们使用学习到的embedding将输入token和输出token转换为$d_{\text{model}}$维的向量。我们还使用普通的线性变换和softmax函数将解码器输出转换为预测的下一个token的概率。 在我们的模型中，两个嵌入层之间和pre-softmax线性变换共享相同的权重矩阵，类似于(cite)。在embedding层中，我们将这些权重乘以$ \sqrt{d_{\text{model}}}  $</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><h2 id="2-4-位置编码"><a href="#2-4-位置编码" class="headerlink" title="2.4 位置编码"></a>2.4 位置编码</h2><p>由于我们的模型不包含循环和卷积，<strong>为了让模型利用序列的顺序，我们必须加入一些序列中token的相对或者绝对位置的信息</strong>。为此，我们将“位置编码”添加到编码器和解码器堆栈底部的输入embeddinng中。位置编码和embedding的维度相同，也是$d_{\text{model}}$, 所以这两个向量可以相加。有多种位置编码可以选择，例如通过学习得到的位置编码和固定的位置编码 (<a href="https://arxiv.org/pdf/1705.03122.pdf" title="cite">cite</a>)。</p><p>在这项工作中，我们使用不同频率的正弦和余弦函数：</p><script type="math/tex; mode=display">PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})</script><script type="math/tex; mode=display">PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})</script><p>其中$pos$是位置，$i$ 是维度。也就是说，位置编码的每个维度对应于一个正弦曲线。 这些波长形成一个从 $2\pi$ 到 $10000 \cdot 2\pi$的集合级数。我们选择这个函数是因为我们假设它会让模型很容易学习对相对位置的关注，因为对任意确定的偏移k , $PE_{pos+k}$可以表示为 $PE_{pos}$的线性函数。</p><p>此外，我们会将编码器和解码器堆栈中的embedding和位置编码的和再加一个dropout。对于基本模型，我们使用的dropout比例是$P_{drop}=0.1$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implement the PE function.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * </span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;pe&quot;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + self.pe[:, : x.size(<span class="number">1</span>)].requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><p>位置编码将根据位置添加正弦波。波的频率和偏移对于每个维度都是不同的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">example_positional</span>():</span><br><span class="line">    pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">    y = pe.forward(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))</span><br><span class="line"></span><br><span class="line">    data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;embedding&quot;</span>: y[<span class="number">0</span>, :, dim],</span><br><span class="line">                    <span class="string">&quot;dimension&quot;</span>: dim,</span><br><span class="line">                    <span class="string">&quot;position&quot;</span>: <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">100</span>)),</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> dim <span class="keyword">in</span> [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">800</span>)</span><br><span class="line">        .encode(x=<span class="string">&quot;position&quot;</span>, y=<span class="string">&quot;embedding&quot;</span>, color=<span class="string">&quot;dimension:N&quot;</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">show_example(example_positional)</span><br></pre></td></tr></table></figure><p><img src="image/image_52cSZXhK8o.png" alt=""></p><p>我们还尝试使用学习到的位置嵌入（<a href="https://arxiv.org/pdf/1705.03122.pdf" title="cite">cite</a>）来代替，发现这两个版本产生了几乎相同的结果。 我们选择正弦版本是因为它可以让模型推断出比训练期间遇到的序列长度更长的序列长度。</p><h2 id="2-5-完整模型"><a href="#2-5-完整模型" class="headerlink" title="2.5 完整模型"></a>2.5 完整模型</h2><p>在这里，我们定义了一个从超参数到完整模型的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_model</span>(<span class="params"></span></span><br><span class="line"><span class="params">    src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;Helper: Construct a model from hyperparameters.&quot;</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This was important from their code.</span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h2 id="2-6-Inference"><a href="#2-6-Inference" class="headerlink" title="2.6 Inference"></a>2.6 Inference</h2><p>在这里，我们用生成模型的预测。 我们尝试使用我们的transformer 来记住输入。 正如您将看到的那样，由于模型尚未训练，输出是随机生成的。 在下一个教程中，我们将构建训练函数并尝试训练我们的模型来记住从 1 到 10 的数字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inference_test</span>():</span><br><span class="line">    test_model = make_model(<span class="number">11</span>, <span class="number">11</span>, <span class="number">2</span>)</span><br><span class="line">    test_model.<span class="built_in">eval</span>()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    memory = test_model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">        out = test_model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = test_model.generator(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.empty(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Example Untrained Model Prediction:&quot;</span>, ys)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_tests</span>():</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        inference_test()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(run_tests)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Example Untrained Model Prediction: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[0, 3, 4, 4, 4, 4, 4, 4, 4, 4]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0, 10, 10, 10,  3,  2,  5,  7,  9,  6]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  4,  3,  6, 10, 10,  2,  6,  2,  2]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  9,  0,  1,  5, 10,  1,  5, 10,  6]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  1,  5,  1, 10,  1, 10, 10, 10, 10]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  1, 10,  9,  9,  9,  9,  9,  1,  5]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  3,  1,  5, 10, 10, 10, 10, 10, 10]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  3,  5, 10,  5, 10,  4,  2,  4,  2]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[0, 5, 6, 2, 5, 6, 2, 6, 2, 2]])</span><br></pre></td></tr></table></figure><h1 id="3-Training"><a href="#3-Training" class="headerlink" title="3. Training"></a>3. Training</h1><p>本节描述了我们模型的训练机制。</p><blockquote><p>我们在这快速地介绍一些工具，这些工具用于训练一个标准的encoder-decoder模型。首先，我们定义一个批处理对象，其中包含用于训练的 src 和目标句子，以及构建掩码。</p></blockquote><h2 id="3-1-Batches-and-Masking"><a href="#3-1-Batches-and-Masking" class="headerlink" title="3.1 Batches and Masking"></a>3.1 Batches and Masking</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Batch</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Object for holding a batch of data with mask during training.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, src, tgt=<span class="literal">None</span>, pad=<span class="number">2</span></span>):  <span class="comment"># 2 = &lt;blank&gt;</span></span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(-<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> tgt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.tgt = tgt[:, :-<span class="number">1</span>]</span><br><span class="line">            self.tgt_y = tgt[:, <span class="number">1</span>:]</span><br><span class="line">            self.tgt_mask = self.make_std_mask(self.tgt, pad)</span><br><span class="line">            self.ntokens = (self.tgt_y != pad).data.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_std_mask</span>(<span class="params">tgt, pad</span>):</span><br><span class="line">        <span class="string">&quot;Create a mask to hide padding and future words.&quot;</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(-<span class="number">2</span>)</span><br><span class="line">        tgt_mask = tgt_mask &amp; subsequent_mask(tgt.size(-<span class="number">1</span>)).type_as(</span><br><span class="line">            tgt_mask.data</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure><blockquote><p>接下来我们创建一个通用的训练和评估函数来跟踪损失。我们传入一个通用的损失函数，也用它来进行参数更新。</p></blockquote><h2 id="3-2-Training-Loop"><a href="#3-2-Training-Loop" class="headerlink" title="3.2 Training Loop"></a>3.2 Training Loop</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TrainState</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Track number of steps, examples, and tokens processed&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    step: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># Steps in the current epoch</span></span><br><span class="line">    accum_step: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># Number of gradient accumulation steps</span></span><br><span class="line">    samples: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># total # of examples used</span></span><br><span class="line">    tokens: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># total # of tokens processed</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_epoch</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data_iter,</span></span><br><span class="line"><span class="params">    model,</span></span><br><span class="line"><span class="params">    loss_compute,</span></span><br><span class="line"><span class="params">    optimizer,</span></span><br><span class="line"><span class="params">    scheduler,</span></span><br><span class="line"><span class="params">    mode=<span class="string">&quot;train&quot;</span>,</span></span><br><span class="line"><span class="params">    accum_iter=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">    train_state=TrainState(<span class="params"></span>),</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Train a single epoch&quot;&quot;&quot;</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    total_tokens = <span class="number">0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    tokens = <span class="number">0</span></span><br><span class="line">    n_accum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_iter):</span><br><span class="line">        out = model.forward(</span><br><span class="line">            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask</span><br><span class="line">        )</span><br><span class="line">        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)</span><br><span class="line">        <span class="comment"># loss_node = loss_node / accum_iter</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span> <span class="keyword">or</span> mode == <span class="string">&quot;train+log&quot;</span>:</span><br><span class="line">            loss_node.backward()</span><br><span class="line">            train_state.step += <span class="number">1</span></span><br><span class="line">            train_state.samples += batch.src.shape[<span class="number">0</span>]</span><br><span class="line">            train_state.tokens += batch.ntokens</span><br><span class="line">            <span class="keyword">if</span> i % accum_iter == <span class="number">0</span>:</span><br><span class="line">                optimizer.step()</span><br><span class="line">                optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line">                n_accum += <span class="number">1</span></span><br><span class="line">                train_state.accum_step += <span class="number">1</span></span><br><span class="line">            scheduler.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss</span><br><span class="line">        total_tokens += batch.ntokens</span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">40</span> == <span class="number">1</span> <span class="keyword">and</span> (mode == <span class="string">&quot;train&quot;</span> <span class="keyword">or</span> mode == <span class="string">&quot;train+log&quot;</span>):</span><br><span class="line">            lr = optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">            elapsed = time.time() - start</span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                (</span><br><span class="line">                    <span class="string">&quot;Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f &quot;</span></span><br><span class="line">                    + <span class="string">&quot;| Tokens / Sec: %7.1f | Learning Rate: %6.1e&quot;</span></span><br><span class="line">                )</span><br><span class="line">                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)</span><br><span class="line">            )</span><br><span class="line">            start = time.time()</span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">        <span class="keyword">del</span> loss</span><br><span class="line">        <span class="keyword">del</span> loss_node</span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens, train_state</span><br></pre></td></tr></table></figure><h2 id="3-3-Training-Data-and-Batching"><a href="#3-3-Training-Data-and-Batching" class="headerlink" title="3.3 Training Data and Batching"></a>3.3 Training Data and Batching</h2><p>我们在包含约450万个句子对的标准WMT 2014英语-德语数据集上进行了训练。这些句子使用字节对编码进行编码，源语句和目标语句共享大约37000个token的词汇表。对于英语-法语翻译，我们使用了明显更大的WMT 2014英语-法语数据集，该数据集由 3600 万个句子组成，并将token拆分为32000个word-piece词表。</p><p>每个训练批次包含一组句子对，句子对按相近序列长度来分批处理。每个训练批次的句子对包含大约25000个源语言的tokens和25000个目标语言的tokens。</p><h2 id="3-4-Hardware-and-Schedule"><a href="#3-4-Hardware-and-Schedule" class="headerlink" title="3.4 Hardware and Schedule"></a>3.4 Hardware and Schedule</h2><p>我们在一台配备8个 NVIDIA P100 GPU 的机器上训练我们的模型。使用论文中描述的超参数的base models，每个训练step大约需要0.4秒。我们对base models进行了总共10万steps或12小时的训练。而对于big models，每个step训练时间为1.0秒，big models训练了30万steps（3.5 天）。</p><h2 id="3-5-Optimizer"><a href="#3-5-Optimizer" class="headerlink" title="3.5 Optimizer"></a>3.5 Optimizer</h2><p>我们使用Adam优化器，其中$\beta_1=0.9$, $\beta_2=0.98$，并且$ϵ=10^{-9}$。我们根据以下公式在训练过程中改变学习率：</p><script type="math/tex; mode=display">lrate = d^{-0.5}_{model}\cdot min(step\_num^{-0.5}, stem\_num \cdot warmup\_steps^{-1.5})</script><p>这对应于在第一次$warmup_steps$步中线性地增加学习速率，并且随后将其与步数的平方根成比例地减小。我们使用 $warmup_steps=4000$。</p><blockquote><p>注意：这部分非常重要。需要使用此模型设置进行训练。</p><p>该模型针对不同模型大小和优化超参数的曲线示例。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rate</span>(<span class="params">step, model_size, factor, warmup</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    we have to default the step to 1 for LambdaLR function</span></span><br><span class="line"><span class="string">    to avoid zero raising to negative power.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">        step = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> factor * (</span><br><span class="line">        model_size ** (-<span class="number">0.5</span>) * <span class="built_in">min</span>(step ** (-<span class="number">0.5</span>), step * warmup ** (-<span class="number">1.5</span>))</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">example_learning_schedule</span>():</span><br><span class="line">    opts = [</span><br><span class="line">        [<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>],  <span class="comment"># example 1</span></span><br><span class="line">        [<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>],  <span class="comment"># example 2</span></span><br><span class="line">        [<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>],  <span class="comment"># example 3</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    dummy_model = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    learning_rates = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># we have 3 examples in opts list.</span></span><br><span class="line">    <span class="keyword">for</span> idx, example <span class="keyword">in</span> <span class="built_in">enumerate</span>(opts):</span><br><span class="line">        <span class="comment"># run 20000 epoch for each example</span></span><br><span class="line">        optimizer = torch.optim.Adam(</span><br><span class="line">            dummy_model.parameters(), lr=<span class="number">1</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">        )</span><br><span class="line">        lr_scheduler = LambdaLR(</span><br><span class="line">            optimizer=optimizer, lr_lambda=<span class="keyword">lambda</span> step: rate(step, *example)</span><br><span class="line">        )</span><br><span class="line">        tmp = []</span><br><span class="line">        <span class="comment"># take 20K dummy training steps, save the learning rate at each step</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line">            tmp.append(optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>])</span><br><span class="line">            optimizer.step()</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line">        learning_rates.append(tmp)</span><br><span class="line"></span><br><span class="line">    learning_rates = torch.tensor(learning_rates)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Enable altair to handle more than 5000 rows</span></span><br><span class="line">    alt.data_transformers.disable_max_rows()</span><br><span class="line"></span><br><span class="line">    opts_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;Learning Rate&quot;</span>: learning_rates[warmup_idx, :],</span><br><span class="line">                    <span class="string">&quot;model_size:warmup&quot;</span>: [<span class="string">&quot;512:4000&quot;</span>, <span class="string">&quot;512:8000&quot;</span>, <span class="string">&quot;256:4000&quot;</span>][</span><br><span class="line">                        warmup_idx</span><br><span class="line">                    ],</span><br><span class="line">                    <span class="string">&quot;step&quot;</span>: <span class="built_in">range</span>(<span class="number">20000</span>),</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> warmup_idx <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(opts_data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">600</span>)</span><br><span class="line">        .encode(x=<span class="string">&quot;step&quot;</span>, y=<span class="string">&quot;Learning Rate&quot;</span>, color=<span class="string">&quot;model_size:warmup:N&quot;</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">example_learning_schedule()</span><br></pre></td></tr></table></figure><p><img src="image/image_utXlLNZTuy.png" alt=""></p><h2 id="3-6-Regularization"><a href="#3-6-Regularization" class="headerlink" title="3.6 Regularization"></a>3.6 Regularization</h2><h4 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h4><p>在训练过程中，我们使用的label平滑的值为$\epsilon_{ls}=0.1$ (<a href="https://arxiv.org/abs/1512.00567" title="cite">cite</a>)。这让模型不易理解，因为模型学得更加不确定，但提高了准确性和BLEU得分。</p><blockquote><p>我们使用KL div损失实现标签平滑。我们没有使用one-hot独热分布，而是创建了一个分布，we create a distribution that has confidence of the correct word and the rest of the smoothing mass distributed throughout the vocabulary。该分布具有对正确单词的“置信度”和分布在整个词汇表中的“平滑”质量的其余部分。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LabelSmoothing</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implement label smoothing.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, padding_idx, smoothing=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(reduction=<span class="string">&quot;sum&quot;</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, target</span>):</span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>))</span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, true_dist.clone().detach())</span><br></pre></td></tr></table></figure><blockquote><p>Here we can see an example of how the mass is distributed to the words based on confidence. 在这里，我们可以看到一个示例，说明质量如何根据置信度分配给单词。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of label smoothing.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_label_smoothing</span>():</span><br><span class="line">    crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">    predict = torch.FloatTensor(</span><br><span class="line">        [</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">    crit(x=predict.log(), target=torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>]))</span><br><span class="line">    LS_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;target distribution&quot;</span>: crit.true_dist[x, y].flatten(),</span><br><span class="line">                    <span class="string">&quot;columns&quot;</span>: y,</span><br><span class="line">                    <span class="string">&quot;rows&quot;</span>: x,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(LS_data)</span><br><span class="line">        .mark_rect(color=<span class="string">&quot;Blue&quot;</span>, opacity=<span class="number">1</span>)</span><br><span class="line">        .properties(height=<span class="number">200</span>, width=<span class="number">200</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            alt.X(<span class="string">&quot;columns:O&quot;</span>, title=<span class="literal">None</span>),</span><br><span class="line">            alt.Y(<span class="string">&quot;rows:O&quot;</span>, title=<span class="literal">None</span>),</span><br><span class="line">            alt.Color(</span><br><span class="line">                <span class="string">&quot;target distribution:Q&quot;</span>, scale=alt.Scale(scheme=<span class="string">&quot;viridis&quot;</span>)</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">show_example(example_label_smoothing)</span><br></pre></td></tr></table></figure><p><img src="image/image_XUPyWlyIWk.png" alt=""></p><blockquote><p>Label smoothing actually starts to penalize the model if it gets very confident about a given choice. 如果模型对给定的选择非常有信心，标签平滑实际上会开始惩罚模型。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, crit</span>):</span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([[<span class="number">0</span>, x / d, <span class="number">1</span> / d, <span class="number">1</span> / d, <span class="number">1</span> / d]])</span><br><span class="line">    <span class="keyword">return</span> crit(predict.log(), torch.LongTensor([<span class="number">1</span>])).data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">penalization_visualization</span>():</span><br><span class="line">    crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line">    loss_data = pd.DataFrame(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;Loss&quot;</span>: [loss(x, crit) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">100</span>)],</span><br><span class="line">            <span class="string">&quot;Steps&quot;</span>: <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">99</span>)),</span><br><span class="line">        &#125;</span><br><span class="line">    ).astype(<span class="string">&quot;float&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(loss_data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">350</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            x=<span class="string">&quot;Steps&quot;</span>,</span><br><span class="line">            y=<span class="string">&quot;Loss&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">show_example(penalization_visualization)</span><br></pre></td></tr></table></figure><p><img src="image/image_T8iuBJzWQQ.png" alt=""></p><h1 id="4-A-First-Example"><a href="#4-A-First-Example" class="headerlink" title="4.A First Example"></a>4.A First Example</h1><p>我们可以从尝试一个简单的copy任务开始。给定来自小词汇表的一组随机输入符号symbols，目标是生成这些相同的符号。</p><h2 id="4-1-Synthetic-Data"><a href="#4-1-Synthetic-Data" class="headerlink" title="4.1 Synthetic Data"></a>4.1 Synthetic Data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_gen</span>(<span class="params">V, batch_size, nbatches</span>):</span><br><span class="line">    <span class="string">&quot;Generate random data for a src-tgt copy task.&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nbatches):</span><br><span class="line">        data = torch.randint(<span class="number">1</span>, V, size=(batch_size, <span class="number">10</span>))</span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        src = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        tgt = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="4-2-Loss-Computation"><a href="#4-2-Loss-Computation" class="headerlink" title="4.2 Loss Computation"></a>4.2 Loss Computation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleLossCompute</span>:</span><br><span class="line">    <span class="string">&quot;A simple loss compute and train function.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, generator, criterion</span>):</span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = criterion</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x, y, norm</span>):</span><br><span class="line">        x = self.generator(x)</span><br><span class="line">        sloss = (</span><br><span class="line">            self.criterion(</span><br><span class="line">                x.contiguous().view(-<span class="number">1</span>, x.size(-<span class="number">1</span>)), y.contiguous().view(-<span class="number">1</span>)</span><br><span class="line">            )</span><br><span class="line">            / norm</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> sloss.data * norm, sloss</span><br></pre></td></tr></table></figure><h2 id="4-3-Greedy-Decoding"><a href="#4-3-Greedy-Decoding" class="headerlink" title="4.3 Greedy Decoding"></a>4.3 Greedy Decoding</h2><p>为简单起见，此代码使用贪婪解码预测翻译。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decode</span>(<span class="params">model, src, src_mask, max_len, start_symbol</span>):</span><br><span class="line">    memory = model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_len - <span class="number">1</span>):</span><br><span class="line">        out = model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = model.generator(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> ys</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the simple copy task.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_simple_model</span>():</span><br><span class="line">    V = <span class="number">11</span></span><br><span class="line">    criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line">    model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        model.parameters(), lr=<span class="number">0.5</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = LambdaLR(</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        lr_lambda=<span class="keyword">lambda</span> step: rate(</span><br><span class="line">            step, model_size=model.src_embed[<span class="number">0</span>].d_model, factor=<span class="number">1.0</span>, warmup=<span class="number">400</span></span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">80</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        model.train()</span><br><span class="line">        run_epoch(</span><br><span class="line">            data_gen(V, batch_size, <span class="number">20</span>),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(model.generator, criterion),</span><br><span class="line">            optimizer,</span><br><span class="line">            lr_scheduler,</span><br><span class="line">            mode=<span class="string">&quot;train&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        run_epoch(</span><br><span class="line">            data_gen(V, batch_size, <span class="number">5</span>),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(model.generator, criterion),</span><br><span class="line">            DummyOptimizer(),</span><br><span class="line">            DummyScheduler(),</span><br><span class="line">            mode=<span class="string">&quot;eval&quot;</span>,</span><br><span class="line">        )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">    max_len = src.shape[<span class="number">1</span>]</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, max_len)</span><br><span class="line">    <span class="built_in">print</span>(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># execute_example(example_simple_model)</span></span><br></pre></td></tr></table></figure><h1 id="5-A-Real-World-Example"><a href="#5-A-Real-World-Example" class="headerlink" title="5.A Real World Example"></a>5.A Real World Example</h1><p>现在我们考虑一个使用IWSLT德语-英语翻译任务的真实示例。这个任务比论文中考虑的WMT任务小得多，但这个任务也能说明整个（翻译）系统。我们还展示了如何使用多GPU处理，使任务能真正快速地训练。</p><h2 id="5-1-Data-Loading"><a href="#5-1-Data-Loading" class="headerlink" title="5.1 Data Loading"></a>5.1 Data Loading</h2><p>我们将使用torchtext和spacy加载数据集来进行tokenization。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load spacy tokenizer models, download them if they haven&#x27;t been</span></span><br><span class="line"><span class="comment"># downloaded already</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_tokenizers</span>():</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spacy_de = spacy.load(<span class="string">&quot;de_core_news_sm&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        os.system(<span class="string">&quot;python -m spacy download de_core_news_sm&quot;</span>)</span><br><span class="line">        spacy_de = spacy.load(<span class="string">&quot;de_core_news_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spacy_en = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        os.system(<span class="string">&quot;python -m spacy download en_core_web_sm&quot;</span>)</span><br><span class="line">        spacy_en = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> spacy_de, spacy_en</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text, tokenizer</span>):</span><br><span class="line">    <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> tokenizer.tokenizer(text)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">yield_tokens</span>(<span class="params">data_iter, tokenizer, index</span>):</span><br><span class="line">    <span class="keyword">for</span> from_to_tuple <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">yield</span> tokenizer(from_to_tuple[index])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_vocabulary</span>(<span class="params">spacy_de, spacy_en</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_de</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_de)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_en</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_en)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Building German Vocabulary ...&quot;</span>)</span><br><span class="line">    train, val, test = datasets.Multi30k(language_pair=(<span class="string">&quot;de&quot;</span>, <span class="string">&quot;en&quot;</span>))</span><br><span class="line">    vocab_src = build_vocab_from_iterator(</span><br><span class="line">        yield_tokens(train + val + test, tokenize_de, index=<span class="number">0</span>),</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        specials=[<span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>, <span class="string">&quot;&lt;blank&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Building English Vocabulary ...&quot;</span>)</span><br><span class="line">    train, val, test = datasets.Multi30k(language_pair=(<span class="string">&quot;de&quot;</span>, <span class="string">&quot;en&quot;</span>))</span><br><span class="line">    vocab_tgt = build_vocab_from_iterator(</span><br><span class="line">        yield_tokens(train + val + test, tokenize_en, index=<span class="number">1</span>),</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        specials=[<span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>, <span class="string">&quot;&lt;blank&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    vocab_src.set_default_index(vocab_src[<span class="string">&quot;&lt;unk&gt;&quot;</span>])</span><br><span class="line">    vocab_tgt.set_default_index(vocab_tgt[<span class="string">&quot;&lt;unk&gt;&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab_src, vocab_tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_vocab</span>(<span class="params">spacy_de, spacy_en</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> exists(<span class="string">&quot;vocab.pt&quot;</span>):</span><br><span class="line">        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)</span><br><span class="line">        torch.save((vocab_src, vocab_tgt), <span class="string">&quot;vocab.pt&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        vocab_src, vocab_tgt = torch.load(<span class="string">&quot;vocab.pt&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Finished.\nVocabulary sizes:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab_src))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab_tgt))</span><br><span class="line">    <span class="keyword">return</span> vocab_src, vocab_tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_interactive_notebook():</span><br><span class="line">    <span class="comment"># global variables used later in the script</span></span><br><span class="line">    spacy_de, spacy_en = show_example(load_tokenizers)</span><br><span class="line">    vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Finished.</span><br><span class="line">Vocabulary sizes:</span><br><span class="line">59981</span><br><span class="line">36745</span><br></pre></td></tr></table></figure><p>批处理对训练速度非常重要。我们希望有非常均匀的批次，绝对最小的填充。为此，我们必须对默认的torchtext批处理进行一些修改。此代码修补了torchtext的默认批处理，以确保我们通过搜索足够的句子来找到稳定的批处理。</p><h2 id="5-2-Iterators"><a href="#5-2-Iterators" class="headerlink" title="5.2 Iterators"></a>5.2 Iterators</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collate_batch</span>(<span class="params"></span></span><br><span class="line"><span class="params">    batch,</span></span><br><span class="line"><span class="params">    src_pipeline,</span></span><br><span class="line"><span class="params">    tgt_pipeline,</span></span><br><span class="line"><span class="params">    src_vocab,</span></span><br><span class="line"><span class="params">    tgt_vocab,</span></span><br><span class="line"><span class="params">    device,</span></span><br><span class="line"><span class="params">    max_padding=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">    pad_id=<span class="number">2</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    bs_id = torch.tensor([<span class="number">0</span>], device=device)  <span class="comment"># &lt;s&gt; token id</span></span><br><span class="line">    eos_id = torch.tensor([<span class="number">1</span>], device=device)  <span class="comment"># &lt;/s&gt; token id</span></span><br><span class="line">    src_list, tgt_list = [], []</span><br><span class="line">    <span class="keyword">for</span> (_src, _tgt) <span class="keyword">in</span> batch:</span><br><span class="line">        processed_src = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                bs_id,</span><br><span class="line">                torch.tensor(</span><br><span class="line">                    src_vocab(src_pipeline(_src)),</span><br><span class="line">                    dtype=torch.int64,</span><br><span class="line">                    device=device,</span><br><span class="line">                ),</span><br><span class="line">                eos_id,</span><br><span class="line">            ],</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        processed_tgt = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                bs_id,</span><br><span class="line">                torch.tensor(</span><br><span class="line">                    tgt_vocab(tgt_pipeline(_tgt)),</span><br><span class="line">                    dtype=torch.int64,</span><br><span class="line">                    device=device,</span><br><span class="line">                ),</span><br><span class="line">                eos_id,</span><br><span class="line">            ],</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        src_list.append(</span><br><span class="line">            <span class="comment"># warning - overwrites values for negative values of padding - len</span></span><br><span class="line">            pad(</span><br><span class="line">                processed_src,</span><br><span class="line">                (</span><br><span class="line">                    <span class="number">0</span>,</span><br><span class="line">                    max_padding - <span class="built_in">len</span>(processed_src),</span><br><span class="line">                ),</span><br><span class="line">                value=pad_id,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        tgt_list.append(</span><br><span class="line">            pad(</span><br><span class="line">                processed_tgt,</span><br><span class="line">                (<span class="number">0</span>, max_padding - <span class="built_in">len</span>(processed_tgt)),</span><br><span class="line">                value=pad_id,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    src = torch.stack(src_list)</span><br><span class="line">    tgt = torch.stack(tgt_list)</span><br><span class="line">    <span class="keyword">return</span> (src, tgt)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataloaders</span>(<span class="params"></span></span><br><span class="line"><span class="params">    device,</span></span><br><span class="line"><span class="params">    vocab_src,</span></span><br><span class="line"><span class="params">    vocab_tgt,</span></span><br><span class="line"><span class="params">    spacy_de,</span></span><br><span class="line"><span class="params">    spacy_en,</span></span><br><span class="line"><span class="params">    batch_size=<span class="number">12000</span>,</span></span><br><span class="line"><span class="params">    max_padding=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">    is_distributed=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="comment"># def create_dataloaders(batch_size=12000):</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_de</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_de)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_en</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_en)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">        <span class="keyword">return</span> collate_batch(</span><br><span class="line">            batch,</span><br><span class="line">            tokenize_de,</span><br><span class="line">            tokenize_en,</span><br><span class="line">            vocab_src,</span><br><span class="line">            vocab_tgt,</span><br><span class="line">            device,</span><br><span class="line">            max_padding=max_padding,</span><br><span class="line">            pad_id=vocab_src.get_stoi()[<span class="string">&quot;&lt;blank&gt;&quot;</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    train_iter, valid_iter, test_iter = datasets.Multi30k(</span><br><span class="line">        language_pair=(<span class="string">&quot;de&quot;</span>, <span class="string">&quot;en&quot;</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_iter_map = to_map_style_dataset(</span><br><span class="line">        train_iter</span><br><span class="line">    )  <span class="comment"># DistributedSampler needs a dataset len()</span></span><br><span class="line">    train_sampler = (</span><br><span class="line">        DistributedSampler(train_iter_map) <span class="keyword">if</span> is_distributed <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    )</span><br><span class="line">    valid_iter_map = to_map_style_dataset(valid_iter)</span><br><span class="line">    valid_sampler = (</span><br><span class="line">        DistributedSampler(valid_iter_map) <span class="keyword">if</span> is_distributed <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_dataloader = DataLoader(</span><br><span class="line">        train_iter_map,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=(train_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">        sampler=train_sampler,</span><br><span class="line">        collate_fn=collate_fn,</span><br><span class="line">    )</span><br><span class="line">    valid_dataloader = DataLoader(</span><br><span class="line">        valid_iter_map,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=(valid_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">        sampler=valid_sampler,</span><br><span class="line">        collate_fn=collate_fn,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> train_dataloader, valid_dataloader</span><br></pre></td></tr></table></figure><h2 id="5-3-Training-the-System"><a href="#5-3-Training-the-System" class="headerlink" title="5.3 Training the System"></a>5.3 Training the System</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_worker</span>(<span class="params"></span></span><br><span class="line"><span class="params">    gpu,</span></span><br><span class="line"><span class="params">    ngpus_per_node,</span></span><br><span class="line"><span class="params">    vocab_src,</span></span><br><span class="line"><span class="params">    vocab_tgt,</span></span><br><span class="line"><span class="params">    spacy_de,</span></span><br><span class="line"><span class="params">    spacy_en,</span></span><br><span class="line"><span class="params">    config,</span></span><br><span class="line"><span class="params">    is_distributed=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Train worker process using GPU: <span class="subst">&#123;gpu&#125;</span> for training&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    torch.cuda.set_device(gpu)</span><br><span class="line"></span><br><span class="line">    pad_idx = vocab_tgt[<span class="string">&quot;&lt;blank&gt;&quot;</span>]</span><br><span class="line">    d_model = <span class="number">512</span></span><br><span class="line">    model = make_model(<span class="built_in">len</span>(vocab_src), <span class="built_in">len</span>(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.cuda(gpu)</span><br><span class="line">    module = model</span><br><span class="line">    is_main_process = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> is_distributed:</span><br><span class="line">        dist.init_process_group(</span><br><span class="line">            <span class="string">&quot;nccl&quot;</span>, init_method=<span class="string">&quot;env://&quot;</span>, rank=gpu, world_size=ngpus_per_node</span><br><span class="line">        )</span><br><span class="line">        model = DDP(model, device_ids=[gpu])</span><br><span class="line">        module = model.module</span><br><span class="line">        is_main_process = gpu == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    criterion = LabelSmoothing(</span><br><span class="line">        size=<span class="built_in">len</span>(vocab_tgt), padding_idx=pad_idx, smoothing=<span class="number">0.1</span></span><br><span class="line">    )</span><br><span class="line">    criterion.cuda(gpu)</span><br><span class="line"></span><br><span class="line">    train_dataloader, valid_dataloader = create_dataloaders(</span><br><span class="line">        gpu,</span><br><span class="line">        vocab_src,</span><br><span class="line">        vocab_tgt,</span><br><span class="line">        spacy_de,</span><br><span class="line">        spacy_en,</span><br><span class="line">        batch_size=config[<span class="string">&quot;batch_size&quot;</span>] // ngpus_per_node,</span><br><span class="line">        max_padding=config[<span class="string">&quot;max_padding&quot;</span>],</span><br><span class="line">        is_distributed=is_distributed,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        model.parameters(), lr=config[<span class="string">&quot;base_lr&quot;</span>], betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = LambdaLR(</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        lr_lambda=<span class="keyword">lambda</span> step: rate(</span><br><span class="line">            step, d_model, factor=<span class="number">1</span>, warmup=config[<span class="string">&quot;warmup&quot;</span>]</span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line">    train_state = TrainState()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(config[<span class="string">&quot;num_epochs&quot;</span>]):</span><br><span class="line">        <span class="keyword">if</span> is_distributed:</span><br><span class="line">            train_dataloader.sampler.set_epoch(epoch)</span><br><span class="line">            valid_dataloader.sampler.set_epoch(epoch)</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[GPU<span class="subst">&#123;gpu&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> Training ====&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        _, train_state = run_epoch(</span><br><span class="line">            (Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx) <span class="keyword">for</span> b <span class="keyword">in</span> train_dataloader),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(module.generator, criterion),</span><br><span class="line">            optimizer,</span><br><span class="line">            lr_scheduler,</span><br><span class="line">            mode=<span class="string">&quot;train+log&quot;</span>,</span><br><span class="line">            accum_iter=config[<span class="string">&quot;accum_iter&quot;</span>],</span><br><span class="line">            train_state=train_state,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        GPUtil.showUtilization()</span><br><span class="line">        <span class="keyword">if</span> is_main_process:</span><br><span class="line">            file_path = <span class="string">&quot;%s%.2d.pt&quot;</span> % (config[<span class="string">&quot;file_prefix&quot;</span>], epoch)</span><br><span class="line">            torch.save(module.state_dict(), file_path)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[GPU<span class="subst">&#123;gpu&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> Validation ====&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        sloss = run_epoch(</span><br><span class="line">            (Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx) <span class="keyword">for</span> b <span class="keyword">in</span> valid_dataloader),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(module.generator, criterion),</span><br><span class="line">            DummyOptimizer(),</span><br><span class="line">            DummyScheduler(),</span><br><span class="line">            mode=<span class="string">&quot;eval&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(sloss)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_main_process:</span><br><span class="line">        file_path = <span class="string">&quot;%sfinal.pt&quot;</span> % config[<span class="string">&quot;file_prefix&quot;</span>]</span><br><span class="line">        torch.save(module.state_dict(), file_path)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_distributed_model</span>(<span class="params">vocab_src, vocab_tgt, spacy_de, spacy_en, config</span>):</span><br><span class="line">    <span class="keyword">from</span> the_annotated_transformer <span class="keyword">import</span> train_worker</span><br><span class="line"></span><br><span class="line">    ngpus = torch.cuda.device_count()</span><br><span class="line">    os.environ[<span class="string">&quot;MASTER_ADDR&quot;</span>] = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">    os.environ[<span class="string">&quot;MASTER_PORT&quot;</span>] = <span class="string">&quot;12356&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Number of GPUs detected: <span class="subst">&#123;ngpus&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Spawning training processes ...&quot;</span>)</span><br><span class="line">    mp.spawn(</span><br><span class="line">        train_worker,</span><br><span class="line">        nprocs=ngpus,</span><br><span class="line">        args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, <span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">vocab_src, vocab_tgt, spacy_de, spacy_en, config</span>):</span><br><span class="line">    <span class="keyword">if</span> config[<span class="string">&quot;distributed&quot;</span>]:</span><br><span class="line">        train_distributed_model(</span><br><span class="line">            vocab_src, vocab_tgt, spacy_de, spacy_en, config</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        train_worker(</span><br><span class="line">            <span class="number">0</span>, <span class="number">1</span>, vocab_src, vocab_tgt, spacy_de, spacy_en, config, <span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_trained_model</span>():</span><br><span class="line">    config = &#123;</span><br><span class="line">        <span class="string">&quot;batch_size&quot;</span>: <span class="number">32</span>,</span><br><span class="line">        <span class="string">&quot;distributed&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;num_epochs&quot;</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">&quot;accum_iter&quot;</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">&quot;base_lr&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">&quot;max_padding&quot;</span>: <span class="number">72</span>,</span><br><span class="line">        <span class="string">&quot;warmup&quot;</span>: <span class="number">3000</span>,</span><br><span class="line">        <span class="string">&quot;file_prefix&quot;</span>: <span class="string">&quot;multi30k_model_&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    model_path = <span class="string">&quot;multi30k_model_final.pt&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> exists(model_path):</span><br><span class="line">        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)</span><br><span class="line"></span><br><span class="line">    model = make_model(<span class="built_in">len</span>(vocab_src), <span class="built_in">len</span>(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&quot;multi30k_model_final.pt&quot;</span>))</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_interactive_notebook():</span><br><span class="line">    model = load_trained_model()</span><br></pre></td></tr></table></figure><p>一旦训练，我们就可以对模型进行解码以生成一组翻译。这里我们简单地翻译验证集中的第一句话。这个数据集非常小，所以贪婪搜索的翻译结果相当准确。</p><h1 id="6-Additional-Components-BPE-Search-Averaging"><a href="#6-Additional-Components-BPE-Search-Averaging" class="headerlink" title="6.Additional Components: BPE, Search, Averaging"></a>6.Additional Components: BPE, Search, Averaging</h1><p>以上内容主要涵盖了transformer模型本身，但其实还有四个附加功能我们没有涉及。不过我们在<a href="https://github.com/opennmt/opennmt-py" title="OpenNMT-py">OpenNMT-py</a>中实现了所有这些附加功能。</p><ol><li><strong>BPE/Word-piece</strong>：我们可以使用一个<strong>库</strong>首先将数据预处理为子词单元。可以参见Rico Sennrich的<a href="https://github.com/rsennrich/subword-nmt" title="subword-nmt">subword-nmt</a> 来实现。这些模型会将训练数据转换为如下所示：<code>▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden</code></li><li><strong>Shared Embeddings</strong>：当使用具有共享词汇表的 BPE 时，我们可以在源/目标/生成器之间共享相同的权重向量。 有关详细信息，请参阅（引用）。 要将其添加到模型中，只需执行以下操作：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="literal">False</span>:</span><br><span class="line">    model.src_embed[<span class="number">0</span>].lut.weight = model.tgt_embeddings[<span class="number">0</span>].lut.weight</span><br><span class="line">    model.generator.lut.weight = model.tgt_embed[<span class="number">0</span>].lut.weight</span><br></pre></td></tr></table></figure></li><li>Beam Search：这有点太复杂了，无法在这里介绍。有关pytorch实现，请参阅<a href="https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/Beam.py" title="OpenNMT-py">OpenNMT-py</a>。</li><li>Model Averaging：论文对最后k个检查点进行平均以得到集成效果。如果我们有一堆模型，我们可以这样做：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">average</span>(<span class="params">model, models</span>):</span><br><span class="line">    <span class="string">&quot;Average models into model&quot;</span></span><br><span class="line">    <span class="keyword">for</span> ps <span class="keyword">in</span> <span class="built_in">zip</span>(*[m.params() <span class="keyword">for</span> m <span class="keyword">in</span> [model] + models]):</span><br><span class="line">        ps[<span class="number">0</span>].copy_(torch.<span class="built_in">sum</span>(*ps[<span class="number">1</span>:]) / <span class="built_in">len</span>(ps[<span class="number">1</span>:]))</span><br></pre></td></tr></table></figure></li></ol><h1 id="7-Results"><a href="#7-Results" class="headerlink" title="7.Results"></a>7.Results</h1><p>在WMT 2014 英语-德语翻译任务中,big transformer模型(表2中的Transformer (big)) 比之前报道的最佳模型(包括集成模型)高出2.0 BLEU以上, 新的最高BLEU分数为28.4。该模型的配置列于表3的底部，模型在8个P100 GPU上训练3.5天。即使是我们的基础模型，其表现也超过了之前的模型和集成模型，且模型训练成本比之前模型要小的多。</p><p>在WMT 2014英语-法语翻译任务中，我们的big model的BLEU得分达到41.0分，优于之前发布的所有单一模型，训练成本不到之前最先进模型的1/4。英语-法语翻译任务训练的Transformer (big)模型使用的丢弃率为Pdrop = 0.1,而不是0.3。</p><p><img src="image/image_13KURgL3xT.png" alt=""></p><p>通过上一节讲的几个附加功能，OpenNMT-py在EN-DE WMT上得分可以达到26.9。下面，我将这些参数加载到我的代码重现中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load data and model for output checks</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">check_outputs</span>(<span class="params"></span></span><br><span class="line"><span class="params">    valid_dataloader,</span></span><br><span class="line"><span class="params">    model,</span></span><br><span class="line"><span class="params">    vocab_src,</span></span><br><span class="line"><span class="params">    vocab_tgt,</span></span><br><span class="line"><span class="params">    n_examples=<span class="number">15</span>,</span></span><br><span class="line"><span class="params">    pad_idx=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">    eos_string=<span class="string">&quot;&lt;/s&gt;&quot;</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    results = [()] * n_examples</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(n_examples):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\nExample %d ========\n&quot;</span> % idx)</span><br><span class="line">        b = <span class="built_in">next</span>(<span class="built_in">iter</span>(valid_dataloader))</span><br><span class="line">        rb = Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx)</span><br><span class="line">        greedy_decode(model, rb.src, rb.src_mask, <span class="number">64</span>, <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        src_tokens = [</span><br><span class="line">            vocab_src.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> rb.src[<span class="number">0</span>] <span class="keyword">if</span> x != pad_idx</span><br><span class="line">        ]</span><br><span class="line">        tgt_tokens = [</span><br><span class="line">            vocab_tgt.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> rb.tgt[<span class="number">0</span>] <span class="keyword">if</span> x != pad_idx</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Source Text (Input)        : &quot;</span></span><br><span class="line">            + <span class="string">&quot; &quot;</span>.join(src_tokens).replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Target Text (Ground Truth) : &quot;</span></span><br><span class="line">            + <span class="string">&quot; &quot;</span>.join(tgt_tokens).replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        )</span><br><span class="line">        model_out = greedy_decode(model, rb.src, rb.src_mask, <span class="number">72</span>, <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">        model_txt = (</span><br><span class="line">            <span class="string">&quot; &quot;</span>.join(</span><br><span class="line">                [vocab_tgt.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> model_out <span class="keyword">if</span> x != pad_idx]</span><br><span class="line">            ).split(eos_string, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            + eos_string</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Model Output               : &quot;</span> + model_txt.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_model_example</span>(<span class="params">n_examples=<span class="number">5</span></span>):</span><br><span class="line">    <span class="keyword">global</span> vocab_src, vocab_tgt, spacy_de, spacy_en</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Preparing Data ...&quot;</span>)</span><br><span class="line">    _, valid_dataloader = create_dataloaders(</span><br><span class="line">        torch.device(<span class="string">&quot;cpu&quot;</span>),</span><br><span class="line">        vocab_src,</span><br><span class="line">        vocab_tgt,</span><br><span class="line">        spacy_de,</span><br><span class="line">        spacy_en,</span><br><span class="line">        batch_size=<span class="number">1</span>,</span><br><span class="line">        is_distributed=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Loading Trained Model ...&quot;</span>)</span><br><span class="line"></span><br><span class="line">    model = make_model(<span class="built_in">len</span>(vocab_src), <span class="built_in">len</span>(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.load_state_dict(</span><br><span class="line">        torch.load(<span class="string">&quot;multi30k_model_final.pt&quot;</span>, map_location=torch.device(<span class="string">&quot;cpu&quot;</span>))</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Checking Model Outputs:&quot;</span>)</span><br><span class="line">    example_data = check_outputs(</span><br><span class="line">        valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model, example_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># execute_example(run_model_example)</span></span><br></pre></td></tr></table></figure><h2 id="7-1-Attention-Visualization"><a href="#7-1-Attention-Visualization" class="headerlink" title="7.1 Attention Visualization"></a>7.1 Attention Visualization</h2><p>就算使用贪婪解码，翻译效果看起来也不错。我们可以进一步将其可视化，以查看注意力的每一层发生了什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mtx2df</span>(<span class="params">m, max_row, max_col, row_tokens, col_tokens</span>):</span><br><span class="line">    <span class="string">&quot;convert a dense matrix to a data frame with row and column indices&quot;</span></span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        [</span><br><span class="line">            (</span><br><span class="line">                r,</span><br><span class="line">                c,</span><br><span class="line">                <span class="built_in">float</span>(m[r, c]),</span><br><span class="line">                <span class="string">&quot;%.3d %s&quot;</span></span><br><span class="line">                % (r, row_tokens[r] <span class="keyword">if</span> <span class="built_in">len</span>(row_tokens) &gt; r <span class="keyword">else</span> <span class="string">&quot;&lt;blank&gt;&quot;</span>),</span><br><span class="line">                <span class="string">&quot;%.3d %s&quot;</span></span><br><span class="line">                % (c, col_tokens[c] <span class="keyword">if</span> <span class="built_in">len</span>(col_tokens) &gt; c <span class="keyword">else</span> <span class="string">&quot;&lt;blank&gt;&quot;</span>),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> r &lt; max_row <span class="keyword">and</span> c &lt; max_col</span><br><span class="line">        ],</span><br><span class="line">        <span class="comment"># if float(m[r,c]) != 0 and r &lt; max_row and c &lt; max_col],</span></span><br><span class="line">        columns=[<span class="string">&quot;row&quot;</span>, <span class="string">&quot;column&quot;</span>, <span class="string">&quot;value&quot;</span>, <span class="string">&quot;row_token&quot;</span>, <span class="string">&quot;col_token&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attn_map</span>(<span class="params">attn, layer, head, row_tokens, col_tokens, max_dim=<span class="number">30</span></span>):</span><br><span class="line">    df = mtx2df(</span><br><span class="line">        attn[<span class="number">0</span>, head].data,</span><br><span class="line">        max_dim,</span><br><span class="line">        max_dim,</span><br><span class="line">        row_tokens,</span><br><span class="line">        col_tokens,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data=df)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .encode(</span><br><span class="line">            x=alt.X(<span class="string">&quot;col_token&quot;</span>, axis=alt.Axis(title=<span class="string">&quot;&quot;</span>)),</span><br><span class="line">            y=alt.Y(<span class="string">&quot;row_token&quot;</span>, axis=alt.Axis(title=<span class="string">&quot;&quot;</span>)),</span><br><span class="line">            color=<span class="string">&quot;value&quot;</span>,</span><br><span class="line">            tooltip=[<span class="string">&quot;row&quot;</span>, <span class="string">&quot;column&quot;</span>, <span class="string">&quot;value&quot;</span>, <span class="string">&quot;row_token&quot;</span>, <span class="string">&quot;col_token&quot;</span>],</span><br><span class="line">        )</span><br><span class="line">        .properties(height=<span class="number">400</span>, width=<span class="number">400</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_encoder</span>(<span class="params">model, layer</span>):</span><br><span class="line">    <span class="keyword">return</span> model.encoder.layers[layer].self_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_decoder_self</span>(<span class="params">model, layer</span>):</span><br><span class="line">    <span class="keyword">return</span> model.decoder.layers[layer].self_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_decoder_src</span>(<span class="params">model, layer</span>):</span><br><span class="line">    <span class="keyword">return</span> model.decoder.layers[layer].src_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_layer</span>(<span class="params">model, layer, getter_fn, ntokens, row_tokens, col_tokens</span>):</span><br><span class="line">    <span class="comment"># ntokens = last_example[0].ntokens</span></span><br><span class="line">    attn = getter_fn(model, layer)</span><br><span class="line">    n_heads = attn.shape[<span class="number">1</span>]</span><br><span class="line">    charts = [</span><br><span class="line">        attn_map(</span><br><span class="line">            attn,</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">            h,</span><br><span class="line">            row_tokens=row_tokens,</span><br><span class="line">            col_tokens=col_tokens,</span><br><span class="line">            max_dim=ntokens,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_heads)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">assert</span> n_heads == <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> alt.vconcat(</span><br><span class="line">        charts[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># | charts[1]</span></span><br><span class="line">        | charts[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># | charts[3]</span></span><br><span class="line">        | charts[<span class="number">4</span>]</span><br><span class="line">        <span class="comment"># | charts[5]</span></span><br><span class="line">        | charts[<span class="number">6</span>]</span><br><span class="line">        <span class="comment"># | charts[7]</span></span><br><span class="line">        <span class="comment"># layer + 1 due to 0-indexing</span></span><br><span class="line">    ).properties(title=<span class="string">&quot;Layer %d&quot;</span> % (layer + <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="7-2-Encoder-Self-Attention"><a href="#7-2-Encoder-Self-Attention" class="headerlink" title="7.2 Encoder Self Attention"></a>7.2 Encoder Self Attention</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">viz_encoder_self</span>():</span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[</span><br><span class="line">        <span class="built_in">len</span>(example_data) - <span class="number">1</span></span><br><span class="line">    ]  <span class="comment"># batch object for the final example</span></span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model, layer, get_encoder, <span class="built_in">len</span>(example[<span class="number">1</span>]), example[<span class="number">1</span>], example[<span class="number">1</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># &amp; layer_viz[1]</span></span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># &amp; layer_viz[3]</span></span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        <span class="comment"># &amp; layer_viz[5]</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_encoder_self)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Preparing Data ...</span><br><span class="line">Loading Trained Model ...</span><br><span class="line">Checking Model Outputs:</span><br><span class="line"></span><br><span class="line">Example 0 ========</span><br><span class="line"></span><br><span class="line">Source Text (Input)        : &lt;s&gt; Zwei Frauen <span class="keyword">in</span> pinkfarbenen T-Shirts und &lt;unk&gt; unterhalten sich vor einem &lt;unk&gt; . &lt;/s&gt;</span><br><span class="line">Target Text (Ground Truth) : &lt;s&gt; Two women wearing pink T - shirts and blue jeans converse outside clothing store . &lt;/s&gt;</span><br><span class="line">Model Output               : &lt;s&gt; Two women <span class="keyword">in</span> pink shirts and face are talking <span class="keyword">in</span> front of a &lt;unk&gt; . &lt;/s&gt;</span><br></pre></td></tr></table></figure><p><img src="image/image_jh_mPP__BE.png" alt=""></p><h2 id="7-3-Decoder-Self-Attention"><a href="#7-3-Decoder-Self-Attention" class="headerlink" title="7.3 Decoder Self Attention"></a>7.3 Decoder Self Attention</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">viz_decoder_self</span>():</span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[<span class="built_in">len</span>(example_data) - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model,</span><br><span class="line">            layer,</span><br><span class="line">            get_decoder_self,</span><br><span class="line">            <span class="built_in">len</span>(example[<span class="number">1</span>]),</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">1</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">3</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">5</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_decoder_self)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Preparing Data ...</span><br><span class="line">Loading Trained Model ...</span><br><span class="line">Checking Model Outputs:</span><br><span class="line"></span><br><span class="line">Example 0 ========</span><br><span class="line"></span><br><span class="line">Source Text (Input)        : &lt;s&gt; Eine Gruppe von Männern <span class="keyword">in</span> Kostümen spielt Musik . &lt;/s&gt;</span><br><span class="line">Target Text (Ground Truth) : &lt;s&gt; A group of men <span class="keyword">in</span> costume play music . &lt;/s&gt;</span><br><span class="line">Model Output               : &lt;s&gt; A group of men <span class="keyword">in</span> costumes playing music . &lt;/s&gt;</span><br></pre></td></tr></table></figure><p><img src="image/image_zbgs80BnJA.png" alt=""></p><h2 id="7-4-Decoder-Src-Attention"><a href="#7-4-Decoder-Src-Attention" class="headerlink" title="7.4 Decoder Src Attention"></a>7.4 Decoder Src Attention</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">viz_decoder_src</span>():</span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[<span class="built_in">len</span>(example_data) - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model,</span><br><span class="line">            layer,</span><br><span class="line">            get_decoder_src,</span><br><span class="line">            <span class="built_in">max</span>(<span class="built_in">len</span>(example[<span class="number">1</span>]), <span class="built_in">len</span>(example[<span class="number">2</span>])),</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">            example[<span class="number">2</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">1</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">3</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">5</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_decoder_src)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Preparing Data ...</span><br><span class="line">Loading Trained Model ...</span><br><span class="line">Checking Model Outputs:</span><br><span class="line"></span><br><span class="line">Example 0 ========</span><br><span class="line"></span><br><span class="line">Source Text (Input)        : &lt;s&gt; Ein kleiner Junge verwendet einen Bohrer , um ein Loch <span class="keyword">in</span> ein Holzstück zu machen . &lt;/s&gt;</span><br><span class="line">Target Text (Ground Truth) : &lt;s&gt; A little boy using a drill to make a hole <span class="keyword">in</span> a piece of wood . &lt;/s&gt;</span><br><span class="line">Model Output               : &lt;s&gt; A little boy uses a machine to be working <span class="keyword">in</span> a hole <span class="keyword">in</span> a <span class="built_in">log</span> . &lt;/s&gt;</span><br></pre></td></tr></table></figure><p><img src="image/image_Mrv-JBg_Ia.png" alt=""></p><h1 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8.Conclusion"></a>8.Conclusion</h1><p>希望这段代码对未来的研究有用。 如果您有任何问题，请联系我们。</p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何阅读论文</title>
      <link href="/paper_reading/0.%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87/"/>
      <url>/paper_reading/0.%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87/</url>
      
        <content type="html"><![CDATA[<p>解读视频：<a href="https://www.bilibili.com/video/BV1H44y1t75x/?spm_id_from=333.999.0.0">https://www.bilibili.com/video/BV1H44y1t75x/?spm_id_from=333.999.0.0</a></p><p>花三遍，读一篇论文</p><h2 id="第一遍（花时最少，做海选）"><a href="#第一遍（花时最少，做海选）" class="headerlink" title="第一遍（花时最少，做海选）"></a>第一遍（花时最少，做海选）</h2><p>第一遍读论文的时候，需要去关注标题和摘要</p><ul><li>读完摘要之后，直接跳到结论这边</li><li>读完这三个部分，大致就知道这篇论文是在讲什么东西了</li></ul><h2 id="第二遍（对相关论文做以进一步精选）"><a href="#第二遍（对相关论文做以进一步精选）" class="headerlink" title="第二遍（对相关论文做以进一步精选）"></a>第二遍（对相关论文做以进一步精选）</h2><p>第二遍里面我们就要对整个文章完整过一遍，然后知道每一块到底在干什么东西，我们可以沿着从标题一直往下读到最后，但是这个时候也不需要注意太多的细节，以及一些公式的证明等等。</p><ul><li><strong>关注的地方</strong>：第二遍阅读的时候，最重要是搞明白那些重要的图和表，都要知道他每一个字在干什么事情 ；作者提出的方法和别人提出的方法是怎么进行对比的？之间差距有多大？这个时候可能你还没有特别搞懂他在干什么。但是不要紧，你可以将不懂的地方标记下来，留到之后第三遍进行阅读</li><li><strong>达到的效果</strong>：第二遍阅读完之后，你就对整个论文的各个部分，都有一个大概的了解，中间可以把作者引用的别人的相关文献圈出来，比如作者是在某某某的方法上进行了改进，做了哪些改进之类的。这里需要注意的是，如果你发现作者引用的这些重要文献是你没有读过的，那么你需要把它圈出来，作为你的稍后阅读清单</li><li><strong>对后续的影响</strong>：这一遍阅读之后，你需要再继续思考一下这篇论文的质量以及和自己研究方向的契合程度，决定一下自己要不要进行第三遍的完完全全彻底的精读</li></ul><h2 id="第三遍（重点研读）"><a href="#第三遍（重点研读）" class="headerlink" title="第三遍（重点研读）"></a>第三遍（重点研读）</h2><p>第三遍是最后一遍了，也是最详细的一遍，这里就需要自己知道每一句话在干什么，每一段在说什么</p><p>一边读，可以一边在脑子里面思考一些问题：</p><ul><li>比如说，如果要是我来写这篇文章，我会如何组织这个结构？</li><li>读实验部分的时候，可以思考一下，作者是如何描述自己的实验的，你可以思考，如果换自己来做的话，能不能比作者做得更好？</li></ul><p>这一遍读的时候，一定要明白作者每句话，每个字在说什么，并且最好可以脑补出它整个流程是什么样子的，似乎是自己在做实验，写论文一样。如果有困难的话，可以借助思维导图或者流程图这样的工具，把他的整个流程以可视化的形式展现出来，帮助自己理解。</p>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PaperReading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 Rainbow</title>
      <link href="/paper_reading/3.2.Rainbow/"/>
      <url>/paper_reading/3.2.Rainbow/</url>
      
        <content type="html"><![CDATA[<h1 id="Rainbow"><a href="#Rainbow" class="headerlink" title="Rainbow"></a>Rainbow</h1><h1 id="Rainbow-Combining-Improvements-in-Deep-Reinforcement-Learning"><a href="#Rainbow-Combining-Improvements-in-Deep-Reinforcement-Learning" class="headerlink" title="Rainbow: Combining Improvements in Deep Reinforcement Learning"></a>Rainbow: Combining Improvements in Deep Reinforcement Learning</h1><h1 id="Rainbow-结合深度强化学习的改进"><a href="#Rainbow-结合深度强化学习的改进" class="headerlink" title="Rainbow: 结合深度强化学习的改进"></a>Rainbow: 结合深度强化学习的改进</h1><p>论文地址：<a href="https://arxiv.org/abs/1710.02298" title="https://arxiv.org/abs/1710.02298">https://arxiv.org/abs/1710.02298</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>深度强化学习社区对DQN算法进行了几项独立改进。然而，尚不清楚这些扩展中的哪一个是互补的，并且可以有效地结合。本文研究了DQN算法的六个扩展，并实证研究了它们的组合。我们的实验表明，在数据效率和最终性能方面，该组合在雅达利2600基准上提供了最先进的性能。我们还提供了详细的消融研究结果，该研究显示了每个部件对整体性能的贡献。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall per-formance.</p><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h2><p>最近在将强化学习（RL）扩展到复杂顺序决策问题方面取得的许多成功都是由深度Q网络算法（DQN；Mnih等人，20132015）开创的。DQN将Q学习与卷积神经网络和经验回放相结合，能够从原始像素中学习如何玩许多雅达利游戏。从那时起，人们提出了许多技巧，以提高其速度或稳定性。Double DQN（DDQN；van Hasselt、Guez和Silver 2016）通过解耦选择和评估引导动作，解决了Q学习的高估偏差问题（van Haselt 2010）。优先经验回放（Schaul，2015）通过更频繁地重放需要学习的内容，提高了数据效率。Dueling Network（Wang，2016）通过将最有动作价值分解为最优状态价值和最优优势， 有助于跨动作进行学习。从A3C中使用的多步TD（Sutton 1988；Sutton和Barto 1998）中学习（Mnih，2016），改变了偏差-方差权衡，有助于更快地将新获得到的奖励传播到较早访过的结点。分布Q学习（Bellemare、Dabney和Munos 2017）学习现有收益的分类分布，而不是估计平均值。Noisy DQN（Fortunato等人，2017）使用随机网络层进行探索。当然，这份清单还远远不够详尽。</p><p>这些算法中的每一个都能够单独实现显著的性能改进。由于它们是通过解决根本不同的问题来实现的，而且它们建立在同一个网络上，因此它们可以合理地结合在一起。在某些情况下，已经做到了这一点：Prioritized DDQN和 Dueling DDQN都使用 Double DQN，Dueling DDQN还与优先经验回放相结合。在本文中，我们建议研究一种结合上述所有技巧的方法。我们展示了如何整合这些不同的方法，并且它们实际上在很大程度上是互补的。事实上，他们的结合在Arcade Learning Environment（Bellemare et al.2013）的57款Atari 2600游戏的基准测试平台上，在效率和性能方面都取得了很好的结果。最后，我们展示了研究的结果，以帮助理解不同方法对性能和效率的贡献。</p><p><img src="image/image_eYnp9irvB4.png" alt=""></p><p>图1：57款雅达利游戏中人类正常化表现的中位数。我们将我们的集成智能体（Rainbow彩虹色）与DQN（灰色）和六个已发布的baseline进行了比较。请注意，我们在7M帧后达到了DQN的最佳性能，超过了44M帧内的任何基线，并达到了显著提高的最终性能。在5个点上使用移动平均值平滑曲线。</p><h2 id="2-背景"><a href="#2-背景" class="headerlink" title="2.背景"></a>2.背景</h2><p>强化学习解决了智能体在环境中行动以最大化标量奖励信号的问题。没有向智能体提供直接监督，例如，从未直接告知智能体最佳行动。</p><h3 id="2-1智能体和环境"><a href="#2-1智能体和环境" class="headerlink" title="2.1智能体和环境"></a>2.1智能体和环境</h3><p>在每个离散时间步$t=0,1,2…$，环境向智能体提供当前状态$S_t$，智能体通过选择动作$A_t$进行行动，然后环境提供下一个奖励$R_{t+1}$、折扣系数$\gamma_{t+1}$和状态$S_{t+1}$。这种交互被形式化为马尔可夫决策过程（MDP），即元组$<S、 A、T、r、γ>$, 其中S是有限状态集，a是有限动作集，$T(s,a,a’)=P[S_{t+1}=s’|S_t=s, A_t=a]$ 是（随机）转移函数，$r(s,a)=E[R_{t+1}|S_t=s,A_t=a]$是奖励函数，$γ\in [0, 1]$是折扣系数。在我们的实验中，MDP是具有常数$γ_t=γ$的，除了在$γ_t=0$时，但算法以一般形式表示。</p><p>在智能体端，动作选择由策略$π$给出，策略$π$定义每个状态的动作概率分布。在t时刻，状态为$S_t$，我们定义折扣回报函数$G_t=\sum_{k=0}^{\infty} \gamma_t^{(k)} R_{t+k+1}$，为智能体计算未来奖励的折扣总和，其中，未来$k$步奖励的折扣由该时间之前的折扣乘积给出$\gamma_t^{(k)}=\Pi_{i=1}^k \gamma_{t+i}$。智能体的目标是通过找到一个好的政策来最大化预期的折扣回报。</p><p>策略可以直接学习，也可以根据其他学习来构建。在基于价值的强化学习中，当从给定状态开始执行策略 $\pi$ 时，智能体可以获得预期折扣收益或价值的估计，$v_π(s)=E_π[G_t|S_t=s]$，或状态动作价值函数$q_π(s，a)=E_\pi[G_t |S_t=s,A_t=a]$。从状态动作价值函数中导出新策略的常用方法是 $ \epsilon-greed<br>ly  $方法。意味着以概率 $1-\epsilon$ 的概率取价值最高的动作，否则以 $\epsilon$ 的概率随机选取动作。这类策略用于引入一种探索形式：通过根据其当前估计随机选择次优行为，智能体可以在适当时发现并纠正其估计。主要的限制是，很难找到延伸到未来的替代行动方案；这激发了对更直接探索形式的研究。</p><h3 id="2-2深度强化学习和DQN"><a href="#2-2深度强化学习和DQN" class="headerlink" title="2.2深度强化学习和DQN"></a>2.2深度强化学习和DQN</h3><p>对于过大的状态和动作空间，很难独立学习每个状态和动作对的Q值估计。在深度强化学习中，我们使用深度（即多层）神经网络表示智能体的各种组件，如策略$π(s,a)$或值$q(s,b)$。这些网络的参数通过梯度下降来训练，最小化损失函数。</p><p>在DQN（Mnih，2015）中，通过使用卷积神经网络来近似拟合给定状态$S_t$的动作值（以原始像素帧的形式作为输入送到神经网络），成功地将深度神经网络和强化学习相结合。在每个步骤中，智能体都会根据当前状态选择一个 $\epsilon - greedily$ 动作值，并将转换$(St, At, Rt+1, γt+1,St+1)$添加到经验回放缓冲区（Lin 1992），该缓冲区保存最后一百万个经验。采用随机梯度下降法对神经网络参数进行优化，最小化损失函数</p><script type="math/tex; mode=display">(R_{t+1} + \gamma_{t+1} \underset{a'}{max} q_\theta^-(S_{t+1}, a') - q_\theta(S_t, A_t))^2 ~~~~~~~~~~~~~~~~~~~~~~ (1)</script><p>其中t是从经验回放池中随机选取的时间步长。损失的梯度仅反向传播到策略网络的参数$θ$中（也用于选择动作）；$\overline θ$表示目标网络的参数；没有直接优化，而是从策略网络的定期复制参数。使用RMSprop（Tieleman和Hinton 2012），一种随机梯度下降的变体，对从经验回放池中的数据均匀采样，进行小批次梯度优化。这意味着，在上述损失中，时间索引t将是来自最后一百万次经验的随机时间索引，而不是当前时间。使用经验回放和目标网络可以相对稳定地学习Q值，并在几款雅达利游戏中获得超人的表现。</p><h2 id="3-DQN扩展"><a href="#3-DQN扩展" class="headerlink" title="3.DQN扩展"></a>3.DQN扩展</h2><p>DQN是一个重要的里程碑，但目前已知该算法的若干局限性，并提出了许多扩展。我们选择了六个扩展，每个扩展都解决了一个问题并提高了整体性能。为了保持选择的大小可管理，我们选择了一组解决不同问题的扩展（例如，只是众多解决探索中的一个）。</p><h3 id="3-1-Double-Q-learning"><a href="#3-1-Double-Q-learning" class="headerlink" title="3.1 Double Q-learning"></a>3.1 Double Q-learning</h3><p>由于等式1中的最大化步骤，传统的Q学习受到高估偏差的影响，这可能会影响学习。Double Q-learning（van Hasselt 2010）通过解耦来解决这种高估，在原始DQN网络中最大化选择动作，在目标网络中评估动作的好坏。使用损失函数如下，可以将其与DQN（van Hasselt、Guez和Silver 2016）有效结合</p><script type="math/tex; mode=display">(R_{t+1}+\gamma_{t+1}q_\theta^-(S_{t+1}, \underset{a'}{argmax~} q_\theta(S_{t+1},a')) - q_\theta(S_t, A_t))^2</script><p>这一变化被证明可以减少对DQN的有害高估，从而提高性能。</p><h3 id="3-2-优先经验回放"><a href="#3-2-优先经验回放" class="headerlink" title="3.2 优先经验回放"></a>3.2 优先经验回放</h3><p>DQN从经验回放池中均匀采样。理想情况下，我们希望更频繁地对那些需要学习错误的样本进行采样。作为学习的智能体，优先经验回放（Schaul et al.2015）以相对于上次遇到的绝对TD误差的概率$p_t$采样：</p><script type="math/tex; mode=display">p_t \propto |R_{t+1} + \gamma_{t+1} \underset{a'}{max~} q_\theta^-(S_{t+1}, a') - q_\theta(S_t, A_t)|^w</script><p>其中ω是决定分布形状的超参数。新的样本以最大优先级（权重）插入到重放缓冲区中，从而提供对最近样本的偏差。请注意，随机采样也可能是受欢迎的，即使在对它们的了解所剩无几的情况下。</p><h3 id="3-3-Dueling网络"><a href="#3-3-Dueling网络" class="headerlink" title="3.3 Dueling网络"></a>3.3 Dueling网络</h3><p>对决网络是为基于价值的强化学习神经网络架构。它具有两个计算输出，即状态价值输出和最优势输出，共享部分卷积层，并由一个特殊的聚合器合并（Wang，2016）。这对应于以下动作值的分解：</p><script type="math/tex; mode=display">q_\theta(s,a)=v_\eta f_\xi(s) + a_\psi(f_\xi(s)-a)-\frac{\sum_{a'} a_\psi(f_\xi(s), a')}{N_{actions}}</script><p>其中$ξ$、$η$和$ψ$分别是共享卷积层$f_ξ$、状态价值输出$v_η$和优势输出$a_ψ$的参数；$θ={ξ，η，ψ}$是它们的连接。</p><h3 id="3-4-多步TD"><a href="#3-4-多步TD" class="headerlink" title="3.4 多步TD"></a>3.4 多步TD</h3><p>Q学习累积一个奖励，然后在下一步使用贪婪策略来自举更新动作。或者，可以使用前视多步目标（Sutton 1988）。我们将给定状态$S_t$的选择n步奖励定义为：</p><script type="math/tex; mode=display">R_t^{(n)} = \sum_{k=0}^{n-1} \gamma_t^{(k)} R_{t+k+1}     ~~~~~~~~~~~~~~~~~(2)</script><p>然后通过最小化损失来定义DQN的多步TD损失函数，</p><script type="math/tex; mode=display">(R_t^{(n)}+\gamma_t^{(n)}~ \underset{a'}{q_\theta^-}(S_{t+n}, a') - q_\theta(S_t, A_t))^2</script><p>具有适当调整n的多步目标通常会导致更快的学习（Sutton和Barto 1998）。</p><h3 id="3-5-分布式RL"><a href="#3-5-分布式RL" class="headerlink" title="3.5 分布式RL"></a>3.5 分布式RL</h3><p>我们可以学习近似收益的分布而不是预期收益。最近，Bellemare、Dabney和Munos（2017）提出将概率分布放置在离散的$z$上，对这种分布进行建模，其中$z$是带有$N_{atoms} \in N^+$原子，定义为 $z^i=v_{min}+(i-1)\frac{v_{max}-v_{min}}{N_{atoms}-1}$，其中，$i \in \{1,…, N_{atoms}\}$. 时间t处的近似分布$d_t$是在该支持下定义的，每个原子i上的概率密度为$p^i_θ(S_t, a_t)$，使得$d_t=(z, p_θ(S_t, a_t))$。目标是更新θ，使该分布与实际收益分布紧密匹配。</p><p>要了解概率密度，关键是返回的分布满足Bellman方程的一个变体。对于给定状态$S_t$和动作$A_t$，最优策略$π^<em>$下的收益分布，应匹配通过下一状态$S_{t+1}$的分布和动作$a^</em>_{t+1}=\pi^<em>(S_{t+1})$定义的目标分布，根据折扣系数将其收敛，并根据奖励（或随机情况下的奖励分配）将计算奖励。然后，通过首先构建对目标分布，然后最小化分布$d_t$和目标分布之间的Kullbeck-Leibler发散，目标Q学习的分布变体$ d’_t=(R_{t+1} + \gamma_{t+1} z, p^-_\theta(S_{t+1}, \overline a^</em>_{t_1}))  $：</p><script type="math/tex; mode=display">D_{KL}(\Phi_z d'_t || d_t) ~~~~~~~~~~ (3)</script><p>这里，$Φ_z$是目标分布在z上的L2范数，再状态$S_{t+1}$， $a^∗_{t+1}=\underset{a}{argmax~}q_θ^-(S_{t+1},a)$）是相对于平均值$q_{\overline θ} = z^T p_\theta(S_{t+1},a)$更真实。</p><p>与不同分布情况一样，我们可以使用参数$\overline θ$的冻结副本来构建目标分布。参数化分布可以用神经网络表示，如在DQN中，但使用$N_{atoms}×N_{actions}$输出。softmax独立地应用于输出的每个动作维度，以确保每个动作的分布被适当地归一化。</p><h3 id="3-6-噪声网络"><a href="#3-6-噪声网络" class="headerlink" title="3.6 噪声网络"></a>3.6 噪声网络</h3><p>探索使用的局限性$  \epsilon -greedy $ 策略在诸如《蒙特祖马的复仇》这样的游戏中是明确的，在游戏中必须执行许多动作才能获得第一笔奖励。Noisy Nets（Fortunato，2017）提出了一种噪声线性层，</p><script type="math/tex; mode=display">y=(b+wx)+(b_{noisy} \odot \epsilon^b + (w_{noisy} \odot \epsilon^w)x), ~~~~~~~(4)</script><p>其中，$\epsilon^b$和$\epsilon^w$是随机变量，$\odot$ 表示元素乘积。然后可以使用该变换来代替标准线性$y＝b+wx$。随着时间的推移，网络可以学会忽略噪声流，但会在状态空间的不同部分以不同的速率这样做，从而允许以自退火的形式进行状态条件探索。</p><h2 id="4-集成智能体"><a href="#4-集成智能体" class="headerlink" title="4.集成智能体"></a>4.集成智能体</h2><p>在本文中，我们将上述所有组件集成到一个集成智能体中，我们称之为Rainbow。</p><p>首先，我们将1-step 分布损失（3）替换为多步变量。我们通过根据累积折扣收缩$S_{t+n}$中的价值分布，并将其采样为截断的n步折扣收益来构建目标分布。这对应于将目标分布定义为$d^{(n)}_t = (R_t^{(n)} - \gamma_t^{(n)}z, p_\theta^-(S_{t+n}, a^*_{t+n}))$，损失为：</p><script type="math/tex; mode=display">D_{KL}(\Phi_zd_t^{(n)} || d_t)</script><p>其中，$Φz$是z的投影。</p><p>我们通过使用根据 <em>在线网络</em> 选择的$S_{t+n}$中的贪婪动作$a^*_{t+n}$作为目标动作，将多步分布损失与双Q学习相结合， 并使用目标网络评估这种动作。</p><p>在标准比例优先经验重放（Schaul et al.2015）中，绝对TD误差被用于确定采样的优先级。这可以在分布设置中使用平均作用值进行计算。然而，在我们的实验中，所有分布 Rainbow 变种都通过KL损失来优先考虑过渡，因为这是算法所最小化的：</p><script type="math/tex; mode=display">p_t \propto (D_{KL}(\Phi_zd_t^{(n)} || d_t))^w</script><p>作为优先权的KL损失可能对有噪声的随机环境更具鲁棒性，因为即使在收益不确定的情况下，损失也会继续减少。</p><p>网络结构是一种分布式Q函数和对决网络一起使用的结构。网络具有共享卷积层$f_ξ(s)$，然后输出$ N_{atoms}<br>  $维的状态价值$v_η$和$ N_{atoms}<br>  $维的优势头$a_\xi$，其中$a^i_ξ(f_ξ(s),a)$将表示对应于原子i和动作a的输出，对于每个原子$z_i$，如在对决DQN中一样，将价值流和优势流聚合，然后通过softmax层，以获得用于估计收益分布的归一化参数分布</p><script type="math/tex; mode=display">p_\theta^i(s,a)=\frac{exp(v_\eta^i(\phi) + a^i_\psi(\phi,a) - \overline a^i_\psi(s))}{\sum_j exp(v_\eta^i(\phi) + a^i_\psi(\phi,a) - \overline a^i_\psi(s))}</script><p>其中$  \phi=f_\xi(s) $,  $\overline a^i_\phi(s)=\frac{1}{N_{actions}} \sum_{a’} a^i_\psi(\phi, a’)$</p><p>然后，我们用等式（4）中描述的噪声等效物替换所有线性层。在这些噪声线性层中，我们使用因子化高斯噪声（Fortunato，2017）来减少独立噪声变量的数量。</p><h2 id="5-实验方法"><a href="#5-实验方法" class="headerlink" title="5.实验方法"></a>5.实验方法</h2><p>我们现在描述用于配置和评估集成智能体的方法和设置。</p><h3 id="5-1-评估方法"><a href="#5-1-评估方法" class="headerlink" title="5.1 评估方法"></a>5.1 评估方法</h3><p>我们评估了57款雅达利游戏中2600个智能体上的效果（Bellemare，2013）。我们遵循Mnih等人（2015）和van Hasselt等人（2016）的训练和评估程序。在训练期间，通过暂停学习并评估500K帧的最新智能体，在环境中每1M步评估智能体的平均分数。如van Hasselt等人（2016）所述，训练轮次在108K帧（或30分钟的模拟播放）时被截断。</p><p>智能体的得分在每场游戏中被标准化，因此0%对应于随机代理，100%对应于人类专家的平均得分。标准化分数可以在所有Atari级别上进行汇总，以比较不同代理的性能。跟踪所有游戏中人类正常化表现的中位数是很常见的。我们还考虑了智能体的表现高于人类表现的一小部分的游戏数量，以理解中位数的改进来自何处。人类正常化的平均表现可能信息量较小，因为它主要由一些游戏（如亚特兰蒂斯）主导，在这些游戏中，智能体的得分比人类高出几个数量级。</p><p>除了跟踪中值性能作为环境步骤的函数之外，在训练结束时，我们使用两种不同的测试方案重新评估最佳智能体。在无操作启动机制中，我们在每个轮次开始时插入一个随机数（最多30个）的无操作操作（正如我们在训练中所做的那样）。在人类开始游戏中，使用从人类专家轨迹的初始部分随机采样的点来初始化每个轮次初始值（Nair等人，2015）；这两种方法之间的差异表明，智能体在多大程度上对自己的轨迹过拟合。</p><p>由于空间限制，我们专注于游戏的总体结果。然而，在附录中，我们提供了所有游戏和所有智能体的完整学习曲线，以及无操作和人工启动测试制度中原始和标准化分数的详细对比表。</p><h3 id="5-2-超参数调整"><a href="#5-2-超参数调整" class="headerlink" title="5.2 超参数调整"></a>5.2 超参数调整</h3><p>Rainbow的所有组件都有许多超参数。超参数的组合空间太大，无法进行穷举搜索，因此我们进行了有限的调整。对于每个组件，我们从介绍该组件的文章中使用的值开始，并通过手动坐标下降调整超参数中最敏感的参数。</p><p>DQN及其变体在前200K轮次期间不执行学习更新，以确保足够不相关的更新。我们发现，有了优先经验回放，可以在80K轮次后更快地开始学习。</p><p>DQN从探索开始$\epsilon$对应于随机均匀作用；它将前4M帧上的探测量退火到最终值0.1（在以后的变型中降低到0.01）。无论何时使用噪声的网络，我们都采取贪心策略（$\epsilon = 0$）， $σ_0$超参数的值为0.5，用于初始化噪声中的权重。对于没有噪声网络的智能体，我们使用$\epsilon - greedy$策略，但是降低探索速度比以前更快，在前250K帧中为0.01。</p><p>我们使用了Adam优化器（Kingma和Ba 2014），我们发现它对学习率的选择比RMSProp更不敏感。DQN使用$α=0.00025$的学习率。在所有Rainbow变体中，我们使用$α/4$的学习率，选自$｛α/2、α/4、α/6｝$，超参数Adam的值为$1.5×10^{-4}$。</p><p>对于优先级经验回放，我们使用推荐的比例变量，优先级指数$ω$为0.5，并在训练过程中将重要性采样指数$β$从0.4线性增加到1。比较${0.4、0.5、0.7}$的值，调整优先级指数$ω$。使用分布DQN的KL损失作为优先级，我们观察到性能对ω的选择非常稳健。</p><p>多步TD中n的值是Rainbow的一个敏感超参数。我们比较了n=1、3和5的值。我们观察到，n=3和5最初都表现得很好，但到最后，总体上n=3表现得最好。</p><p>超级参数（见表1）在所有57款游戏中都是相同的，即Rainbow智能体实际上是一个在所有游戏中都表现良好的单一智能体设置。</p><p>表1 Rainbow 超参数</p><p><img src="image/image_be-fxwSpIp.png" alt=""></p><p><img src="image/image_cpWH5yX0p1.png" alt=""></p><p>图2：每一个图都显示了几个智能体的游戏数量，其中他们至少达到了人类性能的给定分数，作为时间的函数。从左到右，我们考虑20%、50%、100%、200%和500%的阈值。在第一行，我们将Rainbow与基线进行比较。在第二排，我们将Rainbow与它的消融进行了比较。</p><h2 id="6-分析"><a href="#6-分析" class="headerlink" title="6.分析"></a>6.分析</h2><p>在本节中，我们分析了主要的实验结果。首先，我们表明Rainbow比已发布的DQN变体更受欢迎。然后我们进行消融研究，比较几种不同的消融，每种消融都对应于从Rainbow中去除一种成分。</p><h3 id="6-1-与已发布baseline的比较"><a href="#6-1-与已发布baseline的比较" class="headerlink" title="6.1 与已发布baseline的比较"></a>6.1 与已发布baseline的比较</h3><p>在图1中，我们将Rainbow的表现（根据游戏中人类归一化得分的中位数来衡量）与A3C、DQN、DDQN、优先DDQN和对决DDQN的相应曲线进行了比较。我们感谢Dueling和Prioritized代理的作者提供了这些代理的学习曲线，并报告了我们自己对DQN、A3C、DDQN、Distributional DQN和Noisy DQN的重新运行。Rainbow的性能在数据效率和最终性能方面都明显优于任何基线。请注意，我们在7M帧后匹配了DQN的最终性能，在44M帧中超过了这些基线的最佳最终性能，并达到了显著改进的最终性能。</p><p>在对智能体的最终评估中，训练结束后，Rainbow在无操作制度下的中位数得分为223%；在人类起步阶段，我们测得的中位数为153%。在表2中，我们将这些分数与个别基线的公布中值分数进行了比较。</p><p>在图2（顶行）中，我们绘制了智能体达到人类标准化性能特定水平的游戏数量。从左到右，子图显示了不同代理实现了20%、50%、100%、200%和500%人类标准化性能的游戏数量。这使我们能够确定性能的总体改进来自何处。请注意，Rainbow和其他智能体之间的性能差距在各个级别的性能上都是显而易见的：Rainbow智能体正在提高基线智能体已经很好的游戏的分数，以及在基线智能体与人类性能相差很远的游戏中的分数。</p><h3 id="6-2-学习速度"><a href="#6-2-学习速度" class="headerlink" title="6.2 学习速度"></a>6.2 学习速度</h3><p>与最初的DQN设置一样，我们在单个GPU上运行每个智能体。匹配DQN最终性能所需的7M帧对应于不到10小时的时间。200M帧的完整运行对应于大约10天，并且在所有讨论的变体之间，这一变化小于20%。文献中包含了许多可替代的训练设置，它们通过利用并行性来提高作为挂钟时间函数的性能，例如，Nair（2015）、Salimans（2017）和Mnih（2016）。在如此不同的硬件/计算资源之间正确地关联性能是非常重要的，因此我们只关注算法变化。虽然我们认为它们是重要的和互补的，但我们将可伸缩性和并行性的问题留给未来的工作。</p><p>表2：Rainbow和基线的最佳智能体的中位数标准化分数。对于标有星号的方法，分数来自相应的论文。DQN的分数来自对决网络的论文，因为DQN论文没有报告所有57场比赛的分数。其他分数来自我们自己的实现。</p><p><img src="image/image_TwrSrMRwve.png" alt=""></p><p><img src="image/image_DgM8zGSGEA.png" alt=""></p><p>图3：57款雅达利游戏中人类正常化表现的中位数，作为时间的函数。我们将我们的集成智能体（彩虹色）与DQN（灰色）和六种不同的消融（虚线）进行比较。在5个点上使用移动平均值平滑曲线。</p><h3 id="6-3-消融研究"><a href="#6-3-消融研究" class="headerlink" title="6.3 消融研究"></a>6.3 消融研究</h3><p>由于Rainbow将几个不同的想法集成到一个智能体中，我们进行了额外的实验，以了解在这种特定组合的背景下，各个组件的贡献。</p><p>为了更好地了解每个成分对Rainbow的贡献，我们进行了消融研究。在每次消融中，我们从完整的Rainbow组合中移除一个组件。图3显示了完整Rainbow和六种消融变体的中位数标准化评分的比较。图2（下一行）显示了这些消融如何相对于人类标准化性能的不同阈值进行的更详细的细分，图4显示了每个游戏中每次消融的增益或损失，在整个学习过程中的平均值。</p><p>优先经验回放和多步TD是Rainbow最关键的两个组成部分，因为删除其中一个组成部分会导致中位数的大幅下降。毫不奇怪，这两个因素的移除都会影响早期表现。也许更令人惊讶的是，取消多步TD也会影响最终成绩。放大单个游戏（图4），我们看到两个组件在整个游戏中的帮助几乎是一致的（在57场游戏中，完整的彩虹在53场游戏中的表现优于消融）。</p><p>分布Q学习在与智能体性能相关的100%之前的技术中排名靠后。值得注意的是，在早期学习中，没有明显的差异，如图3所示，在最初的4000万帧中，分布消融与完全消融效果一样好。然而，如果没有分布Q学习，智能体的性能就会开始落后。当结果与图2中的人类表现相分离时，我们发现分布消融主要似乎滞后于高于人类水平或接近人类水平的游戏。</p><p>就中位数表现而言，包括噪声网络时，智能体表现更好；当这些被移除并且探索使用传统的 $\epsilon - greedy$ 策略，总体性能较差（图3中的红线）。虽然移除“噪声网络”导致几场游戏的成绩大幅下降，但在其他游戏中也有小幅上升（图4）。</p><p>总的来说，当从完整的Rainbow中移除对决网络时，我们没有观察到显著的差异。然而，如图4所示，中位数分数掩盖了对决网络的影响在不同游戏之间存在差异的事实。图2显示，对决网络可能在高于人类表现水平的游戏（#games\&gt;200%）上有所改善，而在低于人类表现的游戏（#games\&gt;20%）上有所下降。</p><p>同样，在双Q学习的情况下，观察到的中位性能差异（图3）是有限的，根据游戏的不同，组件有时会导致性能下降或帮助（图4）。为了进一步研究双重Q 学习的作用，我们将我们训练过的智能体的预测与根据折扣奖励计算的实际折扣回报进行了比较。将Rainbow与双Q学习被消融的智能体进行比较，我们发现实际回报通常高于10，因此不在分布的支持范围内，从−10到+10。这导致回报被低估，而不是高估。我们假设，将值限制在这个限制范围内可以抵消Q学习的高估偏差。然而，请注意，如果分布的支持扩大，双Q学习的重要性可能会增加。</p><p>在附录中，我们展示了每个游戏的最终表现和Rainbow的学习曲线、消融和基线。</p><p><img src="image/image_l5uqxT0nbP.png" alt=""></p><p>图4：所有57款雅达利游戏中消融的性能下降。性能是学习曲线下的区域，相对于Rainbow智能体和DQN进行标准化。DQN表现优于Rainbow的两个游戏被省略。每场比赛都会突出显示导致最强跌落的消融。取消优先级或多步TD会降低大多数游戏的性能，但每个组件的贡献因游戏而异。</p><h2 id="7-讨论"><a href="#7-讨论" class="headerlink" title="7.讨论"></a>7.讨论</h2><p>我们已经证明，DQN的若干改进可以成功地集成到单个学习算法中，从而实现最先进的性能。此外，我们已经表明，在集成算法中，除了一个组件之外，所有组件都提供了明显的性能优势。还有许多我们无法包含的算法组件，这将是集成智能体进一步实验的有希望的候选。在众多可能的候选人中，我们将在下面讨论几个。</p><p>我们在这里关注的是Q学习家族中基于价值的方法。我们没有考虑纯粹基于策略的RL算法，如信任区域策略优化（Schulman，2015），也没有考虑行actor-critic方法（Mnih，2016；O’Donoghue，2016）。</p><p>许多算法利用数据序列来提高学习效率。最优收紧（He et al.2016）使用多步收益来构建额外的不等式边界，而不是使用它们来代替Q学习中使用的1步目标。跟踪允许n步回报的软组合（Sutton 1988）。然而，与Rainbow中使用的多步目标相比，顺序方法在每个梯度上都利用了更多的计算。此外，引入优先序列重放提出了如何存储、重放和优先序列的问题。</p><p>偶发控制（Blundell，2016）也关注数据效率，并显示在某些领域非常有效。它通过使用情景记忆作为补充学习系统来改善早期学习，能够立即重演成功的动作序列。</p><p>除了Noisy Nets，许多其他探索方法也可能是有用的算法成分：在这些Bootstrapped DQN（Osband等人2016）、内在动机（Stadie、Levine和Abbeel 2015）和基于计数的探索（Bellemare等人2016）中。这些替代组件的集成是进一步研究的富有成果的课题。</p><p>在本文中，我们专注于学习更新，而没有探索其他计算架构。在A3C（Mnih等人，2016）、Gorila（Nair等人，2015）或Evolution Strategies（Salimans等人，2017）中，从环境的并行副本进行异步学习可以有效地加速学习，至少在时间方面是如此。然而，请注意，它们的数据效率可能较低。</p><p>分层RL也成功地应用于几个复杂的雅达利游戏。在HRL的成功应用中，我们重点介绍了h-DQN（Kulkarni等人，2016a）和Feudal Networks（Vezhnevets等人，2017）。</p><p>通过利用辅助任务，如像素控制或特征控制（Jaderberg et al.2016）、监督预测（Dosovitskiy and Koltun 2016）或后续特征（Kulkarni et al.2016b），状态表示也可以变得更有效。</p><p>为了根据基线公平地评估Rainbow，我们遵循了折扣奖励、固定动作重复和帧堆叠的常见领域修改，但这些可能会被其他学习算法改进所消除。波普艺术规范化（van Hasselt et al.2016）允许删除奖励这苦，同时保持类似的表现水平。细粒度动作重复（Sharma、Lakshminarayanan和Ravindran 2017）能够学习如何重复动作。循环状态网络（Hausknecht and Stone 2015）可以学习时间状态表示，取代观测帧的固定堆栈。总的来说，我们认为将真实游戏暴露给智能体是未来研究的一个有希望的方向。</p><h2 id="8-参考文献"><a href="#8-参考文献" class="headerlink" title="8.参考文献"></a>8.参考文献</h2>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> PaperReading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 一种简单的蒙特卡洛树搜索并行化方法</title>
      <link href="/paper_reading/3.1.WU_UCT/"/>
      <url>/paper_reading/3.1.WU_UCT/</url>
      
        <content type="html"><![CDATA[<h1 id="监控未观察样本-一种简单的蒙特卡洛树搜索并行化方法"><a href="#监控未观察样本-一种简单的蒙特卡洛树搜索并行化方法" class="headerlink" title="监控未观察样本: 一种简单的蒙特卡洛树搜索并行化方法"></a>监控未观察样本: 一种简单的蒙特卡洛树搜索并行化方法</h1><h1 id="Watch-the-Unobserved-a-Sample-Approach-to-Parallelizing-Monte-Carlo-TreeSearch"><a href="#Watch-the-Unobserved-a-Sample-Approach-to-Parallelizing-Monte-Carlo-TreeSearch" class="headerlink" title="Watch the Unobserved: a Sample Approach to Parallelizing Monte Carlo TreeSearch"></a>Watch the Unobserved: a Sample Approach to Parallelizing Monte Carlo TreeSearch</h1><p>论文地址：<a href="https://openreview.net/forum?id=BJlQtJSKDB">https://openreview.net/forum?id=BJlQtJSKDB</a></p><p>Github ：<a href="https://github.com/liuanji/WU-UCT">https://github.com/liuanji/WU-UCT</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>蒙特卡洛树搜索 (MCTS) 算法在许多具有挑战性的基准测试上(例如,围棋等)取得了巨大成功.然而,它们通常需要大量部署,这使得它们的应用成本很高. 此外,由于 MCTS固有的顺序性质,并行化 MCTS 也极具挑战性:每次模拟都严重依赖从先前模拟的数据 (例如,节点访问计数),用于实现有效的探索和利用的权衡. 尽管存在这些困难,我们还是开发了一种算法WU-UCT, 来有效地并行化 MCTS, 它实现了线性加速,并且随着线程数量的增加表现出有限的性能损失. WU-UCT的关键思想是我们引入一组统计数据来跟踪正在进行未结束的模拟样本(称为未观察样本) 的数量. 当我们将最耗时的扩展和模拟步骤并行化时,这些数据被用来修正UCT算法中的选择步骤, 以保证正确的探索和利用的权衡. 在“JOY CITY” 游戏基准和 Atari Game 基准上的实验证明了 WU-UCT与传统的UCT算法相比, 在线性加速和性能方面有明显的提升.</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract:"></a>Abstract:</h2><p>Monte Carlo Tree Search (MCTS) algorithms have achieved great success on many challenging benchmarks (e.g., Computer Go). However, they generally require a large number of rollouts, making their applications costly. Furthermore, it is also extremely challenging to parallelize MCTS due to its inherent sequential nature: each rollout heavily relies on the statistics (e.g., node visitation counts) estimated from previous simulations to achieve an effective exploration-exploitation tradeoff. In spite of these difficulties, we develop an algorithm, WU-UCT1, to effectively parallelize MCTS, which achieves linear speedup and exhibits only limited performance loss with an increasing number of workers. The key idea in WU-UCT is a set of statistics that we introduce to track the number of on-going yet incomplete simulation queries (named as unobserved samples). These statistics are used to modify the UCT tree policy in the selection steps in a principled manner to retain effective exploration-exploitation tradeoff when we parallelize the most time-consuming expansion and simulation steps. Experiments on a proprietary benchmark and the Atari Game benchmark demonstrate the linear speedup and the superior performance of WU-UCT comparing to existing techniques.</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><p>最近, 蒙特卡罗树搜索 (MCTS) 算法, 如UCT (Kocsis, 2006) , 在解决许多具有挑战性的人工智能 (AI) 问题方面取得了巨大成功, 包括视频游戏 (Guo, 2016) 和围棋 (Silver, 2016年) . 然而, 它们依赖于大量的与环境交互 (例如数百万), 来构建用于决策的搜索树, 这导致了很高的时间复杂性 (Browne, 2012). 由于这个原因, 并行化MCTS的需求越来越大. 然而, 在不降低性能的情况下并行化 MCTS 是困难的 (Segal, 2010; Mirsoleimani, 2018a; Chaslot, 2008), 主要是因为每次 MCTS 迭代都需要来自先前迭代的信息以提供有效的探索和利用. 具体来说, 并行化 MCTS 将不可避免地覆盖这些迭代信息, 我们将在第 2.2 节中展示这种信息丢失可能会导致性能显着下降. 因此, 关键问题是如何获取和利用更多可用信息来消除并行化造成的信息损失, 并帮助算法实现更好的探索和利用的权衡.</p><p>为此, 我们提出了 WU-UCT (Watch the Unobserved in UCT), 这是一种新颖的并行 MCTS 算法, 可以在有限的性能损失下实现线性加速. 这是通过概念创新 (第 3.1 节) 以及有效的真实系统运行 (第 3.2 节) 来实现的. 具体来说, WU-UCT 克服上述挑战的关键思想是用于跟踪正在进行但未完成的模拟样本数量, 称为未观察样本. 我们将这些新的样本与原始观察样本相结合, 用来修正UCT算法中的选择步骤, 正如我们将在第 4 节中展示的那样, 这在并行化过程中有效地保持了探索和利用的权衡. 我们提出的方法已成功部署在真实的系统中, 用于有效且准确地评估手机游戏“Joy City”中用户通过关卡的比率 (称为用户通过率), 目的是减少他们的设计周期. 在这个基准上, 我们展示了 WU-UCT 在预测用户通过率 (第 5.1 节) 方面实现了接近最优的线性加速和卓越的性能. 我们在 Atari Game 基准上进一步评估 WU-UCT, 并将其与最先进的并行 MCTS 算法 (第 5.2 节) 进行比较, 这也证明了我们卓越的加速和性能.</p><h2 id="2-并行化MCTS的困难"><a href="#2-并行化MCTS的困难" class="headerlink" title="2.并行化MCTS的困难"></a>2.并行化MCTS的困难</h2><p>我们首先介绍 MCTS 和 UCT 算法, 以及它们在并行化方面的困难.</p><h3 id="2-1-蒙特卡洛树搜索和上限置信度-UCT"><a href="#2-1-蒙特卡洛树搜索和上限置信度-UCT" class="headerlink" title="2.1 蒙特卡洛树搜索和上限置信度 (UCT)"></a>2.1 蒙特卡洛树搜索和上限置信度 (UCT)</h3><p>马尔可夫决策过程(MDP) $&lt; S,\ A,\ R,\ P,\ \gamma &gt;$,智能体与环境交互以最大化长期累积奖励. 具体来说, 智能体在状态$s_{t} \in S\ $下根据策略 $\pi$ 选择动作 $a_{t} \in A$,故通过MDP转换到下一个状态$s_{t + 1}\sim P\left( \left. \ S_{t + 1} \right|S_{t},\ a_{t} \right)$,并且获得奖励$R\left( s_{t},\ a_{t} \right)$.智能体的目标时学习一个最优策略$\ \pi^{*}$, 使得长期累积奖励最大化:</p><script type="math/tex; mode=display">\max_{\pi}{\ E}_{a_{t} \sim \pi,\ s_{t + 1}}P\lbrack\sum_{t = 0}^{\infty}{\gamma^{t}R\left( s_{t},\ a_{t} \right)|s_{0} = s}\rbrack</script><p>其中 $s\  \in \ S$ 表示初始状态, $\gamma\  \in \ (0,\ 1\rbrack$是折扣因子. 为了解决上述问题, 已经开发了许多强化学习 (RL) 算法(Sutton&amp;Barto, 2018) , 包括无模型算法 (Mnih, 2013；2016；Williams,1992；Konda&amp;Tsitsiklis, 2000；Schulman, 2015；2017) 和基于模型的算法(Nagabandi, 2018；Weber, 2017；Bertsekas,2005；Deisenroth&amp;Rasmussen, 2011) . 蒙特卡罗树搜索 (MCTS)是一种基于模型的RL算法, 它选取每个动作的最优动作 (Browne et al., 2012).具体来说, 它使用MDP模型 (或采样) 通过构建搜索树 (图1(a))来确定每个步骤的最优动作, 其中每个节点 $s$ 表示一个动作 $a$ 访问状态,<br>从$s$开始的每个边表示一个可以在该状态下采取的操作,以及节点$s’$表示采取动作$a$后的状态. 如图1(a) 所示,MCTS重复执行四个顺序步骤: 选择、扩展、模拟和反向传播.选择步骤是遍历现有搜索树, 直到满足叶节点 (或其他终止条件) ,方法是根据树策略选择每个节点的动作 (边) .一种广泛使用的节点选择策略是树的置信上限 (UCT) (Kocsis, 2006年) :</p><script type="math/tex; mode=display">a_{s} = \arg{\operatorname{}{\{ V_{s^{'}} + \beta\ \sqrt{\frac{2\log N_{s}}{N_{s'}}}\}}}</script><p>其中 $C(s)$ 表示 $s$ 的所有子节点集; 第一项 $V_{s’}$ 表示从节点 $s’$状态开始可以收到的长期累积奖励的估计值, 第二项表示估计的不确定性(置信区间的大小). 根据置信上限计算置信区间(UCT), 使用 $N_{s}$ 和$N_{s’}$, 表示节点 $s$ 和 $s’$ 的被访问次数. 因此,UCT策略的关键思想是根据预期回报的估计 (即, 置信上限) 选择最佳动作,从而在利用 (第一项) 和探索 (第二项) 之间取得平衡, $\beta$ 控制平衡.一旦选择过程到达搜索树的叶子节点 (或满足其他终止条件),我们将根据先前的策略通过添加新的叶子节点来扩展节点. 然后, 在模拟步骤中,我们通过在环境中使用默认 (模拟) 策略运行模拟来估计其值函数 (累积奖励)${\hat{V}}_{s}$ 最后, 再反向传播,将$V_{s}$和$N_{s}$从叶子节点$s_{T}$递归的更新到所选路径的根节点$s_{0}$($t \sim \lbrack 0,\ T - 1\rbrack$)</p><script type="math/tex; mode=display">N_{s_{T}} \leftarrow N_{s_{t}} + 1, {\hat{V}}_{s_{t}} \leftarrow R\left( s_{t},\ a_{t} \right) + \gamma\ {\hat{V}}_{s_{t + 1}},V_{s_{t}} \leftarrow (\left( N_{s_{t}} - 1 \right)V_{s_{t}} + \ {\hat{V}}_{s_{t}}\ )/N_{s_{t}}</script><p>其中, ${\hat{V}}_{s_{T}}$是$s_{T}$的模拟返回； $a_{t}$表示在状态$s_{t}$下, 在 (2) 之后选择的动作.</p><p><img src="images/1685880394192.png" alt="1685880394192"></p><ul><li>a.每个 (非并行) MCTS操作由四个连续步骤组成: 选择、扩展、模拟和反向传播, 其中扩展和模拟步骤通常最耗时.</li></ul><p><img src="images/1685880420448.png" alt="1685880420448"></p><p>图1: MCTS及其并行化. (a) MCTS概述. (b) 理想的并行化:假设最新的统计数据$\{ V_{s},N_{s}\}$ (彩色)在模拟开始时就可供所有线程使用 (实际上不现实). (c)并行化MCTS的关键挑战是: 线程只能访问过时的$\{ V_{s},N_{s}\}$ (灰色),导致诸如探索失败之类的问题. (d) WU-UCT跟踪不完整模拟查询的数量(表示为$O_{s}$ ), 并以原则方式修改UCT策略, 以保持有效的探索和利用的权衡.它实现了与理想的并行化相当的加速和性能.</p><h3 id="2-2-MCTS并行化的本质困难"><a href="#2-2-MCTS并行化的本质困难" class="headerlink" title="2.2  MCTS并行化的本质困难"></a>2.2  MCTS并行化的本质困难</h3><p>上述讨论表明, MCTS算法本质上是顺序的:新扩展中的每个选择步骤都需要完成之前的扩展,以便为UCT树策略提供更新的数据 $V_{s}$ 和 $N_{s}$.尽管最新数据的要求不是强制性的,但实际上需要它来实现有效的探索和利用的权衡 (Auer, 2002年). 具体来说,最新数据有助于UCT树策略识别和删除非奖励的分支,广泛访问奖励路径以获得更多深度. 同样,为了实现最佳性能, 当使用多个线程时,还必须确保每个线程在自己的选择步骤中使用最新的数据 (图1 (b) 中的彩色$V_{s}$ 和 $N_{s}$). 然而, 根据以下观察结果, 将MCTS并行化是不可能的.首先, 与其他两个步骤相比,扩展步骤和模拟步骤通常更耗时, 因为它们涉及与环境 (或模拟器)的大量交互. 因此, 如图1(c) 所示, 当线程C启动选择一个新步骤时,其他线程A和B很可能仍处于模拟或扩展步骤中. 这会阻止他们更新其他线程 (如C)的 (全局) 数据信息. 在不同的线程中使用过时的数据信息 (灰色的 $\text{Vs}$ 和  $\text{Ns}$), 如果设定固定的加速目标的情况下, 可能由于探索和利用失败,导致性能大幅度下降, 我们将在第4节中进行详细讨论. 举个例子, 图1 (c)说明了探索的失败, 由于线程C在其选择步骤中与线程A穿过相同的路径.具体来说, 如果线程A和C在开始自己的选择步骤之间的数据信息没有变化,他们将选择相同的节点, 这将大大减少探索的多样性. 因此,我们在并行化MCTS时要解决的关键问题是如何跟踪正确的数据信息,并以正确性的方式修正UCT策略, 以在不同线程之间保持有效的探索和利用的权衡.</p><h2 id="3-WU-UCT"><a href="#3-WU-UCT" class="headerlink" title="3.WU-UCT"></a>3.WU-UCT</h2><p>在本节中, 我们首先提出了WU-UCT算法的概念 (第3.1节) , 然后我们介绍了一个使用Master-Worker（主-辅）架构的真实系统的实现 (第3.2节) .</p><h3 id="3-1-UCT树中未观察样本"><a href="#3-1-UCT树中未观察样本" class="headerlink" title="3.1 UCT树中未观察样本"></a>3.1 UCT树中未观察样本</h3><p>正如我们前面指出的, 在并行化MCTS时,我们要解决的关键问题是如何向每个线程提供最新的数据$\{ Vs,\ Ns\}$,以便他们能够在其选择步骤中实现有效的探索和利用权衡. 在图1(b)中的理想并行化中, 假设情况就是这样. 算法上, 它与顺序MCTS等效,由不同的线程并行执行. 不幸的是, 在实践中,每个线程可用的数据$\{ Vs,\ Ns\}$通常已经过时,因为其他线程的模拟和扩展步骤缓慢且不完整. 具体来说,由于在模拟完成之前无法观察到估计值$\ {\hat{V}}_{s}$,并且线程不应等待更新的数据继续进行, 因此数据$\{ Vs,\ Ns\}$的 (部分)丢失是不可避免的. 现在的问题是:有没有其他办法来解决这个问题？答案是肯定的, 解释如下.</p><p>为了弥补一般并行化和理想情况之间的差距,我们仔细检查了它们在统计数据可用性方面的差异. 如统计数据的颜色所示,它们在 $\{ Vs,\ Ns\}$ 中的唯一差异是由正在进行的模拟过程引起的.尽管只有在模拟步骤完成后才能更新 $\text{Vs}$,但实际上只要线程启动新的扩展, 就可以使用最新的 $\text{Ns}$ 信息.这是我们用来在WU-UCT算法中实现有效并行化的关键. 基于此,我们引入了另一个变量 $\text{Os}$, 以计算已启动但尚未完成的访问的数量,我们称之为未观察样本. 也就是说, 我们的新统计数据 $\text{Os}$监控未观察到的样本数, 然后用于将UCT树策略更正为以下形式:</p><script type="math/tex; mode=display">a_{s} = \max_{s^{'} \sim C(s)}\{ V_{s^{'}} + \beta\sqrt{\frac{2lo\operatorname{g}\left( N_{s} + O_{s} \right)}{N_{s'} + O_{s'}}}\}</script><p>上述修改后的节点选择策略是, 当有 $Os$ 个线程模拟 (查询) 节点时, 节点 $s$的置信区间最终会在完成模拟后缩小. 因此, 添加 $\text{Os}$ 和 $Os’$对于探索来说, 要事先考虑这样一个事实, 并让其他线程意识到这一点.尽管形式简单, 但它提供了一种正确的方法,以在并行环境下保持有效的探索和利用的权衡；它纠正了探索和利用的权衡的置信度.由于置信度会立即更新 (即在模拟开始时),因此可以保证最新启动的线程可以获得到额外的统计数据,这会阻止他们广泛查询同一个节点以及找到更好的节点供他们查询. 例如,当需要多个分支进行探索时, 允许他们被均匀地探索. 相反,当一个节点被充分访问时 (即最大的 $\text{Ns}$ 和 $Ns’$ ),从未观察到的样本中添加 $\text{Os}$ 和 $Os’$ 几乎没有影响, 因为置信区间在$Vs’$ 周围充分缩小, 允许广泛利用最有价值的分支.</p><h3 id="3-2-使用-Master-Worker-架构的系统实现"><a href="#3-2-使用-Master-Worker-架构的系统实现" class="headerlink" title="3.2 使用 Master-Worker 架构的系统实现"></a>3.2 使用 Master-Worker 架构的系统实现</h3><p>继续解释 WU-UCT 的系统实现, 其整体架构如图 2(a) 所示. 具体来说, 我们使用master-worker 架构来实现 WU-UCT 算法, 考虑以下几点. 首先,由于与选择和反向传播步骤相比, 扩展和模拟步骤更耗时,因此它们应该密集并行化. 事实上, 它们相对容易并行化 (例如,可以独立执行不同的模拟). 其次, 正如我们之前所讨论的,不同的线程需要访问最新的统计数据 $\{ Vs,\ Ns,\ Os\}$以实现正确的探索和利用的权衡. 为此,选择和反向传播步骤的集中式架构更可取,因为它允许对统计信息的检索和更新添加严格的限制, 使其保持最新. 具体来说,我们使用一个集中的主线程来维护一组全局统计数据 (除了游戏状态等其他数据),并让它负责反向传播步骤 (即更新全局统计数据) 和选择步骤(即利用全局统计数据). 如图2(a) 所示, 主线程重复执行扩展,直到达到预定义的模拟次数. 在每次扩展期间, 它会选择节点进行查询,将扩展和模拟任务分配给不同的线程, 并收集返回的结果以更新全局统计信息.特别是, 我们使用以下不完全更新和完全更新 (如图 2(a) 所示)沿遍历的路径跟踪 $\text{Ns}$ 和 $\text{Os}$ (见图 1(d)).</p><script type="math/tex; mode=display">\left\lbrack 不完全更新\right\rbrack\text{\ \ }O_{s} \leftarrow O_{s} + 1 \\\left\lbrack 完全更新 \right\rbrack O_{s} \leftarrow O_{s} - 1;N_{s} \leftarrow N_{s} + 1</script><p>在模拟任务开始之前执行不完全更新,使更新后的统计数据可以在全局范围内即时可用；在模拟返回可用后完成完整更新,类似于顺序算法中的反向传播步骤. 此外, $\text{Vs}$也在完整更新步骤中更新. 当我们并行化耗时较长的扩展和模拟步骤时,主线程和子线程之间明确的分工提供了顺序选择和反向传播步骤.它通过集中式主线程确保所有线程的最新统计数据,并在没有太多性能下降的情况下实现线性加速 (实验结果见第 5 节).</p><p><img src="images/1685880443650.png" alt="1685880443650"></p><p>图 2: WU-UCT 的系统架构及其时间消耗示意图. (a) 绿色块和任务缓冲区在 master 处操作, 而蓝色块由<br>worker 执行.(b-c) 两个游戏基准的时间消耗 (第 5 节) </p><p>为了证明上述基本原理,对提出的 WU-UCT s算法进行了一组时间分析, 并在 图2(b)-(c) 中显示结果. 我们展示了主线程和子线程不同部分的时间消耗. 首先, 我们关注子线程, 模拟线程的占用率接近 100%, 模拟步骤完全并行化. 尽管扩展线程没有得到充分利用, 但扩展步骤最大程度地并行化, 因为所需的模拟和扩展任务的数量是相同的. 这表明在扩展线程的数量和模拟线程的数量之间存在一个最佳 (任务相关) 比率, 以最少的资源 (例如内存) 完全并行化这两个步骤. 回到主线程, 在这两个基准测试中, 我们看到在模拟和扩展步骤上花费的时间明显占主导地位, 即使它们都由 16 个线程并行化. 我们最后关注并行化带来的通信开销. 尽管与模拟和反向传播相比更耗时, 但与扩展和模拟步骤所用的时间相比, 通信开销可以忽略不计. 尽管与模拟和反向传播相比更耗时, 但与扩展和模拟步骤所用的时间相比, 通信开销可以忽略不计. </p><h2 id="4-监控为观察到的样本的好处"><a href="#4-监控为观察到的样本的好处" class="headerlink" title="4.监控为观察到的样本的好处"></a>4.监控为观察到的样本的好处</h2><p>在本节中, 我们将讨论在 WU-UCT 中监控未观察样本的好处, 并将其与几种流行的并行 MCTS 算法 (图3) 进行比较, 包括叶子结点并行化(LeafP)、具有虚拟损失的树并行化(TreeP)和根结点并行化(RootP) . LeafP 并行化了叶子结点模拟, 从而产生了一个有效的十六进制游戏求解器 (Wang, 2018). 具有虚拟损失的 TreeP 最近在挑战现实任务 (如围棋) 方面取得了巨大成功 (Silver, 2016) . RootP 在不同的线程处并行化根节点的子树, 并在所有线程完成模拟后汇总子树的统计信息 (Soejima, 2010) .</p><p><img src="images/1685880466228.png" alt="1685880466228"></p><ul><li>叶子结点并行化(LeafP): 模拟过程中，多个线程(如A、B、C)同时查询同一个节点。 依次执行选择、展开和反向传播 (类似).</li><li>树并行化(TreeP): 虚拟损失 $r_{\text{VL}}$从已被某个线程遍历的节点的$V_{s}$中减去.$r_{\text{VL}}$将在反向传播期间添加回节点.</li><li>c. 根结点并行化(RootP):不同的线程在本地内存中执行树搜索，每个线程从不同的子节点开始。</li></ul><p>图3: 三种流行的并行 MCTS 算法. LeafP 并行化模拟步骤, TreeP 使用虚拟损失来鼓励探索, RootP 并行化根节点的子树.</p><p>我们认为, 通过引入额外的统计数据 $\text{Os}$, WU-UCT比上述方法实现了更好的探索和利用的权衡. 首先, LeafP 和 TreeP代表了这种权衡中的两个极端. LeafP 在探索方面缺乏多样性,因为它的所有线程都被分配到模拟同一个节点, 这会导致探索崩溃从而性能下降,其方式与一般的并行化非常相似 (见图1(c)). 相比之下, 尽管 TreeP中使用的虚拟损失可以鼓励探索多样性,但这种硬加性的惩罚可能会导致利用失败: 即使线程确定同一节点是最佳的,他们也不太可能共同模拟同一节点 (Mirsoleimani, 2017). RootP试图通过让线程执行独立的树搜索来避免这些问题. 但是,这会减少每个线程的扩展次数, 从而降低 UCT 策略的准确性.与上述三种方法不同, WU-UCT 通过以下方式实现了更好的探索和利用的权衡.它通过使用 $\text{Os}$ 来“惩罚”具有许多正在进行的模拟的节点来鼓励探索.同时, 它允许多个线程利用最优回报的节点, 因为当 $\text{Ns}$ 变大时,这种“惩罚”就会消失.</p><h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5.实验"></a>5.实验</h2><p>本节在真实系统上评估所提出的 WU-UCT 算法, 以预测手机游戏“Joy City”的用户通过率 (第5.1节) 以及专有 Atari 游戏基准测试 (第5.2节), 旨在展示卓越的性能和接近线性加速.</p><h3 id="5-1-“Joy-City”游戏的实验"><a href="#5-1-“Joy-City”游戏的实验" class="headerlink" title="5.1  “Joy City”游戏的实验"></a>5.1  “Joy City”游戏的实验</h3><p>“Joy City”是一款关卡类游戏, 玩法多样且富有挑战性. 玩家点击以消除游戏板上的连接项目. 要通过一个级别, 玩家必须在给定的步骤数内完成某些目标. 用于通过关卡的步数 (称为游戏步数) 是主要的性能指标, 它将大师与初学者区分开来. 由于其大量的游戏状态 (超过 $12^{9 \times 9}$) 和过渡中的高随机性, 这是一项具有挑战性的强化学习任务. 制作系统的目标是准确预测不同游戏关卡的用户通过率, 为游戏设计提供有用且快速的反馈. 在 WU-UCT 的支持下, 系统运行速度提高了 16 倍, 同时准确地预测了用户通过率 (8.6% MAE). 在本小节中, 我们集中分析使用两个典型游戏关卡 (Level-35 和 Level-58) 6 的 WU-UCT 的加速和性能.</p><p>我们使用不同数量的扩展和模拟线程 (从 1 到 16) 评估 WU-UCT, 并在图4 (a) - (b) 中报告加速效果. 对于所有实验, 我们将模拟总数固定为 500. 首先, 当我们拥有相同数量的扩展线程和模拟线程时, WU-UCT 实现了线性加速. 此外, 图4 还表明扩展线程和模拟线程都至关重要, 因为降低任一组的线程数量都会降低加速. 除了接近线性的加速特性外, WU-UCT 随着线程数量的增加而减少的性能损失可以忽略不计, 如图4(c)-(d) 所示. Level-35 和 Level-58 的性能标准差 (以平均游戏步数衡量) 在不同数量的扩展和模拟线程中分别仅为 0.67 和 1.22, 远小于它们的平均游戏步数 (12 和30). </p><p><img src="images/1685880478094.png" alt="1685880478094"></p><p>图 4: WU-UCT 加速和性能. 结果平均超过 10 次运行. WU-UCT实现了线性加速, 性能损失可忽略不计 (以游戏步数衡量) . </p><h3 id="5-2-ATARI-游戏基准测试"><a href="#5-2-ATARI-游戏基准测试" class="headerlink" title="5.2 ATARI 游戏基准测试"></a>5.2 ATARI 游戏基准测试</h3><p>我们进一步评估了 Atari Games (Bellemare., 2013) 上的 WU-UCT, 这是强化学习 (RL) 和规划算法的经典基准 (Guo, 2014). Atari Games 是 MCTS 算法的理想测试平台, 因为它的规划范围很长 (数千) 、稀疏奖励和复杂的游戏策略. 我们将 WU-UCT 与第 4 节中讨论的三种并行 MCTS 算法进行比较: TreeP、LeafP 和 RootP. 我们还测试了经典 UCT (比 WU-UCT 大约慢 16 倍) 和 PPO (Schulman, 2017) 的结果作为参考. 通常, 经典 UCT 的性能为并行 UCT 算法设置了一个上限. 之所以包含 PPO, 是因为我们使用精炼的 PPO 策略网络 (Hinton, 2015；Rusu, 2015) 作为所有其他算法的推出策略. 它被认为是并行和经典 UCT 算法的性能下限. 所有实验共进行了 128 个模拟步骤, 所有并行算法使用 16 个线程. </p><p>我们首先比较了 WU-UCT 与 15 个 Atari 游戏的基线之间的性能 (通过平均获得奖励衡量) , 该游戏由 16 个模拟线程和 1 个扩展线程完成 (为了公平比较, 因为基线不平行于扩展步骤). 每个任务重复 10 次, 平均值和标准差见表1. 由于选择期间更好的探索和利用的权衡, WU-UCT 在 15 个任务中的 12 个中优于所有其他并行算法. 成对的 t 检验进一步表明, WU-UCT 在 g根节点为7、9 的 7 个任务中的表现明显优于 TreeP、LeafP 和 RootP (通过 Bonferroni 方法调整, p 值 &lt; 0.0011). 接下来, 我们测试模拟线程对加速性能的影响. 在图5中, 我们比较了4、8和16个模拟线程的平均返回时间消耗(每一步). 条形图表明, WU-UCT随着线程数量的增加几乎没有性能损失, 而基线在严重并行化时表现出显著的性能下降. WU-UCT还实现了与基线相比最快的速度, 这得益于高效的master-worker体系结构(章节3.2). 总之, 我们提出的WU-UCT不仅在相同数量的线程下显著优于基线方法, 而且随着并行化水平的提高, 性能损失可以忽略不计. </p><p><img src="images/1685880489893.png" alt="1685880489893"></p><p>图5: WU-UCT的速度和性能测试, 以及四款Atari游戏的三条基线. 所有实验均重复三次, 并报告其平均值和标准差(仅针对插曲奖励). 对于WU-UCT, 扩展线程数量固定为1. </p><p>表1: 15 款 Atari 游戏的表现. 报告了 10 次试验的平均情节回报 (±标准偏差). 并行算法中最好的平均分数以粗体突出显示. </p><p><img src="images/1685880502000.png" alt="1685880502000"></p><h2 id="6-相关工作"><a href="#6-相关工作" class="headerlink" title="6.相关工作"></a>6.相关工作</h2><p><strong>MCTS</strong> 蒙特卡洛树搜索是一种规划方法, 用于在确定性(Silver, 2016)或随机(Schafer, 2008)环境下进行最优决策. 它对人工智能应用产生了深远的影响(Browne, 2012), 甚至被应用于预测和模仿人类行为(van Opheusden, 2016). 最近, 有大量的工作将MCTS和其他学习方法相结合, 使两种方法相互改进. 例如, Guo等人(2014)利用MCTS的力量来提高无模型RL方法的性能; Shen等人(2018)弥补了MCTS和基于图的搜索之间的差距, 优于RL和知识库完成基线. </p><p><strong>并行MCTS</strong> 许多并行化MCTS方法的方法已经被开发出来, 其目标是双重的: 在保持算法性能的同时, 使用大量线程的情况下实现接近线性的加速. 流行的MCTS并行化方法包括叶子并行化、根并行化和树并行化(Chaslo, 2008). 叶子并行化的目的是通过分配多个线程查询同一个节点来收集更好的统计信息(Cazenave &amp; Jouandeau, 2007). 然而, 这是以浪费树搜索的多样性为代价的. 因此, 尽管在客户机-服务器网络体系结构的帮助下, 它的性能显著下降, 但却有接近理想的加速(Kato &amp; Takeuchi, 2010). 在根并行化中, 构建多个搜索树并分配给不同的线程. 主线程用于同步来自不同树的统计数据, 这将导致在现实任务中更好的表现(Bourki, 2010). 然而, 一个关于围棋的案例研究揭示了它的劣势, 即使是少量的线程(Soejima, 2010). 另一方面, 树并行化使用多个线程遍历、执行查询和更新共享搜索树. 它显著受益于两种技术. 首先, 添加一个虚拟损失以避免由不同的线程查询同一节点(Chaslot, 2008). 这已被应用于MCTS的各种成功应用中, 如围棋(Silver, 2016)和斗地注(Whitehouse, 2011). 此外, 体系结构方面的改进, 如使用管道(Mirsoleimani, 2018b)或无锁结构(Mirsoleimani, 2018a), 大大提高了算法的速度. 然而, 尽管虚拟损失能够增加多样性, 但即使在四个线程的情况下, 虚拟损失也会降低性能 (Mirsoleimani., 2017;Bourki, 2010). 最后, Zhong等人也提出了了统计未观察样本来调整<em>机械臂选择</em>的置信区间的想法. 然而, 它主要关注并行化<em>多臂阈值老虎机</em>问题 (Chen, 2014) , 而不是像我们所做的树搜索问题. </p><h2 id="7-结论"><a href="#7-结论" class="headerlink" title="7.结论"></a>7.结论</h2><p>本文提出了 WU-UCT, 一种新颖的并行 MCTS 算法, 通过监控未观察样本的数量来解决并行化过程中统计数据过时的问题. 基于新设计的统计数据, 它有正确地修正了UCT节点选择策略, 实现了有效的探索和利用的权衡. 连同我们以效率为导向的系统实现, WU-UCT 实现了近乎最佳的线性加速, 并且在广泛的任务中只有有限的性能损失, 包括部署的真实系统和 Atari 游戏.</p><h2 id="8-感谢"><a href="#8-感谢" class="headerlink" title="8.感谢"></a>8.感谢</h2><p>这项工作得到了腾讯人工智能实验室和西雅图人工智能实验室、快手公司的支持. 感谢Xiangru Lian对系统实现的帮助. </p><h2 id="参考文献-References"><a href="#参考文献-References" class="headerlink" title="参考文献(References):"></a>参考文献(References):</h2><p>。。。。。。</p>]]></content>
      
      
      <categories>
          
          <category> PaperReading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> PaperReading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.强化学习基础</title>
      <link href="/easy_rl_exercise/1.%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
      <url>/easy_rl_exercise/1.%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h1 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h1><ul><li><strong>强化学习（reinforcement learning，RL）</strong>：智能体可以在与复杂且不确定的环境进行交互时，尝试使所获得的奖励最大化的算法。</li><li><strong>动作（action）</strong>： 环境接收到的智能体基于当前状态的输出。</li><li><strong>状态（state）</strong>：智能体从环境中获取的状态。</li><li><strong>奖励（reward）</strong>：智能体从环境中获取的反馈信号，这个信号指定了智能体在某一步采取了某个策略以后是否得到奖励，以及奖励的大小。</li><li><strong>探索（exploration）</strong>：在当前的情况下，继续尝试新的动作。其有可能得到更高的奖励，也有可能一无所有。</li><li><strong>开发（exploitation）</strong>：在当前的情况下，继续尝试已知的可以获得最大奖励的过程，即选择重复执行当前动作。</li><li><strong>深度强化学习（deep reinforcement learning）</strong>：不需要手动设计特征，仅需要输入状态就可以让系统直接输出动作的一个端到端（end-to-end）的强化学习方法。通常使用神经网络来拟合价值函数（value function）或者策略网络（policy network）。</li><li><strong>全部可观测（full observability）、完全可观测（fully observed）和部分可观测（partially observed）</strong>：当智能体的状态与环境的状态等价时，我们就称这个环境是全部可观测的；当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的；一般智能体不能观察到环境的所有状态时，我们称这个环境是部分可观测的。</li><li><strong>部分可观测马尔可夫决策过程（partially observable Markov decision process，POMDP）</strong>：即马尔可夫决策过程的泛化。部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是其假设智能体无法感知环境的状态，只能知道部分观测值。</li><li><strong>动作空间（action space）、离散动作空间（discrete action space）和连续动作空间（continuous action space）</strong>：在给定的环境中，有效动作的集合被称为动作空间，智能体的动作数量有限的动作空间称为离散动作空间，反之，则被称为连续动作空间。</li><li><strong>基于策略的（policy-based）</strong>：智能体会制定一套动作策略，即确定在给定状态下需要采取何种动作，并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。</li><li><strong>基于价值的（valued-based）</strong>：智能体不需要制定显式的策略，它维护一个价值表格或者价值函数，并通过这个价值表格或价值函数来执行使得价值最大化的动作。</li><li><strong>有模型（model-based）结构</strong>：智能体通过学习状态的转移来进行决策。</li><li><strong>免模型（model-free）结构</strong>：智能体没有直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数或者策略网络进行决策。</li></ul><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><h4 id="1-1-强化学习的基本结构是什么？"><a href="#1-1-强化学习的基本结构是什么？" class="headerlink" title="1-1 强化学习的基本结构是什么？"></a><strong>1-1</strong> 强化学习的基本结构是什么？</h4><p>本质上是<strong>智能体与环境的交互</strong>。具体地，当智能体在环境中得到当前时刻的状态后，其会基于此状态输出一个动作，这个动作会在环境中被执行并输出下一个状态和当前的这个动作得到的奖励。智能体在环境里存在的目标是最大化期望累积奖励。</p><h4 id="1-2-强化学习相对于监督学习为什么训练过程会更加困难？"><a href="#1-2-强化学习相对于监督学习为什么训练过程会更加困难？" class="headerlink" title="1-2 强化学习相对于监督学习为什么训练过程会更加困难？"></a><strong>1-2</strong> 强化学习相对于监督学习为什么训练过程会更加困难？</h4><ol><li>强化学习处理的大多是<strong>序列数据</strong>，其很难像监督学习的样本一样满足独立同分布条件。</li><li>强化学习有<strong>奖励的延迟</strong>，即智能体的动作作用在环境中时，环境对于智能体状态的奖励存在延迟，使得反馈不实时。</li><li>监督学习有正确的标签，模型可以通过标签修正自己的预测来更新模型，而强化学习相当于一个“<strong>试错</strong>”的过程，其完全根据环境的“反馈”更新对自己最有利的动作。</li></ol><h4 id="1-3-强化学习的基本特征有哪些？"><a href="#1-3-强化学习的基本特征有哪些？" class="headerlink" title="1-3 强化学习的基本特征有哪些？"></a><strong>1-3</strong> 强化学习的基本特征有哪些？</h4><ol><li>有试错探索过程，即需要通过探索环境来获取对当前环境的理解。</li><li>强化学习中的智能体会从环境中获得延迟奖励。</li><li>强化学习的训练过程中时间非常重要，因为数据都是时间关联的，而不是像监督学习中的数据大部分是满足独立同分布的。</li><li>强化学习中智能体的动作会影响它从环境中得到的反馈。</li></ol><h4 id="1-4-近几年强化学习发展迅速的原因有哪些？"><a href="#1-4-近几年强化学习发展迅速的原因有哪些？" class="headerlink" title="1-4 近几年强化学习发展迅速的原因有哪些？"></a><strong>1-4</strong> 近几年强化学习发展迅速的原因有哪些？</h4><ol><li><strong>算力的提升</strong>使我们可以更快地通过试错等方法来使得智能体在环境里面获得更多的信息，从而取得更大的奖励。</li><li>我们有了<strong>深度强化学习</strong>这样一个端到端的训练方法，可以把特征提取、价值估计以及决策部分一起优化，这样就可以得到一个更强的决策网络。</li></ol><h4 id="1-5-状态和观测有什么关系？"><a href="#1-5-状态和观测有什么关系？" class="headerlink" title="1-5 状态和观测有什么关系？"></a><strong>1-5</strong> 状态和观测有什么关系？</h4><p><strong>状态是对环境的完整描述</strong>，不会隐藏环境信息。观测是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用同一个实值向量、矩阵或者更高阶的张量来表示状态和观测。</p><h4 id="1-6-一个强化学习智能体由什么组成？"><a href="#1-6-一个强化学习智能体由什么组成？" class="headerlink" title="1-6 一个强化学习智能体由什么组成？"></a><strong>1-6</strong> 一个强化学习智能体由什么组成？</h4><ol><li><strong>策略函数</strong>，智能体会用策略函数来选取它下一步的动作，策略包括随机性策略和确定性策略。</li><li><strong>价值函数</strong>，我们用价值函数来对当前状态进行评估，即进入现在的状态可以对后面的奖励带来多大的影响。价值函数的值越大，说明进入该状态越有利。</li><li><strong>模型</strong>，其表示智能体对当前环境状态的理解，它决定系统是如何运行的。</li></ol><h4 id="1-7-根据强化学习智能体的不同，我们可以将其分为哪几类？"><a href="#1-7-根据强化学习智能体的不同，我们可以将其分为哪几类？" class="headerlink" title="1-7 根据强化学习智能体的不同，我们可以将其分为哪几类？"></a><strong>1-7</strong> 根据强化学习智能体的不同，我们可以将其分为哪几类？</h4><ol><li><strong>基于价值的智能体</strong>。显式学习的是价值函数，隐式地学习智能体的策略。因为这个策略是从学到的价值函数里面推算出来的。</li><li><strong>基于策略的智能体</strong>。其直接学习策略，即直接给智能体一个状态，它就会输出对应动作的概率。当然在基于策略的智能体里面并没有去学习智能体的价值函数。</li><li>另外还有一种智能体，它把以上两者结合。把基于价值和基于策略的智能体结合起来就有了<strong>演员-评论员智能体</strong>。这一类智能体通过学习策略函数和价值函数以及两者的交互得到更佳的状态。</li></ol><h4 id="1-8-基于策略迭代和基于价值迭代的强化学习方法有什么区别？"><a href="#1-8-基于策略迭代和基于价值迭代的强化学习方法有什么区别？" class="headerlink" title="1-8 基于策略迭代和基于价值迭代的强化学习方法有什么区别？"></a><strong>1-8</strong> 基于策略迭代和基于价值迭代的强化学习方法有什么区别？</h4><ol><li>基于策略迭代的强化学习方法，智能体会制定一套动作策略，即<strong>确定在给定状态下需要采取何种动作，并根据该策略进行操作</strong>。强化学习算法直接对策略进行优化，使得制定的策略能够获得最大的奖励；基于价值迭代的强化学习方法，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。</li><li>基于价值迭代的方法只能应用在离散的环境下，例如围棋或某些游戏领域，对于行为集合规模庞大或是动作连续的场景，如机器人控制领域，其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作)。</li><li>基于价值迭代的强化学习算法有 Q-learning、Sarsa 等，基于策略迭代的强化学习算法有策略梯度算法等。</li><li>此外，演员-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，从而取得更好的效果。</li></ol><h4 id="1-9-有模型学习和免模型学习有什么区别？"><a href="#1-9-有模型学习和免模型学习有什么区别？" class="headerlink" title="1-9 有模型学习和免模型学习有什么区别？"></a><strong>1-9</strong> 有模型学习和免模型学习有什么区别？</h4><p>针对是否需要对真实环境建模，强化学习可以分为有模型学习和免模型学习。</p><p>有模型学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。</p><p>总体来说，有模型学习相比免模型学习仅仅多出一个步骤，即对真实环境进行建模。免模型学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。免模型学习的泛化性要优于有模型学习，原因是有模型学习需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。</p><h4 id="1-10-如何通俗理解强化学习？"><a href="#1-10-如何通俗理解强化学习？" class="headerlink" title="1-10 如何通俗理解强化学习？"></a><strong>1-10</strong> 如何通俗理解强化学习？</h4><p>环境和奖励函数不是我们可以控制的，两者是在开始学习之前就已经事先确定的。我们唯一能做的事情是调整策略，使得智能体可以在环境中得到最大的奖励。另外，策略决定了智能体的行为，策略就是给一个外界的输入，然后它会输出现在应该要执行的动作。</p><h1 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h1><h4 id="1-1-友善的面试官-看来你对于强化学习还是有一定了解的呀，那么可以用一句话谈一下你对于强化学习的认识吗？"><a href="#1-1-友善的面试官-看来你对于强化学习还是有一定了解的呀，那么可以用一句话谈一下你对于强化学习的认识吗？" class="headerlink" title="1-1 友善的面试官: 看来你对于强化学习还是有一定了解的呀，那么可以用一句话谈一下你对于强化学习的认识吗？"></a><strong>1-1</strong> 友善的面试官: 看来你对于强化学习还是有一定了解的呀，那么可以用一句话谈一下你对于强化学习的认识吗？</h4><p>强化学习包含环境、动作和奖励3部分，其本质是智能体通过与环境的交互，使其做出的动作对应的决策得到的总奖励最大，或者说是期望最大。</p><h4 id="1-2-友善的面试官-请问，你认为强化学习、监督学习和无监督学习三者有什么区别呢？"><a href="#1-2-友善的面试官-请问，你认为强化学习、监督学习和无监督学习三者有什么区别呢？" class="headerlink" title="1-2 友善的面试官: 请问，你认为强化学习、监督学习和无监督学习三者有什么区别呢？"></a><strong>1-2</strong> 友善的面试官: 请问，你认为强化学习、监督学习和无监督学习三者有什么区别呢？</h4><p>首先强化学习和无监督学习是不需要有标签样本的，而监督学习需要许多有标签样本来进行模型的构建和训练。</p><p>其次对于强化学习与无监督学习，无监督学习直接基于给定的数据进行建模，寻找数据或特征中隐藏的结构，一般对应聚类问题；强化学习需要通过延迟奖励学习策略来得到模型与目标的距离，这个距离可以通过奖励函数进行定量判断，这里我们可以将奖励函数视为正确目标的一个稀疏、延迟形式。</p><p>另外，强化学习处理的多是序列数据，样本之间通常具有强相关性，但其很难像监督学习的样本一样满足独立同分布条件。</p><h4 id="1-3-友善的面试官-根据你的理解，你认为强化学习的使用场景有哪些呢？"><a href="#1-3-友善的面试官-根据你的理解，你认为强化学习的使用场景有哪些呢？" class="headerlink" title="1-3 友善的面试官: 根据你的理解，你认为强化学习的使用场景有哪些呢？"></a><strong>1-3</strong> 友善的面试官: 根据你的理解，你认为强化学习的使用场景有哪些呢？</h4><p>7个字总结就是“多序列决策问题”，或者说是对应的模型未知，需要通过学习逐渐逼近真实模型的问题。并且当前的动作会影响环境的状态，即具有马尔可夫性的问题。同时应满足所有状态是可重复到达的条件，即满足可学习条件。</p><h4 id="1-4-友善的面试官-请问强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢？"><a href="#1-4-友善的面试官-请问强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢？" class="headerlink" title="1-4 友善的面试官: 请问强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢？"></a><strong>1-4</strong> 友善的面试官: 请问强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢？</h4><p>深度学习中的损失函数的目的是使预测值和真实值之间的差距尽可能小，而强化学习中的损失函数的目的是使总奖励的期望尽可能大。</p><h4 id="1-5-友善的面试官-你了解有模型和免模型吗？两者具体有什么区别呢？"><a href="#1-5-友善的面试官-你了解有模型和免模型吗？两者具体有什么区别呢？" class="headerlink" title="1-5 友善的面试官: 你了解有模型和免模型吗？两者具体有什么区别呢？"></a><strong>1-5</strong> 友善的面试官: 你了解有模型和免模型吗？两者具体有什么区别呢？</h4><p>我认为两者的区别主要在于是否需要对真实的环境进行建模，免模型方法不需要对环境进行建模，直接与真实环境进行交互即可，所以其通常需要较多的数据或者采样工作来优化策略，这也使其对于真实环境具有更好的泛化性能；而有模型方法需要对环境进行建模，同时在真实环境与虚拟环境中进行学习，如果建模的环境与真实环境的差异较大，那么会限制其泛化性能。现在通常使用有模型方法进行模型的构建工作。</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10.模仿学习</title>
      <link href="/easy_rl_exercise/10.%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/"/>
      <url>/easy_rl_exercise/10.%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h1><ul><li><strong>模仿学习（imitation learning，IL）</strong>：其讨论我们没有奖励或者无法定义奖励但是有与环境进行交互时怎么进行智能体的学习。这与我们平时处理的问题有些类似，因为通常我们无法从环境中得到明确的奖励。模仿学习又被称为示范学习（learning from demonstration）、学徒学习（apprenticeship learning）以及观察学习（learning by watching）等。</li><li><strong>行为克隆（behavior cloning）</strong>：类似于机器学习中的监督学习，通过收集专家的状态与动作等对应信息，来训练我们的网络。在使用时，输入状态就可以输出对应的动作。</li><li><strong>数据集聚合（dataset aggregation）</strong>：用来应对在行为克隆中专家提供不到数据的情况，其希望收集专家在各种极端状态下的动作。</li><li><strong>逆强化学习（inverse reinforcement learning，IRL）</strong>：逆强化学习先找出奖励函数，再用强化学习找出最优演员。这么做是因为我们没有环境中的奖励，但是有专家的示范，使用逆强化学习，我们可以推断专家是因为何种奖励函数才会采取这些动作。有了奖励函数以后就可以使用一般的强化学习方法找出最优演员。</li><li><strong>第三人称视角模仿学习（third person imitation learning）</strong>：一种把第三人称视角所观察到的经验泛化为第一人称视角的经验的技术。</li></ul><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><h4 id="10-1-具体的模仿学习方法有哪些？"><a href="#10-1-具体的模仿学习方法有哪些？" class="headerlink" title="10-1 具体的模仿学习方法有哪些？"></a><strong>10-1</strong> 具体的模仿学习方法有哪些？</h4><p>行为克隆、逆强化学习或者称为逆最优控制。</p><h4 id="10-2-行为克隆存在哪些问题呢？对应的解决方法有哪些？"><a href="#10-2-行为克隆存在哪些问题呢？对应的解决方法有哪些？" class="headerlink" title="10-2 行为克隆存在哪些问题呢？对应的解决方法有哪些？"></a><strong>10-2</strong> 行为克隆存在哪些问题呢？对应的解决方法有哪些？</h4><p>（1）首先，如果只收集专家的示范（看到某一个状态输出的动作），那么所有的结果会是非常有限的。所以我们要收集专家在各种极端状态下的动作或者说要收集更多、更复杂的数据，可以使用数据集聚合方法。</p><p>（2）另外，使用传统意义上的行为克隆，智能体会完全复制专家的行为，不管专家的行为是否合理，智能体都会硬把它记下来。智能体是一个网络，网络的容量是有限的。就算给网络足够的训练数据，它在训练数据集上得到的正确率往往也不是100\%。所以这个时候，什么该学、什么不该学就变得很重要。实际上，极少数专家的行为是没有意义的，但是使用它们的示范至少不会产生较坏的影响。</p><p>（3）还有，在进行行为克隆的时候，训练数据和测试数据往往是不匹配的。我们可以用数据集聚合来缓解这个问题。具体来说，在训练和测试的时候，数据分布是不同的。因为在强化学习中，动作会影响到接下来的状态。我们先有状态 $s_1$ ，然后采取动作 $a_1$ ，动作 $a_1$ 会决定接下来的状态 $s_2$ 。如果 $\pi^<em>$ 与 $\hat{\pi}$ 一模一样，那么我们训练时看到的状态与测试时看到的状态会是一样的，这样模型的泛化性能就会变得比较差。而且， $\pi^</em>$ 和 $\hat{\pi}$ 可能有一点儿误差，虽然这个误差在监督学习中，由于每一个样本都是独立的，因此影响不大，但对强化学习来说，可能在某个地方，也许智能体无法完全复制专家的行为，最后得到的结果就会差很多。所以行为克隆并不能够完全解决模仿学习的问题，我们可以使用另外一个比较好的方法，即逆强化学习。</p><h4 id="10-3-逆强化学习是怎么运行的呢？"><a href="#10-3-逆强化学习是怎么运行的呢？" class="headerlink" title="10-3 逆强化学习是怎么运行的呢？"></a><strong>10-3</strong> 逆强化学习是怎么运行的呢？</h4><p>首先，我们有一个专家，其策略为 $\hat{\pi}$，这个专家负责与环境交互，给我们 $\hat{\tau_1}$ ～ $\hat{\tau_n}$，我们需要将其中的状态-动作序列都记录下来。然后对于演员，其策略为$\pi$，也需要进行一样的交互和序列的记录。接着我们需要指定一个奖励函数，并且保证专家对应的分数一定要比演员的要高，用这个奖励函数继续学习并更新我们的训练，同时套用一般条件下的强化学习方法进行演员网络的更新。在这个过程中，我们也要同时进行一开始指定的奖励函数的更新，使得演员得分越来越高，但是不超过专家的得分。最终的奖励函数应该让专家和演员对应的奖励函数都达到比较高的分数，并且从最终的奖励函数中无法分辨出两者。</p><h4 id="10-4-逆强化学习方法与生成对抗网络在图像生成中有什么异曲同工之处？"><a href="#10-4-逆强化学习方法与生成对抗网络在图像生成中有什么异曲同工之处？" class="headerlink" title="10-4 逆强化学习方法与生成对抗网络在图像生成中有什么异曲同工之处？"></a><strong>10-4</strong> 逆强化学习方法与生成对抗网络在图像生成中有什么异曲同工之处？</h4><p>在生成对抗网络中，我们有一些比较好的图片数据集，也有一个生成器，一开始其不知道要生成什么样的图片，只能随机生成。另外，我们有一个判别器，其用来给生成的图片打分，专家生成的图片得分高，生成器生成的图片得分低。有了判别器以后，生成器会想办法去“骗”判别器。生成器希望判别器也给它生成的图片打高分。整个过程与逆强化学习的过程是类似的。我们一一对应起来看。</p><p>（1）生成的图片就是专家的判别结果，生成器就是演员，生成器会生成很多的图片并让演员与环境进行交互，从而产生很多轨迹。这些轨迹与环境交互的记录等价于生成对抗网络中的生成图片。</p><p>（2）逆强化学习中的奖励函数就是判别器。奖励函数给专家的实例打高分，给演员的交互结果打低分。</p><p>（3）考虑两者的过程，在逆强化学习中，演员会想办法从已经学习到的奖励函数中获得高分，然后迭代地循环。这个过程其实是与生成对抗网络的训练过程一致的。</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.马尔可夫决策过程</title>
      <link href="/easy_rl_exercise/2.%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"/>
      <url>/easy_rl_exercise/2.%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h1><ul><li><strong>马尔可夫性质（Markov property，MP）</strong>：如果某一个过程未来的状态与过去的状态无关，只由现在的状态决定，那么其具有马尔可夫性质。换句话说，一个状态的下一个状态只取决于它的当前状态，而与它当前状态之前的状态都没有关系。</li><li><strong>马尔可夫链（Markov chain）</strong>： 概率论和数理统计中具有马尔可夫性质且存在于离散的指数集（index set）和状态空间（state space）内的随机过程（stochastic process）。</li><li><strong>状态转移矩阵（state transition matrix）</strong>：状态转移矩阵类似于条件概率（conditional probability），其表示当智能体到达某状态后，到达其他所有状态的概率。矩阵的每一行描述的是从某节点到达所有其他节点的概率。</li><li><strong>马尔可夫奖励过程（Markov reward process，MRP）</strong>： 本质是马尔可夫链加上一个奖励函数。在马尔可夫奖励过程中，状态转移矩阵和它的状态都与马尔可夫链的一样，只多了一个奖励函数。奖励函数是一个期望，即在某一个状态可以获得多大的奖励。</li><li><strong>范围（horizon）</strong>：定义了同一个回合（episode）或者一个完整轨迹的长度，它是由有限个步数决定的。</li><li><strong>回报（return）</strong>：把奖励进行折扣（discounted），然后获得的对应的奖励。</li><li><strong>贝尔曼方程（Bellman equation）</strong>：其定义了当前状态与未来状态的迭代关系，表示当前状态的价值函数可以通过下个状态的价值函数来计算。贝尔曼方程因其提出者、动态规划创始人理查德 ⋅\cdot⋅ 贝尔曼（Richard Bellman）而得名，同时也被叫作“动态规划方程”。贝尔曼方程即 $V(s)=R(s)+ \gamma \sum_{s’ \in S}P(s’|s)V(s’)$ ，特别地，其矩阵形式为 $\mathrm{V}=\mathrm{R}+\gamma \mathrm{PV}$。</li><li><strong>蒙特卡洛算法（Monte Carlo algorithm，MC algorithm）</strong>： 可用来计算价值函数的值。使用本节中小船的例子，当得到一个马尔可夫奖励过程后，我们可以从某一个状态开始，把小船放到水中，让它随波流动，这样就会产生一个轨迹，从而得到一个折扣后的奖励 $g$。当积累该奖励到一定数量后，用它直接除以轨迹数量，就会得到其价值函数的值。</li><li><strong>动态规划算法（dynamic programming，DP）</strong>： 其可用来计算价值函数的值。通过一直迭代对应的贝尔曼方程，最后使其收敛。当最后更新的状态与上一个状态差距不大的时候，动态规划算法的更新就可以停止。</li><li><strong>Q函数（Q-function）</strong>： 其定义的是某一个状态和某一个动作所对应的有可能得到的回报的期望。</li><li><strong>马尔可夫决策过程中的预测问题</strong>：即策略评估问题，给定一个马尔可夫决策过程以及一个策略 $\pi$ ，计算它的策略函数，即每个状态的价值函数值是多少。其可以通过动态规划算法解决。</li><li><strong>马尔可夫决策过程中的控制问题</strong>：即寻找一个最佳策略，其输入是马尔可夫决策过程，输出是最佳价值函数（optimal value function）以及最佳策略（optimal policy）。其可以通过动态规划算法解决。</li><li><strong>最佳价值函数</strong>：搜索一种策略 π\pi<em>π</em> ，使每个状态的价值最大，$V^*$ 就是到达每一个状态的极大值。在极大值中，我们得到的策略是最佳策略。最佳策略使得每个状态的价值函数都取得最大值。所以当我们说某一个马尔可夫决策过程的环境可解时，其实就是我们可以得到一个最佳价值函数。</li></ul><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><h4 id="2-1-为什么在马尔可夫奖励过程中需要有折扣因子？"><a href="#2-1-为什么在马尔可夫奖励过程中需要有折扣因子？" class="headerlink" title="2-1 为什么在马尔可夫奖励过程中需要有折扣因子？"></a><strong>2-1</strong> 为什么在马尔可夫奖励过程中需要有折扣因子？</h4><ol><li>首先，是有些马尔可夫过程是环状的，它并没有终点，所以我们想避免无穷的奖励。</li><li>另外，我们想把不确定性也表示出来，希望尽可能快地得到奖励，而不是在未来的某个时刻得到奖励。</li><li>接上一点，如果这个奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面才可以得到奖励。</li><li>还有，在有些时候，折扣因子也可以设为0。当它被设为0后，我们就只关注它当前的奖励。我们也可以把它设为1，设为1表示未来获得的奖励与当前获得的奖励是一样的。</li></ol><p>所以，折扣因子可以作为强化学习智能体的一个超参数进行调整，然后就会得到不同行为的智能体。 </p><h4 id="2-2-为什么矩阵形式的贝尔曼方程的解析解比较难求得？"><a href="#2-2-为什么矩阵形式的贝尔曼方程的解析解比较难求得？" class="headerlink" title="2-2 为什么矩阵形式的贝尔曼方程的解析解比较难求得？"></a><strong>2-2</strong> 为什么矩阵形式的贝尔曼方程的解析解比较难求得？</h4><p>通过矩阵求逆的过程，我们就可以把 $V$ 的解析解求出来。但是这个矩阵求逆的过程的复杂度是 $O(N^3)$ ，所以当状态非常多的时候，比如从10个状态到1000个状态，到100万个状态，那么当我们有100万个状态的时候，转移矩阵就会是一个100万乘100万的矩阵。对于这样一个大矩阵进行求逆是非常困难的，所以这种通过解析解去解的方法，只能应用在很小量的马尔可夫奖励过程中。</p><h4 id="2-3-计算贝尔曼方程的常见方法有哪些，它们有什么区别？"><a href="#2-3-计算贝尔曼方程的常见方法有哪些，它们有什么区别？" class="headerlink" title="2-3 计算贝尔曼方程的常见方法有哪些，它们有什么区别？"></a><strong>2-3</strong> 计算贝尔曼方程的常见方法有哪些，它们有什么区别？</h4><ol><li><strong>蒙特卡洛方法</strong>：可用来计算价值函数的值。以本书中的小船示例为例，当得到一个马尔可夫奖励过程后，我们可以从某一个状态开始，把小船放到水中，让它“随波逐流”，这样就会产生一条轨迹，从而得到一个折扣后的奖励 g 。当积累该奖励到一定数量后，直接除以轨迹数量，就会得到其价值函数的值。</li><li><strong>动态规划方法</strong>：可用来计算价值函数的值。通过一直迭代对应的贝尔曼方程，最后使其收敛。当最后更新的状态与上一个状态区别不大的时候，通常是小于一个阈值 $\gamma$ 时，更新就可以停止。</li><li>以上两者的结合方法：我们也可以使用<strong>时序差分学习方法</strong>，其为动态规划方法和蒙特卡洛方法的结合。</li></ol><h4 id="2-4-马尔可夫奖励过程与马尔可夫决策过程的区别是什么？"><a href="#2-4-马尔可夫奖励过程与马尔可夫决策过程的区别是什么？" class="headerlink" title="2-4 马尔可夫奖励过程与马尔可夫决策过程的区别是什么？"></a><strong>2-4</strong> 马尔可夫奖励过程与马尔可夫决策过程的区别是什么？</h4><p>相对于马尔可夫奖励过程，马尔可夫决策过程多了一个决策过程，其他的定义与马尔可夫奖励过程是类似的。由于多了一个决策，多了一个动作，因此状态转移也多了一个条件，即执行一个动作，导致未来状态的变化，其不仅依赖于当前的状态，也依赖于在当前状态下智能体采取的动作决定的状态变化。对于价值函数，它也多了一个条件，多了一个当前的动作，即当前状态以及采取的动作会决定当前可能得到的奖励的多少。</p><p>另外，两者之间是有转换关系的。具体来说，已知一个马尔可夫决策过程以及一个策略 $\pi$ 时，我们可以把马尔可夫决策过程转换成马尔可夫奖励过程。在马尔可夫决策过程中，状态的转移函数 $P(s’|s,a)$ 是基于它的当前状态和当前动作的，因为我们现在已知策略函数，即在每一个状态，我们知道其采取每一个动作的概率，所以我们就可以直接把这个动作进行加和，就可以得到对于马尔可夫奖励过程的一个转移概率。同样地，对于奖励，我们可以把动作去掉，这样就会得到一个类似于马尔可夫奖励过程的奖励。</p><h4 id="2-5-马尔可夫决策过程中的状态转移与马尔可夫奖励过程中的状态转移的结构或者计算方面的差异有哪些？"><a href="#2-5-马尔可夫决策过程中的状态转移与马尔可夫奖励过程中的状态转移的结构或者计算方面的差异有哪些？" class="headerlink" title="2-5 马尔可夫决策过程中的状态转移与马尔可夫奖励过程中的状态转移的结构或者计算方面的差异有哪些？"></a><strong>2-5</strong> 马尔可夫决策过程中的状态转移与马尔可夫奖励过程中的状态转移的结构或者计算方面的差异有哪些？</h4><p>对于马尔可夫链，它的转移概率是直接决定的，即从当前时刻的状态通过转移概率得到下一时刻的状态值。但是对于马尔可夫决策过程，其中间多了一层动作的输出，即在当前这个状态，首先要决定采取某一种动作，再通过状态转移函数变化到另外一个状态。所以在当前状态与未来状态转移过程中多了一层决策性，这是马尔可夫决策过程与之前的马尔可夫过程的不同之处。在马尔可夫决策过程中，动作是由智能体决定的，所以多了一个组成部分，智能体会采取动作来决定未来的状态转移。</p><h4 id="2-6-我们如何寻找最佳策略，寻找最佳策略方法有哪些？"><a href="#2-6-我们如何寻找最佳策略，寻找最佳策略方法有哪些？" class="headerlink" title="2-6 我们如何寻找最佳策略，寻找最佳策略方法有哪些？"></a><strong>2-6</strong> 我们如何寻找最佳策略，寻找最佳策略方法有哪些？</h4><p>本质来说，当我们取得最佳价值函数后，我们可以通过对Q函数进行最大化，从而得到最佳价值。然后，我们直接对Q函数取一个让动作最大化的值，就可以直接得到其最佳策略。具体方法如下，</p><ol><li><p><strong>穷举法</strong>（一般不使用）：假设我们有有限个状态、有限个动作可能性，那么每个状态我们可以采取 $A$ 种动作策略，那么总共就是 $|A|^{|S|}$ 个可能的策略。我们可以把他们穷举一遍，然后算出每种策略的价值函数，对比一下就可以得到最佳策略。但是这种方法的效率极低。</p></li><li><p><strong>策略迭代</strong>： 一种迭代方法，其由两部分组成，以下两个步骤一直在迭代进行，最终收敛，其过程有些类似于机器学习中的EM算法（期望-最大化算法）。第一个步骤是策略评估，即当前我们在优化这个策略 $\pi$ ，在优化过程中通过评估从而得到一个更新的策略；第二个步骤是策略提升，即取得价值函数后，进一步推算出它的Q函数，得到它的最大值。</p></li><li><p><strong>价值迭代</strong>： 我们一直迭代贝尔曼最优方程，通过迭代，其能逐渐趋向于最佳策略，这是价值迭代方法的核心。我们为了得到最佳的 $V^<em>$ ，对于每个状态的 $V^</em>$ 值，直接使用贝尔曼最优方程进行迭代，迭代多次之后它就会收敛到最佳策略及其对应的状态，这里是没有策略函数的。</p></li></ol><h1 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h1><h4 id="2-1-友善的面试官：请问马尔可夫过程是什么？马尔可夫决策过程又是什么？其中马尔可夫最重要的性质是什么呢？"><a href="#2-1-友善的面试官：请问马尔可夫过程是什么？马尔可夫决策过程又是什么？其中马尔可夫最重要的性质是什么呢？" class="headerlink" title="2-1 友善的面试官：请问马尔可夫过程是什么？马尔可夫决策过程又是什么？其中马尔可夫最重要的性质是什么呢？"></a><strong>2-1</strong> 友善的面试官：请问马尔可夫过程是什么？马尔可夫决策过程又是什么？其中马尔可夫最重要的性质是什么呢？</h4><p>马尔可夫过程是一个二元组 $<S,P>$ ， $S$ 为状态集合， $P$ 为状态转移函数；</p><p>马尔可夫决策过程是一个五元组 $<S,P,A,R,\gamma>$， 其中 $R$ 表示从 $S$ 到 $S’$ 能够获得的奖励期望， $\gamma$ 为折扣因子， $A$ 为动作集合；</p><p>马尔可夫最重要的性质是下一个状态只与当前状态有关，与之前的状态无关，也就是 $p(s_{t+1} | s_t)= p(s_{t+1}|s_1,s_2,…,s_t)$。</p><h4 id="2-2-友善的面试官：请问我们一般怎么求解马尔可夫决策过程？"><a href="#2-2-友善的面试官：请问我们一般怎么求解马尔可夫决策过程？" class="headerlink" title="2-2 友善的面试官：请问我们一般怎么求解马尔可夫决策过程？"></a><strong>2-2</strong> 友善的面试官：请问我们一般怎么求解马尔可夫决策过程？</h4><p>我们求解马尔可夫决策过程时，可以直接求解贝尔曼方程或动态规划方程：</p><script type="math/tex; mode=display">V(s)=R(S)+ \gamma \sum_{s' \in S}p(s'|s)V(s')</script><p>特别地，其矩阵形式为 $\mathrm{V}=\mathrm{R}+\gamma \mathrm{PV}$。但是贝尔曼方程很难求解且计算复杂度较高，所以可以使用动态规划、蒙特卡洛以及时序差分等方法求解。</p><h4 id="2-3-友善的面试官：请问如果数据流不具备马尔可夫性质怎么办？应该如何处理？"><a href="#2-3-友善的面试官：请问如果数据流不具备马尔可夫性质怎么办？应该如何处理？" class="headerlink" title="2-3 友善的面试官：请问如果数据流不具备马尔可夫性质怎么办？应该如何处理？"></a><strong>2-3</strong> 友善的面试官：请问如果数据流不具备马尔可夫性质怎么办？应该如何处理？</h4><p>如果不具备马尔可夫性，即下一个状态与之前的状态也有关，若仅用当前的状态来求解决策过程，势必导致决策的泛化能力变差。为了解决这个问题，可以利用循环神经网络对历史信息建模，获得包含历史信息的状态表征，表征过程也可以使用注意力机制等手段，最后在表征状态空间求解马尔可夫决策过程问题。</p><h4 id="2-4-友善的面试官：请分别写出基于状态价值函数的贝尔曼方程以及基于动作价值函数的贝尔曼方程。"><a href="#2-4-友善的面试官：请分别写出基于状态价值函数的贝尔曼方程以及基于动作价值函数的贝尔曼方程。" class="headerlink" title="2-4 友善的面试官：请分别写出基于状态价值函数的贝尔曼方程以及基于动作价值函数的贝尔曼方程。"></a><strong>2-4</strong> 友善的面试官：请分别写出基于状态价值函数的贝尔曼方程以及基于动作价值函数的贝尔曼方程。</h4><ol><li>基于状态价值函数的贝尔曼方程：$V_{\pi}(s) = \sum_{a}{\pi(a|s)}\sum_{s’,r}{p(s’,r|s,a)[r(s,a)+\gamma V_{\pi}(s’)]}$；</li><li>基于动作价值函数的贝尔曼方程：$Q_{\pi}(s,a)=\sum_{s’,r}p(s’,r|s,a)[r(s’,a)+\gamma V_{\pi}(s’)]$。</li></ol><h4 id="2-5-友善的面试官：请问最佳价值函数-V-和最佳策略-pi-为什么等价呢？"><a href="#2-5-友善的面试官：请问最佳价值函数-V-和最佳策略-pi-为什么等价呢？" class="headerlink" title="2-5 友善的面试官：请问最佳价值函数 $V^$ 和最佳策略 $\pi^*$ 为什么等价呢？"></a><strong>2-5</strong> 友善的面试官：请问最佳价值函数 $V^<em>$ 和最佳策略 $\pi^*</em>$ 为什么等价呢？</h4><p>最佳价值函数的定义为$V^* (s)=\max_{\pi} V_{\pi}(s)$，即我们搜索一种策略 $\pi$ 来让每个状态的价值最大。</p><p>$V^<em>$ 就是到达每一个状态其的最大价值，同时我们得到的策略就可以说是最佳策略，即 $\pi^{</em>}(s)=\underset{\pi}{\arg \max }~ V_{\pi}(s)$ 。最佳策略使得每个状态的价值函数都取得最大值。所以如果我们可以得到一个最佳价值函数，就可以说某一个马尔可夫决策过程的环境被解。在这种情况下，其最佳价值函数是一致的，即其达到的上限的值是一致的，但这里可能有多个最佳策略对应于相同的最佳价值。</p><h4 id="2-6-友善的面试官：能不能手写一下第n步的价值函数更新公式呀？另外，当-n越来越大时，价值函数的期望和方差是分别变大还是变小呢？"><a href="#2-6-友善的面试官：能不能手写一下第n步的价值函数更新公式呀？另外，当-n越来越大时，价值函数的期望和方差是分别变大还是变小呢？" class="headerlink" title="2-6 友善的面试官：能不能手写一下第n步的价值函数更新公式呀？另外，当 n越来越大时，价值函数的期望和方差是分别变大还是变小呢？"></a><strong>2-6</strong> 友善的面试官：能不能手写一下第n步的价值函数更新公式呀？另外，当 n越来越大时，价值函数的期望和方差是分别变大还是变小呢？</h4><p><em>n</em> 越大，方差越大，期望偏差越小。价值函数的更新公式如下：</p><script type="math/tex; mode=display">Q\left(S, A\right) \leftarrow Q\left(S, A\right)+\alpha\left[\sum_{i=1}^{n} \gamma^{i-1} r_{t+i}+\gamma^{n} \max _{a} Q\left(S',a\right)-Q\left(S, A\right)\right]</script>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.表格型方法</title>
      <link href="/easy_rl_exercise/3.%E8%A1%A8%E6%A0%BC%E5%9E%8B%E6%96%B9%E6%B3%95/"/>
      <url>/easy_rl_exercise/3.%E8%A1%A8%E6%A0%BC%E5%9E%8B%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h1><ul><li><strong>概率函数和奖励函数</strong>：概率函数定量地表达状态转移的概率，其可以表现环境的随机性。但是实际上，我们经常处于一个未知的环境中，即概率函数和奖励函数是未知的。</li><li><strong>Q表格</strong>：其表示形式是表格，其中表格的横轴为动作（智能体的动作），纵轴为环境的状态，每一个坐标点对应某时刻智能体和环境的状态，并通过对应的奖励反馈选择被执行的动作。一般情况下，Q表格是一个已经训练好的表格，不过我们也可以每执行一步，就对Q表格进行更新，然后用下一个状态的Q值来更新当前状态的Q值（即时序差分方法）。</li><li><strong>时序差分（temporal difference，TD）方法</strong>：一种Q函数（Q值）的更新方式，流程是使用下一步的Q值 $Q(s_{t+1},a_{t+1})$ 来更新当前步的Q值 $Q(s_t,a_t)$。完整的计算公式如下：$Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha [r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]$ 。</li><li><strong>Sarsa算法</strong>：一种更新前一时刻状态的单步更新的强化学习算法，也是一种同策略学习算法。该算法由于每次更新Q函数时需要知道前一步的状态、动作、奖励以及当前时刻的状态、将要执行的动作，即 $s_{t}$、$a_{t}$、$r_{t+1}$、$s_{t+1}$、$a_{t+1}$ 这几个值，因此被称为 Sarsa 算法。智能体每进行一次循环，都会用 $s_{t}$、$a_{t}$、$r_{t+1}$、$s_{t+1}$、$a_{t+1}$ 对前一步的Q值（函数）进行一次更新。</li></ul><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><h4 id="3-1-构成强化学习的马尔可夫决策过程的四元组有哪些变量？"><a href="#3-1-构成强化学习的马尔可夫决策过程的四元组有哪些变量？" class="headerlink" title="3-1 构成强化学习的马尔可夫决策过程的四元组有哪些变量？"></a><strong>3-1</strong> 构成强化学习的马尔可夫决策过程的四元组有哪些变量？</h4><p>状态、动作、状态转移概率和奖励，分别对应$(S,A,P,R)$，后面有可能会加上折扣因子构成五元组。</p><h4 id="3-2-请通俗地描述强化学习的“学习”流程。"><a href="#3-2-请通俗地描述强化学习的“学习”流程。" class="headerlink" title="3-2 请通俗地描述强化学习的“学习”流程。"></a><strong>3-2</strong> 请通俗地描述强化学习的“学习”流程。</h4><p>可以将强化学习的“学习”流程类比于人类的学习流程。人类学习就是尝试每一条路，并记录尝试每一条路后的最终结果。在人类尝试的过程中，其实就可以慢慢地了解到哪一条路（对应于强化学习中的状态概念）会更好。我们用价值函数 $V(s)$ 来定量表达该状态的优劣，然后用Q函数来判断在什么状态下做什么动作能够得到最大奖励，在强化学习中我们用Q函数来表示状态-动作值。</p><h4 id="3-3-请描述基于Sarsa算法的智能体的学习过程。"><a href="#3-3-请描述基于Sarsa算法的智能体的学习过程。" class="headerlink" title="3-3 请描述基于Sarsa算法的智能体的学习过程。"></a><strong>3-3</strong> 请描述基于Sarsa算法的智能体的学习过程。</h4><p>对于环境和智能体。两者每交互一次以后，智能体都会向环境输出动作，接着环境会反馈给智能体当前时刻的状态和奖励。那么智能体此时会进行两步操作：</p><ol><li>使用已经训练好的Q表格，对应环境反馈的状态和奖励选取对应的动作进行输出。</li><li>我们已经拥有了$(s_{t}, a_{t}, r_{t+1}, s_{t+1}, a_{t+1})$  这几个值，并直接使用 $a_{t+1}$ 更新我们的Q表格。</li></ol><h4 id="3-4-Q学习算法和Sarsa算法的区别是什么？"><a href="#3-4-Q学习算法和Sarsa算法的区别是什么？" class="headerlink" title="3-4 Q学习算法和Sarsa算法的区别是什么？"></a><strong>3-4</strong> Q学习算法和Sarsa算法的区别是什么？</h4><p>Sarsa算法是Q学习算法的改进（这句话可参考论文 “On-Line Q-Learning Using Connectionist Systems”的摘要部分），详细描述如下。</p><ol><li>首先，Q学习是异策略的时序差分学习方法，而 Sarsa 算法是同策略的时序差分学习方法。</li><li>其次，Sarsa算法在更新Q表格的时候所用到的 $a’$ 是获取下一个Q值时一定会执行的动作。这个动作有可能是用 $\varepsilon$-贪心方法采样出来的，也有可能是 $\mathrm{max}_Q$ 对应的动作，甚至是随机动作。</li><li>但是Q学习在更新Q表格的时候所用到的Q值 $Q(S’,a’)$ 对应的动作不一定是下一步会执行的动作，因为下一步实际会执行的动作可能是因为进一步的探索而得到的。Q学习默认的动作不是通过行为策略来选取的，它默认 $a’$ 为最佳策略对应的动作，所以Q学习算法在更新的时候，不需要传入 $a’$ ，即 $a_{t+1}$ 。</li><li>更新公式的对比（区别只在目标计算部分）。</li></ol><p>Sarsa算法的公式：$r_{t+1}+\gamma Q(s_{t+1}, a_{t+1})$ 。</p><p>Q学习算法的公式：$r_{t+1}+\gamma \underset{a}{\max} Q\left(s_{t+1}, a\right)$ 。</p><p>总结起来，Sarsa算法实际上是用固有的策略产生 {$S,A,R,S’,A’$} 这一条轨迹，然后使用 $Q(s_{t+1},a_{t+1})$ 更新原本的Q值 $Q(s_t,a_t)$ 。但是Q学习算法并不需要知道实际上选择的动作，它默认下一个动作就是Q值最大的那个动作。所以Sarsa算法的动作通常会更加“保守胆小”，而对应的Q学习算法的动作会更加“莽撞激进”。</p><h4 id="3-5-同策略和异策略的区别是什么？"><a href="#3-5-同策略和异策略的区别是什么？" class="headerlink" title="3-5 同策略和异策略的区别是什么？"></a><strong>3-5</strong> 同策略和异策略的区别是什么？</h4><p>Sarsa算法就是一个典型的同策略算法，它只用一个 $\pi$ ，为了兼顾探索和开发，它在训练的时候会显得有点儿“胆小怕事”。它在解决悬崖寻路问题的时候，会尽可能地远离悬崖边，确保哪怕自己不小心向未知区域探索了一些，也还是处在安全区域内，不至于掉入悬崖中。</p><p>Q学习算法是一个比较典型的异策略算法，它有目标策略（target policy），用 $\pi$ 来表示。此外还有行为策略（behavior policy），用 $\mu$ 来表示。它分离了目标策略与行为策略，使得其可以大胆地用行为策略探索得到的经验轨迹来优化目标策略。这样智能体就更有可能探索到最优的策略。</p><p>比较Q学习算法和Sarsa算法的更新公式可以发现，Sarsa算法并没有选取最大值的操作。因此，Q学习算法是非常激进的，其希望每一步都获得最大的奖励；Sarsa算法则相对来说偏保守，会选择一条相对安全的迭代路线。</p><h1 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h1><h4 id="3-1-友善的面试官：同学，你能否简述同策略和异策略的区别呢？"><a href="#3-1-友善的面试官：同学，你能否简述同策略和异策略的区别呢？" class="headerlink" title="3-1 友善的面试官：同学，你能否简述同策略和异策略的区别呢？"></a><strong>3-1</strong> 友善的面试官：同学，你能否简述同策略和异策略的区别呢？</h4><p>同策略和异策略的根本区别在于生成样本的策略和参数更新时的策略是否相同。对于同策略，行为策略和要优化的策略是同一策略，更新了策略后，就用该策略的最新版本对数据进行采样；对于异策略，其使用任意行为策略来对数据进行采样，并利用其更新目标策略。例如，Q学习在计算下一状态的预期奖励时使用了最大化操作，直接选择最优动作，而当前策略并不一定能选择到最优的动作，因此这里生成样本的策略和学习时的策略不同，所以Q学习算法是异策略算法；相对应的Sarsa算法则是基于当前的策略直接执行一次动作选择，然后用动作和对应的状态更新当前的策略，因此生成样本的策略和学习时的策略相同，所以Sarsa算法为同策略算法。</p><h4 id="3-2-友善的面试官：能否细致地讲一下Q学习算法，最好可以写出其-Q-s-a-的更新公式。另外，它是同策略还是异策略，原因是什么呢？"><a href="#3-2-友善的面试官：能否细致地讲一下Q学习算法，最好可以写出其-Q-s-a-的更新公式。另外，它是同策略还是异策略，原因是什么呢？" class="headerlink" title="3-2 友善的面试官：能否细致地讲一下Q学习算法，最好可以写出其 $Q(s,a)$ 的更新公式。另外，它是同策略还是异策略，原因是什么呢？"></a><strong>3-2</strong> 友善的面试官：能否细致地讲一下Q学习算法，最好可以写出其 $Q(s,a)$ 的更新公式。另外，它是同策略还是异策略，原因是什么呢？</h4><p>Q学习是通过计算最优动作价值函数来求策略的一种时序差分的学习方法，其更新公式为</p><script type="math/tex; mode=display">Q(s, a) \leftarrow Q(s, a) + \alpha [r(s,a) + \gamma \max_{a'} Q(s', a') - Q(s, a)]</script><p>其是异策略的，由于Q更新使用了下一个时刻的最大值，因此其只关心哪个动作使得 $Q(s_{t+1}, a)$ 取得最大值，而实际上到底采取了哪个动作（行为策略），Q学习并不关心。这表明优化策略并没有用到行为策略的数据，所以说它是异策略的。</p><h4 id="3-3-友善的面试官：好的，看来你对于Q学习算法很了解，那么能否讲一下与Q学习算法类似的Sarsa算法呢，最好也可以写出其对应的-Q-s-a-更新公式。另外，它是同策略还是异策略，为什么？"><a href="#3-3-友善的面试官：好的，看来你对于Q学习算法很了解，那么能否讲一下与Q学习算法类似的Sarsa算法呢，最好也可以写出其对应的-Q-s-a-更新公式。另外，它是同策略还是异策略，为什么？" class="headerlink" title="3-3 友善的面试官：好的，看来你对于Q学习算法很了解，那么能否讲一下与Q学习算法类似的Sarsa算法呢，最好也可以写出其对应的 $Q(s,a)$ 更新公式。另外，它是同策略还是异策略，为什么？"></a><strong>3-3</strong> 友善的面试官：好的，看来你对于Q学习算法很了解，那么能否讲一下与Q学习算法类似的Sarsa算法呢，最好也可以写出其对应的 $Q(s,a)$ 更新公式。另外，它是同策略还是异策略，为什么？</h4><p>Sarsa算法可以算是Q学习算法的改进，其更新公式为</p><script type="math/tex; mode=display">Q(s, a) \leftarrow Q(s, a) + \alpha [r(s,a) + \gamma  Q(s', a') - Q(s, a)]</script><p>其为同策略的，Sarsa算法必须执行两次动作得到 $(s,a,r,s’,a’)$ 才可以更新一次；而且 $a’$ 是在特定策略 $\pi$ 的指导下执行的动作，因此估计出来的 $Q(s,a)$ 是在该策略 $\pi$ 下的Q值，样本生成用的 $\pi$ 和估计的 $\pi$ 是同一个，因此是同策略。</p><h4 id="3-4-友善的面试官：请问基于价值的方法和基于策略的方法的区别是什么？"><a href="#3-4-友善的面试官：请问基于价值的方法和基于策略的方法的区别是什么？" class="headerlink" title="3-4 友善的面试官：请问基于价值的方法和基于策略的方法的区别是什么？"></a><strong>3-4</strong> 友善的面试官：请问基于价值的方法和基于策略的方法的区别是什么？</h4><ol><li>生成策略上的差异，前者确定，后者随机。基于价值的方法中动作-价值对的估计值最终会收敛（通常是不同的数，可以转化为0～1的概率），因此通常会获得一个确定的策略；基于策略的方法不会收敛到一个确定的值，另外他们会趋向于生成最佳随机策略。如果最佳策略是确定的，那么最优动作对应的值函数的值将远大于次优动作对应的值函数的值，值函数的大小代表概率的大小。</li><li>动作空间是否连续，前者离散，后者连续。基于价值的方法，对于连续动作空间问题，虽然可以将动作空间离散化处理，但离散间距的选取不易确定。过大的离散间距会导致算法取不到最优动作，会在最优动作附近徘徊；过小的离散间距会使得动作的维度增大，会和高维度动作空间一样导致维度灾难，影响算法的速度。而基于策略的方法适用于连续的动作空间，在连续的动作空间中，可以不用计算每个动作的概率，而是通过正态分布选择动作。</li><li>基于价值的方法，例如Q学习算法，是通过求解最优价值函数而间接地求解最优策略；基于策略的方法，例如REINFORCE等算法直接将策略参数化，通过策略搜索、策略梯度或者进化方法来更新参数以最大化回报。基于价值的方法不易扩展到连续动作空间，并且当同时采用非线性近似、自举等策略时会有收敛问题。策略梯度具有良好的收敛性。</li><li>另外，对于价值迭代和策略迭代，策略迭代有两个循环，一个是在策略估计的时候，为了求当前策略的价值函数需要迭代很多次；另一个是外面的大循环，即策略评估、策略提升。价值迭代算法则是一步到位，直接估计最优价值函数，因此没有策略提升环节。</li></ol><h4 id="3-5-友善的面试官：请简述一下时序差分方法。"><a href="#3-5-友善的面试官：请简述一下时序差分方法。" class="headerlink" title="3-5 友善的面试官：请简述一下时序差分方法。"></a><strong>3-5</strong> 友善的面试官：请简述一下时序差分方法。</h4><p>时序差分算法是使用广义策略迭代来更新Q函数的方法，核心是使用自举，即价值函数的更新使用下一个状态的价值函数来估计当前状态的价值。也就是使用下一步的Q值 $Q(s_{t+1},a_{t+1})$ 来更新当前步的Q值 $Q(s_t,a_t) $。完整的计算公式如下：</p><script type="math/tex; mode=display">Q(s_t,a_t) \leftarrow     Q(s_t,a_t) + \alpha [r_{t+1}+\gamma Q(s_{t+1},a_{t+1})]</script><h4 id="3-6-友善的面试官：请问蒙特卡洛方法和时序差分方法是无偏估计吗？另外谁的方差更大呢？为什么？"><a href="#3-6-友善的面试官：请问蒙特卡洛方法和时序差分方法是无偏估计吗？另外谁的方差更大呢？为什么？" class="headerlink" title="3-6 友善的面试官：请问蒙特卡洛方法和时序差分方法是无偏估计吗？另外谁的方差更大呢？为什么？"></a><strong>3-6</strong> 友善的面试官：请问蒙特卡洛方法和时序差分方法是无偏估计吗？另外谁的方差更大呢？为什么？</h4><p>蒙特卡洛方法是无偏估计，时序差分方法是有偏估计；蒙特卡洛方法的方差较大，时序差分方法的方差较小，原因在于时序差分方法中使用了自举，实现了基于平滑的效果，导致估计的价值函数的方差更小。</p><h4 id="3-7-友善的面试官：能否简单说一下动态规划方法、蒙特卡洛方法和时序差分方法的异同点？"><a href="#3-7-友善的面试官：能否简单说一下动态规划方法、蒙特卡洛方法和时序差分方法的异同点？" class="headerlink" title="3-7 友善的面试官：能否简单说一下动态规划方法、蒙特卡洛方法和时序差分方法的异同点？"></a><strong>3-7</strong> 友善的面试官：能否简单说一下动态规划方法、蒙特卡洛方法和时序差分方法的异同点？</h4><p>相同点：都用于进行价值函数的描述与更新，并且所有方法都基于对未来事件的展望计算一个回溯值。</p><p>不同点：蒙特卡洛方法和时序差分方法属于免模型方法，而动态规划属于有模型方法；时序差分方法和蒙特卡洛方法，因为都是免模型的方法，所以对于后续状态的获知也都是基于试验的方法；时序差分方法和动态规划方法的策略评估，都能基于当前状态的下一步预测情况来得到对于当前状态的价值函数的更新。</p><p>另外，时序差分方法不需要等到试验结束后才能进行当前状态的价值函数的计算与更新，而蒙特卡洛方法需要与环境交互，产生一整条马尔可夫链并直到最终状态才能进行更新。时序差分方法和动态规划方法的策略评估不同之处为免模型和有模型，动态规划方法可以凭借已知转移概率推断出后续的状态情况，而时序差分方法借助试验才能知道。</p><p>蒙特卡洛方法和时序差分方法的不同在于，蒙特卡洛方法进行了完整的采样来获取长期的回报值，因而在价值估计上会有更小的偏差，但是也正因为收集了完整的信息，所以价值的方差会更大，原因在于其基于试验的采样得到，和真实的分布有差距，不充足的交互导致较大方差。而时序差分方法则相反，因为它只考虑了前一步的回报值，其他都是基于之前的估计值，因而其价值估计相对来说具有偏差大方差小的特点。</p><p>三者的联系：对于TD($\lambda$)方法，如果 $\lambda = 0$ ，那么此时等价于时序差分方法，即只考虑下一个状态；如果 $\lambda = 1$ ，等价于蒙特卡洛方法，即考虑 $T-1$ 个后续状态直到整个试验结束。</p>]]></content>
      
      
      <categories>
          
          <category> Cython </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4.DQN算法</title>
      <link href="/easy_rl_exercise/4.DQN%E7%AE%97%E6%B3%95/"/>
      <url>/easy_rl_exercise/4.DQN%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h1><ul><li><strong>深度Q网络（deep Q-network，DQN）</strong>：基于深度学习的Q学习算法，其结合了价值函数近似（value function approximation）与神经网络技术，并采用目标网络和经验回放等方法进行网络的训练。</li><li><strong>状态-价值函数（state-value function）</strong>：其输入为演员某一时刻的状态，输出为一个标量，即当演员在对应的状态时，预期的到过程结束时间段内所能获得的价值。</li><li><strong>状态-价值函数贝尔曼方程（state-value function Bellman equation）</strong>：基于状态-价值函数的贝尔曼方程，它表示在状态 $s_t$ 下对累积奖励 $G_t$ 的期望。</li><li><strong>Q函数（Q-function）</strong>： 其也被称为动作价值函数（action-value function）。其输入是一个状态-动作对，即在某一具体的状态采取对应的动作，假设我们都使用某个策略 $\pi$ ，得到的累积奖励的期望值有多大。</li><li><strong>目标网络（target network）</strong>：其可解决在基于时序差分的网络中，优化目标 $Q_{\pi}\left(s_{t}, a_{t}\right) = r_{t}+Q_{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$ 左右两侧会同时变化使得训练过程不稳定，从而增大回归的难度的问题。目标网络选择将右边部分，即 $r_{t}+Q_{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$ 固定，通过改变左边部分，即 $Q_{\pi}\left(s_{t}, a_{t}\right)$ 中的参数进行回归，这也是深度Q网络应用中比较重要的技巧。</li><li><strong>探索（exploration）</strong>：我们在使用Q函数的时候，我们的策略完全取决于Q函数，这有可能导致出现对应的动作是固定的某几个数值的情况，而不像策略梯度中的输出是随机的，我们再从随机分布中采样选择动作。这会导致我们继续训练的输入值一样，从而“加重”输出的固定性，导致整个模型的表达能力急剧下降，这就是探索-利用窘境（exploration-exploitation dilemma）问题。我们可以使用 $\varepsilon$-贪心和玻尔兹曼探索（Boltzmann exploration）等探索方法进行优化。</li><li><strong>经验回放（experience replay）</strong>：其会构建一个回放缓冲区（replay buffer）来保存许多经验，每一个经验的形式如下：在某一个状态 $s_t$，采取某一个动作 $a_t$，得到奖励 $r_t$，然后进入状态 $s_{t+1}$。我们使用 $\pi$ 与环境交互多次，把收集到的经验都存储在回放缓冲区中。当我们的缓冲区“装满”后，就会自动删去最早进入缓冲区的经验。在训练时，对于每一轮迭代都有相对应的批量（batch）（与我们训练普通的网络一样，都是通过采样得到的），然后用这个批量中的经验去更新我们的Q函数。综上，Q函数在采样和训练的时候，会用到过去的经验，所以这里称这个方法为经验回放，其也是深度Q网络应用中比较重要的技巧。</li><li><strong>双深度Q网络（double DQN）</strong>：在双深度Q网络中存在两个Q网络，第一个Q网络决定哪一个动作的Q值最大，从而决定对应的动作。另一方面，Q值是用 $Q’$ 计算得到的，这样就可以避免过度估计的问题。具体地，假设我们有两个Q函数并且第一个Q函数高估了它现在执行的动作 $a$ 的值，这没关系，只要第二个Q函数 $Q’$ 没有高估动作 $a$ 的值，那么计算得到的就还是正常的值。</li><li><strong>竞争深度Q网络（dueling DQN）</strong>：将原来的深度Q网络的计算过程分为两步。第一步计算一个与输入有关的标量 $\mathrm{V(s)}$；第二步计算一个向量 $\mathrm{A(s,a)}$ 对应每一个动作。最后的网络将两步的结果相加，得到我们最终需要的Q值。用一个公式表示就是 $\mathrm{Q(s,a)=V(s)+A(s,a)}$ 。另外，竞争深度Q网络，使用状态价值函数与动作价值函数来评估Q值。</li><li><strong>优先级经验回放（prioritized experience replay，PER）</strong>：这个方法是为了解决我们在第6章中提出的经验回放方法的不足而提出的。我们在使用经验回放时，均匀地取出回放缓冲区（reply buffer）中的采样数据，这里并没有考虑数据间的权重大小。但是我们应该将那些训练效果不好的数据对应的权重加大，即其应该有更大的概率被采样到。综上，优先级经验回放不仅改变了被采样数据的分布，还改变了训练过程。</li><li><strong>噪声网络（noisy net）</strong>：其在每一个回合开始的时候，即智能体要和环境交互的时候，在原来的Q函数的每一个参数上加上一个高斯噪声（Gaussian noise），把原来的Q函数变成 $\tilde{Q}$ ，即噪声Q函数。同样，我们把每一个网络的权重等参数都加上一个高斯噪声，就得到一个新的网络 $\tilde{Q}$ 。我们会使用这个新的网络与环境交互直到结束。</li><li><strong>分布式Q函数（distributional Q-function）</strong>：对深度Q网络进行模型分布，将最终网络的输出的每一类别的动作再进行分布操作。</li><li><strong>彩虹（rainbow）</strong>：将7个技巧/算法综合起来的方法，7个技巧分别是——深度Q网络、双深度Q网络、优先级经验回放的双深度Q网络、竞争深度Q网络、异步优势演员-评论员算法（A3C）、分布式Q函数、噪声网络，进而考察每一个技巧的贡献度或者与环境的交互是否是正反馈的。</li></ul><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><h4 id="4-1-为什么在深度Q网络中采用价值函数近似的表示方法？"><a href="#4-1-为什么在深度Q网络中采用价值函数近似的表示方法？" class="headerlink" title="4-1 为什么在深度Q网络中采用价值函数近似的表示方法？"></a><strong>4-1</strong> 为什么在深度Q网络中采用价值函数近似的表示方法？</h4><p>首先深度Q网络为基于深度学习的Q学习算法，而在Q学习中，我们使用表格来存储每一个状态下动作的奖励，即我们在正文中介绍的动作价值函数 $Q(s,a)$ 。但是在我们的实际任务中，状态量通常数量巨大，并且在连续任务中会遇到维度灾难等问题，使用真正的价值函数通常是不切实际的，所以使用了与价值函数近似的表示方法。</p><h4 id="4-2-评论员的输出通常与哪几个值直接相关？"><a href="#4-2-评论员的输出通常与哪几个值直接相关？" class="headerlink" title="4-2 评论员的输出通常与哪几个值直接相关？"></a><strong>4-2</strong> 评论员的输出通常与哪几个值直接相关？</h4><p>与状态和演员直接相关。我们在讨论输出时通常是针对一个演员衡量一个状态的好坏，也就是状态、价值从本质上来说是依赖于演员的。不同的演员在相同的状态下也会有不同的输出。</p><h4 id="4-3-我们通常怎么衡量状态价值函数-V-pi-s-？其优势和劣势分别有哪些？"><a href="#4-3-我们通常怎么衡量状态价值函数-V-pi-s-？其优势和劣势分别有哪些？" class="headerlink" title="4-3 我们通常怎么衡量状态价值函数 $V_{\pi}(s)$ ？其优势和劣势分别有哪些？"></a><strong>4-3</strong> 我们通常怎么衡量状态价值函数 $V_{\pi}(s)$ ？其优势和劣势分别有哪些？</h4><p>（1）<strong>基于蒙特卡洛的方法</strong>：本质上就是让演员与环境交互。评论员根据统计结果，将演员和状态对应起来，即如果演员看到某一状态 $s_a$ ，将预测接下来的累积奖励有多大，如果看到另一个状态 $s_b$，将预测接下来的累积奖励有多大。但是其普适性不好，其需要匹配到所有的状态。如果我们面对的是一个简单的例如贪吃蛇游戏等状态有限的问题还可以应对，但是如果我们面对的是一个图片型的任务，我们几乎不可能将所有的状态（对应每一帧的图像）的都“记录”下来。总之，其不能对未出现过的输入状态进行对应价值的输出。</p><p>（2）<strong>基于蒙特卡洛的网络方法</strong>：为了弥补上面描述的基于蒙特卡洛的方法的不足，我们将其中的状态价值函数 $V_{\pi}(s)$ 定义为一个网络，其可以对于从未出现过的输入状态，根据网络的泛化和拟合能力，“估测”出一个价值输出。</p><p>（3）<strong>基于时序差分的网络方法</strong>，即基于时序差分的网络：与我们在前4章介绍的蒙特卡洛方法与时序差分方法的区别一样，基于时序差分的网络方法和基于蒙特卡洛的网络方法的区别也相同。在基于蒙特卡洛的方法中，每次我们都要计算累积奖励，也就是从某一个状态 $s_a$ 一直到游戏结束的时候，得到的所有奖励的总和。所以要应用基于蒙特卡洛的方法时，我们必须至少把游戏玩到结束。但有些游戏要玩到游戏结束才能够更新网络花费的时间太长了，因此我们会采用基于时序差分的网络方法。基于时序差分的网络方法不需要把游戏玩到结束，只要在游戏某一个状态 $s_t$ 的时候，采取动作 $a_t$ 得到奖励 $r_t$ ，进入状态 $s_{t+1}$，就可以应用基于时序差分的网络方法。其公式与之前介绍的时序差分方法类似，即 $V_{\pi}\left(s_{t}\right)=V_{\pi}\left(s_{t+1}\right)+r_{t}$。</p><p>（4）<strong>基于蒙特卡洛方法和基于时序差分方法的区别</strong>在于： 蒙特卡洛方法本身具有很大的随机性，我们可以将其 $G_a$ 视为一个随机变量，所以其最终的偏差很大。而对于时序差分，其具有随机的变量 $r$ 。因为在状态 $s_t$ 采取同一个动作，所得的奖励也不一定是一样的，所以对于时序差分方法来说，$r$ 是一个随机变量。但是相对于蒙特卡洛方法的 $G_a$ 来说，$r$ 的随机性非常小，这是因为 $G_a$ 本身就是由很多的 $r$ 组合而成的。从另一个角度来说，在时序差分方法中，我们的前提是 $r_t=V_{\pi}\left(s_{t+1}\right)-V_{\pi}\left(s_{t}\right)$ ，但是我们通常无法保证 $V_{\pi}\left(s_{t+1}\right)$ 、$V_{\pi}\left(s_{t}\right)$ 计算的误差为0。所以当 $V_{\pi}\left(s_{t+1}\right)$ 、$V_{\pi}\left(s_{t}\right)$ 计算得不准确，得到的结果也会是不准确的。总之，两者各有优劣。</p><p>（5）目前，基于时序差分的方法是比较常用的，基于蒙特卡洛的方法其实是比较少用的。</p><h4 id="4-4-基于本章正文介绍的基于蒙特卡洛的网络方法，我们怎么训练模型呢？或者我们应该将其看作机器学习中什么类型的问题呢？"><a href="#4-4-基于本章正文介绍的基于蒙特卡洛的网络方法，我们怎么训练模型呢？或者我们应该将其看作机器学习中什么类型的问题呢？" class="headerlink" title="4-4 基于本章正文介绍的基于蒙特卡洛的网络方法，我们怎么训练模型呢？或者我们应该将其看作机器学习中什么类型的问题呢？"></a><strong>4-4</strong> 基于本章正文介绍的基于蒙特卡洛的网络方法，我们怎么训练模型呢？或者我们应该将其看作机器学习中什么类型的问题呢？</h4><p>理想状态下，我们期望对于一个输入状态，输出其无误差的奖励价值。对于价值函数，如果输入状态是 $s_a$，正确的输出价值应该是 $G_a$。如果输入状态是 $s_b$，正确的输出价值应该是 $G_b$。所以在训练的时候，其就是一个典型的机器学习中的回归问题。我们实际中需要输出的仅仅是一个非精确值，即我们希望在输入状态 $s_a$ 的时候，输出价值与 $G_a$ 越近越好；输入 $s_b$ 的时候，输出价值与 $G_b$ 越近越好。其训练方法与我们在训练卷积神经网络等深度神经网络时的方法类似。</p><h4 id="4-5-基于本章正文中介绍的基于时序差分的网络方法，具体地，我们应该怎么训练模型呢？"><a href="#4-5-基于本章正文中介绍的基于时序差分的网络方法，具体地，我们应该怎么训练模型呢？" class="headerlink" title="4-5 基于本章正文中介绍的基于时序差分的网络方法，具体地，我们应该怎么训练模型呢？"></a><strong>4-5</strong> 基于本章正文中介绍的基于时序差分的网络方法，具体地，我们应该怎么训练模型呢？</h4><p>基于时序差分网络的核心函数为 $V_{\pi}\left(s_{t}\right)=V_{\pi}\left(s_{t+1}\right)+r_{t}$。我们将状态 $s_t$ 输入网络，因为将 $s_t$ 输入网络会得到输出 $V_{\pi}(s_t)$，同样将 $s_{t+1}$ 输入网络会得到$V_{\pi}(s_{t+1})$。同时核心函数 $V_{\pi}\left(s_{t}\right)=V_{\pi}\left(s_{t+1}\right)+r_{t}$  告诉我们， $V_{\pi}(s_t)$ 减 $V_{\pi}(s_{t+1})$ 的值应该是 $r_t$。我们希望它们两个相减的损失值与 $r_t$ 尽可能地接近。这也是网络的优化目标，我们称之为损失函数。</p><h4 id="4-6-动作价值函数和状态价值函数的有什么区别和联系？"><a href="#4-6-动作价值函数和状态价值函数的有什么区别和联系？" class="headerlink" title="4-6 动作价值函数和状态价值函数的有什么区别和联系？"></a><strong>4-6</strong> 动作价值函数和状态价值函数的有什么区别和联系？</h4><p>（1）状态价值函数的输入是一个状态，它根据状态计算出当前这个状态以后的累积奖励的期望值是多少。</p><p>（2）动作价值函数的输入是状态-动作对，即在某一个状态采取某一个动作，同时假设我们都使用策略 $\pi$ ，得到的累积奖励的期望值是多少。</p><h4 id="4-7-请介绍Q函数的两种表示方法。"><a href="#4-7-请介绍Q函数的两种表示方法。" class="headerlink" title="4-7 请介绍Q函数的两种表示方法。"></a><strong>4-7</strong> 请介绍Q函数的两种表示方法。</h4><ol><li>使用<strong>状态-动作</strong>对表示时，即当Q函数的输入是状态-动作对时，输出就是一个标量。</li><li>仅使用<strong>状态</strong>表示时，即当Q函数的输入仅是一个状态时，输出就是多个价值。</li></ol><h4 id="4-8-当得到了Q函数后，我们应当如何找到更好的策略-pi’-呢？或者说-pi’-的本质是什么？"><a href="#4-8-当得到了Q函数后，我们应当如何找到更好的策略-pi’-呢？或者说-pi’-的本质是什么？" class="headerlink" title="4-8 当得到了Q函数后，我们应当如何找到更好的策略 $\pi’$ 呢？或者说 $\pi’$ 的本质是什么？"></a><strong>4-8</strong> 当得到了Q函数后，我们应当如何找到更好的策略 $\pi’$ 呢？或者说 $\pi’$ 的本质是什么？</h4><p>首先， $\pi’$ 由 $\pi^{\prime}(s)=\underset{a}{\arg \max} Q_{\pi}(s, a)$ 计算而得，其表示假设我们已经学习出 $\pi$ 的Q函数，对于某一个状态 $s$ ，把所有可能的动作 $a$ 一一代入这个Q函数，看看哪一个动作 $a$ 可以让Q函数的价值最大，那么该动作就是 $\pi’$ 将会执行的动作。所以根据以上方法决定动作的策略 $\pi’$ 一定比原来的策略 $\pi$ 要好，即 $V_{\pi^{\prime}}(s) \geqslant V_{\pi}(s)$ 。</p><h4 id="4-9-解决探索-利用窘境问题的探索的方法有哪些？"><a href="#4-9-解决探索-利用窘境问题的探索的方法有哪些？" class="headerlink" title="4-9 解决探索-利用窘境问题的探索的方法有哪些？"></a><strong>4-9</strong> 解决探索-利用窘境问题的探索的方法有哪些？</h4><p>（1） <strong>$\varepsilon$-贪心</strong>： 我们有 $1-\varepsilon$ 的概率（通常 $\varepsilon$ 很小）完全按照Q函数决定动作，但是有 $\varepsilon$ 的概率使得动作是随机的。通常在实现上， $\varepsilon$的值会随着时间递减。也就是在最开始的时候，因为还不知道哪个动作是比较好的，所以我们会花比较大的力气做探索。接下来随着训练的次数越来越多，我们已经比较确定哪一种策略是比较好的，就会减少探索，从而把 $\varepsilon$ 的值变小，主要根据Q函数来决定未来的动作，随机性就会变小。</p><p>（2） <strong>玻尔兹曼探索</strong>：这个方法比较像策略梯度。在策略梯度里面，网络的输出是一个期望动作空间上的一个概率分布，我们根据概率分布去采样。所以也可以根据Q值确定一个概率分布，假设某一个动作的Q值越大，代表它越好，我们采取这个动作的概率就越高。</p><h4 id="4-10-我们使用经验回放有什么好处？"><a href="#4-10-我们使用经验回放有什么好处？" class="headerlink" title="4-10 我们使用经验回放有什么好处？"></a><strong>4-10</strong> 我们使用经验回放有什么好处？</h4><p>（1）首先，在强化学习的整个过程中，最花时间的过程是与环境交互，使用GPU乃至TPU来训练网络相对来说是比较快的。而用回放缓冲区可以减少与环境交互的次数。因为在训练的时候，我们的经验不需要通通来自于某一个策略（或者当前时刻的策略）。一些由过去的策略所得到的经验可以放在回放缓冲区中被使用多次，被反复地再利用，这样采样到的经验才能被高效地利用。</p><p>（2）另外，在训练网络的时候，我们其实希望一个批量里面的数据越多样越好。如果一个批量里面的数据都是同性质的，我们训练出的模型的拟合能力可能不会很乐观。如果一个批量里面都是一样的数据，在训练的时候，拟合效果会比较差。如果回放缓冲区里面的经验通通来自于不同的策略，那么采样到的一个批量里面的数据会是比较多样的。这样可以保证我们的模型的性能至少不会很差。</p><h4 id="4-11-在经验回放中我们观察-pi-的价值，发现里面混杂了一些不是-pi-的经验，这会有影响吗？"><a href="#4-11-在经验回放中我们观察-pi-的价值，发现里面混杂了一些不是-pi-的经验，这会有影响吗？" class="headerlink" title="4-11 在经验回放中我们观察 $\pi$ 的价值，发现里面混杂了一些不是 $\pi$ 的经验，这会有影响吗？"></a><strong>4-11</strong> 在经验回放中我们观察 $\pi$ 的价值，发现里面混杂了一些不是 $\pi$ 的经验，这会有影响吗？</h4><p>没影响。这并不是因为过去的 $\pi$ 与现在的 $\pi’$ 很相似，就算过去的$\pi$ 不是很相似，其实也是没有关系的。主要的原因是我们并不是去采样一条轨迹，我们只能采样一个经验，所以与是不是异策略是没有关系的。就算是异策略，就算是这些经验不是来自 $\pi$，我们还是可以使用这些经验来估测 $Q_{\pi}(s,a)$。</p><h4 id="4-12-为什么传统的深度Q网络的效果并不好？可以参考其公式-Q-s-t-a-t-r-t-max-a-Q-s-t-1-a-来描述。"><a href="#4-12-为什么传统的深度Q网络的效果并不好？可以参考其公式-Q-s-t-a-t-r-t-max-a-Q-s-t-1-a-来描述。" class="headerlink" title="4-12 为什么传统的深度Q网络的效果并不好？可以参考其公式 $Q(s_t ,a_t)=r_t+\max_{a}Q(s_{t+1},a)$ 来描述。"></a><strong>4-12</strong> 为什么传统的深度Q网络的效果并不好？可以参考其公式 $Q(s_t ,a_t)=r_t+\max_{a}Q(s_{t+1},a)$ 来描述。</h4><p>因为实际应用时，需要让 $Q(s_t ,a_t)$ 与 $r_t+\max_{a}Q(s_{t+1},a)$ 尽可能相等，即与我们的目标越接近越好。可以发现，目标值很容易一不小心就被设置得太高，因为在计算该目标值的时候，我们实际上在做的事情是看哪一个动作 $a$ 可以得到最大的Q值，就把它加上去，使其成为我们的目标。</p><p>例如，现在有4个动作，本来它们得到的Q值都是差不多的，它们得到的奖励也都是差不多的，但是在估算的时候是有误差的。如果第1个动作被高估了，那目标就会执行该动作，然后就会选这个高估的动作的Q值加上 $r_t$ 当作目标值。如果第4个动作被高估了，那目标就会选第4个动作的Q值加上 $r_t$ 当作目标值。所以目标总是会选那个Q值被高估的动作，我们也总是会选那个奖励被高估的动作的Q值当作Q值的最大值的结果去加上 $r_t$ 当作新目标值，因此目标值总是太大。</p><h4 id="4-13-在传统的深度Q网络中，我们应该怎么解决目标值太大的问题呢？"><a href="#4-13-在传统的深度Q网络中，我们应该怎么解决目标值太大的问题呢？" class="headerlink" title="4-13 在传统的深度Q网络中，我们应该怎么解决目标值太大的问题呢？"></a><strong>4-13</strong> 在传统的深度Q网络中，我们应该怎么解决目标值太大的问题呢？</h4><p>我们可以使用双深度Q网络解决这个问题。首先，在双深度Q网络里面，选动作的Q函数与计算价值的Q函数不同。在深度Q网络中，需要穷举所有的动作 $a$，把每一个动作 $a$ 都代入Q函数并计算哪一个动作 $a$ 反馈的Q值最大，就把这个Q值加上 $r_t$ 。但是对于双深度Q网络的两个Q网络，第一个Q网络决定哪一个动作的Q值最大，以此来决定选取的动作。我们的Q值是用 $Q’$ 算出来的，这样有什么好处呢？为什么这样就可以避免过度估计的问题呢？假设我们有两个Q函数，如果第一个Q函数高估了它现在选出来的动作 $a$ 的值，那没关系，只要第二个Q函数 $Q’$ 没有高估这个动作 $a$ 的值，计算得到的就还是正常值。假设反过来是 $Q’$ 高估了某一个动作的值，那也不会产生过度估计的问题。</p><h4 id="4-14-请问双深度Q网络中所谓的-Q-与-Q’-两个网络的功能是什么？"><a href="#4-14-请问双深度Q网络中所谓的-Q-与-Q’-两个网络的功能是什么？" class="headerlink" title="4-14 请问双深度Q网络中所谓的 $Q$ 与 $Q’$ 两个网络的功能是什么？"></a><strong>4-14</strong> 请问双深度Q网络中所谓的 $Q$ 与 $Q’$ 两个网络的功能是什么？</h4><p>在双深度Q网络中存在两个Q网络，一个是目标的Q网络，一个是真正需要更新的Q网络。具体实现方法是<strong>使用需要更新的Q网络选动作，然后使用目标的Q网络计算价值</strong>。双深度Q网络相较于深度Q网络的更改是最少的，它几乎没有增加任何的运算量，甚至连新的网络都不需要。唯一要改变的就是在找最佳动作 $a$ 的时候，本来使用 $Q’$ 来计算，即用目标的Q网络来计算，现在改成用需要更新的Q网络来计算。</p><h4 id="4-15-如何理解竞争深度Q网络的模型变化带来的好处？"><a href="#4-15-如何理解竞争深度Q网络的模型变化带来的好处？" class="headerlink" title="4-15 如何理解竞争深度Q网络的模型变化带来的好处？"></a><strong>4-15</strong> 如何理解竞争深度Q网络的模型变化带来的好处？</h4><p>对于 $\mathrm{Q}(s,a)$ ，其对应的状态由于为表格的形式，因此是离散的，而实际中的状态却不是离散的。对于 $\mathrm{Q}(s,a)$ 的计算公式—— $\mathrm{Q}(s,a)=\mathrm{V}(s)+\mathrm{A}(s,a)$ 。其中的 $\mathrm{V}(s)$ 对于不同的状态都有值， $\mathrm{A}(s,a)$ 对于不同的状态都有不同的动作对应的值。所以从本质上来说，我们最终矩阵 $\mathrm{Q}(s,a)$ 的结果是将每一个 $\mathrm{V}(s)$ 加到矩阵 $\mathrm{A}(s,a)$ 中得到的。从模型的角度考虑，我们的网络直接改变的不是 $\mathrm{Q}(s,a)$ ，而是改变的 $\mathrm{V}$、$\mathrm{A}$ 。但是有时我们更新时不一定会将 $\mathrm{V}(s)$ 和 $\mathrm{Q}(s,a)$ 都更新。将状态和动作对分成两个部分后，我们就不需要将所有的状态-动作对都采样一遍，我们可以使用更高效的估计Q值的方法将最终的 $\mathrm{Q}(s,a)$ 计算出来。</p><h4 id="4-16-使用蒙特卡洛和时序差分平衡方法的优劣分别有哪些？"><a href="#4-16-使用蒙特卡洛和时序差分平衡方法的优劣分别有哪些？" class="headerlink" title="4-16 使用蒙特卡洛和时序差分平衡方法的优劣分别有哪些？"></a><strong>4-16</strong> 使用蒙特卡洛和时序差分平衡方法的优劣分别有哪些？</h4><p>优势：时序差分方法只采样了一步，所以某一步得到的数据是真实值，接下来的都是Q值估测出来的。使用蒙特卡洛和时序差分平衡方法采样比较多步，如采样$N$步才估测价值，所以估测的部分所造成的影响就会比较小。</p><p>劣势：因为智能体的奖励比较多，所以当我们把$N$步的奖励加起来时，对应的方差就会比较大。为了缓解方差大的问题，我们可以通过调整$N$值，在方差与不精确的Q值之间取得一个平衡。这里介绍的参数$N$是超参数，需要微调参数 $N$，例如是要多采样3步、还是多采样5步。</p><h4 id="4-17-深度Q网络相比基于策略梯度的方法为什么训练效果更好、更平稳？"><a href="#4-17-深度Q网络相比基于策略梯度的方法为什么训练效果更好、更平稳？" class="headerlink" title="4-17 深度Q网络相比基于策略梯度的方法为什么训练效果更好、更平稳？"></a><strong>4-17</strong> 深度Q网络相比基于策略梯度的方法为什么训练效果更好、更平稳？</h4><p>在深度Q网络中，只要能够估计出Q函数，就可以找到一个比较好的策略。同样地，只要能够估计出Q函数，就可以增强对应的策略。因为<strong>估计Q函数是一个比较容易的回归问题</strong>，在这个回归问题中，我们可以时刻观察模型训练的效果是不是越来越好（一般情况下我们只需要关注回归的损失有没有下降，就可以判断模型学习得好不好），所以估计Q函数相较于学习一个策略来说是比较容易的。只需要估计Q函数，就可以保证现在一定会得到比较好的策略，同样其也比较容易操作。对比来说，策略梯度方法中的优化目标是最大化总回报，但是我们很难找到一个明确的损失函数来进行优化，其本质上是一个策略搜索问题，也就是一个无约束的优化问题。</p><h4 id="4-18-深度Q网络在处理连续动作时存在什么样的问题呢？对应的解决方法有哪些呢？"><a href="#4-18-深度Q网络在处理连续动作时存在什么样的问题呢？对应的解决方法有哪些呢？" class="headerlink" title="4-18 深度Q网络在处理连续动作时存在什么样的问题呢？对应的解决方法有哪些呢？"></a><strong>4-18</strong> 深度Q网络在处理连续动作时存在什么样的问题呢？对应的解决方法有哪些呢？</h4><p>我们在日常生活中常见的问题大都是包含连续动作的，例如智能体要进行自动驾驶，其就需要决定方向盘要左转几度或右转几度，这就是连续的动作；假设智能体是一个机器人，它身上有50个关节，它的每一个动作就对应到这50个关节的角度，这些角度也是连续的。</p><p>然而在使用深度Q网络时，很重要的一步是要求能够解决对应的优化问题。当我们预估出Q函数 $Q(s,a)$ 以后，必须要找到一个动作，它可以让 $Q(s,a)$ 最大。假设动作是离散的，那么动作 $a$ 的可能性是有限的。但如果动作是连续的，我们就不能像对离散的动作一样，穷举所有可能的动作了。</p><p>为了解决这个问题，有以下几种方案。</p><ol><li>第一个方案：我们可以使用<strong>采样方法</strong>，即随机采样出$N$个可能的动作，然后一个一个代入Q函数中，计算对应的$N$个Q值，并比较哪一个最大。但是这个方案因为使用采样方法所以不会非常精确。</li><li>第二个方案：我们将这个<strong>连续动作问题，建模为一个优化问题</strong>，从而可以用梯度上升去最大化我们的目标函数。具体地，我们将动作视为变量，使用梯度上升更新动作对应的Q值。但是这个方案通常时间花销比较大，因为其需要迭代计算。</li><li>第三个方案：<strong>设计一个特别的网络架构</strong>，即设计一个特别的Q函数，使得求解让Q函数最大化的动作 $a$ 变得非常容易。也就是这里的Q函数不是一个广义的Q函数，我们可以使用特殊方法设计Q函数，使得寻找让这个Q函数最大的动作 $a$ 非常容易。但是这个方案的Q函数不能随意设计，其必须有一些额外的限制。</li><li>第四个方案：<strong>不用深度Q网络</strong>，毕竟用其处理连续动作比较麻烦。</li></ol><h1 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h1><h4 id="4-1-友善的面试官：请问深度Q网络是什么？其两个关键性的技巧分别是什么？"><a href="#4-1-友善的面试官：请问深度Q网络是什么？其两个关键性的技巧分别是什么？" class="headerlink" title="4-1 友善的面试官：请问深度Q网络是什么？其两个关键性的技巧分别是什么？"></a><strong>4-1</strong> 友善的面试官：请问深度Q网络是什么？其两个关键性的技巧分别是什么？</h4><p>深度Q网络是基于深度学习的Q学习算法，其结合了价值函数近似与神经网络技术，并采用了<strong>目标网络</strong>和<strong>经验回放技巧</strong>进行网络的训练。</p><h4 id="4-2-友善的面试官：那我们继续分析！你刚才提到的深度Q网络中的两个技巧————目标网络和经验回放，其具体作用是什么呢？"><a href="#4-2-友善的面试官：那我们继续分析！你刚才提到的深度Q网络中的两个技巧————目标网络和经验回放，其具体作用是什么呢？" class="headerlink" title="4-2 友善的面试官：那我们继续分析！你刚才提到的深度Q网络中的两个技巧————目标网络和经验回放，其具体作用是什么呢？"></a><strong>4-2</strong> 友善的面试官：那我们继续分析！你刚才提到的深度Q网络中的两个技巧————目标网络和经验回放，其具体作用是什么呢？</h4><p>在深度Q网络中某个动作价值函数的更新依赖于其他动作价值函数。如果我们一直更新价值网络的参数，会导致更新目标不断变化，也就是我们在追逐一个不断变化的目标，这样势必会不太稳定。为了解决基于时序差分的网络中，优化目标 $Q_{\pi}\left(s_{t}, a_{t}\right) =r_{t}+Q_{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$ 左右两侧会同时变化使得训练过程不稳定，从而增大回归难度的问题，目标网络选择将优化目标的右边即 $r_{t}+Q_{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$ 固定，通过改变优化目标左边的网络参数进行回归。</p><p>对于经验回放，其会构建一个回放缓冲区，用来保存许多数据，每一个数据的内容包括：状态 $s_t$、采取的动作 $a_t$、得到的奖励 $r_t$、下一个状态 $s_{t+1}$。我们使用 $\pi$ 与环境交互多次，把收集到的数据都放到回放缓冲区中。当回放缓冲区“装满”后，就会自动删去最早进入缓冲区的数据。在训练时，对于每一轮迭代都有相对应的批量（与我们训练普通网络一样，通过采样得到），然后用这个批量中的数据去更新Q函数。即Q函数在采样和训练的时候会用到过去的经验数据，也可以消除样本之间的相关性。</p><h4 id="4-3-友善的面试官：深度Q网络和Q学习有什么异同点？"><a href="#4-3-友善的面试官：深度Q网络和Q学习有什么异同点？" class="headerlink" title="4-3 友善的面试官：深度Q网络和Q学习有什么异同点？"></a><strong>4-3</strong> 友善的面试官：深度Q网络和Q学习有什么异同点？</h4><p>整体来说，从名称就可以看出，两者的目标价值以及价值的更新方式基本相同。但有如下不同点：</p><ol><li>首先，深度Q网络将Q学习与深度学习结合，用深度网络来近似动作价值函数，而Q学习则是采用表格进行存储。</li><li>深度Q网络采用了经验回放的技巧，从历史数据中随机采样，而Q学习直接采用下一个状态的数据进行学习。</li></ol><h4 id="4-4-友善的面试官：请问，随机性策略和确定性策略有什么区别吗？"><a href="#4-4-友善的面试官：请问，随机性策略和确定性策略有什么区别吗？" class="headerlink" title="4-4 友善的面试官：请问，随机性策略和确定性策略有什么区别吗？"></a><strong>4-4</strong> 友善的面试官：请问，随机性策略和确定性策略有什么区别吗？</h4><p>随机性策略表示为某个状态下动作取值的分布，确定性策略在每个状态只有一个确定的动作可以选。从熵的角度来说，确定性策略的熵为0，没有任何随机性。随机性策略有利于我们进行适度的探索，确定性策略不利于进行探索。</p><h4 id="4-5-友善的面试官：请问不打破数据相关性，神经网络的训练效果为什么就不好？"><a href="#4-5-友善的面试官：请问不打破数据相关性，神经网络的训练效果为什么就不好？" class="headerlink" title="4-5 友善的面试官：请问不打破数据相关性，神经网络的训练效果为什么就不好？"></a><strong>4-5</strong> 友善的面试官：请问不打破数据相关性，神经网络的训练效果为什么就不好？</h4><p>在神经网络中通常使用随机梯度下降法。随机的意思是我们随机选择一些样本来增量式地估计梯度，比如常用的批量训练方法。如果样本是相关的，就意味着前后两个批量很可能也是相关的，那么估计的梯度也会呈现出某种相关性。但是在极端条件下，后面的梯度估计可能会抵消掉前面的梯度估计量，从而使得训练难以收敛。</p><h4 id="4-6-友善的面试官：深度Q网络都有哪些变种？引入状态奖励的是哪种？"><a href="#4-6-友善的面试官：深度Q网络都有哪些变种？引入状态奖励的是哪种？" class="headerlink" title="4-6 友善的面试官：深度Q网络都有哪些变种？引入状态奖励的是哪种？"></a><strong>4-6</strong> 友善的面试官：深度Q网络都有哪些变种？引入状态奖励的是哪种？</h4><p>深度Q网络有3个经典的变种：双深度Q网络、竞争深度Q网络、优先级双深度Q网络。</p><ol><li><strong>双深度Q网络</strong>：将动作选择和价值估计分开，避免Q值被过高估计。</li><li><strong>竞争深度Q网络</strong>：将Q值分解为状态价值和优势函数，得到更多有用信息。</li><li><strong>优先级双深度Q网络</strong>：将经验池中的经验按照优先级进行采样。</li></ol><h4 id="4-7-友善的面试官：请简述双深度Q网络原理。"><a href="#4-7-友善的面试官：请简述双深度Q网络原理。" class="headerlink" title="4-7 友善的面试官：请简述双深度Q网络原理。"></a><strong>4-7</strong> 友善的面试官：请简述双深度Q网络原理。</h4><p>深度Q网络由于总是选择当前最优的动作价值函数来更新当前的动作价值函数，因此存在过估计问题（估计的价值函数值大于真实的价值函数值）。为了解耦这两个过程，双深度Q网络使用两个价值网络，一个网络用来执行动作选择，然后用另一个网络的价值函数对应的动作值更新当前网络。</p><h4 id="4-8-友善的面试官：请问竞争深度Q网络模型有什么优势呢？"><a href="#4-8-友善的面试官：请问竞争深度Q网络模型有什么优势呢？" class="headerlink" title="4-8 友善的面试官：请问竞争深度Q网络模型有什么优势呢？"></a><strong>4-8</strong> 友善的面试官：请问竞争深度Q网络模型有什么优势呢？</h4><p>对于 $\boldsymbol{Q}(s,a)$ ，其对应的状态由于为表格的形式，因此是离散的，而实际的状态大多不是离散的。对于Q值 $\boldsymbol{Q}(s,a)=V(s)+\boldsymbol{A}(s,a)$ 。其中的 $V(s)$ 是对于不同的状态都有值， $\boldsymbol{A}(s,a)$ 对于不同的状态都有不同的动作对应的值。所以本质上，我们最终的矩阵 $\boldsymbol{Q}(s,a)$ 是将每一个 $V(s)$ 加到矩阵 $\boldsymbol{A}(s,a)$ 中得到的。但是有时我们更新时不一定会将 $V(s)$ 和 $\boldsymbol{Q}(s,a)$ 都更新。我们将其分成两个部分后，就不需要将所有的状态-动作对都采样一遍，我们可以使用更高效的估计Q值的方法将最终的 $\boldsymbol{Q}(s,a)$ 计算出来。</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5.策略梯度</title>
      <link href="/easy_rl_exercise/5.%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/"/>
      <url>/easy_rl_exercise/5.%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<h1 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h1><ul><li><strong>策略（policy）</strong>：在每一个演员中会有对应的策略，这个策略决定了演员的后续动作。具体来说，策略就是对于外界的输入，输出演员现在应该要执行的动作。一般地，我们将策略写成 $\pi$ 。</li><li><strong>回报（return）</strong>：一个回合（episode）或者试验（trial）得到的所有奖励的总和，也被人们称为总奖励（total reward）。一般地，我们用 $R$ 来表示它。</li><li><strong>轨迹（trajectory）</strong>：一个试验中我们将环境输出的状态 $s$ 与演员输出的动作 $a$ 全部组合起来形成的集合称为轨迹，即 $\tau=\left\{s_{1}, a_{1}, s_{2}, a_{2}, \cdots, s_{t}, a_{t}\right\}$ 。</li><li><strong>奖励函数（reward function）</strong>：用于反映在某一个状态采取某一个动作可以得到的奖励分数，这是一个函数。即给定一个状态-动作对 ($s_1$,$a_1$) ，奖励函数可以输出 $r_1$ 。给定 ($s_2$,$a_2$)，它可以输出 $r_2$。 把所有的 $r$ 都加起来，我们就得到了 $R(\tau)$ ，它代表某一个轨迹 $\tau$ 的奖励。</li><li><strong>期望奖励（expected reward）</strong>：$\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)=E_{\tau \sim p_{\theta}(\tau)}[R(\tau)]$。</li><li><strong>REINFORCE</strong>：基于策略梯度的强化学习的经典算法，其采用回合更新的模式。</li></ul><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><h4 id="5-1-如果我们想让机器人自己玩视频游戏，那么强化学习中的3个组成部分（演员、环境、奖励函数）具体分别代表什么？"><a href="#5-1-如果我们想让机器人自己玩视频游戏，那么强化学习中的3个组成部分（演员、环境、奖励函数）具体分别代表什么？" class="headerlink" title="5-1 如果我们想让机器人自己玩视频游戏，那么强化学习中的3个组成部分（演员、环境、奖励函数）具体分别代表什么？"></a><strong>5-1</strong> 如果我们想让机器人自己玩视频游戏，那么强化学习中的3个组成部分（演员、环境、奖励函数）具体分别代表什么？</h4><p>演员做的事情就是操控游戏的摇杆，比如向左、向右、开火等操作；环境就是游戏的主机，负责控制游戏的画面、控制怪物如何移动等；奖励函数就是当执行什么动作、发生什么状况的时候，我们可以得到多少分数，比如击杀一只怪兽得到20分、被对手暴击扣除10分、完成任务得到10分等。</p><h4 id="5-2-在一个过程中，一个具体的轨迹-s-1-a-1-s-2-a-2-出现的概率取决于什么？"><a href="#5-2-在一个过程中，一个具体的轨迹-s-1-a-1-s-2-a-2-出现的概率取决于什么？" class="headerlink" title="5-2 在一个过程中，一个具体的轨迹{$s_1 , a_1 , s_2 , a_2$}出现的概率取决于什么？"></a><strong>5-2</strong> 在一个过程中，一个具体的轨迹{$s_1 , a_1 , s_2 , a_2$}出现的概率取决于什么？</h4><ol><li>一部分是<strong>环境</strong>的行为，即环境的函数内部的参数或内部的规则是什么形式的。 $p(s_{t+1}|s_t,a_t)$ 这一项代表的是环境，环境这一项通常是无法控制的，因为它是已经客观存在的，或者其形式是提前制定好的。</li><li>另一部分是<strong>智能体</strong>的行为，我们能控制的是 $p_\theta(a_t|s_t)$ ，即给定一个状态 $s_t$，演员要采取什么样的动作 $a_t$ 取决于演员的参数 $\theta$，所以这部分是我们可以控制的。随着演员动作的不同，每个同样的轨迹，它会因为不同的概率从而表现出不同的行为。</li></ol><h4 id="5-3-当我们最大化期望奖励时，应该使用什么方法？"><a href="#5-3-当我们最大化期望奖励时，应该使用什么方法？" class="headerlink" title="5-3 当我们最大化期望奖励时，应该使用什么方法？"></a><strong>5-3</strong> 当我们最大化期望奖励时，应该使用什么方法？</h4><p>应该使用梯度上升法，因为要让期望奖励越大越好，所以是梯度上升法。梯度上升法在更新参数的时候要添加梯度信息。要进行梯度上升，我们先要计算期望奖励 $\bar{R}$ 的梯度。我们对 $\bar{R}$ 取一个梯度，这里只有 $p_{\theta}(\tau)$ 是与 $\theta$ 有关的，所以 $p_{\theta}(\tau)$ 为梯度的部分。</p><h4 id="5-4-我们应该如何理解策略梯度的公式呢？"><a href="#5-4-我们应该如何理解策略梯度的公式呢？" class="headerlink" title="5-4 我们应该如何理解策略梯度的公式呢？"></a><strong>5-4</strong> 我们应该如何理解策略梯度的公式呢？</h4><p>策略梯度的公式如下：</p><script type="math/tex; mode=display">\begin{aligned}E_{\tau \sim p_{\theta}(\tau)}\left[R(\tau) \nabla \log p_{\theta}(\tau)\right] &\approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(\tau^{n}\right) \\&=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)\end{aligned}</script><p>$p_{\theta}(\tau)$ 里面有两项，$p(s_{t+1}|s_t,a_t)$ 来自环境，$p_\theta(a_t|s_t)$ 来自智能体。 $p(s_{t+1}|s_t,a_t)$ 由环境决定，从而与 $\theta$ 无关，因此 $\nabla \log p(s_{t+1}|s_t,a_t) =0$ ， $\nabla p_{\theta}(\tau)=\nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)$。</p><p>具体来说：</p><p>（1）假设在状态 $s_t$ 时执行动作 $a_t$，最后发现轨迹 $\tau$ 的奖励是正的，那我们就要增大这一项的概率，即增大在状态 $s_t$ 时执行动作 $a_t$ 的概率；</p><p>（2）反之，在状态 $s_t$ 时执行动作 $a_t$ 会导致轨迹 $\tau$ 的奖励变成负的，我们就要减小这一项的概率。</p><h4 id="5-5-我们可以使用哪些方法来进行梯度提升的计算？"><a href="#5-5-我们可以使用哪些方法来进行梯度提升的计算？" class="headerlink" title="5-5 我们可以使用哪些方法来进行梯度提升的计算？"></a><strong>5-5</strong> 我们可以使用哪些方法来进行梯度提升的计算？</h4><p>用梯度提升来更新参数，对于原来的参数 $\theta$ ，可以将原始的 $\theta$ 加上更新梯度，再乘一个学习率。通常学习率也需要调整，与神经网络一样，我们可以使用 Adam、RMSProp、SGD 等优化器对其进行调整。</p><h4 id="5-6-进行基于策略梯度的优化的技巧有哪些？"><a href="#5-6-进行基于策略梯度的优化的技巧有哪些？" class="headerlink" title="5-6 进行基于策略梯度的优化的技巧有哪些？"></a><strong>5-6</strong> 进行基于策略梯度的优化的技巧有哪些？</h4><p>（1）<strong>增加基线</strong>：为了防止所有奖励都为正，从而导致每一个状态和动作的变换，都会使得每一项变换的概率上升，我们把奖励减去一项 $b$，称之为基线。当减去 $b$ 后，就可以让奖励 $R(\tau^n)-b$ 有正有负。所以如果得到的总奖励 $R(\tau^n)$ 大于 $b$ ，就让它的概率增大。如果总奖励小于 $b$，就算它是正的，值很小也是不好的，就需要让这一项的概率减小。如果奖励 $R(\tau^n)$ 小于 $b$ ，就要让采取这个动作的奖励下降，这样也符合常理。但是使用基线会让本来奖励很大的“动作”的奖励变小，从而降低更新速率。</p><p>（2）<strong>指派合适的分数</strong>：首先，原始权重是整个回合的总奖励。现在改成从某个时间点 $t$ 开始，假设动作是在时间点 $t$ 被执行的，从时间点 $t$，一直到游戏结束所有奖励的总和大小，才真正代表这个动作是好的还是不好的；接下来我们再进一步，把未来的奖励打一个折扣，我们称由此得到的奖励的和为折扣回报。</p><p>（3）综合以上两种技巧，我们将其统称为<strong>优势函数</strong>，用 $A$ 来代表优势函数。优势函数取决于状态和动作，即我们需计算的是在某一个状态 $s$ 采取某一个动作 $a$ 的时候，优势函数有多大。</p><p>（4）优势函数的意义在于衡量假设我们在某一个状态 $s_t$ 执行某一个动作 $a_t$，相较于其他可能动作的优势。它在意的不是绝对的好，而是相对的好，即相对优势，因为会减去一个基线 $b$ 。 $A_{\theta}\left(s_{t}, a_{t}\right)$ 通常可以由一个网络预估出来，这个网络叫作评论员。</p><h4 id="5-7-对于策略梯度的两种方法，蒙特卡洛强化学习和时序差分强化学习两种方法有什么联系和区别？"><a href="#5-7-对于策略梯度的两种方法，蒙特卡洛强化学习和时序差分强化学习两种方法有什么联系和区别？" class="headerlink" title="5-7 对于策略梯度的两种方法，蒙特卡洛强化学习和时序差分强化学习两种方法有什么联系和区别？"></a><strong>5-7</strong> 对于策略梯度的两种方法，蒙特卡洛强化学习和时序差分强化学习两种方法有什么联系和区别？</h4><p>（1）两者的更新频率不同。蒙特卡洛强化学习方法是每一个回合更新一次，即需要经历完整的状态序列后再更新，比如贪吃蛇游戏，贪吃蛇“死了”即游戏结束后再更新。而时序差分强化学习方法是每一步就更新一次，比如贪吃蛇游戏，贪吃蛇每移动一次（或几次）就进行更新。相对来说，时序差分强化学习方法比蒙特卡洛强化学习方法更新的频率更高。</p><p>（2）时序差分强化学习方法能够在知道一个小步后就进行学习，相比于蒙特卡洛强化学习方法，其更加快速和灵活。</p><p>（3）具体例如：假如我们要优化开车去公司的通勤时间。对于此问题，每一次通勤，我们将到达不同的路口。对于时序差分强化学习方法，其会对每一个经过的路口计算时间，例如在路口 A 就开始更新预计到达路口 B、路口 C $\cdots \cdots$ ，以及到达公司的时间；对于蒙特卡洛强化学习方法，其不会每经过一个路口就更新时间，而是到达最终的目的地后，再修改到达每一个路口和到达公司对应的时间。</p><h4 id="5-8-请详细描述REINFORCE算法的计算过程。"><a href="#5-8-请详细描述REINFORCE算法的计算过程。" class="headerlink" title="5-8 请详细描述REINFORCE算法的计算过程。"></a><strong>5-8</strong> 请详细描述REINFORCE算法的计算过程。</h4><p>首先我们需要根据一个确定好的策略模型来输出每一个可能动作的概率，对于所有动作的概率，我们使用采样方法（或者是随机的方法）选择一个动作与环境进行交互，同时环境会给我们反馈整个回合的数据。将此回合数据输入学习函数中，并根据回合数据进行损失函数的构造，通过Adam等优化器的优化，再更新我们的策略模型。</p><h1 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h1><h4 id="5-1-友善的面试官：同学来吧，给我手动推导一下策略梯度公式的计算过程。"><a href="#5-1-友善的面试官：同学来吧，给我手动推导一下策略梯度公式的计算过程。" class="headerlink" title="5-1 友善的面试官：同学来吧，给我手动推导一下策略梯度公式的计算过程。"></a><strong>5-1</strong> 友善的面试官：同学来吧，给我手动推导一下策略梯度公式的计算过程。</h4><p>首先我们的目的是最大化奖励函数，即调整 $\theta$ ，使得期望回报最大，可以用公式表示如下：</p><script type="math/tex; mode=display">J(\theta)=E_{\tau \sim p_{\theta(\tau)}}\left[\sum_tr(s_t,a_t)\right]</script><p>其中 $\tau$ 表示从开始到结束的一条完整轨迹。通常对于最大化问题，我们可以使用梯度上升算法找到最大值，即</p><script type="math/tex; mode=display">\theta^* = \theta + \alpha\nabla J({\theta})</script><p>所以我们仅仅需要计算并更新 $\nabla J({\theta})$ ，也就是计算奖励函数 $J({\theta})$ 关于 $\theta$ 的梯度，也就是策略梯度，计算方法如下：</p><script type="math/tex; mode=display">\nabla_{\theta}J(\theta) = \int {\nabla}_{\theta}p_{\theta}(\tau)r(\tau) \mathrm{d}{\tau}=\int p_{\theta}{\nabla}_{\theta} \mathrm{log}p_{\theta}(\tau)r(\tau)\mathrm{d}{\tau}=E_{\tau \sim p_{\theta}(\tau)}[{\nabla}_{\theta}\mathrm{log}p_{\theta}(\tau)r(\tau)]</script><p>接着我们继续展开，对于 $p_{\theta}(\tau)$ ，即 $p_{\theta}(\tau|{\theta})$ ：</p><script type="math/tex; mode=display">p_{\theta}(\tau|{\theta}) = p(s_1)\prod_{t=1}^T \pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)</script><p>取对数后为：</p><script type="math/tex; mode=display">\mathrm{log}p_{\theta}(\tau|{\theta}) = \mathrm{log}p(s_1)+\sum_{t=1}^T \mathrm{log}\pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)</script><p>继续求导：</p><script type="math/tex; mode=display">\nabla \mathrm{log}p_{\theta}(\tau|{\theta}) = \sum_{t=1}^T \nabla_{\theta}\mathrm{log} \pi_{\theta}(a_t|s_t)</script><p>代入第3个式子，可以将其化简为：</p><script type="math/tex; mode=display">\begin{aligned}    \nabla_{\theta}J(\theta)     &= E_{\tau \sim p_{\theta}(\tau)}[{\nabla}_{\theta}\mathrm{log}p_{\theta}(\tau)r(\tau)] \\    &= E_{\tau \sim p_{\theta}}[(\nabla_{\theta}\mathrm{log}\pi_{\theta}(a_t|s_t))(\sum_{t=1}^Tr(s_t,a_t))] \\    &= \frac{1}{N}\sum_{i=1}^N[(\sum_{t=1}^T\nabla_{\theta}\mathrm{log} \pi_{\theta}(a_{i,t}|s_{i,t}))(\sum_{t=1}^Nr(s_{i,t},a_{i,t}))]    \end{aligned}</script><h4 id="5-2-友善的面试官：可以说一下你所了解的基于策略梯度优化的技巧吗？"><a href="#5-2-友善的面试官：可以说一下你所了解的基于策略梯度优化的技巧吗？" class="headerlink" title="5-2 友善的面试官：可以说一下你所了解的基于策略梯度优化的技巧吗？"></a><strong>5-2</strong> 友善的面试官：可以说一下你所了解的基于策略梯度优化的技巧吗？</h4><p>（1）<strong>增加基线</strong>：为了防止所有奖励都为正，从而导致每一个状态和动作的变换，都会使得每一个变换的概率上升，我们把奖励减去一项 $b$，称 $b$ 为基线。当减去 $b$ 以后，就可以让奖励 $R(\tau^n)-b$ 有正有负。如果得到的总奖励 $R(\tau^n)$ 大于 $b$ ，就让它的概率上升。如果总奖励小于 $b$，就算它是正的，值很小也是不好的，就需要让它的概率下降。如果总奖励小于 $b$ ，就要让采取这个动作的奖励下降，这样也符合常理。但是使用基线会让本来奖励很大的“动作”的奖励变小，降低更新速率。</p><p>（2）<strong>指派合适的分数</strong>：首先，原始权重是整个回合的总奖励。现在改成从某个时间点 $t$ 开始，假设这个动作是在时间点 $t$ 被执行的，那么从时间点 $t$ ，一直到游戏结束所有奖励的总和，才真的代表这个动作是好的还是不好的；接下来我们再进一步，把未来的奖励打一个折扣，这里我们称由此得到的奖励的和为折扣回报。</p><p>（3）综合以上两种技巧，我们将其统称为<strong>优势函数</strong>，用 $A$ 来代表优势函数。优势函数取决于状态和动作，即我们需计算的是在某一个状态 $s$ 采取某一个动作 $a$ 的时候，优势函数有多大。</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6.演员-评论家算法</title>
      <link href="/easy_rl_exercise/6.%E6%BC%94%E5%91%98-%E8%AF%84%E8%AE%BA%E5%AE%B6%E7%AE%97%E6%B3%95/"/>
      <url>/easy_rl_exercise/6.%E6%BC%94%E5%91%98-%E8%AF%84%E8%AE%BA%E5%AE%B6%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h1><ul><li><strong>优势演员-评论员（advantage actor-critic，A2C）算法</strong>：一种改进的演员-评论员（actor-critic）算法。</li><li><strong>异步优势演员-评论员（asynchronous advantage actor-critic，A3C）算法</strong>：一种改进的演员-评论员算法，通过异步的操作，实现强化学习模型训练的加速。</li><li><strong>路径衍生策略梯度（pathwise derivative policy gradient）</strong>：一种使用Q学习来求解连续动作的算法，也是一种演员-评论员算法。其会对演员提供价值最大的动作，而不仅仅是提供某一个动作的好坏程度。</li></ul><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><h4 id="6-1-完整的优势演员-评论员算法的工作流程是怎样的？"><a href="#6-1-完整的优势演员-评论员算法的工作流程是怎样的？" class="headerlink" title="6-1 完整的优势演员-评论员算法的工作流程是怎样的？"></a><strong>6-1</strong> 完整的优势演员-评论员算法的工作流程是怎样的？</h4><p>在传统的方法中，我们有一个策略 $\pi$ 以及一个初始的演员与环境交互、收集数据以及反馈。通过每一步得到的反馈，我们进一步更新我们的策略 $\pi$ ，通常我们使用的更新方式是策略梯度。但是对于演员-评论员算法，我们不是直接使用每一步得到的数据和反馈进行策略 $\pi$ 的更新，而是使用这些数据和反馈进行价值函数的估计，这里我们通常使用的算法包括时序差分和蒙特卡洛等算法以及基于它们的优化算法。接下来我们再基于价值函数来更新策略，公式如下：</p><script type="math/tex; mode=display">\nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(r_{t}^{n}+V_{\pi}\left(s_{t+1}^{n}\right)-V_{\pi}\left(s_{t}^{n}\right)\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)</script><p>其中 $r_{t}^{n}+V_{\pi}\left(s_{t+1}^{n}\right)-V_{\pi}\left(s_{t}^{n}\right)$ 为优势函数。我们通过以上方法得到新的策略后，再与环境交互，然后重复预估价值函数的操作，用价值函数来更新我们的策略。以上的整个方法我们称为优势演员-评论员算法。</p><h4 id="6-2-在实现演员-评论员算法的时候有哪些技巧？"><a href="#6-2-在实现演员-评论员算法的时候有哪些技巧？" class="headerlink" title="6-2 在实现演员-评论员算法的时候有哪些技巧？"></a><strong>6-2</strong> 在实现演员-评论员算法的时候有哪些技巧？</h4><p>（1）<strong>预估两个网络</strong>：一个是价值网络；另外一个是策略网络。价值网络的输入是一个状态，输出是一个标签；策略网络的输入是一个状态，输出是一个动作的分布。这两个网络中，演员和评论员的输入都是状态，所以它们前面几层是可以共享的。例如，玩雅达利游戏时，输入都是图片。输入的图片都非常复杂，且比较大，通常前期我们都会用一些卷积神经网络来处理这些图片，把图片抽象成深层次的特征，这些网络对演员与评论员网络来说是可以共用的。我们可以让演员与评论员的前面几层共用同一组参数，这一组参数可能是卷积神经网络中的参数。先把输入的像素变成比较高维度的特征信息，然后输入演员网络决定要采取什么样的动作，评论员网络使用价值函数计算期望奖励。</p><p>（2）<strong>探索机制</strong>：其目的是对策略 $\pi$ 的输出分布进行限制，从而使得分布的熵不要太小，即希望不同的动作被采用的概率平均一些。这样在测试的时候，智能体才会多尝试各种不同的动作，才会对环境进行充分探索，从而得到比较好的结果。</p><h4 id="6-3-异步优势演员-评论员算法在训练时有很多的进程进行异步的工作，最后再将他们所获得的“结果”集合到一起。那么其具体是如何运作的呢？"><a href="#6-3-异步优势演员-评论员算法在训练时有很多的进程进行异步的工作，最后再将他们所获得的“结果”集合到一起。那么其具体是如何运作的呢？" class="headerlink" title="6-3 异步优势演员-评论员算法在训练时有很多的进程进行异步的工作，最后再将他们所获得的“结果”集合到一起。那么其具体是如何运作的呢？"></a><strong>6-3</strong> 异步优势演员-评论员算法在训练时有很多的进程进行异步的工作，最后再将他们所获得的“结果”集合到一起。那么其具体是如何运作的呢？</h4><p>异步优势演员-评论员算法，即算法一开始会有一个全局网络，其包含策略部分和价值部分。假设它的参数是 $\theta_1$，假设对于每一个演员都用一个CPU训练，每一个演员工作前都会将全局网络的参数复制进来。然后演员与环境进行交互，每一个演员与环境交互后，都会计算出梯度并且更新全局网络的参数。这里要注意的是，所有的演员都是并行运行的。所以每个演员都是在全局网络复制了参数以后，执行完再把参数传回去。所以当第一个演员执行完想要把参数传回去的时候，本来它要的参数是 $\theta_1$，等它把梯度传回去的时候，可能原来的参数已经被覆盖，变成 $\theta_2$ 了。</p><h4 id="6-4-对比经典的Q学习算法，路径衍生策略梯度有哪些改进之处？"><a href="#6-4-对比经典的Q学习算法，路径衍生策略梯度有哪些改进之处？" class="headerlink" title="6-4 对比经典的Q学习算法，路径衍生策略梯度有哪些改进之处？"></a><strong>6-4</strong> 对比经典的Q学习算法，路径衍生策略梯度有哪些改进之处？</h4><p>（1）把 $Q(s,a)$ 换成了 $\pi$。经典的Q学习算法是用 $Q(s,a)$ 来决定在状态 $s_t$ 产生哪一个动作 $a_{t}$ ，路径衍生策略梯度是直接用 $\pi$ 来决定。面对前者，我们需要解决最大值的问题，现在的路径衍生策略梯度直接训练了一个演员网络。其输入状态 $s_t$ 就会告诉我们应该采取哪一个动作 $a_{t}$。综上，经典的Q学习算法输入状态 $s_t$，采取哪一个动作 $a_t$ 是 $Q(s,a)$ 决定的，在路径衍生策略梯度里面，我们会直接用 $\pi$ 来决定。</p><p>（2）经典的Q学习算法计算在 $s_{i+1}$ 下对应的策略采取的动作 $a$ 得到的Q值，我们会采取让 $\hat{Q}$ 最大的动作 $a$。现在的路径衍生策略梯度因为我们不需要再求解决最大化的问题，所以我们直接把状态 $s_{i+1}$ 代入策略 $\pi$ 中，就会得到在状态 $s_{i+1}$ 下，哪一个动作会带给我们最大的Q值，就执行这个动作。在Q函数中，有两个Q网络，一个是真正的Q网络，另外一个是目标Q网络。实际上在执行时，也会有两个演员网络，一个真正要学习的演员网络 $\pi$ 和一个目标演员网络 $\hat{\pi}$ 。</p><p>（3）经典的Q学习算法只需要学习Q函数，路径衍生策略梯度需要多学习一个策略 $\pi$，其目的在于最大化Q函数，希望得到的演员可以让Q函数的输出尽可能的大，这与生成对抗网络里面的生成器的概念类似。</p><p>（4）与原来的Q函数一样，我们要把目标Q网络取代掉，路径衍生策略梯度中也要把目标策略取代掉。</p><h1 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h1><h4 id="6-1-友善的面试官：请简述一下异步优势演员-评论员算法（A3C），另外A3C是同策略还是异策略的模型呀？"><a href="#6-1-友善的面试官：请简述一下异步优势演员-评论员算法（A3C），另外A3C是同策略还是异策略的模型呀？" class="headerlink" title="6-1 友善的面试官：请简述一下异步优势演员-评论员算法（A3C），另外A3C是同策略还是异策略的模型呀？"></a><strong>6-1</strong> 友善的面试官：请简述一下异步优势演员-评论员算法（A3C），另外A3C是同策略还是异策略的模型呀？</h4><p>A3C是异步优势演员-评论员算法，其中，评论员学习价值函数，同时有多个演员并行训练并且不时与全局参数同步。A3C旨在并行训练，是同策略算法。 </p><h4 id="6-2-友善的面试官：请问演员-评论员算法有何优点呢？"><a href="#6-2-友善的面试官：请问演员-评论员算法有何优点呢？" class="headerlink" title="6-2 友善的面试官：请问演员-评论员算法有何优点呢？"></a><strong>6-2</strong> 友善的面试官：请问演员-评论员算法有何优点呢？</h4><p>（1）相比以价值函数为中心的算法，演员-评论员算法应用了策略梯度的技巧，这能让它在连续动作或者高维动作空间中选取合适的动作，而Q学习做这件事会很困难。</p><p>（2）相比单纯策略梯度，演员-评论员算法应用了Q学习或其他策略评估的做法，使得演员-评论员算法能进行单步更新而不是回合更新，比单纯的策略梯度的效率要高。</p><h4 id="6-3-友善的面试官：请问异步优势演员-评论员算法具体是如何异步更新的？"><a href="#6-3-友善的面试官：请问异步优势演员-评论员算法具体是如何异步更新的？" class="headerlink" title="6-3 友善的面试官：请问异步优势演员-评论员算法具体是如何异步更新的？"></a><strong>6-3</strong> 友善的面试官：请问异步优势演员-评论员算法具体是如何异步更新的？</h4><p>下面是异步优势演员-评论员算法的大纲，由于其为异步多线程算法，我们只对其中某一单线程进行分析。</p><p>（1）定义全局参数 $\theta$ 和 $w$ 以及特定线程参数 $\theta’$ 和 $w’$。</p><p>（2）初始化时间步 $t=1$。</p><p>（3）当 $T \leqslant T_{\mathrm{max}}$:</p><ul><li>重置梯度：$\mathrm{d} \theta = 0$ 并且 $\mathrm{d}w = 0$。</li><li>将特定于线程的参数与全局参数同步：$\theta’ = \theta$ 以及 $w’=w$。</li><li>令 $t_{\mathrm{start}} =t$ 并且随机采样一个初始状态 $s_t$。</li><li>当 （$s_t!=$ 终止状态）并且$t−t_{\mathrm{start}} \leqslant t_{\mathrm{max}}$。<ul><li>根据当前线程的策略选择当前执行的动作 $a_t\sim\pi_{\theta’}(a_t|s_t)$，执行动作后接收奖励 $r_t$ 然后转移到下一个状态 $s_{t+1}$。</li><li>更新 $t$ 以及 $T$：$t=t+1$ 并且 $T=T+1$。</li></ul></li><li>初始化保存累积奖励估计值的变量。</li><li>对于 $i=t_1, \dots ,t_{\mathrm{start}}$:<ul><li>$r \gets \gamma r+r_i$；这里的 $r$ 是 $G_i$ 的蒙特卡洛估计。</li><li>累积关于参数 $\theta’$ 的梯度：$\mathrm{d} \theta \gets \mathrm{d}\theta + \nabla_{\theta’} \mathrm{log} \pi_{\theta’}(a_i|s_i)(r−V_{w’}(s_i))$。</li><li>累积关于参数 $w’$ 的梯度：$\mathrm{d}w \gets \mathrm{d}w+ \mathrm{\partial} (r-V_{w’}(s_i))^2 / \mathrm{\partial} w’$。</li></ul></li><li>分别使用 $\mathrm{d}\theta$ 以及 $\mathrm{d}w$ 异步更新 $\theta$ 以及 $w$。</li></ul><h4 id="6-4-友善的面试官：演员-评论员算法中，演员和评论员两者的区别是什么？"><a href="#6-4-友善的面试官：演员-评论员算法中，演员和评论员两者的区别是什么？" class="headerlink" title="6-4 友善的面试官：演员-评论员算法中，演员和评论员两者的区别是什么？"></a><strong>6-4</strong> 友善的面试官：演员-评论员算法中，演员和评论员两者的区别是什么？</h4><p>演员是策略模块，输出动作；</p><p>评论员是判别器，用来计算价值函数。</p><h4 id="6-5-友善的面试官：演员-评论员算法框架中的评论员起了什么作用？"><a href="#6-5-友善的面试官：演员-评论员算法框架中的评论员起了什么作用？" class="headerlink" title="6-5 友善的面试官：演员-评论员算法框架中的评论员起了什么作用？"></a><strong>6-5</strong> 友善的面试官：演员-评论员算法框架中的评论员起了什么作用？</h4><p>评论员衡量当前决策的好坏。结合策略模块，当评论员判别某个动作的选择是有益的时候，策略就更新参数以增大该动作出现的概率，反之减小该动作出现的概率。</p><h4 id="6-6-友善的面试官：简述异步优势演员-评论员算法的优势函数。"><a href="#6-6-友善的面试官：简述异步优势演员-评论员算法的优势函数。" class="headerlink" title="6-6 友善的面试官：简述异步优势演员-评论员算法的优势函数。"></a><strong>6-6</strong> 友善的面试官：简述异步优势演员-评论员算法的优势函数。</h4><p>优势函数的计算公式为 $A(s,a)=Q(s,a)-V(s)=r+\gamma V(s’)-V(s)$ ，其可以定量地表示选择动作 $a$ 的优势。即当动作 $a$ 低于价值函数的平均值的时候，优势函数为负值；反之为正值。其是一个标量，具体来说：</p><p>（1）如果 $A(s,a)&gt;0$ ，梯度被推向正方向；</p><p>（2）如果 $A(s,a)&lt;0$ ，即我们的动作比该状态下的平均值还差，则梯度被推向反方向。</p><p>这样就需要两个价值函数，所以可以使用时序差分方法做误差估计：$A(s,a)=r+\gamma V(s’)-V(s)$ 。</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7.DDPG算法</title>
      <link href="/easy_rl_exercise/7.DDPG%E7%AE%97%E6%B3%95/"/>
      <url>/easy_rl_exercise/7.DDPG%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h1><ul><li><strong>深度确定性策略梯度（deep deterministic policy gradient，DDPG）</strong>：在连续控制领域经典的强化学习算法，是深度Q网络在处理连续动作空间的一个扩充方法。具体地，从命名就可以看出，“深度”表明使用了深度神经网络；“确定性”表示其输出的是一个确定的动作，可以用于连续动作环境；“策略梯度”代表的是它用到的是策略网络，并且每步都会更新一次，其是一个单步更新的策略网络。其与深度Q网络都有目标网络和经验回放的技巧，在经验回放部分是一致的，在目标网络的更新上有些许不同。</li></ul><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><h4 id="7-1-请解释随机性策略和确定性策略，两者有什么区别？"><a href="#7-1-请解释随机性策略和确定性策略，两者有什么区别？" class="headerlink" title="7-1 请解释随机性策略和确定性策略，两者有什么区别？"></a><strong>7-1</strong> 请解释随机性策略和确定性策略，两者有什么区别？</h4><p>（1）对于随机性策略 $\pi_\theta(a_t|s_t)$ ，我们输入某一个状态 $s$，采取某一个动作 $a$ 的可能性并不是百分之百的，而是有一个概率的，就好像抽奖一样，根据概率随机抽取一个动作。</p><p>（2）对于确定性策略 $\mu_{\theta}(s_t)$ ，其没有概率的影响。当神经网络的参数固定之后，输入同样的状态，必然输出同样的动作，这就是确定性策略。</p><h4 id="7-2-对于连续动作的控制空间和离散动作的控制空间，如果我们都采取策略网络，应该分别如何操作？"><a href="#7-2-对于连续动作的控制空间和离散动作的控制空间，如果我们都采取策略网络，应该分别如何操作？" class="headerlink" title="7-2 对于连续动作的控制空间和离散动作的控制空间，如果我们都采取策略网络，应该分别如何操作？"></a><strong>7-2</strong> 对于连续动作的控制空间和离散动作的控制空间，如果我们都采取策略网络，应该分别如何操作？</h4><p>首先需要说明的是，对于连续动作的控制空间，Q学习、深度Q网络等算法是没有办法处理的，所以我们需要使用神经网络进行处理，因为其可以既输出概率值，也可以输出确定的策略 $\mu_{\theta}(s_t)$ 。</p><p>（1）要输出离散动作，最后输出的激活函数使用 Softmax 即可。其可以保证输出的是动作概率，而且所有的动作概率加和为1。</p><p>（2）要输出连续的动作，可以在输出层中加一层tanh激活函数，其可以把输出限制到 $[-1,1]$ 。我们得到这个输出后，就可以根据实际动作的一个范围再做缩放，然后将其输出给环境。比如神经网络输出一个浮点数2.8，经过tanh激活函数之后，它就可以被限制在 $[-1,1]$ ，输出0.99。假设小车的速度的动作范围是 $[-2,2]$ ，那我们就按比例将之从 $[-1,1]$ 扩大到 $[-2,2]$ ，0.99乘2，最终输出的就是1.98，将其作为小车的速度或者推小车的力输出给环境。</p><h1 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h1><h4 id="7-1-友善的面试官：请简述一下深度确定性策略梯度算法。"><a href="#7-1-友善的面试官：请简述一下深度确定性策略梯度算法。" class="headerlink" title="7-1 友善的面试官：请简述一下深度确定性策略梯度算法。"></a><strong>7-1</strong> 友善的面试官：请简述一下深度确定性策略梯度算法。</h4><p>深度确定性策略梯度算法使用演员-评论员结构，但是输出的不是动作的概率，而是具体动作，其可以用于连续动作的预测。优化的目的是将深度Q网络扩展到连续的动作空间。另外，其含义如其名：</p><p>（1）深度是因为用了深度神经网络；</p><p>（2）确定性表示其输出的是一个确定的动作，可以用于连续动作的环境；</p><p>（3）策略梯度代表的是它用到的是策略网络。强化算法每个回合就会更新一次网络，但是深度确定性策略梯度算法每个步骤都会更新一次策略网络，它是一个单步更新的策略网络。</p><h4 id="7-2-友善的面试官：请问深度确定性策略梯度算法是同策略算法还是异策略算法？请说明具体原因并分析。"><a href="#7-2-友善的面试官：请问深度确定性策略梯度算法是同策略算法还是异策略算法？请说明具体原因并分析。" class="headerlink" title="7-2 友善的面试官：请问深度确定性策略梯度算法是同策略算法还是异策略算法？请说明具体原因并分析。"></a><strong>7-2</strong> 友善的面试官：请问深度确定性策略梯度算法是同策略算法还是异策略算法？请说明具体原因并分析。</h4><p>异策略算法。（1）深度确定性策略梯度算法是优化的深度Q网络，其使用了经验回放，所以为异策略算法。（2）因为深度确定性策略梯度算法为了保证一定的探索，对输出动作加了一定的噪声，行为策略不再是优化的策略。</p><h4 id="7-3-友善的面试官：你是否了解过分布的分布式深度确定性策略梯度算法（distributed-distributional-deep-deterministic-policy-gradient，D4PG）呢？请描述一下吧。"><a href="#7-3-友善的面试官：你是否了解过分布的分布式深度确定性策略梯度算法（distributed-distributional-deep-deterministic-policy-gradient，D4PG）呢？请描述一下吧。" class="headerlink" title="7-3 友善的面试官：你是否了解过分布的分布式深度确定性策略梯度算法（distributed distributional deep deterministic policy gradient，D4PG）呢？请描述一下吧。"></a><strong>7-3</strong> 友善的面试官：你是否了解过分布的分布式深度确定性策略梯度算法（distributed distributional deep deterministic policy gradient，D4PG）呢？请描述一下吧。</h4><p>分布的分布式深度确定性策略梯度算法（distributed distributional deep deterministic policy gradient，D4PG)，相对于深度确定性策略梯度算法，其优化部分如下。 </p><p>（1）分布式评论员：不再只估计Q值的期望值，而是估计期望Q值的分布，即将期望Q值作为一个随机变量来估计。</p><p>（2）$N$步累计回报：计算时序差分误差时，D4PG计算的是$N$步的时序差分目标值而不仅仅只有一步，这样就可以考虑未来更多步骤的回报。</p><p>（3）多个分布式并行演员：D4PG使用$K$个独立的演员并行收集训练数据并存储到同一个回放缓冲区中。</p><p>（4）优先经验回放（prioritized experience replay，PER）：使用一个非均匀概率从回放缓冲区中进行数据采样。</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>8.PPO算法</title>
      <link href="/easy_rl_exercise/8.PPO%E7%AE%97%E6%B3%95/"/>
      <url>/easy_rl_exercise/8.PPO%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h1><ul><li><strong>同策略（on-policy）</strong>：要学习的智能体和与环境交互的智能体是同一个时对应的策略。</li><li><strong>异策略（off-policy）</strong>：要学习的智能体和与环境交互的智能体不是同一个时对应的策略。</li><li><strong>重要性采样（important sampling）</strong>：使用另外一种分布，来逼近所求分布的一种方法，在强化学习中通常和蒙特卡洛方法结合使用，公式如下：</li></ul><script type="math/tex; mode=display">    \int f(x) p(x) \mathrm{d} x=\int f(x) \frac{p(x)}{q(x)} q(x) \mathrm{d} x=E_{x \sim q}[f(x){\frac{p(x)}{q(x)}}]=E_{x \sim p}[f(x)]</script><p>我们在已知 $q$ 的分布后，可以使用上式计算出从 $p$ 这个分布采样 $x$ 代入 $f$ 以后得到的期望值。</p><ul><li><strong>近端策略优化（proximal policy optimization，PPO）</strong>：避免在使用重要性采样时由于在 $\theta$ 下的 $p_{\theta}\left(a_{t} | s_{t}\right)$ 与在  $\theta ‘$ 下的 $p_{\theta’}\left(a_{t} | s_{t}\right)$ 相差太多，导致重要性采样结果偏差较大而采取的算法。具体来说就是在训练的过程中增加一个限制，这个限制对应 $\theta$ 和 $\theta’$ 输出的动作的KL散度，来衡量 $\theta$ 与 $\theta’$ 的相似程度。</li></ul><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><h4 id="8-1-基于同策略的策略梯度有什么可改进之处？或者说其效率较低的原因在于什么？"><a href="#8-1-基于同策略的策略梯度有什么可改进之处？或者说其效率较低的原因在于什么？" class="headerlink" title="8-1 基于同策略的策略梯度有什么可改进之处？或者说其效率较低的原因在于什么？"></a><strong>8-1</strong> 基于同策略的策略梯度有什么可改进之处？或者说其效率较低的原因在于什么？</h4><p>经典策略梯度的大部分时间花在数据采样上，即当我们的智能体与环境交互后，我们就要进行策略模型的更新。但是对于一个回合我们仅能更新策略模型一次，更新完后我们就要花时间重新采样数据，然后才能再次进行如上的更新。</p><p>所以我们可以使用异策略的方法，即使用另一个不同的策略和演员，与环境进行交互并用所采样的数据进行原先策略的更新。这样等价于使用同一组数据，在同一个回合，我们对整个策略模型更新了多次，这样会更加有效率。</p><h4 id="8-2-使用重要性采样时需要注意的问题有哪些？"><a href="#8-2-使用重要性采样时需要注意的问题有哪些？" class="headerlink" title="8-2 使用重要性采样时需要注意的问题有哪些？"></a><strong>8-2</strong> 使用重要性采样时需要注意的问题有哪些？</h4><p>我们可以在重要性采样中将 $p$ 替换为任意的 $q$，但是本质上要求两者的分布不能差太多，即使我们补偿了不同数据分布的权重 $\frac{p(x)}{q(x)}$ 。 $E_{x \sim p}[f(x)]=E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]$ ，当我们对于两者的采样次数都比较多时，最终的结果会是较为接近的。但是通常我们不会取理想数量的采样数据，所以如果两者的分布相差较大，最后结果的方差将会很大。</p><h4 id="8-3-基于异策略的重要性采样中的数据是从-theta’-中采样出来的，从-theta-换成-theta’-有什么优势？"><a href="#8-3-基于异策略的重要性采样中的数据是从-theta’-中采样出来的，从-theta-换成-theta’-有什么优势？" class="headerlink" title="8-3 基于异策略的重要性采样中的数据是从 $\theta’$ 中采样出来的，从 $\theta$ 换成 $\theta’$ 有什么优势？"></a><strong>8-3</strong> 基于异策略的重要性采样中的数据是从 $\theta’$ 中采样出来的，从 $\theta$ 换成 $\theta’$ 有什么优势？</h4><p>使用基于异策略的重要性采样后，我们不用 $\theta$ 与环境交互，而是由另外一个策略 $\theta’$ 进行示范。 $\theta’$ 的任务就是示范给 $\theta$ 看，它和环境交互，告诉 $\theta$ 它与环境交互会发生什么事，以此来训练 $\theta$ 。我们要训练的是 $\theta$ ，$\theta’$ 只负责做示范，负责与环境交互，所以采样出来的数据与 $\theta$ 本身是没有关系的。所以就可以让 $\theta’$ 与环境交互采样大量数据，$\theta$ 可以更新参数多次。一直到 $\theta$ 训练到一定的程度、参数更新多次以后，$\theta’$ 再重新采样，这就是同策略换成异策略的妙处。</p><h4 id="8-4-在本节中近端策略优化中的KL散度指的是什么？"><a href="#8-4-在本节中近端策略优化中的KL散度指的是什么？" class="headerlink" title="8-4 在本节中近端策略优化中的KL散度指的是什么？"></a><strong>8-4</strong> 在本节中近端策略优化中的KL散度指的是什么？</h4><p>本质来说，KL散度是一个函数，其度量的是两个动作（对应的参数分别为 $\theta$ 和 $\theta’$ ）间的行为距离，而不是参数距离。这里的行为距离可以理解为在相同状态下输出动作的差距（概率分布上的差距），概率分布即KL散度。</p><h1 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h1><h4 id="8-1-友善的面试官：请问什么是重要性采样呀？"><a href="#8-1-友善的面试官：请问什么是重要性采样呀？" class="headerlink" title="8-1 友善的面试官：请问什么是重要性采样呀？"></a><strong>8-1</strong> 友善的面试官：请问什么是重要性采样呀？</h4><p>使用另外一种分布，来逼近所求分布的一种方法，算是一种期望修正的方法，公式如下：</p><script type="math/tex; mode=display">\int f(x) p(x) \mathrm{d} x=\int f(x) \frac{p(x)}{q(x)} q(x) \mathrm{d} x=E_{x \sim q}[f(x){\frac{p(x)}{q(x)}}]=E_{x \sim p}[f(x)]</script><p>我们在已知 $q$ 的分布后，可以使用上式计算出从 $p$ 分布的期望值。也就可以使用 $q$ 来对 $p$ 进行采样了，即重要性采样。</p><h4 id="8-2-友善的面试官：请问同策略和异策略的区别是什么？"><a href="#8-2-友善的面试官：请问同策略和异策略的区别是什么？" class="headerlink" title="8-2 友善的面试官：请问同策略和异策略的区别是什么？"></a><strong>8-2</strong> 友善的面试官：请问同策略和异策略的区别是什么？</h4><p>我可以用一句话概括两者的区别，即生成样本的策略（价值函数）和网络参数更新时的策略（价值函数）是否相同。具体来说，同策略,生成样本的策略（价值函数）与网络更新参数时使用的策略（价值函数）相同。Sarsa算法就是同策略的，其基于当前的策略直接执行一次动作，然后用价值函数的值更新当前的策略，因此生成样本的策略和学习时的策略相同，算法为同策略算法。该算法会遭遇探索-利用窘境，仅利用目前已知的最优选择，可能学不到最优解，不能收敛到局部最优，而加入探索又降低了学习效率。 $\varepsilon$-贪心算法是这种矛盾下的折中，其优点是直接了当、速度快，缺点是不一定能够找到最优策略。异策略，生成样本的策略（价值函数）与网络更新参数时使用的策略（价值函数）不同。例如，Q学习算法在计算下一状态的预期奖励时使用了最大化操作，直接选择最优动作，而当前策略并不一定能选择到最优动作，因此这里生成样本的策略和学习时的策略不同，即异策略算法。</p><h4 id="8-3-友善的面试官：请简述一下近端策略优化算法。其与信任区域策略优化算法有何关系呢？"><a href="#8-3-友善的面试官：请简述一下近端策略优化算法。其与信任区域策略优化算法有何关系呢？" class="headerlink" title="8-3 友善的面试官：请简述一下近端策略优化算法。其与信任区域策略优化算法有何关系呢？"></a><strong>8-3</strong> 友善的面试官：请简述一下近端策略优化算法。其与信任区域策略优化算法有何关系呢？</h4><p>近端策略优化算法借鉴了信任区域策略优化算法，通过采用一阶优化，在采样效率、算法表现以及实现和调试的复杂度之间取得了新的平衡。这是因为近端策略优化算法会在每一次迭代中尝试计算新的策略，让损失函数最小化，并且保证每一次新计算出的策略能够和原策略相差不大。换句话说，其为在避免使用重要性采样时由于在 $\theta$ 下的 $p_{\theta}\left(a_{t} | s_{t}\right)$ 与在 $\theta’$ 下的 $p_{\theta’}\left(a_{t} | s_{t}\right)$ 差太多，导致重要性采样结果偏差较大而采取的算法。</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>9.稀疏奖励</title>
      <link href="/easy_rl_exercise/9.%E7%A8%80%E7%96%8F%E5%A5%96%E5%8A%B1/"/>
      <url>/easy_rl_exercise/9.%E7%A8%80%E7%96%8F%E5%A5%96%E5%8A%B1/</url>
      
        <content type="html"><![CDATA[<h1 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h1><ul><li><strong>设计奖励（reward shaping）</strong>：当智能体与环境进行交互时，我们人为设计一些奖励，从而“指挥”智能体，告诉其采取哪一个动作是最优的。需要注意的是，这个奖励区别于环境的奖励。其可以提高我们估算Q函数时的准确性。</li><li><strong>内在好奇心模块（intrinsic curiosity module，ICM）</strong>：其代表好奇心驱动这个技术中的增加新的奖励函数以后的奖励函数。</li><li><strong>课程学习（curriculum learning）</strong>：一种广义的用在强化学习中训练智能体的方法，其在输入训练数据的时候，采取由易到难的顺序进行输入，也可以人为设计它的学习过程。这个方法在机器学习和强化学习中普遍使用。</li><li><strong>逆课程学习（reverse curriculum learning）</strong>：相较于课程学习，逆课程学习为更广义的方法。其从最终最理想的状态 [我们称之为黄金状态（gold state）] 开始，依次去寻找距离黄金状态最近的状态作为想让智能体达到的阶段性的“理想”状态。当然，我们会在此过程中有意地去掉一些极端的状态，即太简单、太难的状态。综上，逆课程学习是从黄金状态反推的方法。</li><li><strong>分层强化学习（hierarchical reinforcement learning）</strong>：将一个大型的任务，横向或者纵向地拆解成由多个智能体去执行的子任务。其中，有一些智能体负责比较高层次的任务，如负责定目标，定完目标后，再将目标分配给其他的智能体执行。</li></ul><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><h4 id="9-1-解决稀疏奖励的方法有哪些？"><a href="#9-1-解决稀疏奖励的方法有哪些？" class="headerlink" title="9-1 解决稀疏奖励的方法有哪些？"></a><strong>9-1</strong> 解决稀疏奖励的方法有哪些？</h4><p>设计奖励、好奇心驱动的奖励、课程学习、逆课程学习、分层强化学习等。</p><h4 id="9-2-设计奖励存在什么主要问题？"><a href="#9-2-设计奖励存在什么主要问题？" class="headerlink" title="9-2 设计奖励存在什么主要问题？"></a><strong>9-2</strong> 设计奖励存在什么主要问题？</h4><p>主要的问题是我们人为设计的奖励需要领域知识，需要我们自己设计出让环境与智能体更好地交互的奖励，这需要不少的经验知识，并且需要我们根据实际的效果进行调整。</p><h4 id="9-3-内在好奇心模块是什么？我们应该如何设计内在好奇心模块？"><a href="#9-3-内在好奇心模块是什么？我们应该如何设计内在好奇心模块？" class="headerlink" title="9-3 内在好奇心模块是什么？我们应该如何设计内在好奇心模块？"></a><strong>9-3</strong> 内在好奇心模块是什么？我们应该如何设计内在好奇心模块？</h4><p>内在好奇心模块代表好奇心驱动技术中增加新的奖励函数以后的奖励函数。具体来说，其在更新计算时会考虑3个新的部分，分别是状态 $s_1$、动作 $a_1$ 和状态 $s_2$。根据 $s_1$ 、$a_1$、$a_2$，它会输出另外一个新的奖励 $r_1^i$。所以在内在好奇心模块中，我们的总奖励并不是只有 $r$ 而已，还有 $r^i$。它不是只把所有的 $r$ 相加，还把所有 $r^i$ 相加一并当作总奖励。所以，基于内在好奇心模块的智能体在与环境交互的时候，不是只希望 $r$ 越大越好，还同时希望 $r^i$ 越大越好，希望从内在好奇心模块里面得到的总奖励越大越好。</p><p>对于如何设计内在好奇心模块，其输入就像前面所说的一样，包括3部分，即现在的状态 $s_1$、在这个状态采取的动作 $a_1$、下一个状态 $s_{t+1}$，对应的输出就是奖励 $r_1^i$。输入、输出的映射是通过网络构建的，其使用状态 $s_1$ 和动作 $a_1$ 去预测下一个状态 $\hat{s}_{t+1}$ ，然后继续评判预测的状态 $\hat{s}_{t+1}$ 和真实状态 $s_{t+1}$ 的相似性，越不相似得到的奖励就越大。通俗来说这个奖励就是，如果未来的状态越难被预测，那么得到的奖励就越大。这就是好奇心机制，其倾向于让智能体做一些风险比较大的动作，从而提高其探索的能力。</p><p>同时，为了进一步增强网络的表达能力，我们通常将内在好奇心模块的输入优化为特征提取，特征提取器的输入就是状态，输出是一个特征向量，其可以表示这个状态最主要和最重要的特征，把没有意义的事物过滤。</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.Cython概述</title>
      <link href="/program_language/cython/1.Cython%E6%A6%82%E8%BF%B0/"/>
      <url>/program_language/cython/1.Cython%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="1、关于Cython"><a href="#1、关于Cython" class="headerlink" title="1、关于Cython"></a>1、关于Cython</h1><h2 id="1-1-Cython"><a href="#1-1-Cython" class="headerlink" title="1.1 Cython"></a>1.1 Cython</h2><p>Cython 是一门<strong>编程语言</strong>，它将 C、C++ 的静态类型系统融合在了 Python 身上。文件的后缀是 <code>.pyx</code>，它是 Python 的一个超集；语法是 Python 语法和 C 语法的混血，当然我们说它是 Python 的一个超集，因此你写纯 Python 代码也是可以的。</p><p>cython 是一个<strong>编译器</strong>，负责将 Cython 源代码翻译成高效的 C 或者 C++ 源代码；Cython 源文件被编译之后的最终形式可以是 Python 的扩展模块（<code>.pyd</code>），也可以是一个独立的可执行文件。</p><p>因此 Cython 的强大之处就在于它将 Python 和 C 结合了起来，可以让你像写 Python 代码一样的同时还可以获得 C 的高效率；所以我们看到 Cython 相当于是高级语言 Python 和低级语言 C 之间的一个融合。</p><h2 id="1-2-Cython和CPython的区别"><a href="#1-2-Cython和CPython的区别" class="headerlink" title="1.2 Cython和CPython的区别"></a>1.2 Cython和CPython的区别</h2><p>Python 是一门语言，它有自己的语法规则。按照 Python 语言规定的语法规则编写的代码就是 Python 源代码，但是源代码只是一个或多个普通的文本文件，需要使用 Python 语言对应的解释器来执行它。</p><p>而 Python 解释器也会按照同样的语法规则来对我们编写的 Python 源代码进行分词、语法解析等等，如果我们编写的代码不符合 Python 的语法规则，那么会报出语法错误，也就是 SyntaxError。如果符合语法规范的话，那么会顺利的生成抽象语法树（Abstract Syntax Tree，简称AST），然后将 AST 编译成指令集合，也就是所谓的字节码（bytes code），最后再执行字节码。</p><p>所以我们看到 Python 源代码是需要 Python 解释器来操作的，我们想做一些事情的话，如果光写成源代码是不行的，必须要由 <strong>Python 解释器将我们的代码解释成机器可以识别的指令进行执行</strong>才可以。而 <strong>CPython 正是 Python 语言对应的解释器</strong>，并且它也是官方实现的标准解释器，同时还是是使用最广泛的一种解释器。基本上我们使用的解释器都是 CPython，也就是从官网下载、然后安装之后所得到的。</p><p><strong>标准解释器 CPython 是由 C 语言实现的</strong>，除了 CPython 之外还有 Jython（java实现的 Python 解释器）、PyPy（Python 语言实现的 Python 解释器）等等。总之设计出一门语言，还要有相应的解释器才可以；至于编译型语言，则是对应的编译器。</p><p>最后重点来了， CPython 解释器是由 C 实现的，它给 Python 语言提供了 C 级别的接口，也就是熟知的 Python/C API。比如：Python 中的列表，底层对应的是 PyListObject；字典则对应 PyDictObject，等等。所以当我们在Python中创建一个列表，那么 CPython 在执行的时候，就会在底层创建一个 PyListObject。因为 CPython 是用C来实现的，最终肯定是将 Python 代码翻译成 C 级别的代码，然后再变成机器码交给 CPU 执行。而 Cython 也是如此，Cython 代码也是要被翻译成 C 代码的，而实现这一点的就是 cython 编译器（本质上是一个第三方模块，所以同样依赖于 CPython）。因此 Cython 是一门语言，它并不是Python 解释器的另一种实现，它的地位和 CPython 不是等价的，不过和 Python 是平级的。</p><p>因此 Cython 是一门语言，可以通过 Cython 源代码生成高效的扩展模块，同样需要 CPython 来进行调用。</p><h1 id="2、Python、C、C扩展、Cython比较"><a href="#2、Python、C、C扩展、Cython比较" class="headerlink" title="2、Python、C、C扩展、Cython比较"></a>2、Python、C、C扩展、Cython比较</h1><p>以简单的斐波那契数列为例，来测试一下它们执行效率的差异：</p><p><strong>Python代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fib</span>(<span class="params">n</span>):</span><br><span class="line">    a, b = <span class="number">0.0</span>, <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        a, b = a + b, a</span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><p><strong>C代码</strong>：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">double</span> <span class="title function_">cfib</span><span class="params">(<span class="type">int</span> n)</span> </span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line">    <span class="type">double</span> a=<span class="number">0.0</span>, b=<span class="number">1.0</span>, tmp;</span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;n; ++i) </span><br><span class="line">    &#123;</span><br><span class="line">        tmp = a; a = a + b; b = tmp;</span><br><span class="line">    &#125;</span><br><span class="line"> <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面便是 C 实现的一个斐波那契数列，可能有人好奇为什么使用浮点型，而不是整型呢？答案是 C 中的整型是有范围的，所以使用 double，而且 Python 中 float 在底层对应的是PyFloatObject、其内部也是通过 double 来存储的。</p><p>然后是 <strong>C 扩展</strong>:</p><p>注意：C 扩展不是我们的重点，写 C 扩展和写 Cython 本质是一样的，都是为 Python 编写扩展模块，但是写 Cython 绝对要比写 C 扩展简单的多。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> PyObject * <span class="title function_">fib</span><span class="params">(PyObject *self, PyObject *n)</span> </span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (!PyLong_CheckExact(n)) &#123;</span><br><span class="line">        PyErr_Format(PyExc_ValueError, <span class="string">&quot;function fib excepted int, not %s&quot;</span>, Py_TYPE(n) -&gt; tp_name);</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    PyObject *z;</span><br><span class="line">    <span class="type">double</span> a = <span class="number">0.0</span>, b = <span class="number">1.0</span>, tmp;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; PyLong_AsLong(n); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        tmp = a; a = a + b; b = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    z = PyFloat_FromDouble(a);</span><br><span class="line">    <span class="keyword">return</span> z;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Cython:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fib</span>(<span class="params"><span class="built_in">int</span> n</span>):</span><br><span class="line">    cdef <span class="built_in">int</span> i</span><br><span class="line">    cdef double a = <span class="number">0.0</span>, b = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        a, b = a + b, a</span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><p> Python 中所有的变量都是一个 <code>PyObject *</code>，在底层中就是 C 的一个指针。PyObject（C 的一个结构体）内部有两个成员，分别是 <code>ob_refcnt</code>：保存对象的引用计数、<code>ob_type *</code>：保存对象类型的指针。不管是整型、字符串、元组、字典，亦或是其它的什么，所有指向它们的变量都是一个 <code>PyObject *</code>，当进行操作的时候，首先要通过 <code>-&gt; ob_type</code> 来获取对应的类型的指针，再进行转化。</p><p>比如：这里的 a 和 b，我们虽然知道无论进行哪一层循环，结果指向的都是浮点数，但是 Python 解释器不会做这种推断。每一次相加都要进行检测，判断到底是什么类型并进行转化，然后执行加法的时候，再去找内部的 <strong>add</strong> 方法，将两个对象相加，创建一个新的对象，执行结束后再将这个新对象的指针转成 PyObject *，然后返回。并且 Python 中的对象都是在堆上分配空间，再加上 a 和 b 不可变，所以每一次循环都会创建新的对象，并将之前的对象给回收掉。</p><h2 id="2-1-效率差异"><a href="#2-1-效率差异" class="headerlink" title="2.1 效率差异"></a>2.1 效率差异</h2><p><img src="./image/1_1.jpg" alt="效率差异"></p><p>提升的倍数，指的是相对于纯 Python 来说在效率上提升了多少倍；第二列是fib(0)，显然它没有真正进行循环，fib(0) 测量的是<strong>调用一个函数所需要花费的开销</strong>；而倒数第二列 “循环体耗时” 指的是执行 fib(90) 的时候，排除函数调用本身的开销，也就是执行内部循环体所花费的时间。</p><ul><li>Python：各方面都是表现最差的那一个。从 fib(0) 来看，调用一个函数要花 590 纳秒。和 C 相比慢了这么多，原因就在于 Python 调用一个函数的时候需要创建一个栈帧，而这个栈帧是分配在堆上的，而且结束之后还要涉及栈帧的销毁等等。至于 fib(90)，显然无需分析了。</li><li>C：显然此时没有和 Python 运行时的交互，因此消耗的性能最小。</li><li>C扩展：看一下循环体耗时，发现 C 扩展和纯 C 是差不多的，区别就是函数调用上花的时间比较多。原因就在于在调用扩展模块的函数时，需要先将 Python 中的数据转成 C 中的数据，然后在 C 计算斐波那契数列，计算完了再将 C 中的数据转成 Python 中的数据。C 扩展本质也是 C 语言，只不过在编写的时候遵循 Python 提供的 API 规范，可以将 C 代码编译成 pyd 文件，直接让 Python 来调用。</li><li>Cython：单独看循环体耗时的话，我们看到纯 C、C 扩展、Cython 都是差不多的，但是编写 Cython 显然是最方便的。 Cython 做的事情和 C 扩展本质是类似的，都是为 Python 提供扩展模块，所以对于 Cython 来说，将 Python 的数据转成 C 的数据、进行计算、然后再转成 Python 中的数据返回，这一过程是无可避免的。但是我们看到 Cython 在函数调用时的耗时相比 C 扩展却要少很多，主要是 Cython 生成的C代码是经过高度优化的。</li></ul><h2 id="2-2-Python的for循环为什么这么慢？"><a href="#2-2-Python的for循环为什么这么慢？" class="headerlink" title="2.2 Python的for循环为什么这么慢？"></a>2.2 Python的for循环为什么这么慢？</h2><p>通过循环体耗时我们看到，Python 的 for 循环真的是出了名的慢，那么原因是什么呢？</p><h3 id="2-2-1-Python-的-for-循环机制"><a href="#2-2-1-Python-的-for-循环机制" class="headerlink" title="2.2.1 Python 的 for 循环机制"></a>2.2.1 Python 的 for 循环机制</h3><p>Python 在遍历一个可迭代对象的时候，会先调用这个可迭代对象内部的<code>__iter__</code> 方法返回其对应的迭代器，然后再不断地调用这个迭代器的 <code>__next__</code>方法，将值一个一个的迭代出来，直到迭代器抛出 StopIteration 异常，for循环捕捉，终止循环。而迭代器是有状态的，Python 解释器需要时刻记录迭代器的迭代状态。</p><h3 id="2-2-2-python的算数操作"><a href="#2-2-2-python的算数操作" class="headerlink" title="2.2.2 python的算数操作"></a>2.2.2 python的算数操作</h3><p>Python 由于其动态特性，使得其<strong>无法做任何基于类型的优化</strong>。比如：循环体中的 a + b，这个 a、b 指向的可以是整数、浮点数、字符串、元组、列表，甚至是实现了魔法方法 <code>__add__</code> 的类的实例对象，等等。尽管我们知道是浮点数，但是 Python 不会做这种假设，所以每一次执行 a + b 的时候，都会检测其类型到底是什么？然后判断内部是否有<code>__add__</code>方法，以及两者能不能相加，然后条件满足的话再调用对应的 <code>__add__</code> 方法，将 a 和 b 作为参数，将 a 和 b 指向的对象进行相加。计算出结果之后，再返回其指针转成 PyObject * 返回。</p><p>而对于 C 和 Cython 来说，在创建变量的时候就实现规定了类型。就是这个类型，不是其它的，因此编译之后的 a + b 只是一条简单的机器指令。这对比下来，Python 尼玛能不慢吗。</p><h3 id="2-2-3-Python中对象的内存分配"><a href="#2-2-3-Python中对象的内存分配" class="headerlink" title="2.2.3 Python中对象的内存分配"></a>2.2.3 Python中对象的内存分配</h3><p>Python 中的对象是分配在<strong>堆</strong>上面的，因为 Python 中的对象本质上就是 C 中的 malloc 函数为结构体在堆区申请的一块内存。在堆区进行内存的分配和释放是需要付出很大的代价的，而栈则要小很多，并且它是由操作系统维护的，会自动回收，效率极高。而堆显然没有此待遇，而恰恰 Python 的对象都是分配在堆上的，尽管 Python 引入了内存池机制使得其在一定程度上避免了和操作系统的频繁交互，并且还引入了小整数对象池以及针对字符串的intern机制。但事实上，当涉及到对象（任意对象、包括标量）的创建和销毁时，都会增加动态分配内存、以及 Python 内存子系统的开销。而 float 对象又是不可变的，因此每循环一次都会创建和销毁一次，所以效率依旧是不高的。</p><p>而 Cython 分配的变量，这里是 a 和 b，它们就不再是指针了（我们说 Python 中的变量本质上都是一个指针），而是分配在栈上的双精度浮点数。而栈上分配的效率远远高于堆，因此非常适合 for 循环，所以效率要比 Python 高很多。</p><p>所以在 for 循环方面，C 和 Cython 要比纯 Python 快了一个数量级以上，这并不是奇怪的事情，因为 Python 每次迭代都要做很多的工作。</p><h2 id="2-3-使用Cython保证C代码"><a href="#2-3-使用Cython保证C代码" class="headerlink" title="2.3 使用Cython保证C代码"></a>2.3 使用Cython保证C代码</h2><p>C代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// fib.h</span></span><br><span class="line"><span class="type">double</span> <span class="title function_">cfib</span><span class="params">(<span class="type">int</span> n)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// fib.c</span></span><br><span class="line"><span class="type">double</span> <span class="title function_">cfib</span><span class="params">(<span class="type">int</span> n)</span> </span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line">    <span class="type">double</span> a=<span class="number">0.0</span>, b=<span class="number">1.0</span>, tmp;</span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;n; ++i) </span><br><span class="line">    &#123;</span><br><span class="line">        tmp = a; a = a + b; b = tmp;</span><br><span class="line">    &#125;</span><br><span class="line"> <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Cython代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;cfib.h&quot;</span>:</span><br><span class="line">    double cfib(<span class="built_in">int</span> n)</span><br><span class="line">   </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fib</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="comment"># 调用 C 实现的斐波那契函数</span></span><br><span class="line">    <span class="keyword">return</span> cfib(n)</span><br></pre></td></tr></table></figure><p>cdef extern from 可以看成是导入头文件，内部相当于定义要使用的 C 函数。</p>]]></content>
      
      
      <categories>
          
          <category> PL </category>
          
          <category> Cython </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.Cython编译运行</title>
      <link href="/program_language/cython/2.Cython%E7%BC%96%E8%AF%91%E8%BF%90%E8%A1%8C/"/>
      <url>/program_language/cython/2.Cython%E7%BC%96%E8%AF%91%E8%BF%90%E8%A1%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>Python 和 C、C++ 之间一个最重要的差异就是 Python 是解释型语言，而 C、C++ 是编译型语言。</p><p>而Cython 同 C、C++ 类似，在源代码运行之前也需要一个编译的步骤，不过这个编译可以是隐式的，也可以是显式的。而自动编译 Cython 的一个很棒的特性就是它使用起来和纯 Python 是差不多的，无论是显式还是隐式，都可以将 Python 的一部分（计算密集）使用 Cython 重写，因此 Cython 的编译需求可以达到最小化。因为没有必要将所有的代码都用 Cython 编写，而是将那些需要优化的代码使用 Cython 编写即可。</p><p>编译Cython有以下几个选择：</p><ul><li><code>Cython 代码可以在 IPython 解释器中进行编译，并交互式运行。</code></li><li><code>Cython 代码可以在导入的时候自动编译。</code></li><li><code>Cython 代码可以通过类似于 Python 的 disutils 模块的编译工具进行独立编译。</code></li><li><code>Cython代码可以被继承到标准的编译系统，例如：make、CMake、SCons。</code></li></ul><p>这些选择可以让我们在几个特定的场景应用 Cython，从一端的快速交互式探索到另一端的快速构建。</p><h1 id="2、Cython编译Pipeline"><a href="#2、Cython编译Pipeline" class="headerlink" title="2、Cython编译Pipeline"></a>2、Cython编译Pipeline</h1><p>因为 Cython 是 Python 的超集，所以 Python 解释器无法直接运行 Cython 的代码，那么如何才能将 Cython 代码变成 Python 解释器可以识别的有效代码呢？答案是通过 Cython 编译 <strong>Pipeline</strong>。</p><p>Pipeline 的职责就是将 Cython 代码转换成 Python 解释器可以直接导入并使用的 Python 扩展模块，这个 Pipeline 可以在不受用户干预的情况下自动运行（使 Cython 感觉像 Python 一样），也可以在需要更多控制时由用户显式的运行。</p><p>Pipeline 由两步组成：第一步是由 cython 编译器负责将 Cython 转换成经过优化并且依赖当前平台的 <strong>C、C++ 代码</strong>；第二步是使用标准的 C、C++ 编译器将第一步得到的 C、C++ 代码进行编译并<strong>生成标准的扩展模块</strong>，并且这个扩展模块是依赖特定的平台的。如果是在 Linux 或者 Mac OS，那么得到的扩展模块的后缀名为 .so，如果是在 Windows 平台，那么得到的扩展模块的后缀名为 .pyd（扩展模块 .pyd 本质上是一个 DLL 文件）。不管是什么平台，最终得到的都会是一个成熟的 Python 扩展模块，它是可以直接被 Python 解释器进行 import 的。</p><blockquote><p>Cython编译器是一种 源到源 的编译器，并且生成的扩展模块也是经过高度优化的，因此 Cython 生成的 C 代码编译得到的扩展模块 比 手写的 C 代码编译得到的扩展模块 运行的要快并不是一件稀奇的事情。因为 Cython 生成的 C 代码是经过高度精炼，所以大部分情况下比手写所使用的算法更优，而且 <strong>Cython 生成的 C 代码支持所有的通用 C 编译器</strong>，生成的扩展模块同时支持许多不同的 Python 版本。</p></blockquote><p>所以 Cython 和 C 扩展本质上干的事情是一样的，都是将符合 Python/C API 的 C 代码编译成 Python 扩展模块，只不过写 Cython 的话我们不需要直接面对 C，Cython 编译器会自动将 Cython 代码翻译成 C 代码，然后再将其编译成扩展模块。所以两者本质是一样的，只不过 C 比较复杂，而且难编程；但是 Cython 简单，语法本来就和 Python 很相似，所以我们选择编写 Cython，然后让 cython 编译器帮我们把 Cython 代码翻译成 C 的代码。而且重点是，cython 编译器是经过优化的，如果我们能写出很棒的 Cython 代码，那么 cython 编译器在编译之后就会得到同样高质量的 C 代码。</p><h1 id="3、安装"><a href="#3、安装" class="headerlink" title="3、安装"></a>3、安装</h1><h2 id="3-1-C-C-编译器"><a href="#3-1-C-C-编译器" class="headerlink" title="3.1 C / C++编译器"></a>3.1 C / C++编译器</h2><h2 id="3-2-安装cython编译器"><a href="#3-2-安装cython编译器" class="headerlink" title="3.2 安装cython编译器"></a>3.2 安装cython编译器</h2><p>安装 cython 编译器的话，可以直接通过 <code>pip install cython</code> 即可。因此我们看到 cython 编译器只是 Python 的一个第三方包，因此运行 Cython 代码同样要借助 Python 解释器。</p><p>在终端中输入 cython -V，看看是否会提示 cython 的版本，如果正常显示，那么证明安装成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\35b180&gt;cython -V</span><br><span class="line">Cython version 0.29.23</span><br><span class="line"></span><br><span class="line">C:\Users\35b180&gt;cython --version</span><br><span class="line">Cython version 0.29.23</span><br></pre></td></tr></table></figure><p>代码查看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> Cython <span class="keyword">import</span> __version__</span><br><span class="line"><span class="built_in">print</span>(__version__)  <span class="comment"># 0.29.14</span></span><br></pre></td></tr></table></figure><h2 id="3-3-disutils"><a href="#3-3-disutils" class="headerlink" title="3.3 disutils"></a>3.3 disutils</h2><p>Python 有一个标准库 disutils，可以用来<strong>构建、打包、分发</strong> Python 工程。而其中一个有用的特性就是它可以借助 C 编译器将 C 源码编译成扩展模块，并且这个模块是自带的、考虑了平台、架构、Python 版本等因素，因此我们在任意地方使用disutils都可以得到扩展模块。</p><p>注意：上面 disutils 只是帮我们完成了 Pipeline 的第二步，第一步则是需要 cython 来完成。</p><p>以我们之前说的斐波那契数列为栗：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fib.pyx</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fib</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;这是一个扩展模块&quot;&quot;&quot;</span></span><br><span class="line">    cdef <span class="built_in">int</span> i</span><br><span class="line">    cdef double a=<span class="number">0.0</span>, b=<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        a, b = a + b, a</span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><p>编译文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># setup_exe.py</span></span><br><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> Cython.Build <span class="keyword">import</span> cythonize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们说构建扩展模块的过程分为两步: </span></span><br><span class="line"><span class="comment">## 1. 将 Cython 代码翻译成 C 代码; </span></span><br><span class="line"><span class="comment">## 2. 根据 C 代码生成扩展模块</span></span><br><span class="line"><span class="comment"># 而第一步要由 cython 编译器完成, 通过 cythonize; </span></span><br><span class="line"><span class="comment"># 第二步要由 distutils 完成, 通过 distutils.core 下的 setup</span></span><br><span class="line">setup(ext_modules=cythonize(<span class="string">&quot;fib.pyx&quot;</span>, language=<span class="number">3</span>))</span><br><span class="line"><span class="comment"># 里面的 language=3 表示只需要兼容 python3 即可, 而默认是 2 和 3 都兼容</span></span><br><span class="line"><span class="comment"># 强烈建议加上这个参数, 因为目前为止我们只需要考虑 python3 即可</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cythonize 负责将 Cython 代码转成 C 代码, </span></span><br><span class="line"><span class="comment"># 这里我们可以传入单个文件, 也可以是多个文件组成的列表,</span></span><br><span class="line"><span class="comment"># 或者一个glob模式, 会匹配满足模式的所有 Cython 文件; </span></span><br><span class="line"><span class="comment"># 然后 setup 根据 C 代码生成扩展模块</span></span><br></pre></td></tr></table></figure><p>使用如下命令编译：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python .\setup_exe.py build</span><br></pre></td></tr></table></figure><p>在我们执行命令之后，当前目录会多出一个 build 目录，里面的结构如下。</p><p><img src="./image/2_1.jpg" alt="build"></p><p>重点是那个 fib.cp37-win_amd64.pyd 文件，该文件就是根据 fib.pyx 生成的扩展模块，至于其它的可以直接删掉了。把这个文件单独拿出来测试一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test_fib.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> fib</span><br><span class="line"><span class="comment"># 我们看到该 pyd 文件直接就被导入了, </span></span><br><span class="line"><span class="comment"># 至于中间的 cp38-win_amd64 指的是对应的解释器版本、操作系统等信息</span></span><br><span class="line"><span class="built_in">print</span>(fib)  <span class="comment"># &lt;module &#x27;fib&#x27; from &#x27;D:\\satori\\fib.cp38-win_amd64.pyd&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 我们在里面定义了一个 fib 函数, 在 fib.pyx 里面定义的函数在编译成扩展模块之后可以直接使用</span></span><br><span class="line">    <span class="built_in">print</span>(fib.fib(<span class="string">&quot;xx&quot;</span>))</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    <span class="keyword">import</span> traceback</span><br><span class="line">    <span class="built_in">print</span>(traceback.format_exc())</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Traceback (most recent call last):</span></span><br><span class="line"><span class="string">      File &quot;D:/satori/1.py&quot;, line 7, in &lt;module&gt;</span></span><br><span class="line"><span class="string">        print(fib.fib(&quot;xx&quot;))</span></span><br><span class="line"><span class="string">      File &quot;fib.pyx&quot;, line 6, in fib.fib</span></span><br><span class="line"><span class="string">        for i in range(n):</span></span><br><span class="line"><span class="string">    TypeError: an integer is required</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 因为我们定义的是fib(int n), 而传入的不是整型, 所以直接报错</span></span><br><span class="line"><span class="built_in">print</span>(fib.fib(<span class="number">20</span>))  <span class="comment"># 6765.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们的注释</span></span><br><span class="line"><span class="built_in">print</span>(fib.fib.__doc__)  <span class="comment"># 这是一个扩展模块</span></span><br></pre></td></tr></table></figure><h2 id="3-4-引入C源文件"><a href="#3-4-引入C源文件" class="headerlink" title="3.4 引入C源文件"></a>3.4 引入C源文件</h2><p>C代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cfib.h</span></span><br><span class="line"><span class="type">double</span> <span class="title function_">cfib</span><span class="params">(<span class="type">int</span> n)</span>;  <span class="comment">// 定义一个函数声明</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//cfib.c</span></span><br><span class="line"><span class="type">double</span> <span class="title function_">cfib</span><span class="params">(<span class="type">int</span> n)</span> &#123;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line">    <span class="type">double</span> a=<span class="number">0.0</span>, b=<span class="number">1.0</span>, tmp;</span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;n; ++i) &#123;</span><br><span class="line">        tmp = a; a = a + b; b = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">   <span class="keyword">return</span> a;</span><br><span class="line">&#125; <span class="comment">// 函数体的实现</span></span><br></pre></td></tr></table></figure><p>cython代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fib.pyx</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 cdef extern from 导入头文件, 写上里面的函数</span></span><br><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;cfib.h&quot;</span>:</span><br><span class="line">    double cfib(<span class="built_in">int</span> n)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后 Cython 可以直接调用</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fib_with_c</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;调用 C 编写的斐波那契数列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> cfib(n)</span><br></pre></td></tr></table></figure><p>编译文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># setup_exe.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup, Extension</span><br><span class="line"><span class="keyword">from</span> Cython.Build <span class="keyword">import</span> cythonize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们看到之前是直接往 cythonize 里面传入一个文件名即可</span></span><br><span class="line"><span class="comment"># 但是现在我们传入了一个 Extension 对象, 通过 Extension 对象的方式可以实现更多功能</span></span><br><span class="line"><span class="comment"># 这里指定的 name 表示编译之后的文件名, 显然编译之后会得到 wrapper_cfib.cp38-win_amd64.pyd</span></span><br><span class="line"><span class="comment"># 如果是之前的方式, 那么得到的就是 fib.cp38-win_amd64.pyd, 默认会和 .pyx 文件名保持一致, 这里我们可以自己指定</span></span><br><span class="line"><span class="comment"># sources 则是代表源文件, 这里我们只需要指定 pyx 和 c 源文件即可, 因为头文件也在同一个目录中</span></span><br><span class="line"><span class="comment"># 如果不在, 那么还需要通过 include_dirs 指定头文件的所在目录, 不然 extern from &quot;cfib.h&quot; 就报错了</span></span><br><span class="line">ext = Extension(name=<span class="string">&quot;wrapper_cfib&quot;</span>, sources=[<span class="string">&quot;fib.pyx&quot;</span>, <span class="string">&quot;cfib.c&quot;</span>])</span><br><span class="line">setup(ext_modules=cythonize(ext))</span><br></pre></td></tr></table></figure><p>调用测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> wrapper_cfib</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(wrapper_cfib.fib_with_c(<span class="number">20</span>))  <span class="comment"># 6765.0</span></span><br><span class="line"><span class="built_in">print</span>(wrapper_cfib.fib_with_c.__doc__)  <span class="comment"># 调用 C 编写的斐波那契数列</span></span><br></pre></td></tr></table></figure><p><strong>创建扩展</strong>模块的方法：</p><ul><li>单个 pyx 文件：直接通过 <code>cythonize(&quot;xxx.pyx&quot;)</code>即可</li><li>如果 pyx 文件还引入了 C 文件, 那么通过 <code>cythonize(Extension(name=&quot;xx&quot;, sources=[&quot;&quot;, &quot;&quot;]))</code>的方式即可; name 是编译之后的扩展模块的名字, sources 是你要编译的源文件, 我们这里是一个 pyx 文件一个 C 文件;</li></ul><h1 id="4、通过IPython动态交互Cython"><a href="#4、通过IPython动态交互Cython" class="headerlink" title="4、通过IPython动态交互Cython"></a>4、通过IPython动态交互Cython</h1><p>使用 distutils 编译 Cython 代码可以控制每一步的执行过程，当时也意味着在使用之前必须要先经过独立的编译，不涉及到交互式。而 Python 的一大特性就是交互式，比如 IPython，所以需要想个法子让 Cython 也支持交互式，而实现的办法就是使用魔法命令。</p><p>打开IPython，演示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">PS E:\&gt; IPython</span><br><span class="line">Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)]</span><br><span class="line">Type &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line"></span><br><span class="line">IPython 5.5.0 -- An enhanced Interactive Python.</span><br><span class="line">?         -&gt; Introduction and overview of IPython&#x27;s features.</span><br><span class="line"><span class="meta prompt_">%</span><span class="language-bash">quickref -&gt; Quick reference.</span></span><br><span class="line">help      -&gt; Python&#x27;s own help system.</span><br><span class="line">object?   -&gt; Details about &#x27;object&#x27;, use &#x27;object??&#x27; for extra details.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在 IPython 上运行，执行 %load_ext cython 便会加载 Cython 的一些魔法函数</span></span><br><span class="line">In [1]: %load_ext cython</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后神奇的一幕出现了，加上一个魔法命令，就可以直接写 Cython 代码</span></span><br><span class="line">In [2]: %%cython</span><br><span class="line">   ...: def fib(int n):</span><br><span class="line">   ...:     cdef int i</span><br><span class="line">   ...:     cdef double a = 0.0, b = 1.0</span><br><span class="line">   ...:     for i in range(n):</span><br><span class="line">   ...:         a, b = a + b, a</span><br><span class="line">   ...:     return a</span><br><span class="line">   ...:</span><br><span class="line">_cython_magic_5a185622c60ceb32fc21f7d71ba187d2.c</span><br><span class="line">  正在创建库 C:\Users\35b180\.ipython\cython\Users\35b180\.ipython\cython\_cython_magic_5a185622c60ceb32fc21f7d71ba187d2.cp37-win_amd64.lib 和对象 C:\Users\35b180\.ipython\cython\Users\35b180\.ipython\cython\_cython_magic_5a185622c60ceb32fc21f7d71ba187d2.cp37-win_amd64.exp</span><br><span class="line">正在生成代码</span><br><span class="line">已完成代码的生成</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">测试用时，平均花费82.6ns</span></span><br><span class="line">In [3]: %timeit fib(50)</span><br><span class="line">10000000 loops, best of 5: 85.2 ns per loop</span><br></pre></td></tr></table></figure><p>首先 IPython 中存在一些魔法命令，这些命令以一个或<strong>两个百分号开头</strong>，它们提供了普通 Python 解释器所不能提供的功能。<code>%load_ext cython</code>会加载 cython 的一些魔法函数，如果执行成功将不会有任何的输出。然后重点来了，<code>%%cython</code>允许我们在 IPython 解释器中直接编写 Cython 代码，当我们按下两次回车时，显然这个代码块就结束了。但是<strong>里面的 Cython 代码会被 copy 到名字唯一的 .pyx 文件中，并将其编译成扩展模块，编译成功之后 IPython 会再将该模块内的所有内容导入到当前的环境中，以便我们使用</strong>。</p><p>因此上述的编译过程、编译完成之后的导入过程，都是在按下两次回车键之后自动发生的。但是不管怎么样，它都涉及到编译成扩展模块的过程，包括后面要说的即时编译，只不过这一步不需要你手动做了。</p><p>当然相比 IPython，我们更常用 jupyter notbook，既然 Cython 在前者中可以使用，那么后者肯定也是可以的。</p><p>jupyter notebook 底层也是使用了 IPython，所以它的原理和 IPython 是等价的，会先将代码块 copy 到名字唯一的 .pyx 文件中，然后进行编译。编译完毕之后再将里面的内容导入进来，而第二次编译的时候由于单元格里面的内容没有变化，所以不再进行编译了。</p><p>另外在编译的时候如果指定了 <code>--annotate</code>选项，那么还可以看到对应的代码分析。</p><h1 id="5、使用pximport即时编译"><a href="#5、使用pximport即时编译" class="headerlink" title="5、使用pximport即时编译"></a>5、使用pximport即时编译</h1><p>因为 Cython 是以 Python 为中心的，所以希望 Python 解释器在导包的时候能够自动识别 Cython 文件，导入 Cython 就像导入常规、动态的 Python 文件一样。但是不好意思，Python 在导包的时候并不会自动识别以 .pyx 结尾的文件，但是可以通过 <code>pyximport</code>来改变这一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fib.pyx</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>(<span class="params"><span class="built_in">int</span> a, <span class="built_in">int</span> b</span>):</span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># setup_exec.py</span></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line"><span class="comment"># 这里同样指定 language_level=3, 则表示针对的是py3, 因为这种方式也是要编译的</span></span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 执行完之后, Python 解释器在导包的时候就会识别 Cython 文件了, 当然会先进行编译</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> fib</span><br><span class="line"><span class="built_in">print</span>(fib.foo(<span class="number">11</span>, <span class="number">22</span>))  <span class="comment"># 33</span></span><br></pre></td></tr></table></figure><p>正如我们上面演示的那样，使用 <code>pyximport</code>可以让我们省去 cythonize 和 distutils 这两个步骤（注意：这两个步骤还是存在的，只是不用我们做了）。另外， Cython 源文件不会立刻编译，只有当被导入的时候才会编译，并且即便后续 Cython 源文件被修改了，pyximport 也会自动检测，当重新导入的时候也会再度重新编译，机制就和 Python 的 pyc 文件是一个道理。</p><p>自动编译之后的 pyd 文件位于 <code>~/.pyxbld/lib.xxx</code> 中</p><p>但是问题来了，如果包含 Cython 文件中还引入了其它的 C 文件该怎么办呢？直接导入会报错</p><h1 id="6、控制pyximport并管理依赖"><a href="#6、控制pyximport并管理依赖" class="headerlink" title="6、控制pyximport并管理依赖"></a>6、控制pyximport并管理依赖</h1><p>手动编译的时候，可以直接指定依赖的 C 文件的位置，但是直接导入 .pyx 文件的时候是并不知道这些依赖在哪里的。所以应该还要定义一个 <code>.pyxbld</code> 文件，.pyxbld 文件要和 .pyx 文件具有相同的基名，比如为了指定 fib.pyx 文件的依赖，那么 .pyxbld 文件就应该叫做 fib.pyxbld，并且它们要位于同一目录中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fib.pyxbld</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> distutils.extension <span class="keyword">import</span> Extension</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_ext</span>(<span class="params">modname, pyxfilename</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    如果 .pyxbld 文件中定义了这个函数, 那么在编译之前会进行调用, 并自动往里面进行传参</span></span><br><span class="line"><span class="string">    modname 是编译之后的扩展模块名, 显然这里就是 fib</span></span><br><span class="line"><span class="string">    pyxfilename 是编译的 .pyx 文件, 显然是 fib.pyx, 注意: .pyx 和 .pyxbld 要具有相同的基名称</span></span><br><span class="line"><span class="string">    然后它要返回一个我们之前说的 Extension 对象</span></span><br><span class="line"><span class="string">    :param modname:</span></span><br><span class="line"><span class="string">    :param pyxfilename:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> Extension(modname,</span><br><span class="line">                     sources=[pyxfilename, <span class="string">&quot;cfib.c&quot;</span>],</span><br><span class="line">                     <span class="comment"># include_dir 表示在当前目录中寻找头文件</span></span><br><span class="line">                     include_dirs=[<span class="string">&quot;.&quot;</span>])</span><br><span class="line">    <span class="comment"># 我们看到整体还是类似的逻辑, 因为编译这一步是怎么也绕不过去的</span></span><br><span class="line">    <span class="comment"># 区别就是手动编译还是自动编译, 如果是自动编译, 显然限制会比较多</span></span><br><span class="line">    <span class="comment"># 如果想解除限制, 那么我们看到这和手动编译没啥区别了, 甚至还要更麻烦一些</span></span><br></pre></td></tr></table></figure><p>此时再来直接导入看看，会不会得到正确的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 全局编码为 utf-8</span></span><br><span class="line"><span class="keyword">import</span> _locale</span><br><span class="line">_locale._getdefaultlocale = (<span class="keyword">lambda</span> *args: [<span class="string">&#x27;en_US&#x27;</span>, <span class="string">&#x27;utf8&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line"><span class="comment"># 这里同样指定 language_level=3, 则表示针对的是py3, 因为这种方式也是要编译的</span></span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 执行完之后, Python 解释器在导包的时候就会识别 Cython 文件了, 当然会先进行编译</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> fib</span><br><span class="line"><span class="built_in">print</span>(fib.fib_with_c(<span class="number">50</span>))  <span class="comment"># 12586269025.0</span></span><br></pre></td></tr></table></figure><p>.pyxbld 文件中除了通过定义 make_ext 函数的方式外，还可以定义 <code>make_setup_args</code>函数。对于 <code>make_ext</code> 函数，在编译的时候会自动传递两个参数：modname、pyxfilename。但如果定义的是 make_setup_args 函数，那么在编译时就不会传递任何参数，一些都由你自己决定。</p><p>但这里还有一个问题，首先 Cython 源文件一旦改变了，那么<strong>再导入的时候就会重新编译</strong>；但如果 Cython 源文件（.pyx）依赖的 C 文件改变了呢？这个时候导入的话还会自动重新编译吗？答案是<strong>会的</strong>（没想到吧），cython 编译器不仅会检测 Cython 文件的变化，还会检测它<strong>依赖的 C 文件的变化</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> PL </category>
          
          <category> Cython </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.Cython语法介绍</title>
      <link href="/program_language/cython/3.Cython%E8%AF%AD%E6%B3%95%E4%BB%8B%E7%BB%8D/"/>
      <url>/program_language/cython/3.Cython%E8%AF%AD%E6%B3%95%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>Cython 和 Python 的差别从大方向上来说无非两个，</p><ul><li>运行时解释和预先编译；</li><li>动态类型和静态类型。</li></ul><h1 id="2、解释执行-VS-编译执行"><a href="#2、解释执行-VS-编译执行" class="headerlink" title="2、解释执行 VS 编译执行"></a>2、解释执行 VS 编译执行</h1><p>对比一下 Python 虚拟机执行 Python 代码和操作系统执行已经编译的 C 代码之间的差别:</p><h2 id="2-1-Python执行过程"><a href="#2-1-Python执行过程" class="headerlink" title="2.1 Python执行过程"></a>2.1 Python执行过程</h2><p>Python代码在运行之前，会先被编译成 <strong>pyc 文件</strong>（里面存储的是 Python 底层的PyCodeObject 对象），然后读取里面的 PyCodeObject 对象，执行内部的<strong>字节码</strong>。而字节码是能够被 Python 虚拟机解释或者执行的基础指令集，并且虚拟机独立于平台，因此在一个平台生成的字节码可以在任意平台运行。</p><p>虚拟机将一个高级字节码翻译成一个或者多个可以被操作系统执行、最终被CPU执行的低级操作（指令）。这种虚拟化很常见并且十分灵活，可以带来很多好处：其中一个好处就是不会被挑剔的编译器嫌弃（相较于编译型语言，在一个平台编译的可执行文件在其它平台上可能就用不了了），而缺点是运行速度比本地编译好的代码慢。</p><h2 id="2-2-C执行过程"><a href="#2-2-C执行过程" class="headerlink" title="2.2 C执行过程"></a>2.2 C执行过程</h2><p>在 C 的角度，由于不存在虚拟机或者解释器，因此也就不存在所谓的高级字节码。C 代码会被直接翻译、或者编译成机器码，可以直接以一个可执行文件或者动态库（dll 或者 so）的形式存在。但是注意：它<strong>依赖于当前的操作系统</strong>，是为当前平台和架构量身打造的，因为可以直接被 CPU 执行，而且级别非常低（伴随着速度快），所以它与所在的操作系统是有关系的。</p><h2 id="2-3-扩展模块"><a href="#2-3-扩展模块" class="headerlink" title="2.3 扩展模块"></a>2.3 扩展模块</h2><p> Windows 中存在<strong>dll（动态链接库）</strong>、Linux中存在 <strong>so（共享文件）</strong>。如果只是 C 或者 C++、甚至是是 Go 等等编写的普通源文件，然后编译成 dll 或者 so，那么这两者可以通过 ctypes 调用，但是无法通过 import 导入。如果你强行导入，那么会报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: dynamic module does not define module export function</span><br></pre></td></tr></table></figure><p>但是如果是遵循 Python/C API 编写，然后使用 Python 编译成扩展模块的话，尽管该扩展模块在 Linux 上也是 .so、Windows 上是 pyd（pyd也是个dll），但它们是可以直接被 Python 解释器识别被导入的。</p><h1 id="3、动态类型-VS-静态类型"><a href="#3、动态类型-VS-静态类型" class="headerlink" title="3、动态类型 VS 静态类型"></a>3、动态类型 VS 静态类型</h1><p>Python 语言和 C、C++ 之间的另一个重要的差异就是：<strong>前者是动态类型，后者是静态类型</strong>。</p><p>静态类型语言要求在编译的时候就<strong>必须指定变量的类型</strong>，经常会通过显式的声明一个变量来完成这一点，或者在某些情况下编译器会自动推断变量的类型。另一方面，如果一旦声明某个变量，那么之后此作用域该中变量的类型就不可以再改变了。</p><p>看起来限制还蛮多的，那么静态类型可以带来什么好处呢？除了编译时的类型检测，编译器也可以根据静态类型生成适应相应平台的高性能机器码。</p><p>动态语言（针对于 Python）则不一样了，对于动态语言来说，<strong>类型不是和变量绑定的，而是和对象绑定的</strong>，<strong>变量只是一个指向对象的指针罢了</strong>。因此 Python 中如果想创建一个变量，那么必须在创建的同时赋上值，不然 Python 不知道这个变量到底指向哪一个对象。而像 C 这种静态语言，可以创建一个变量的同时不赋上初始值，比如：int n，因为已经知道 n 是一个 int 类型了，而且分配的空间大小已经确定了。</p><p>并且对于动态语言来说，变量即使在同一个作用域中也可以指向任意的对象。并且我们说 Python 中的变量是一个指针，比如：a = 666，相当于创建了一个整型 666，然后让 a 这个变量指向它；如果再来一个 a = “cython”，那么会再创建一个字符串，然后让 a 指向这个字符串，或者说 a 不再存储整型 666 的地址，而是存储新创建的字符串的地址。</p><p>所以在运行 Python 程序时，<strong>解释器要花费很多时间来确认要执行的低阶操作，并抽取相应的数据</strong>。考虑到 Python 设计的灵活性，解释器总是要一种非常通用的方式来确定相应的低阶操作，因为 Python 中的变量在任意时刻可以有任意类型。以上便是所谓的动态解析，而 Python 的通用动态解析是缓慢的。还是以 a + b 为栗：</p><ul><li><code>1. 解释器要检测 a 引用的对象的类型，这在C一级至少需要一次指针查找。</code></li><li><code>2. 解释器从该类型中寻找加法方法的实现，这可能一个或者多个额外的指针查找和内部函数调用。</code></li><li><code>3. 如果解释器找到了相应的方法，那么解释器就有了一个实际的函数调用。</code></li><li><code>4. 解释器会调用这个加法函数，并将 a 和 b 作为参数传递进去。</code></li><li><code>5. 我们说 Python 中的对象在C中都是一个结构体，比如：整型在 C 中是 PyLongObject，内部有引用计数、类型、ob_size、ob_digit，这些成员是什么不必关心，总之其中一个成员肯定是存放具体的值的，其他成员是存储额外的属性的。而加法函数显然要从这两个结构体中筛选出实际的数据，显然这需要指针查找以及将数据从 Python 类型转换到 C 类型。如果成功，那么会执行加法的实际操作；如果不成功，比如类型不对，发现 a 是整型但 b 是个字符串，就会报错。</code></li><li><code>6. 执行完加法操作之后，必须将结果再转回 Python 中的对象，然后获取它的指针、转成 PyObject * 之后才能够返回。</code></li></ul><p>而 C 语言面对 a + b 这种情况，表现则是不同的。因为 C 是静态编译型语言，C 编译器在编译的时候就决定了执行的低阶操作和要传递的参数数据。在运行时，一个编译好的 C 程序几乎跳过了 Python 解释器要必须执行的所有步骤。对于 a + b，编译器提前就确定好了类型，比如整型，那么编译器生成的机器码指令是寥寥可数的：将数据加载至寄存器，相加，存储结果。</p><p>所以我们看到编译后的 C 程序几乎将所有的时间都只花在了调用快速的 C 函数以及执行基本操作上，没有 Python 的那些花里胡哨的动作。并且由于静态语言对变量类型的限制，编译器会生成更快速、更专业的指令，这些指令是为其数据量身打造的。因此某些操作，使用 C 语言可以比使用 Python 快上几百倍甚至几千倍，这简直再正常不过了。</p><p>因此 <strong>Cython</strong> 在性能上可以带来如此巨大的提升的原因就在于，它<strong>将静态类型引入 Python 中，静态类型将 运行时的动态解析 转化成 基于类型优化的机器码</strong>。</p><p>另外，在 Cython 之前我们只能通过在 C 中重新实现 Python 代码来从静态类型中获益，也就是用 C 来编写所谓的扩展模块。而 Cython 可以让我们很容易地写类似于 Python 代码的同时，还能使用 C 的静态类型系统。而我们下面将要学习的第一个、也是最重要的 Cython 关键字：<code>cdef</code>，它是我们通往 C 性能的大门。</p><h1 id="4、通过cdef进行静态类型声明"><a href="#4、通过cdef进行静态类型声明" class="headerlink" title="4、通过cdef进行静态类型声明"></a>4、通过cdef进行静态类型声明</h1><p>Python 中声明变量的方式在 Cython 中也是可以使用的，因为 Python 代码也是合法的 Cython 代码。</p><p>而对于静态类型变量，我们在 Cython 中通过 cdef 关键字并指定类型、变量名的方式进行声明。比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="built_in">int</span> i</span><br><span class="line">cdef <span class="built_in">int</span> j</span><br><span class="line">cdef <span class="built_in">float</span> k</span><br><span class="line"><span class="comment"># 我们看到就像使用 Python 和 C 的混合体一样</span></span><br><span class="line">j = <span class="number">0</span></span><br><span class="line">i = j</span><br><span class="line">k = <span class="number">12.0</span></span><br><span class="line">j = <span class="number">2</span> * k</span><br><span class="line"><span class="keyword">assert</span> i != j</span><br></pre></td></tr></table></figure><p>不仅如此，就连使用 cdef 声明变量的方式也是按照 C 的标准来的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="built_in">int</span> i, j, k</span><br><span class="line">cdef <span class="built_in">float</span> x, y</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">cdef <span class="built_in">int</span> a = <span class="number">1</span>, b = <span class="number">2</span></span><br><span class="line">cdef <span class="built_in">float</span> c = <span class="number">3.0</span>, b = <span class="number">4.1</span></span><br></pre></td></tr></table></figure><p>而在函数内部，cdef 也是要进行缩进的，它们声明的变量也是一个局部变量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>():</span><br><span class="line">   <span class="comment"># 这里的 cdef 是缩进在函数内部的</span></span><br><span class="line">   cdef <span class="built_in">int</span> i</span><br><span class="line">   cdef <span class="built_in">int</span> N = <span class="number">2000</span></span><br><span class="line">   cdef <span class="built_in">float</span> a, b = <span class="number">2.1</span></span><br></pre></td></tr></table></figure><p>并且 cdef 还可以使用类似于 Python 中上下文的方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>():</span><br><span class="line">   <span class="comment"># 这种声明方式也是可以的, 和上面的方式是完全等价的</span></span><br><span class="line">   cdef:</span><br><span class="line">       <span class="built_in">int</span> i</span><br><span class="line">       <span class="built_in">int</span> N = <span class="number">2000</span></span><br><span class="line">       <span class="built_in">float</span> a, b = <span class="number">2.1</span></span><br><span class="line">   <span class="comment"># 但是注意声明的变量要注意缩进</span></span><br><span class="line">   <span class="comment"># Python 对缩进是有讲究的, 它规定了 Python 中的作用域</span></span><br><span class="line">   <span class="comment"># 所以我们看到 Cython 在语法方面还是保留了 Python 的风格</span></span><br></pre></td></tr></table></figure><h2 id="4-1-关于静态和常量"><a href="#4-1-关于静态和常量" class="headerlink" title="4.1 关于静态和常量"></a>4.1 关于静态和常量</h2><p>如果想在函数中<strong>返回一个局部变量的指针</strong>并且外部在接收这个指针之后，还能访问指针指向的值，这个时候该怎么办呢？我们知道 C 函数中的变量是分配在栈上的（不使用 malloc 函数，而是直接创建一个变量），函数结束之后变量对应的值就被销毁了，所以这个时候即使返回一个指针也是无意义的。</p><p>而如果想做到这一点，那么只需要在声明变量的同时在前面加上 <code>static</code>关键字，比如 static int i，这样的话 i 这个变量就不会被分配到栈区，而是会被分配到文字常量区，它的声明周期就不会随着函数的结束而结束，而是伴随着整个程序。</p><p>但是 static 并<strong>不是一个有效的 Cython 关键字</strong>，因此我们无法在 Cython 声明一个 C 的静态变量。除了 static，在C中还有一个 <code>const</code>，用来声明一个不可变的变量，也就是常量，一旦使用 const声明，比如 const int i = 3，那么这个 i 在后续就不可以被修改了。而<strong>在Cython中，const 是支持的</strong>。</p><p><strong>注意</strong>：以上的 int、float 都是 C 中的类型，不是 Python 中的类型（后面会详细说），但除了 int、float 之外，还能在 Cython 中声明哪些 C 的类型呢？</p><p>首先基础类型，像 short、int、long、unsigned short、long long、size_t、ssize_t 等等都是支持的，声明变量的方式均为：<code>cdef 类型 变量</code>，可以声明的时候赋初始值，也可以不赋初始值（这些都是 C 中的类型）；还有指针、数组、定义类型别名、结构体、共同体、函数指针等等也是支持的，我们后面介绍的时候说。</p><p>举个栗子，如果要在 Cython 中定义一个 C 的函数指针要怎么做呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="built_in">int</span> (*signal(<span class="built_in">int</span> (*f)(<span class="built_in">int</span>)))(<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure><p>看到函数指针的声明和 C 也是一模一样的，只需在开头加上一个 cdef 而已。但是不要慌，我们一般不会定义这种函数指针，直接定义一个 Python 函数不香吗？谁没事定义这玩意儿。</p><h2 id="4-2-Cython中的自动类型判断"><a href="#4-2-Cython中的自动类型判断" class="headerlink" title="4.2 Cython中的自动类型判断"></a>4.2 Cython中的自动类型判断</h2><p>在 Cython 中声明一个静态类型变量，使用 cdef 并不是唯一的方法，Cython 会对函数体中没有进行类型声明的变量自动执行类型推断。比如：for 循环中全部都是整型相加，没有涉及到其它类型的变量，那么 Cython 在自动对变量进行推断的时候会发现这个变量可以被优化为静态类型的变量。</p><p><strong>注意</strong>：但是一个程序不会那么智能地对于一个动态类型的语言进行全方位的优化，默认情况下，Cython 只有在确认这么做不会改变代码块的语义之后才会进行类型推断。</p><p>看一下下面这个简单的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">automatic_inference</span>():</span><br><span class="line">   i = <span class="number">1</span></span><br><span class="line">   d = <span class="number">2.0</span></span><br><span class="line">   c = <span class="number">3</span> + <span class="number">4j</span></span><br><span class="line">   r = i * d + c</span><br><span class="line">   <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure><p>在这个例子中，Cython 将字面量 1、3 +4j 以及变量 i、c、r 标记为通用的Python 对象。尽管这些对象的类型和 C 中的类型具有高度的相似性，但是 Cython 会保守地推断整型 i 可能无法代表 C 中的 long（C 中的整数有范围，而 Python没有、可以无限大），因此会将其作为符合 Python 代码语义的 Python 对象。而对于 d = 2.0，则可以自动推断出符合 C 中的 double，因为 Python 中的浮点数对应的值在底层就是使用一个 double 来存储的。所以最终对于用户来讲，变量 d 看似是一个 Python 中的对象，但是 Cython 在执行的时候会讲其视为 C 中的 double 以提高性能。</p><p><strong>注意</strong>：这就是即使我们写纯 Python，cython 编译器也能进行优化的原因，因为会进行推断。但是很明显，我们不应该让 cython 编译器去推断，而是我们来明确指定对应的类型。</p><p>当然我们如果非要 cython 编译器去猜，也是可以的，而且还可以通过 <code>infer_types</code> 编译器指令，<strong>在一些可能会改变 Python 代码语义的情况下给 Cython 留有更多的余地来推断一个变量的类型</strong>。比如：当两个整型相加时可能导致的结果溢出，因为 Python 中的整型在底层是使用数组来存储的，所以不管多大都可以相加，只要内存足够。但是在 C 中不可以，因为 C 中的变量是有明确的类型的，既然是类型，那么空间在一开始就已经确定了。比如 int 使用4个字节，而一旦结果使用4个字节无法表示的时候，就会得到一个意向不到的错误结果。所以如果非要 Cython 来类型推断的话，我们是需要给其留有这样的余地的。</p><p>对于一个函数如果启动这样的类型推断的话，<strong>可以使用 infer_types 的装饰器形式</strong>。不过还是那句话，应该手动指定类型，而不是让 cython 编译器去推断，因为我们是代码的编写者，类型什么的自己最清楚。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> cimport cython</span><br><span class="line"></span><br><span class="line"><span class="meta">@cython.infer_types(<span class="params"><span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">more_inference</span>():</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    d = <span class="number">2.0</span></span><br><span class="line">    c = <span class="number">3</span> + <span class="number">4j</span></span><br><span class="line">    r = i * d + c</span><br><span class="line">    <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure><p>这里出现了一个新的关键字叫做 <code>cimport</code>，至于它的含义我们后面会说，目前只需要知道它和 import 关键字一样，是用来导入模块的即可。然后我们通过装饰器 <code>@cython.infer_types(True)</code>，启动了相应的类型推断，也就是给 Cython 留有更多的猜测空间。</p><p><strong>注意</strong>：当 Cython 支持更多的推断的时候，变量 i 被类型化为 C long；d 和之前一样是 double，而 c 和 r 都是复数变量，复数则依旧使用 Python 中的复数类型。但是注意：并不代表启用 infer_types 时，就万事大吉了；我们知道在不指定 infer_types 的时候，Cython 在推断类型的时候显然是采用最最保险的方法、在保证程序正确执行的情况下进行优化，不能因为为了优化而导致程序出现错误，显然正确性和效率之间正确性是第一位的。而整型由于存在溢出的问题，所以 Cython 是不会自动转化为 C long 的；但是我们通过 infer_types 启动了更多的类型推断，因此在不改变语义的情况下 Cython 是会将整型推断为C long的，但是溢出的问题它不知道，所以在这种情况下是需要我们来要负责确保整型不会出现溢出。</p><h1 id="5、Cython中的C指针"><a href="#5、Cython中的C指针" class="headerlink" title="5、Cython中的C指针"></a>5、Cython中的C指针</h1><p>可以使用 C 的语法在 Cython 中声明一个 C 指针。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cdef double a</span><br><span class="line">cdef double *b = NULL</span><br><span class="line"></span><br><span class="line"><span class="comment"># 和 C 一样, *可以放在类型或者变量的附近</span></span><br><span class="line"><span class="comment"># 但是如果在一行中声明多个指针变量, 那么每一个变量都要带上*</span></span><br><span class="line">cdef double *c, *d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果是这样的话, 则表示声明一个指针变量和一个整型变量</span></span><br><span class="line">cdef <span class="built_in">int</span> *e, f </span><br></pre></td></tr></table></figure><p>既然可以声明指针变量，那么说明能够取得某个变量的地址才对。是的，在 Cython 中通过 <code>&amp;</code>获取一个变量的地址。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cdef double a = <span class="number">3.14</span></span><br><span class="line">cdef double *b = &amp;a </span><br></pre></td></tr></table></figure><p>问题来了，既然可以获取指针，那么能不能通过 * 来获取指针指向的值呢？答案可以获取值，但是方式不是通过 * 来实现。我们知道在 Python 中，*有特殊含义，没错，就是 *args 和 *<em>kwargs，它们允许函数中接收任意个数的参数，并且通过 \</em> 还可以对一个序列进行解包。因此对于 Cython 来讲，无法通过 *p 这种方式来获取 p 指向的内存。<strong>在 Cython 中获取指针指向的内存的方式是通过类似于 p[0] 这种方式，p 是一个指针变量，那么 p[0] 就是 p 指向的内存</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pyx</span></span><br><span class="line"></span><br><span class="line">cdef double a = <span class="number">3.14</span></span><br><span class="line">cdef double *b = &amp;a</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;a = <span class="subst">&#123;a&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 修改b指向的内存</span></span><br><span class="line">b[<span class="number">0</span>] = <span class="number">6.28</span></span><br><span class="line"><span class="comment"># 再次打印a</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;a = <span class="subst">&#123;a&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>这个模块叫做 cython_test.pyx，然后在另一个 py 文件中导入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">a = 3.14</span></span><br><span class="line"><span class="string">a = 6.28</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>pyx 里面有 print 语句，因此导入的时候就自动打印了，看到 a 确实被修改了。因此在 Cython 中可以<strong>通过 &amp; 来获取指针</strong>，也可以<strong>通过 <code>指针[0]</code> 的方式获取指针指向的内存</strong>。唯一的区别就是C里面是使用 * 的方式，而在 Cython 里面如果使用 <code>*b = 6.28</code> 这种方式在语法上则是不被允许的。</p><p>而 C 和 Cython 中关于指针的另一个区别就是该指针在指向一个结构体的时候，假设一个结构体指针叫做 s，里面有两个成员 a 和 b，都是整型。那么对于 C 而言，可以通过 <code>s -&gt; a + s -&gt; b</code> 的方式将两个成员相加，但是对于 Cython 来说，则是 <code>s.a + s.b</code>。看到这个和 Go 是类似的，无论是结构体指针还是结构体本身，都是使用 <code>.</code> 的方式访问结构体内部成员。</p><h1 id="6、静态类型变量和动态类型变量的混合"><a href="#6、静态类型变量和动态类型变量的混合" class="headerlink" title="6、静态类型变量和动态类型变量的混合"></a>6、静态类型变量和动态类型变量的混合</h1><p>Cython <strong>允许静态类型变量和动态类型变量之间进行赋值</strong>，这是一个非常强大的特性。它<strong>允许使用动态的 Python 对象，并且在决定性能的地方能很轻松地将其转化为快速的静态对象</strong>。</p><p>假设有几个静态的 C int 要组合成一个 Python 中的元组，Python/C API 创建和初始化的话很简单，但是却很乏味，需要几十行代码以及大量的错误检查；而在Cython中，只需要像 Python 一样做即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pyx</span></span><br><span class="line"></span><br><span class="line">cdef <span class="built_in">int</span> a, b, c </span><br><span class="line">t = (a, b, c)</span><br></pre></td></tr></table></figure><p>然后我们来导入一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们看到在Cython中没有指定初始值, 所以默认为0</span></span><br><span class="line"><span class="comment"># 比如我们直接 a = int(), 那么 a 也是 0</span></span><br><span class="line"><span class="built_in">print</span>(cython_test.t)  <span class="comment"># (0, 0, 0)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(cython_test.t))  <span class="comment"># &lt;class &#x27;tuple&#x27;&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(cython_test.t[<span class="number">0</span>]))  <span class="comment"># &lt;class &#x27;int&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 虽然t是可以访问的, 但是 a、b、c 是无法访问的，因为它们是 C 中的变量</span></span><br><span class="line"><span class="comment"># 使用 cdef 定义的变量都会被屏蔽掉，在 Python 中是无法使用的</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(cython_test.a)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># module &#x27;cython_test&#x27; has no attribute &#x27;a&#x27;</span></span><br></pre></td></tr></table></figure><p>看到执行的过程很顺畅，这里要说的是：a、b、c 都是静态的整型，Cython 允许使用它们创建动态类型的 Python 元组，然后将该元组分配给 t。所以这个小栗子便体现了 Cython 的美丽和强大之处，可以以显而易见的方式创建一个元组，而无需考虑其它情况。因为 Cython 的目的就在于此，希望概念上简单的事情在实际操作上也很简单。</p><p>虽说如此，但并不是所有东西都可以这么做的。上面的例子之所以有效，是因为Python int 和 C int（short、long等等）有明显的对应关系。如果是指针呢？首先Python 中没有指针这个概念，或者说指针被 Python 隐藏了，只有解释器才能操作指针。因此在 Cython 中，<strong>不可以在函数中返回一个指针，以及打印一个指针、指针</strong>作为 Python 的动态数据结构（如：元组、列表、字典等等）中的某个元素，这些都是不可以的。</p><p>回到元组的那个例子，如果 a、b、c 是一个指针，那么必须要在放入元组之前取消它们的引用，或者说放入元组中的只能是它们指向的值。因为 Python 在语法层面没有指针的概念，所以不能将指针放在元组里面。同理：假设 <code>cdef int a = 3</code>，可以是<code>cdef int *b = &amp;a</code>，但绝不能是 <code>b = &amp;a</code>。因为直接 <code>b = xxx</code> 的话，那么 b 是 Python 中的变量，其类型则需要根据值来推断，然而值是一个指针，所以这是不允许的。</p><p>但是 <code>cdef int b = a</code> 和 <code>b = a</code> 则都是合法的，因为 a 是一个整型，C 中的整型是可以转化成 Python 中的整型的，因此编译的时候会自动转化。只不过如果是前者那么相当于创建了一个 C 的变量 b，Python 导入的时候无法访问；如果是后者，那么相当于创建一个 Python 变量 b，Python 导入的时候可以访问。</p><p>举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="built_in">int</span> a</span><br><span class="line">b = &amp;a</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">cdef int a</span></span><br><span class="line"><span class="string">b = &amp;a</span></span><br><span class="line"><span class="string">   ^</span></span><br><span class="line"><span class="string">------------------------------------------------------------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">cython_test.pyx:5:4: Cannot convert &#x27;int *&#x27; to Python object</span></span><br><span class="line"><span class="string">Traceback (most recent call last):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 我们看到在导入的时候, 编译失败了, 因为 b 是 Python 中的类型, 而 &amp;a 是一个 int*, 所以无法将 int * 转化成 Python 对象</span></span><br></pre></td></tr></table></figure><p>再举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pyx</span></span><br><span class="line">cdef <span class="built_in">int</span> a = <span class="number">3</span></span><br><span class="line">cdef <span class="built_in">int</span> b = a</span><br><span class="line">c = a</span><br><span class="line"></span><br><span class="line"><span class="comment"># setup.exe.py</span></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(cython_test.a)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># module &#x27;cython_test&#x27; has no attribute &#x27;a&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(cython_test.b)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># module &#x27;cython_test&#x27; has no attribute &#x27;b&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(cython_test.c)  <span class="comment"># 3</span></span><br></pre></td></tr></table></figure><p>看到 a 和 b 是 C 中的类型，无法访问，但变量 c 是可以访问的。不过问题又来了，看一下下面的几种情况：</p><p>先定义一个C的变量，然后给这个变量重新赋值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="built_in">int</span> a = <span class="number">3</span></span><br><span class="line">a = <span class="number">4</span></span><br><span class="line"><span class="comment"># Python中能否访问到 a 呢？</span></span><br><span class="line"><span class="comment"># 答案是访问不到的, 虽说是 a = 4, 像是创建 Python 的变量, 但是不好意思, 上面已经创建了 C 的变量 a</span></span><br><span class="line"><span class="comment"># 因此下面再操作 a，都是操作 C 的变量 a, 如果你来一个a = &quot;xxx&quot;, 那么是不合法的</span></span><br><span class="line"><span class="comment"># a 已经是整型了，你再将一个字符串赋值给 a 显然不是合法的</span></span><br></pre></td></tr></table></figure><p>先定义一个 Python 变量，再定义一个同名的 C 变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">b = <span class="number">3</span></span><br><span class="line">cdef <span class="built_in">int</span> b = <span class="number">4</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">b = 3</span></span><br><span class="line"><span class="string">^</span></span><br><span class="line"><span class="string">------------------------------------------------------------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">cython_test.pyx:4:0: Previous declaration is here</span></span><br><span class="line"><span class="string">warning: cython_test.pyx:5:9: cdef variable &#x27;b&#x27; declared after it is used</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 即使一个是 Python 的变量, 一个是 C 的变量, 也依旧不可以重名。不然访问 b 的话，究竟访问哪一个变量呢?</span></span><br><span class="line"><span class="comment"># 所以 b = 3 的时候, 变量就已经被定义了。而 cdef int b = 4 又定义了一遍, 显然是不合法的。</span></span><br><span class="line"><span class="comment"># 不光如此, cdef int c = 4 之后再写上 cdef int c = 5 仍然属于重复定义, 不合法。</span></span><br><span class="line"><span class="comment"># 但 cdef int c = 4 之后，写上 c = 5 是合法的, 因为这相当于改变 c 的值, 并没有重复定义。</span></span><br></pre></td></tr></table></figure><p>先定义一个 Python 变量，再定义一个同名的 Python 变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="built_in">int</span> a = <span class="number">666</span></span><br><span class="line">v = a</span><br><span class="line"><span class="built_in">print</span>(v)</span><br><span class="line">cdef double b = <span class="number">3.14</span></span><br><span class="line">v = b</span><br><span class="line"><span class="built_in">print</span>(v)</span><br><span class="line"><span class="comment"># 这么做是合法的, 其实从 Cython 是 Python 的超集这一点就能理解。</span></span><br><span class="line"><span class="comment"># 主要是：Python 中变量的创建方式和 C 中变量的创建方式是不一样的, Python 中的变量在 C 中是一个指向某个值的指针, 而 C 中的变量就是代表值本身</span></span><br><span class="line"><span class="comment"># cdef int a = 666, 相当于创建了一个变量 a, 这个变量 a 代表的就是 666 本身, 而这个 666 是 C 中整数 666</span></span><br><span class="line"><span class="comment"># 而 v = a 相当于先根据 a 的值、也就是 C 中 整数666 创建一个 Python 的整数 666, 然后再让 v 指向它</span></span><br><span class="line"><span class="comment"># 那么 v = b 也是同理, 因为 v 是 Python 中的变量, 它想指向谁就指向谁; 而 b 是一个 C 中的 double, 可以转成 Python 的 float</span></span><br><span class="line"><span class="comment"># 但如果将一个指针赋值给 v 就不可以了, 因为 Python 中没有哪个数据类型可以和 C 中的指针相对应</span></span><br></pre></td></tr></table></figure><p>再来看一个栗子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num = <span class="number">666</span></span><br><span class="line"></span><br><span class="line">a = num</span><br><span class="line">b = num </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a) == <span class="built_in">id</span>(b))  <span class="comment"># True</span></span><br></pre></td></tr></table></figure><p>首先这个栗子很简单，因为 a 和 b 指向了同一个对象，但如果是下面这种情况呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="built_in">int</span> num = <span class="number">666</span></span><br><span class="line"></span><br><span class="line">a = num</span><br><span class="line">b = num</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a) == <span class="built_in">id</span>(b)) </span><br></pre></td></tr></table></figure><p><em>会发现打印的是 <em>*False</em></em>，因为此时这个 num 是 C 中变量，然后 a = num 会先根据 num 的值创建一个 Python 中的整数，然后再让 a 指向它；同理 b 也是如此，而显然这会创建两个不同 666，虽然值一样，但是地址不一样。</p><p><strong>注意</strong>：这就是 Cython 的方便之处，不需要我们自己转化，而是在编译的时候会自动转化。当然还是按照之前说的，自动转化的前提是可以转化，也就是两者之间要互相对应（比如 Python 的 int 和 C 的 int、long，Python 的 float 和 C 的 float、double 等等）。</p><p>因为 Python int 和 C/C++ int 之间是对应的，所以 Python 会自动转化，那么其它类型呢？Python 类型和 C/C++ 类型之间的对应关系都有哪些呢？</p><p><img src="./image/3_1.jpg" alt="c/python"></p><p><strong>注意</strong>：对于这些 C 的类型，Cython 有更丰富的类型来表示。</p><h2 id="6-1-bint类型"><a href="#6-1-bint类型" class="headerlink" title="6.1 bint类型"></a>6.1 bint类型</h2><p>bint 在 C 中是一个布尔类型，但其实本质上是一个整型，然后会自动转化为 Python 的布尔类型，当然 Python 中布尔类型也是继承自整型。bint 类型有着标准 C 的实现：0为假，非0为真。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pyx</span></span><br><span class="line"></span><br><span class="line">cdef bint flag1 = <span class="number">123</span>  <span class="comment"># 非0是True</span></span><br><span class="line">cdef bint flag2 = <span class="number">0</span>  <span class="comment"># 0是False</span></span><br><span class="line">a = flag1</span><br><span class="line">b = flag2</span><br></pre></td></tr></table></figure><p>这里要进行赋值给 Python 中的变量，不然后续无法访问。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="built_in">print</span>(cython_test.a)  <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(cython_test.b)  <span class="comment"># False</span></span><br></pre></td></tr></table></figure><h2 id="6-2-整数类型与转换溢出"><a href="#6-2-整数类型与转换溢出" class="headerlink" title="6.2 整数类型与转换溢出"></a>6.2 整数类型与转换溢出</h2><p>在 Python2 中，有 int 和 long 两种类型来表示整数。Python2 中的 int 使用 C 中的 long 来存储，是有范围的，而 Python2 中的 long 是没有范围的；但在Python3中，只有int，没有long，而所有的 int 对象都是没有范围的。</p><p>将 Python 中的整型转化成 C 中的整型时，Cython 生成代码会检测是否存在溢出。如果 C 中的 long 无法表示 Python 中的整型，那么运行时会抛出 OverflowError。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># cython_test.pyx</span></span><br><span class="line">i = <span class="number">2</span> &lt;&lt; <span class="number">81</span>  <span class="comment"># 显然 C 中的 int 是存不下的</span></span><br><span class="line">cdef <span class="built_in">int</span> j = i</span><br><span class="line"> </span><br><span class="line"><span class="comment"># setup_exe.py</span></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  ...</span></span><br><span class="line"><span class="string">  ... </span></span><br><span class="line"><span class="string">  File &quot;cython_test.pyx&quot;, line 3, in init cython_test</span></span><br><span class="line"><span class="string">    cdef int j = i</span></span><br><span class="line"><span class="string">ImportError: Building module cython_test failed: [&#x27;OverflowError: Python int too large to convert to C long\n&#x27;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>看到转成 C 的 int 时，如果存不下会自动尝试使用 long，如果还是越界则报错。</p><h2 id="6-3-float类型"><a href="#6-3-float类型" class="headerlink" title="6.3 float类型"></a>6.3 float类型</h2><p>Python 中的 float 对应的值在 C 中也是用 double 来存储的，对于浮点来说可以放心使用。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    PyObject_HEAD</span><br><span class="line">    <span class="type">double</span> ob_fval;</span><br><span class="line">&#125; PyFloatObject;</span><br><span class="line"><span class="comment">// Python 中的对象在底层都是一个结构体, float 对象则是一个 PyFloatObject</span></span><br><span class="line"><span class="comment">// 而 PyObject_HEAD 是一些额外信息：引用计数、指向对应类型的指针</span></span><br><span class="line"><span class="comment">// 而是 ob_fval 则是真正存放具体的值的, 显然这是一个double</span></span><br></pre></td></tr></table></figure><h2 id="6-4-复数类型"><a href="#6-4-复数类型" class="headerlink" title="6.4 复数类型"></a>6.4 复数类型</h2><p>Python 中的复数在 C 中是使用两个 double 来存储的，一个存储实部、一个存储虚部。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="type">double</span> real;</span><br><span class="line">    <span class="type">double</span> imag;</span><br><span class="line">&#125; Py_complex;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    PyObject_HEAD</span><br><span class="line">    Py_complex cval;</span><br><span class="line">&#125; PyComplexObject;</span><br></pre></td></tr></table></figure><p>复数不常用，了解一下即可。</p><h2 id="6-5-bytes类型、str类型"><a href="#6-5-bytes类型、str类型" class="headerlink" title="6.5 bytes类型、str类型"></a>6.5 bytes类型、str类型</h2><p>在 Cython 中如果想创建一个<strong>字节串可以使用 bytes</strong>，而创建一个字符串则是 str 或者 unicode。没错，这些都是 Python 中的类型，关于 C 类型和 Python 类型在 Cython 中的表现我们后面会详细说。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pyx</span></span><br><span class="line"><span class="comment"># 创建一个字节串使用 bytes</span></span><br><span class="line">cdef <span class="built_in">bytes</span> name = <span class="string">&quot;cython&quot;</span>.encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"><span class="comment"># 创建一个字符串可以使用 str, 和 Python 一样</span></span><br><span class="line">cdef <span class="built_in">str</span> where1 = <span class="string">&quot;东方地灵殿&quot;</span></span><br><span class="line"><span class="comment"># 也可以使用 unicode, 但是字符串要有前缀u，两种方式在 Python3 是等价的, 因此建议使用 str</span></span><br><span class="line"><span class="comment"># 之所以会有 unicode 是为了兼容 Python2</span></span><br><span class="line">cdef unicode where2 = <span class="string">u&quot;东方地灵殿&quot;</span></span><br><span class="line"></span><br><span class="line">NAME = name</span><br><span class="line">WHERE1 = where1</span><br><span class="line">WHERE2 = where2</span><br><span class="line"></span><br><span class="line"><span class="comment"># setup_exe.py</span></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="built_in">print</span>(cython_test.NAME.decode(<span class="string">&quot;utf-8&quot;</span>))  <span class="comment"># 古明地觉</span></span><br><span class="line"><span class="built_in">print</span>(cython_test.WHERE1)  <span class="comment"># 东方地灵殿</span></span><br><span class="line"><span class="built_in">print</span>(cython_test.WHERE2)  <span class="comment"># 东方地灵殿</span></span><br><span class="line"><span class="built_in">print</span>(cython_test.NAME.decode(<span class="string">&quot;utf-8&quot;</span>)[<span class="number">2</span>])  <span class="comment"># 地</span></span><br><span class="line"><span class="built_in">print</span>(cython_test.WHERE1[: <span class="number">2</span>] + cython_test.WHERE2[<span class="number">2</span>:])  <span class="comment"># 东方地灵殿</span></span><br></pre></td></tr></table></figure><h1 id="7、使用Python类型进行静态声明"><a href="#7、使用Python类型进行静态声明" class="headerlink" title="7、使用Python类型进行静态声明"></a>7、使用Python类型进行静态声明</h1><p>之前使用 cdef 的时候用的都是 C 中的类型，比如 cdef int、cdef float，当然 Python 中也有这两个，不过使用的确实是 C 中的类型，再或者 cdef unsigned long long 等等也是可以的。那么可不可以使用 Python 中的类型进行静态声明呢，其实细心的话会发现是可以的，因为上面使用了 cdef str 声明变量了。</p><p>不光是 str，只要是在 CPython 中实现了，并且 Cython 有权限访问的话，都可以用来进行静态声明，而 Python 中内建的类型都是满足要求的。换句话说，只要在 Python 中可以直接拿来用的，都可以直接当成 C 的类型来进行声明（bool 类型除外）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pyx</span></span><br><span class="line"><span class="comment"># 声明的时候直接初始化</span></span><br><span class="line">cdef <span class="built_in">tuple</span> b = <span class="built_in">tuple</span>(<span class="string">&quot;123&quot;</span>)</span><br><span class="line">cdef <span class="built_in">list</span> c = <span class="built_in">list</span>(<span class="string">&quot;123&quot;</span>)</span><br><span class="line">cdef <span class="built_in">dict</span> d = &#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;cython&quot;</span>&#125;</span><br><span class="line">cdef <span class="built_in">set</span> e = &#123;<span class="string">&quot;cython&quot;</span>, <span class="string">&quot;python&quot;</span>&#125;</span><br><span class="line">cdef <span class="built_in">frozenset</span> f = <span class="built_in">frozenset</span>([<span class="string">&quot;cython&quot;</span>, <span class="string">&quot;python&quot;</span>])</span><br><span class="line"></span><br><span class="line">A = a</span><br><span class="line">B = b</span><br><span class="line">C = c</span><br><span class="line">D = d</span><br><span class="line">E = e</span><br><span class="line">F = f</span><br><span class="line"></span><br><span class="line"><span class="comment"># setup_exe.py</span></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> cython_test <span class="keyword">import</span> *</span><br><span class="line"><span class="built_in">print</span>(A)  <span class="comment"># cython</span></span><br><span class="line"><span class="built_in">print</span>(B)  <span class="comment"># (&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;)</span></span><br><span class="line"><span class="built_in">print</span>(C)  <span class="comment"># [&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;]</span></span><br><span class="line"><span class="built_in">print</span>(D)  <span class="comment"># &#123;&#x27;name&#x27;: &#x27;cython&#x27;&#125;</span></span><br><span class="line"><span class="built_in">print</span>(E)  <span class="comment"># &#123;&#x27;cython&#x27;, &#x27;python&#x27;&#125;</span></span><br><span class="line"><span class="built_in">print</span>(F)  <span class="comment"># frozenset(&#123;&#x27;cython&#x27;, &#x27;python&#x27;&#125;)</span></span><br></pre></td></tr></table></figure><p>可以看到得到的结果是正确的，完全可以当成 Python 中的类型来使用。这里在使用 Python 中的类型进行静态声明的时候，都赋上了一个初始值，但<strong>如果只是声明没有赋上初始值，那么得到的结果是一个 None</strong>。</p><p><strong>注意</strong>：只要是用 Python 中的类型进行静态声明且不赋初始值，那么结果都是 None。比如：<code>cdef tuple b; B = b</code>，那么 Python 在打印 B 的时候得到的就是 None，而不是一个空元组。不过整型是个例外，因为 int 实际上用的是 C 里面 int，会得到一个 0，当然还有float。</p><blockquote><p>为什么 Cython 可以做到这一点呢？实际上这些结构在 CPython 中都是已经实现好了的，Cython 将它们设置为指向底层中某个数据结构的 C 指针，比如：cdef tuple a，那么 a 就是一个PyTupleObject *，它们可以像普通变量一样使用，当然 Python 中的变量也是一样的，a = tuple()，那么 a 同样是一个 PyTupleObject *。</p><p>同理我们想一下 C 扩展，我们使用 Python/C API 编写扩展模块的时候，也是一样的道理，只不过还是那句话，使用 C 来编写扩展非常的麻烦，因为用 C 来开发本身就是麻烦的事情。所以 Cython 很好的解决了这一点，让我们可以将写 Python 一样写扩展，会自动地将我们的代码翻译成C级的代码。因此从这个角度上讲，Cython 可以做的，使用纯 C 来编写扩展也是完全可以做的，区别就是一个简单方便，一个麻烦。更何况使用 C 编写扩展，需要掌握Python/C API，而且还需要有 Python 解释器方面的知识，门槛还是比较高的，可能一开始掌握套路了还不觉得有什么，但是到后面当你使用 C 来实现一个 Python 中的类的时候，你就知道这是一件相当恐怖的事情了。而在 Cython 中，定义一个类仍然非常简单，像 Python 一样，我们后续系列会说。</p></blockquote><p>另外使用 Python 中的<strong>类型声明变量的时候不可以使用指针的形式</strong>，比如：cdef tuple *t，这么做是不合法的，会报错：<code>Pointer base type cannot be a Python object</code>。此外，我们使用 cdef 的时候指定了类型，那么赋值的时候就不可以那么无拘无束了，比如：<code>cdef tuple a = list(&quot;123&quot;)</code> 就是不合法的，因为声明了 a 指向一个元组，但是我们给了一个字典，那么编译扩展模块的时候就会报错：<code>TypeError: Expected tuple, got list</code>。</p><p>这里再思考一个问题，Cython 创建的变量无法被直接访问，需要将其赋值给 Python 中的变量才可以使用。那么，在赋完值的时候，这两个变量指向的是同一个对象吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pyx</span></span><br><span class="line">cdef <span class="built_in">list</span> a = <span class="built_in">list</span>(<span class="string">&quot;123&quot;</span>)</span><br><span class="line"><span class="comment"># a是一个PyListObject *, 然后b也是一个PyListObject *</span></span><br><span class="line"><span class="comment"># 但是这两位老铁是不是指向同一个PyListObject对象呢？</span></span><br><span class="line">b = a  </span><br><span class="line"><span class="comment"># 打印一下a is b</span></span><br><span class="line"><span class="built_in">print</span>(a <span class="keyword">is</span> b)</span><br><span class="line"><span class="comment"># 修改a的第一个元素之后，再次打印b</span></span><br><span class="line">a[<span class="number">0</span>] = <span class="string">&quot;xxx&quot;</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># setup_exe.py</span></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">[&#x27;xxx&#x27;, &#x27;2&#x27;, &#x27;3&#x27;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>看到 a 和 b 确实是同一个对象，并且 a 在本地修改了之后，会影响到 b。毕竟两个变量指向的是同一个列表、或者 PyListObject 结构体实例，当然我们使用 del 删除一个元素也是同理。</p><p><strong>Cython 中的变量和 Python 中的变量是等价的</strong>，那么 Python 中变量可以使用的 api，Cython 中的变量都可以使用，比如 a.insert、a.append 等等。只不过对于 int 和 float 来说，C 中也存在同名的类型，那么会优先使用 C 的类型，这也是我们期望的结果。</p><blockquote><p>而且一旦使用的是 C 里面的类型，比如 <code>cdef int = 1;cdef float b = 22.33</code>，那么 a 和 b 就不再是 PyLongObject * 和 PyFloatObject * 了，因为它们用的不是 Python 中的类型，而是 C 中的类型。所以 a 和 b 的类型就是 C 中实打实的 int 和 float，并且 a 和 b 也不再是一个指针，它们代表的就是具体的整数和浮点数。</p><p>为什么要在使用 int 和 float 的时候，要选择 C 中 int 和 float 呢？答案很好理解，因为 Cython 本身就是用来加速计算的，而提到计算，显然避不开 int 和 float，因此这两位老铁默认使用的 C 里面类型。事实上单就 Python 中的整型和浮点来说，在运算时底层也是先转化成 C 的类型，然后再操作，最后将操作完的结果再转回 Python 中的类型。而如果默认就使用C的类型，就少了转换这一步了，可以极大提高效率。</p><p>然而即便是 C 中的整型和浮点型，在操作的时候和 C 还是有一些不同的，主要就在于除法和取模。</p></blockquote><p>当操作的是 Python 的 int 时，那么结果是不会溢出的；如果操作的是静态的 C 对象，那么整型可能存在溢出，这些我们是知道的。但是除此之外，还有一个最重要的区别就是除法和取模，在除法和取模上，C 的类型使用的却不是 C 的标准。举个栗子：</p><p>当使用有符号整数计算模的时候，C 和 Python 有着明显不同的行为：比如 <code>-7 % 5</code>，如果是 Python 的话那么结果为 3，C 的话结果为 -2。显然 C 的结果是符合我们正常人思维的，但是为什么 Python 得到的结果这么怪异呢？</p><blockquote><p>事实上不光是 C，Go、Js 也是如此，计算 <code>-7 % 5</code> 的结果都是-2，但 Python 得到 3 主要是因为其内部的机制不同。我们知道 <code>a % b</code>，等于 <code>a - (a / b) * b</code>，其中 <code>a / b</code> 表示两者的商。比如 7 % 2，等于 7 - (7 / 2) * 2 = 7 - 3 * 2 = 1，对于正数，显然以上所有语言计算的结果都是一样的。</p><p>而负数出现差异的原因就在于：C 在计算 a / b 的时候是截断小数点，而 Python 是向下取整。比如上面的 -7 % 5，等于 -7 - (-7 / 5) * 5。-7 / 5 得到的结果是负的一点多，C的话直接截断得到 -1，因此结果是 -7 - (-1) * 5 = -2；但 Python 是向下取整，负的一点多变成 -2，因此结果变成了 -7 - (-2) * 5 = 3。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python中 / 默认是得到浮点, 整除的话使用 //</span></span><br><span class="line"><span class="comment"># 我们看到得到的是 -2</span></span><br><span class="line"><span class="built_in">print</span>(-<span class="number">7</span> // <span class="number">5</span>)  <span class="comment"># -2</span></span><br></pre></td></tr></table></figure><p>因此在除法和取模方面，尤其需要注意。另外即使在Cython中，也是一样的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="built_in">int</span> a = -<span class="number">7</span></span><br><span class="line">cdef <span class="built_in">int</span> b = <span class="number">5</span></span><br><span class="line">cdef <span class="built_in">int</span> c1 = a / b</span><br><span class="line">cdef <span class="built_in">int</span> c2 = a // b</span><br><span class="line"><span class="built_in">print</span>(c1)</span><br><span class="line"><span class="built_in">print</span>(c2)</span><br><span class="line"><span class="built_in">print</span>(-<span class="number">7</span> // <span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>以上打印的结果都是 -2，说明 Cython 默认使用 Python 的语义进行除法，当然还有取模，即使操作的对象是静态类型的 C 标量。这么做原因就在于为了最大程度的和 Python 保持一致，如果想要启动 C 语义都需要显式地进行开启。然后我们看到 a 和 b 是静态类型的 C 变量，它们也是可以使用 // 的，因为 Cython 的目的就像写Python一样。但是我们看到无论是 a / b 还是 a // b 得到的都是 -2，这很好理解。因为在 Cython 中 a 和 b 都是静态的 int，而在C中对两个 int 使用加减乘除得到的依旧是一个 int，因此会将中间得到的浮点数变成整型，至于是直接截断还是向下取整则是和 Python 保持一致的，是按照 Python 的标准来的。至于 a // b 对于整型来说就更不用说了，a // b 本身就表示整除，因此在 Cython 中两个 int 之间使用 / 和使用 // 是一样的。然后我们再来举个浮点数的例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="built_in">float</span> a = -<span class="number">7</span></span><br><span class="line">cdef <span class="built_in">float</span> b = <span class="number">5</span></span><br><span class="line">cdef <span class="built_in">float</span> c1 = a / b</span><br><span class="line">cdef <span class="built_in">float</span> c2 = a // b</span><br><span class="line"><span class="built_in">print</span>(c1)</span><br><span class="line"><span class="built_in">print</span>(c2)</span><br><span class="line"> <span class="keyword">import</span> cython_test</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">-1.399999976158142</span></span><br><span class="line"><span class="string">-2.0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>此时的 a 和 b 都是浮点数，那么 a / b 也是个浮点，所以没有必要截断了，小数位会保留；而 a // b虽然得到的也是浮点（只要 a 和 b 中有一个是浮点，那么 a / b 和 a // b 得到的也是浮点），但它依旧具备整除的意义，所以 a // b 得到结果是 -2.0，然后赋值给一个 float 变量，还是 -2.0。不过为什么 a // b 得到的是 -2.0，可能有人不是很明白，因此关于 Python 中 / 和 // 在不同操作数之间的差异，再举个栗子看一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">7</span> / <span class="number">2</span> == <span class="number">3.5</span>  <span class="comment"># 3.5, 很好理解</span></span><br><span class="line"><span class="number">7</span> // <span class="number">2</span> == <span class="number">3</span>  <span class="comment"># // 表示整除, 因此 3.5 会向下取整, 得到 3</span></span><br><span class="line">-<span class="number">7</span> / <span class="number">2</span> == -<span class="number">3.5</span>  <span class="comment"># -3.5, 很好理解</span></span><br><span class="line">-<span class="number">7</span> // -<span class="number">2</span> = -<span class="number">4</span>  <span class="comment"># // 表示取整, 因此 -3.5 会向下取整, 得到 -4</span></span><br><span class="line"></span><br><span class="line"><span class="number">7.0</span> / <span class="number">2</span> == <span class="number">3.5</span>  <span class="comment"># 3.5, 依旧没问题</span></span><br><span class="line"><span class="number">7.0</span> // <span class="number">2</span> == <span class="number">3.0</span>  <span class="comment"># //两边出现了浮点, 结果也是浮点; 但 // 又是整除, 所以你可以简单认为是先取整(得到 3), 然后变成浮点(得到3.0)</span></span><br><span class="line">-<span class="number">7.0</span> / <span class="number">2</span> == -<span class="number">3.5</span>  <span class="comment"># -3.5, 依旧很简单</span></span><br><span class="line">-<span class="number">7.0</span> // <span class="number">2</span> == -<span class="number">7.8</span> // <span class="number">2</span> == -<span class="number">4.0</span>  <span class="comment"># -3.5 和 -3.9 都会向下取整, 然后得到-4, 但结果是浮点, 所以是-4.0</span></span><br><span class="line"></span><br><span class="line">-<span class="number">7.0</span> / -<span class="number">2</span> == <span class="number">3.5</span>  <span class="comment"># 3.5, 没问题</span></span><br><span class="line">-<span class="number">7.0</span> // -<span class="number">2</span> == <span class="number">3</span>  <span class="comment"># 3.5向下取整, 得到3</span></span><br></pre></td></tr></table></figure><p>所以 Python 的整除或者说地板除还是比较奇葩的，主要原因就<strong>在于其它语言是截断（小数点后面直接不要了），而 Python 是向下取整</strong>。如果是结果为正数的话，截断和向下取整是等价的，所以此时基本所有语言都是一样的；而结果为负数的话，那么截断和向下取整就不同了，因为 -3.14 截断得到的是 -3、但向下取整得到的不是 -3，而是 -4。因此这一点务必要记住，算是 Python 的一个坑吧。话说如果没记错的话，好像只有 Python 采用了向下取整这种方式，别的语言（至少C、js、Go）都是截断的方式。</p><p>还有一个问题，那就是整型和浮点型之间可不可以相互赋值呢？先说结论：</p><ul><li><code>整型赋值给浮点型是可以的</code></li><li><code>但是浮点型赋值给整型不可以</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 7是一个纯数字, 那么它既可以在赋值 int 类型变量时表示整数7</span></span><br><span class="line"><span class="comment"># 也可以在赋值给 float 类型变量时表示 7.0</span></span><br><span class="line">cdef <span class="built_in">int</span> a = <span class="number">7</span></span><br><span class="line">cdef <span class="built_in">float</span> b = <span class="number">7</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 但如果是下面这种形式, 虽然也是可以的, 但是会弹出警告</span></span><br><span class="line">cdef <span class="built_in">float</span> c = a</span><br><span class="line"><span class="comment"># 提示: &#x27;=&#x27;: conversion from &#x27;int&#x27; to &#x27;float&#x27;, possible loss of data</span></span><br><span class="line"><span class="comment"># 因为 a 的值虽然也是 7, 但它已经具有相应的类型了, 就是一个 int, 将 int 赋值给 float 会警告</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 而将浮点型赋值给整型则不行</span></span><br><span class="line"><span class="comment"># 这行代码在编译的时候会报错： Cannot assign type &#x27;double&#x27; to &#x27;int&#x27;</span></span><br><span class="line">cdef <span class="built_in">int</span> d = <span class="number">7.0</span></span><br></pre></td></tr></table></figure><p>使用 cdef int、cdef float 声明的变量不再是指向 Python 中 int对象、float对象的PyLongObject *、PyFloatObject *，其类型就是 C 中的 int、float。尽管整型没有考虑溢出，但是它在做运算的时候遵循 Python 的规则(主要是除法)，那么可不可以让其强制遵循C的规则呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cimport cython</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过@cython.cdivision(True)进行装饰即可完成这一点</span></span><br><span class="line"><span class="meta">@cython.cdivision(<span class="params"><span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">divides</span>(<span class="params"><span class="built_in">int</span> a, <span class="built_in">int</span> b</span>):</span><br><span class="line">    <span class="keyword">return</span> a / b</span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="built_in">print</span>(-<span class="number">7</span> // <span class="number">2</span>)  <span class="comment"># -4</span></span><br><span class="line"><span class="comment"># 函数参数 a 和 b 都是整型, 相除得到还是整型</span></span><br><span class="line"><span class="comment"># 如果是 Python 语义, 那么在转化的时候会向下取整得到 -4, 但这里是 C 语义, 所以是截断得到 -3</span></span><br><span class="line"><span class="built_in">print</span>(cython_test.divides(-<span class="number">7</span>, <span class="number">2</span>))  <span class="comment"># -3</span></span><br></pre></td></tr></table></figure><p>除了这种方式，还可以下面下面两种方式来指定。</p><ul><li>通过上下文管理器的方式</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cimport cython</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">divides</span>(<span class="params"><span class="built_in">int</span> a, <span class="built_in">int</span> b</span>):</span><br><span class="line">    <span class="keyword">with</span> cython.cdivision(<span class="literal">True</span>):</span><br><span class="line">        <span class="keyword">return</span> a / b</span><br></pre></td></tr></table></figure><ul><li>通过注释的方式进行全局声明</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># cython: cdivision=True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">divides</span>(<span class="params"><span class="built_in">int</span> a, <span class="built_in">int</span> b</span>):</span><br><span class="line">    <span class="keyword">return</span> a / b</span><br></pre></td></tr></table></figure><p>如果什么都不指定的话，执行一下看看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">def</span> <span class="title function_">divides</span>(<span class="params"><span class="built_in">int</span> a, <span class="built_in">int</span> b</span>):</span><br><span class="line">    <span class="keyword">return</span> a / b</span><br><span class="line"> <span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(-<span class="number">7</span> // <span class="number">2</span>)  <span class="comment"># -4</span></span><br><span class="line"><span class="built_in">print</span>(cython_test.divides(-<span class="number">7</span>, <span class="number">2</span>))  <span class="comment"># -4</span></span><br></pre></td></tr></table></figure><p>此时就和Python语义是一样的了。</p><p><strong>总结：</strong></p><ul><li><code>使用 cdef int、cdef float 声明的变量不再是 Python 中的 int、float，也不再对应 CPython 中的 PyLongObject * 和PyFloatObject *，而就是 C 中的 int 和 float。</code></li><li><code>虽然是 C 中的 int 和 float，并且也没有像 Python 一样考虑整型溢出的问题(实际上溢出的情况非常少，如果可能溢出的话，就不要使用 C 中的 int 或者 long，而是使用 Python 的 int)，但是在进行运算的时候是遵循 Python 的语义的。因为 Cython 就是为了优化 Python 而生的，因此在各个方面都要和 Python 保持一致。</code></li><li><code>但是也提供了一些方式，禁用掉 Python 的语义，而是采用 C 的语义。方式就是上面说的那三种，它们专门针对于整除和取模，因为加减乘都是一样的，只有除和取模会有歧义。</code></li></ul><p>不过这里还有一个隐患，因为在除法的时候使其遵循 C 的语义，而 C 不会对分母为 0 的情况进行考虑，而 Python 则会进行检测。如果分母为 0，在 Python 中会抛出：ZeroDivisionError，在C中会可能导致未定义的行为（从硬件损坏和数据损害都有可能，好吓人，妈妈我怕）。</p><p>Cython 中还有一个 cdivision_warnings，使用方式和 cdivision 完全一样，表示：当取模的时候如果两个操作数中有一个是负数，那么会抛出警告。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cimport cython</span><br><span class="line"></span><br><span class="line"><span class="meta">@cython.cdivision_warnings(<span class="params"><span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mod</span>(<span class="params"><span class="built_in">int</span> a, <span class="built_in">int</span> b</span>):</span><br><span class="line">    <span class="keyword">return</span> a % b</span><br><span class="line"> <span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># -7 - (2 * -4) == 1</span></span><br><span class="line"><span class="built_in">print</span>(cython_test.mod(-<span class="number">7</span>, <span class="number">2</span>))  </span><br><span class="line"><span class="comment"># 提示我们取整操作在 C 和 Python 有着不同的语义, 同理 cython_test.mod(7, -2) 也会警告</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">RuntimeWarning: division with oppositely signed operands, C and Python semantics differ</span></span><br><span class="line"><span class="string">  return a % b</span></span><br><span class="line"><span class="string">1</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -7 - (-2 * 3) = -1</span></span><br><span class="line"><span class="built_in">print</span>(cython_test.mod(-<span class="number">7</span>, -<span class="number">2</span>))  <span class="comment"># -1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 但是这里的 cython_test.mod(-7, -2) 却没有弹出警告，这是为什么呢？</span></span><br><span class="line"><span class="comment"># 很好理解，我们说只有商是负数的时候才会存在歧义，但是 -7 除以 -2 得到的商是 3.5，是个正数</span></span><br><span class="line"><span class="comment"># 而正数的表现形式在截断和向下取整中都是一致的，所以不会警告</span></span><br><span class="line"><span class="comment"># 同理 cython_test.mod(7, 2) 一样不会警告</span></span><br></pre></td></tr></table></figure><p>另外这里的警告是同时针对 Python 和 C 的，即使我们再使用一层装饰器 <code>@cython.cdivision(True)</code> 装饰、将其改变为 C 的语义的话，也一样会弹出警告的。 cdivision_warnings 意义不是很大，了解一下即可。</p><h1 id="8、用于加速的静态类型"><a href="#8、用于加速的静态类型" class="headerlink" title="8、用于加速的静态类型"></a>8、用于加速的静态类型</h1><p>上面介绍了在 Cython 中使用 Python 的类型进行声明，这咋一看有点古怪，为什么不直接使用 Python 的方式创建变量呢？<code>a = (1, 2, 3)</code> 不香么？为什么非要使用 <code>cdef tuple a = (1, 2, 3)</code> 这种形式呢？答案是 “为了遵循一个通用的 Cython 原则”：提供的静态信息越多，Cython 就越能优化结果。所以 <code>a = (1, 2, 3)</code>，这个 a 可以指向任意的对象，但是 <code>cdef tuple a = (1, 2, 3)</code> 的话，这个 a 只能指向元组，在明确了类型的时候，执行的速度会更快。</p><p>看一个列表的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lst = []</span><br><span class="line">lst.append(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>我们只看 lst.append(1) 这一行，显然它再简单不过了，但是你知道 Python 解释器是怎么操作的吗？</p><ul><li><p><strong>检测类型</strong>，Python 中变量是一个 PyObject *，因为任何对象在底层都嵌套了 PyObject 这个结构体，但具体是什么类型则需要一步检索才知道。通过 <code>PyTypeObject *type = lst -&gt; ob_type</code>，拿到其类型。</p></li><li><p><strong>转化类型</strong>，PyListObject *lst = (PyListObject *)lst</p></li><li><p><strong>查找属性</strong>，调用的是 append 方法，因此调用 PyObject_GetAttrString，参数就是字符串 “append”，找到指向该方法的指针。如果不是 list，但是内部如果有 append 方法也是可以的，然后进行调用。</p></li></ul><p>因此看到一个简单的 append，Python 内部是需要执行以上几个步骤的，但如果实现规定好了类型呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="built_in">list</span> lst = []</span><br><span class="line">lst.append(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>那么此时会有什么差别呢？对 list 对象进行 append 的时候底层调用的 C 一级的函数是 PyList_Append，通过索引赋值的时候调用的是 PyList_SetItem，索引取值的时候调用的是 PyList_GetItem，等等。每一个操作在 C 一级都指向了一个具体的函数，如果提前知道了类型，那么 Cython 生成的代码可以将上面的三步变成一步，没错，直接通过 C api 让 lst.append 指向 PyList_Append 这个 C 一级的函数，这样省去了类型检测、转换、属性查找等步骤，直接调用调用即可。</p><blockquote><p>所以列表解析比普通的 for 循环快也是如此，因为 Python 对内置的结构非常熟悉，当使用的是列表解析式，那么也同样会直接指向 PyList_Append 这个 C 一级的函数。</p></blockquote><p>同理在 Cython 中使用 for 循环的时候，也是如此。如果循环一个可迭代对象，而这个可迭代对象内部的元素都是同一种类型（假设是 dict 对象），那么在循环之前可以先声明循环变量的类型，比如：cdef dict item，然后在 for item in xxx，这样也能提高效率。</p><h1 id="9、引用技术和静态字符串"><a href="#9、引用技术和静态字符串" class="headerlink" title="9、引用技术和静态字符串"></a>9、引用技术和静态字符串</h1><p>Python 会自动管理内存的，解释器 CPython 通过直接的引用计数来判断一个对象是否应该被回收，但是无法解决循环引用，于是 Python 中又提供了垃圾回收来解决这一点。</p><blockquote><p>这里多提一句 Python 中的 gc，Python 判断一个对象回收的标准就是它的引用计数是否为 0，为 0 就被回收。但是这样无法解决循环引用，于是 Python 中的 gc 就是来解决这个问题的。那么它是怎么解决的呢？</p><p>首先什么样的对象会发生循环引用呢？不用说，显然是可变对象，比如：列表、类的实例对象等等，像 int、str 这些不可变对象肯定是不会发生循环引用的，单纯的引用计数足以解决。</p><p>而对于可变对象，Python 会通过分代技术，维护三个链表：零代链表、一代链表、二代链表。将那些可变对象移到链表上，然后通过三色标记模型找到那些发生循环引用的对象，将它们的引用计数减一，从而解决循环引用的问题。不过有人好奇了，为什么是三个链表，一个不行吗？事实上，Python 检测循环引用、或者触发一次 gc 还是要花费一些代价的，对于某些经过 gc 的洗礼之后还活着的对象，我们认为它们是比较稳定的，不应该每次触发 gc 就对它们进行检测。所以 Python 会把零代链表中比较稳定的对象移动到一代链表中，同理一代链表也是如此，不过最多就是二代链表，没有三代链表。当清理零代链表的次数达到 10 次的时候，会清理一次一代链表；清理一代链表达到 10 次的时候，会清理一次二代链表。</p></blockquote><p>而 Cython 处理所有的引用计数问题，确保 Python 对象（无论是Cython动态声明、还是Python动态声明）在引用计数为 0 时被销毁。</p><blockquote><p>很好理解，就是内存管理的问题 Cython 也会负责的。其实不用想也大概能猜到 Cython 会这么做，毕竟 <code>cdef tuple a = (1, 2, 3)</code> 和 <code>a = (1, 2, 3)</code> 底层都对应 PyTupleObject *，只不过后者在操作的时候需要先通过 PyObject * 获取类型 <code>(PyTupleObject \*)</code> 再转化罢了，而前者则省略了这一步。但它们底层都是 CPython 中的结构体，所以内存都由解释器管理。还是那句话，Cython 代码是要被翻译成 C 的代码的，在翻译的时候会自动处理内存的问题，当然这点和 Python 也是一样的。</p></blockquote><p>但是当 Cython 中动态变量和静态变量混合时，那么内存管理会有微妙的影响。举个栗子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># char *, 在 Cython 只能接收一个非 ascii 字符串, 或者 bytes 对象</span></span><br><span class="line"><span class="comment"># 但下面这行代码是编译不过去的</span></span><br><span class="line">cdef char *name = <span class="string">&quot;cython&quot;</span>.encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"><span class="comment"># 不是说后面可以跟一个 bytes 对象吗? </span></span><br><span class="line"><span class="comment"># 但问题是这个 bytes 对象它是一个临时对象, 什么是临时对象呢? 就是创建完了但是没有变量指向它</span></span><br><span class="line"><span class="comment"># 这里的 name 是使用 C 的类型创建的变量, 所以它不会增加这个 bytes 对象的引用计数</span></span><br><span class="line"><span class="comment"># 因此这个 bytes 对象创建出来之后就会被销毁, 编译时会抛出：Storing unsafe C derivative of temporary Python reference</span></span><br><span class="line"><span class="comment"># 告诉我们创建出来的Python对象是临时的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这么做是可以的, 并且 &quot;komeiji satori&quot; 会被解释成 C 中的字符串</span></span><br><span class="line">cdef char *name = <span class="string">&quot;cython&quot;</span></span><br><span class="line"><span class="comment"># 同理 cdef int a = 123; 这个 123 也是 C 中的整型, 但 cdef char *name = &quot;古明地觉&quot; 则不行, 因为它不是ascii字符串</span></span><br></pre></td></tr></table></figure><p>那么如何解决这一点呢？答案是使用变量保存起来就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 这种做法是完全合法的, 因为我们这个 bytes 对象是被 name1 指向了</span></span><br><span class="line">name1 = <span class="string">&quot;cython&quot;</span>.encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">cdef char *buf1 = name1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然鹅这么做是不行的, 编译是通过的, 但是执行的时候会报错：TypeError: expected bytes, str found</span></span><br><span class="line">name2 = <span class="string">&quot;cython&quot;</span></span><br><span class="line">cdef char *buf2 = name2</span><br><span class="line"><span class="comment"># 可能有人觉得, cdef char *buf2 = &quot;komeiji satori&quot;就可以, 为什么赋值给一个变量就不行了</span></span><br><span class="line"><span class="comment"># 因此 char * 它需要接收的是 C 中的字符串, 或者 Python 中的 bytes, 而我们赋值一个变量的时候它就已经是 Python 中的字符串了</span></span><br></pre></td></tr></table></figure><p><strong>因此关于 char * 来总结一下：</strong></p><ul><li><code>cdef char *buf = &quot;cython&quot;.encode(&quot;utf-8&quot;) 理论上是合理的，但是由于这个对象创建完之后就被销毁，所以不行。这个是在编译的时候就会被检测到，因为这属于内存方面的问题。</code></li><li><code>cdef char *buf = &quot;cython&quot; 是可以的，因为此时 &quot;cython&quot; 会被解释成 C 中的字符串。</code></li><li><code>name = &quot;cython&quot;.encode(&quot;utf-8&quot;); cdef char *buf = name 也可以的，因为 name 指向了字节对象，所以不会被销毁，能够提取它的 char 指针。</code></li><li><code>name = &quot;cython&quot;; cdef char *buf = name 则不行，原因在于我们将 &quot;cython&quot; 赋值给了 name，那么这个 name 显然就是 Python 中的字符串，不可以将 Python 中的字符串赋值给 C 中的 char *，只能赋字节串，因此会报错。但该错误是属于赋值出错了，因此它是一个运行时错误，所以编译成扩展模块的时候是可以正常通过的。</code></li></ul><p>不过还是那句话，只有当直接给 char * 变量赋一个 ascii 字符串的时候，才会被当成是 C 中的字符串，如果赋了非 ascii 字符串、或者是 ascii 字符串用变量接收了并且赋的是变量，那么也是不合法的。因此建议，字符串直接使用 str 即可，没有必要使用 char *。</p><p>那么下面的代码有没有问题呢？如果有问题该怎么改呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> word1 = <span class="string">&quot;hello&quot;</span>.encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">word2 = <span class="string">&quot;satori&quot;</span>.encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"></span><br><span class="line">cdef char *word = word1 + word2</span><br></pre></td></tr></table></figure><p>会不会出问题呢？显然会有大问题，尽管 word1 和 word2 指向了相应的 bytes 对象，但是 word1 + word2 则是会创建一个新的 bytes 对象，这个新的 bytes 对象可没有人指向。因此提取其 char * 之后也没用，因为这个新创建的 bytes 对象会被直接销毁。</p><p><strong>而解决的办法有两种：</strong></p><ul><li><code>tmp = word1 + word2; cdef char *word = tmp，使用一个动态的方式创建一个变量指向它，确保它不会被销毁。</code></li><li><code>cdef bytes tmp = word1 + word2; cdef char *word = tmp，道理一样，只不过使用的是静态声明的方式。</code></li></ul><p>另外，其实像上面这种情况并不常见，基本上只有 char * 会有这个问题，因为它比较特殊，底层使用一个指针来表示字符串。和 int、long 不同，cdef long a = 123，这个 123 直接就是 C 中的 long，可以直接使用；但将 Python 中的 bytes 对象赋值给 char *，在 C 的级别 char * 所引用的数据还是由 CPython 进行管理的，char * 缓冲区无法告诉解释器还有一个对象（非 Python 对象）引用它，这就导致了它的引用计数不会加1，而是创建完之后就会被销毁。</p><p>所以需要提前使用 Python 中的变量将其保存起来，这样就不会删除了。而只有char *会面临这个问题，而其它的则无需担心。但是我们完全可以不使用 char *，使用 str 和 bytes 难道不好吗？</p><h1 id="10、Cython的函数"><a href="#10、Cython的函数" class="headerlink" title="10、Cython的函数"></a>10、Cython的函数</h1><p>上面所学的关于动态变量和静态变量的知识也适用于函数，Python 的函数和 C 的函数都有一些共同的属性：函数名称、接收参数、返回值，但是 Python 中的函数更加的灵活这强大。因为 Python 中一切皆对象，所以函数也是一等公民，可以随意赋值、并具有相应的状态和行为，这种抽象是非常有用的。</p><p><strong>一个Python函数可以：</strong></p><ul><li><code>在导入时和运行时动态创建</code></li><li><code>使用 lambda 关键字匿名创建</code></li><li><code>在另一个函数(或其它嵌套范围)中定义</code></li><li><code>从其它函数中返回</code></li><li><code>作为一个参数传递给其它函数</code></li><li><code>使用位置参数和关键字参数调用</code></li><li><code>函数参数可以使用默认值</code></li></ul><p><strong>C函数调用开销最小，比Python函数快几个数量级。一个C函数可以：</strong></p><ul><li><code>可以作为一个参数传递给其它函数，但这样做比 Python 麻烦的多</code></li><li><code>不能在其它函数内部定义，而这在 Python 中不仅可以、而且还非常常见，毕竟 Python 中常用的装饰器就是通过高阶函数加上闭包实现的，而闭包则可以理解为是函数的内部嵌套其它函数。</code></li><li><code>具有不可修改的静态分配名称</code></li><li><code>只能接受位置参数</code></li><li><code>函数参数不支持默认值</code></li></ul><p>正所谓鱼和熊掌不可兼得，Python 的函数调用虽然慢几个数量级（即使没有参数），但是它的灵活性和可扩展性都比 C 强大很多，这是以效率为代价换来的。而 C 的效率虽然高，但是灵活性没有 Python 好，这便是各自的优缺点。</p><h2 id="10-1-在Cython中使用def关键字定义Python函数"><a href="#10-1-在Cython中使用def关键字定义Python函数" class="headerlink" title="10.1 在Cython中使用def关键字定义Python函数"></a>10.1 在Cython中使用def关键字定义Python函数</h2><p>Cython 支持使用 def 关键字定义一个通用的 Python 函数，并且还可以按照我们预期的那样工作。比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pyx</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rec</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> n * rec(n - <span class="number">1</span>)</span><br><span class="line"> <span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># setup_exe.py</span></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(cython_test.rec(<span class="number">20</span>))  <span class="comment"># 2432902008176640000</span></span><br></pre></td></tr></table></figure><p>显然这是一个 Python 语法的函数，参数 n 是接收一个动态的 Python 变量，但它在 Cython 中也是合法的，并且表现形式是一样的。</p><p>即使是普通的 Python 函数，也可以通过 cython 进行编译，但是就调用而言，这两者是没有任何区别的。不过说执行扩展里面的代码时，已经绕过了解释器解释字节码这一过程；但是 Python 代码则不一样，它是需要被解释执行的，因此在运行期间可以随便动态修改内部的属性。举个栗子就很清晰了：</p><p><strong>Python 版本：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件名：a.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="number">123</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 另一个文件</span></span><br><span class="line"><span class="keyword">from</span> a <span class="keyword">import</span> foo</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(foo())  <span class="comment"># 123</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(foo.__name__)  <span class="comment"># foo</span></span><br><span class="line">foo.__name__ = <span class="string">&quot;哈哈&quot;</span></span><br><span class="line"><span class="built_in">print</span>(foo.__name__)  <span class="comment"># 哈哈</span></span><br></pre></td></tr></table></figure><p><strong>Cython 版本：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="number">123</span></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> cython_test <span class="keyword">import</span> foo</span><br><span class="line"><span class="built_in">print</span>(foo())  <span class="comment"># 123</span></span><br><span class="line"><span class="built_in">print</span>(foo.__name__)  <span class="comment"># foo</span></span><br><span class="line"></span><br><span class="line">foo.__name__ = <span class="string">&quot;哈哈&quot;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   ...</span></span><br><span class="line"><span class="string">   ...</span></span><br><span class="line"><span class="string">    foo.__name__ = &quot;哈哈&quot;</span></span><br><span class="line"><span class="string">AttributeError: attribute &#x27;__name__&#x27; of &#x27;builtin_function_or_method&#x27; objects is not writable</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>看到报错了：’builtin_function_or_method’ 的属性 <code>__name__</code>不可写，因为 Python 中的函数是一个类型函数，它是通过解释器的，所以它可以修改自身的一些属性。但是 Cython 代码在编译之后，变成了 builtin_function_or_method，绕过了解释这一步，因为不能对它自身的属性进行修改。事实上，Python 的一些内置函数也是不能修改的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">getattr</span>.__name__ = <span class="string">&quot;xxx&quot;</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># attribute &#x27;__name__&#x27; of &#x27;builtin_function_or_method&#x27; objects is not writable</span></span><br></pre></td></tr></table></figure><p>这些内置的函数直接指向了底层 C 一级的函数，因此它们的属性是不能够被修改的。</p><p>回到刚才的用递归计算阶乘的例子上来，显然 rec 函数里面的 n 是一个动态变量，如果想要加快速度，就要使用静态变量，也就是规定好类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rec</span>(<span class="params">long n</span>):</span><br><span class="line">   <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">       <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">   <span class="keyword">return</span> n * rec(n - <span class="number">1</span>)</span><br></pre></td></tr></table></figure><blockquote><p>此时当我们传递的时候，会将值转成 C 中的 long，如果无法转换则会抛出异常。</p></blockquote><p>另外在 Cython 中定义任何函数，都可以将动态类型的参数和静态类型的参数混合使用。 Cython 允许静态参数具有默认值，并且可以按照位置参数或者关键字参数的方式传递。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这样的话, 我们就可以不传参了, 默认 n 是 20</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rec</span>(<span class="params">long n=<span class="number">20</span></span>):</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> n * rec(n - <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>但是遗憾的是，即便使用了 long n 这种形式定义参数，效率也不会有提升。因为这里的 rec 还是一个 Python 函数，它的返回值也是一个 Python 中的整型，而不是静态的 C long。因此在计算 n * rec(n - 1) 的时候，Cython 必须生成大量代码，从返回的 Python 整数中提取底层的 C long，然后乘上静态类型的变量 n，最后再将结果得到的 C long 打包成 Python 的整型。所以整个过程基本上是没什么变化的。</p><p>那么如何才能提升性能呢？显然这明显可以不使用递归而是使用循环的方式，当然这个我们不谈，因为这个 Cython 没啥关系。想做的是告诉Cython：”这是一个 C long，你要在不创建任何 Python 整型的情况下计算它，会将你最终计算好的结果包装成 Python 中的整型，总之你计算的时候不需要 Python 整数参与。”</p><h2 id="10-2-在Cython中使用cdef关键字定义C函数"><a href="#10-2-在Cython中使用cdef关键字定义C函数" class="headerlink" title="10.2 在Cython中使用cdef关键字定义C函数"></a>10.2 在Cython中使用cdef关键字定义C函数</h2><p>cdef 关键字除了创建变量之外，还可以创建具有 C 语义的函数。cdef 定义的函数其参数和返回值通常都是<strong>静态类型</strong>的，它们可以处理 C 指针、结构体、以及其它一些无法自动转换为 Python 类型的 C 类型。所以把 cdef 定义的函数看成是长得像 Python 函数的 C 函数即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cdef long rec(long n):</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> n * rec(n - <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>之前的例子就可以改写成上面这种形式，看到结构非常相似，主要区别就是指定了返回值的类型。</p><p>但是此时的函数是没有任何 Python 对象参与的，因此不需要从 Python 类型转化成 C 类型。该函数和纯 C 函数一样有效，调用函数的开销最小。另外，即便是 cdef 定义的函数，我们依旧可以创建 Python 对象和动态变量，或者接收它们作为参数也是可以的。但是 cdef 编写的函数应该是在，为了获取 C 的效率、不需要动态变量的情况下编写的。</p><p>当然在 Cython 源文件中可以使用 cdef 定义函数、也可以是用 def 定义函数，这是显然的。cdef 函数返回的类型可以是任何的静态类型（如：指针、结构体、C数组、静态Python类型）。如果省略了返回值，那么默认是object <code>比如:cdef f1():等价于cdef object f1()</code>，也就是说此时返回任何对象都是可以的。关于返回值的问题，举个例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 合法, 返回的是一个 list 对象</span></span><br><span class="line">cdef <span class="built_in">list</span> f1():</span><br><span class="line">    <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等于cdef object f2(): 而 Python 中任何对象都是 object 对象</span></span><br><span class="line">cdef f2():</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 虽然要求返回列表, 但是返回 None 也是可以的(None特殊, 后面会继续说)</span></span><br><span class="line">cdef <span class="built_in">list</span> f3():</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同样道理</span></span><br><span class="line">cdef <span class="built_in">list</span> f4():</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里是会报错的：TypeError: Expected list, got tuple</span></span><br><span class="line">cdef <span class="built_in">list</span> f5():</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span></span><br></pre></td></tr></table></figure><p>使用 cdef 定义的函数，可以被其它的函数（cdef 和 def 都行）调用，但是 <strong>Cython 不允许从外部 Python 代码来调用 cdef 函数</strong>，之前使用 cdef 定义的变量也是如此。因为 Python 中函数也可以看成是变量，所以通常会定义一个 Python 中的函数，然后让 Python 中的函数来调用 cdef 定义的函数，所以此时的 Python 函数就类似于一个包装器，用于向外界提供一个访问的接口。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cdef long _rec(long n):</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> n * rec(n - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rec</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">return</span> _rec(n)</span><br></pre></td></tr></table></figure><p>这种方式是最快的，之前的方式都有大量的 Python 开销。</p><blockquote><p>但不幸的时，这种方式有一个弊端，相信肯定都能想到。那就是 C 中的整数类型（int、long等等）都存在精度问题，而 Python 的整型是不受限制的，只要内存足够。解决办法就是确保不会溢出，或者将 long 换成double。</p><p>这是一个很普遍的问题，基本上所有的语言都是这样子，<strong>只有 Python 在表示整型的时候是没有限制的</strong>。有些时候，Python 数据和 C 数据并不能总是实现完美的映射，需要意识到 C 的局限性。这也是为什么 Cython 不会擅自把 Python 中的 int 变成 C 中的 int、long，因为这两者在极端情况下不是等价的。但是绝大多数情况下，使用 long 是足够的，甚至都不需要 long，int 也足够，至少我平时很少遇见 long 存不下的数字，或者实在不行就用double嘛。</p></blockquote><h2 id="10-3-使用cpdef结合def、cdef"><a href="#10-3-使用cpdef结合def、cdef" class="headerlink" title="10.3 使用cpdef结合def、cdef"></a>10.3 使用cpdef结合def、cdef</h2><p> Cython 定义一个函数可以使用 def 和 cdef，但是还有第三种定义函数的方式，使用 <code>cpdef</code>关键字声明。cpdef 是 def 和 cdef 的混合体，结合了这两种函数的特性，并解决了局限性。之前使用 cdef 定义了一个函数 _rec，但是它没法直接被外部访问，因此又定义了一个 Python 函数 rec 供外部调用，相当于提供了一个接口。所以我们需要定义两个函数，一个是用来执行逻辑的（C版本），另一个是让外部访问的（Python版本，一般这种函数称之为 <strong>Python 包装器</strong>。很形象，C 版本不能被外部访问，因为定义一个 Python 函数将其包起来）。</p><p>但是 cpdef 定义的函数会同时具备这两种身份，怎么理解呢？一个 cpdef 定义的函数会自动为我们提供上面那两个函数的功能，它们具备相同的名称。从 Cython 中调用函数时，会调用 C 的版本，在外部的 Python 中导入并访问时，会调用包装器。这样的话，cpdef 函数就可以将 cdef 函数的性能和 def 函数的可访问性结合起来了。</p><p>因此上面那个例子，我们就可以改写成如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cpdef long rec(long n):</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> n * rec(n - <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如果定义两个函数，这两个函数还不能重名，但是使用 cpdef 就不需要关心了，这样可以更方便。</p><h1 id="10-4-inline-cdef-and-cpdef-functions"><a href="#10-4-inline-cdef-and-cpdef-functions" class="headerlink" title="10.4 inline cdef and cpdef functions"></a>10.4 inline cdef and cpdef functions</h1><p>在 C 和 C++ 中，定义函数时还可以使用一个可选的关键字 inline，这个 inline 是做什么的呢？函数调用是有开销的（话说你效率这么高了，还在乎这一点啊），而使用 inline 关键字定义的函数，那么代码会被放在符号表中，在使用时直接进行替换（像宏一样展开），没有了调用的开销，提高效率。</p><p>Cython 同样支持 inline 关键字，使用时只需要将 inline 放在 cdef 或者 cpdef 后面即可，但是不能放在 def 后面。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cpdef inline unsigned long rec(<span class="built_in">int</span> n):</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> rec(n - <span class="number">1</span>) * n</span><br></pre></td></tr></table></figure><p>inline 如果使用得当，那么可以提高性能，特别是在深度嵌套循环中调用的小型内联函数。因为它们会被多次调用，这个时候通过 inline 可以省去函数调用的开销。</p><p>使用 cpdef 有一个局限性，那就是它要同时兼容 Python 和 C：意味着它的参数和返回值类型必须同时兼容 Python 类型和C类型。但，并非所有的 C 类型都可以用 Python 类型表示，比如：C 指针、void、C 数组等等，它们不可以作为 cpdef 定义的函数的参数类型和返回值类型。</p><h1 id="11、函数和异常处理"><a href="#11、函数和异常处理" class="headerlink" title="11、函数和异常处理"></a>11、函数和异常处理</h1><p>def 定义的函数在 C 的级别总是会返回一个 PyObject*，这个是恒定的不会改变，因为 Python 中的所有变量在底层都是一个 PyObject *。它允许 Cython 正确地从 def 函数中抛出异常，但是 cdef 和 cpdef 可能会返回一个非 Python 类型，因此此时则需要一些其它的异常提示机制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cpdef <span class="built_in">int</span> divide_ints(<span class="built_in">int</span> i, <span class="built_in">int</span> j):</span><br><span class="line"><span class="keyword">return</span> i // j</span><br></pre></td></tr></table></figure><p>如果这里的 j传递了一个 0，会引发 ZeroDivisionError，但是这个异常却没有办法传递给它的调用方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.divide_ints(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.divide_ints(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">ZeroDivisionError: integer division <span class="keyword">or</span> modulo by zero</span><br><span class="line">Exception ignored <span class="keyword">in</span>: <span class="string">&#x27;cython_test.divide_ints&#x27;</span></span><br><span class="line">ZeroDivisionError: integer division <span class="keyword">or</span> modulo by zero</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure><p>异常没法传递，换句话说就是异常没有办法向上抛，即使检测到了这个异常。最终会忽略警告信息，并且也会返回一个错误的值 0。</p><p>为了正确传递此异常，Cython 提供了一个 <strong>except</strong> 字句，允许 cdef、cpdef 函数和调用方通信，说明在执行中发生了、或可能发生了 Python 异常。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cpdef <span class="built_in">int</span> divide_ints(<span class="built_in">int</span> i, <span class="built_in">int</span> j) <span class="keyword">except</span>? -<span class="number">1</span>:</span><br><span class="line"><span class="keyword">return</span> i // j</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.divide_ints(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">  File <span class="string">&quot;cython_test.pyx&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> cython_test.divide_ints</span><br><span class="line">    cpdef <span class="built_in">int</span> divide_ints(<span class="built_in">int</span> i, <span class="built_in">int</span> j)<span class="keyword">except</span>?-<span class="number">1</span>:</span><br><span class="line">  File <span class="string">&quot;cython_test.pyx&quot;</span>, line <span class="number">2</span>, <span class="keyword">in</span> cython_test.divide_ints</span><br><span class="line">    <span class="keyword">return</span> i // j</span><br><span class="line">ZeroDivisionError: integer division <span class="keyword">or</span> modulo by zero</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure><p>我们看到此时异常被正常的传递给调用方了，此时程序就崩溃了，而之前那种情况程序是没有崩溃的。</p><p>这里实现的方式是通过在结尾加上 <code>except ? -1</code> 来实现这一点，这个 <code>except ? -1</code> <strong>允许返回值 -1 充当发生异常时的哨兵</strong>。事实上不仅是 -1，只要在返回值类型的范围内的任何数字都行，它们的作用就是传递异常。但是问题来了，如果函数恰好就返回了 -1 的时候该怎么办呢？看到 <code>except ? -1</code> 中的那个问号了吗，它就是用来做这个的，如果函数恰好返回了一个 -1，那么 Cython 会检测是否有异常回溯栈，有的话会自动展开堆栈。如果将那个问号去掉，看看会有什么结果吧。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cpdef <span class="built_in">int</span> divide_ints(<span class="built_in">int</span> i, <span class="built_in">int</span> j) <span class="keyword">except</span> -<span class="number">1</span>:</span><br><span class="line"><span class="keyword">return</span> i // j</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.divide_ints(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.divide_ints(<span class="number">1</span>, <span class="number">0</span>)  <span class="comment"># 依旧会引发异常，这没问题</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">  File <span class="string">&quot;cython_test.pyx&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> cython_test.divide_ints</span><br><span class="line">    cpdef <span class="built_in">int</span> divide_ints(<span class="built_in">int</span> i, <span class="built_in">int</span> j)<span class="keyword">except</span>-<span class="number">1</span>:</span><br><span class="line">  File <span class="string">&quot;cython_test.pyx&quot;</span>, line <span class="number">2</span>, <span class="keyword">in</span> cython_test.divide_ints</span><br><span class="line">    <span class="keyword">return</span> i // j</span><br><span class="line">ZeroDivisionError: integer division <span class="keyword">or</span> modulo by zero</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>   </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.divide_ints(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">SystemError: &lt;built-<span class="keyword">in</span> function divide_ints&gt; returned NULL without setting an error</span><br></pre></td></tr></table></figure><p>如果使用 C 编写过扩展模块的话，应该会遇见过这个问题。Python 中的函数总会有一个返回值的，所以在 C 中一定会返回一个 PyObject *。如果 Python 中的函数出错了，那么在 C 一级就会返回一个 NULL，并且将发生异常设置进去。如果返回了 NULL 但是没有设置异常的话，就会抛出上面的那个错误。而我们这里的 <code>except -1</code> 表示返回了 -1 就代表发生异常了、底层会返回NULL，但是此时却没有异常，所以提示returned NULL without setting an error。</p><p>所以我们看到 <code>except ? -1</code> 只是单纯为了在发生异常的时候能够往上抛罢了，这里可以是 -1、也可以是其它的什么值。而函数如果也返回了相同的值，那么就会检测异常回溯栈，没有报错就会正常返回。而触发检测的条件就是中的那个 <code>?</code>，如果不指定 <code>?</code>，那么当函数返回了和 <code>except</code> 指定的值相同的值，那么是会报错的，因此这个时候你应该确保函数不可能会返回 <code>except</code> 后面指定的值。所以尽管加上了 <code>?</code> 会牺牲一些效率（因为涉及回溯栈的展开，但实际上是没有什么差别的），但如果没有百分之百的把握确定函数不会返回相同的值，那么就使用 <code>?</code> 做一层检测吧。或者还可以使用 <code>except \*</code>，此时会对返回的任何值都进行检测，但没有什么必要、会产生开销，直接写上 <code>except ? -1</code> 即可。这样只对 -1 进行检测，因为目的是能够在发生异常的时候进行传递。</p><p>另外<strong>只有返回值是C的类型，才需要指定 <code>except ? -1</code></strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cpdef <span class="built_in">tuple</span> divide_ints(<span class="built_in">int</span> i, <span class="built_in">int</span> j):</span><br><span class="line">a = i // j</span><br></pre></td></tr></table></figure><p>这个时候即使给 j 传递了 0，异常也是会向上抛的，因为返回值不再是 C 中的类型，而是 Python 中的类型；如果将这里 tuple 改成 int、或者 long 之后异常还是会被忽略掉的。</p><p>因此，在不指定 <code>except ? -1</code> 的情况下，异常不会向上抛需要满足两个条件：</p><ul><li>必须是C中的对象在操作时发生了错误，这里是 i 和 j 相除发生了错误；</li><li>返回值必须是 C 中的类型。</li></ul><h1 id="12、关于扩展模块中的函数信息"><a href="#12、关于扩展模块中的函数信息" class="headerlink" title="12、关于扩展模块中的函数信息"></a>12、关于扩展模块中的函数信息</h1><p>一个函数可以有很多信息，我们可以通过函数的字节码进行获取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(foo.__code__.co_varnames)  <span class="comment"># (&#x27;a&#x27;, &#x27;b&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> inspect</span><br><span class="line"><span class="built_in">print</span>(inspect.signature(foo))  <span class="comment"># (a, b)</span></span><br></pre></td></tr></table></figure><p>但是对于扩展模块中的函数就不能这样获取了，把上面的 foo 函数定义在 cython_test.pyx 中，然后来看一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> cython_test <span class="keyword">import</span> foo</span><br><span class="line"><span class="built_in">print</span>(foo.__code__)  <span class="comment"># 123</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   ...</span></span><br><span class="line"><span class="string">   ...</span></span><br><span class="line"><span class="string">    print(foo.__code__)  # 123</span></span><br><span class="line"><span class="string">AttributeError: &#x27;builtin_function_or_method&#x27; object has no attribute &#x27;__code__&#x27;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>看到扩展模块内的函数变成 built-in 级别的了，所以一些动态信息已经没有了，即便有也是无法动态修改的，比如之前说的 <code>__name__</code>。因为信息的访问、动态修改都是在解释器解释执行的时候完成的，而扩展模块已经是不需要解释、直接拿来执行就可以，已经是终极形态，所以不像常规定义的 Python 函数，扩展模块内的函数的动态信息是不支持动态修改的，有的甚至无法访问。</p><p>既然这样的话，那如何才能将函数信息体现出来呢？答案是通过 <strong>docstring</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pyx </span></span><br><span class="line">cpdef <span class="built_in">int</span> divide_ints(<span class="built_in">int</span> i, <span class="built_in">int</span> j) <span class="keyword">except</span> ? -<span class="number">1</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param i: 第一个整型i </span></span><br><span class="line"><span class="string">    :param j: 第二个整型j</span></span><br><span class="line"><span class="string">    :return: i和j相除</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> i // j</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="built_in">print</span>(cython_test.divide_ints.__doc__)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param i: 第一个整型i </span></span><br><span class="line"><span class="string">    :param j: 第二个整型j</span></span><br><span class="line"><span class="string">    :return: i和j相除</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>这是我们向外界进行描述的最好方式，甚至是唯一方式。</p><h1 id="13、类型转换"><a href="#13、类型转换" class="headerlink" title="13、类型转换"></a>13、类型转换</h1><p>C 和 Python 在数值类型上都有各自的成熟规则，但是这里介绍的是 C 类型，因为 Cython 使用的是 C 类型。</p><p>类型转换在 C 中很常见，尤其是指针，Cython 也提供了相似的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里是将其它类型的指针变量 v 转成了int *</span></span><br><span class="line">cdef <span class="built_in">int</span> *ptr_i = &lt;<span class="built_in">int</span> *&gt;v</span><br><span class="line"><span class="comment"># 在 C 中, 类似于 int *ptr_i = (int *)v, 只不过小括号变成尖括号</span></span><br></pre></td></tr></table></figure><p>显式的转换在 C 中是不被检测的，因此可以对类型进行完全的控制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pyx</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_address</span>(<span class="params">a</span>):</span><br><span class="line">    <span class="comment"># Python 中的变量本质上就是个指针, 所以这里转成 void *</span></span><br><span class="line">    cdef void *v = &lt;void*&gt; a</span><br><span class="line">    <span class="comment"># 而指针存储的值是一个地址, 一串 16进制数, 我们将其转成 long long, 因为一个 long 存不下</span></span><br><span class="line">    cdef long long addr = &lt;long long&gt; v</span><br><span class="line">    <span class="comment"># 然后再通过内置函数 id 获取地址, 因此两个地址是一样的</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Cython address:&quot;</span>, addr)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Python id :&quot;</span>, <span class="built_in">id</span>(a))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line">cython_test.print_address(<span class="string">&quot;cython语言&quot;</span>)</span><br><span class="line">cython_test.print_address([])</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Cython address: 2230547577424</span></span><br><span class="line"><span class="string">Python id : 2230547577424</span></span><br><span class="line"><span class="string">Cython address: 2230548032896</span></span><br><span class="line"><span class="string">Python id : 2230548032896</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>这里传递的对象显然是一个 PyObject *，然后这里先转成 void *，然后再转成 long long，将地址使用十进制表示，这一点和内置函数 id 做的事情是相同的。</p><p>也可以对 Python 中的类型进行强制转换，转换之后的类型可以是内置的、也可以是自己定义的，来看比较做作的例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">a</span>):</span><br><span class="line">    cdef <span class="built_in">list</span> lst1 = <span class="built_in">list</span>(a)</span><br><span class="line">    <span class="built_in">print</span>(lst1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(lst1))</span><br><span class="line"></span><br><span class="line">    cdef <span class="built_in">list</span> lst2 = &lt;<span class="built_in">list</span>&gt; a</span><br><span class="line">    <span class="built_in">print</span>(lst2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(lst2))</span><br><span class="line"> <span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line">cython_test.func(<span class="string">&quot;123&quot;</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;]</span></span><br><span class="line"><span class="string">&lt;class &#x27;list&#x27;&gt;</span></span><br><span class="line"><span class="string">123</span></span><br><span class="line"><span class="string">&lt;class &#x27;str&#x27;&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">cython_test.func((<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[1, 2, 3]</span></span><br><span class="line"><span class="string">&lt;class &#x27;list&#x27;&gt;</span></span><br><span class="line"><span class="string">(1, 2, 3)</span></span><br><span class="line"><span class="string">&lt;class &#x27;tuple&#x27;&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>看到使用 list(a) 转换是正常的，但是 <list> a 则没有实现转换，还是原本的类型。这里的 <list> 作用是接收一个列表然后将其转化为静态的列表，换句话说就是将 PyObject * 转成 PyListObject *。如果接收的不是一个list，那么会转换失败。在早期的 Cython 中会引发一个SystemError，但目前不会了，尽管这里的 lst2 我们定义的时候使用的是 cdef list，但如果转化失败还保留原来的类型。</p><p>可如果我们希望在无法转化的时候报错，这个时候要怎么做呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">a</span>):</span><br><span class="line">    <span class="comment"># 将 &lt;list&gt; 换成 &lt;list?&gt; 即可</span></span><br><span class="line">    cdef <span class="built_in">list</span> lst2 = &lt;<span class="built_in">list</span>?&gt; a</span><br><span class="line">    <span class="built_in">print</span>(lst2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(lst2))</span><br></pre></td></tr></table></figure><p>此时传递其它对象就会报错了，比如我们传递了一个元组，会报出 TypeError: Expected list, got tuple。</p><h1 id="14、声明并使用结构体、共同体、枚举"><a href="#14、声明并使用结构体、共同体、枚举" class="headerlink" title="14、声明并使用结构体、共同体、枚举"></a>14、声明并使用结构体、共同体、枚举</h1><p>Cython 也支持声明、创建、操作 C 中的结构体、共同体、枚举。先看一下 C 中没有使用 typedef的结构体、共同体的声明。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> struct mycpx &#123;</span><br><span class="line">    <span class="built_in">float</span> a;</span><br><span class="line">    <span class="built_in">float</span> b;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">union uu &#123;</span><br><span class="line">    <span class="built_in">int</span> a;</span><br><span class="line">    short b, c;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>如果使用 Cython 创建的话，那么是如下形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cdef struct mycpx:</span><br><span class="line">    <span class="built_in">float</span> real</span><br><span class="line">    <span class="built_in">float</span> imag</span><br><span class="line">    </span><br><span class="line">cdef union uu:</span><br><span class="line">    <span class="built_in">int</span> a</span><br><span class="line">    short b, c</span><br></pre></td></tr></table></figure><p>这里的 cdef 也可以写成 ctypedef 的形式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">ctypedef struct mycpx:</span><br><span class="line">    <span class="built_in">float</span> real</span><br><span class="line">    <span class="built_in">float</span> imag</span><br><span class="line">    </span><br><span class="line">ctypedef union uu:</span><br><span class="line">    <span class="built_in">int</span> a</span><br><span class="line">    short b, c</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时我们相当于为结构体和共同体起了一个别名叫：mycpx、uu</span></span><br><span class="line">cdef mycpx zz  <span class="comment"># 此时的 zz 就是一个 mycpx 类型的变量</span></span><br><span class="line"><span class="comment"># 当然无论结构体是使用 cdef 声明的还是 ctypedef 声明的，变量 zz 的声明都是一样的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 但是变量的赋值方式有以下几种</span></span><br><span class="line"><span class="comment"># 1. 创建的时候直接赋值</span></span><br><span class="line">cdef mycpx a = mycpx(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 也可以支持关键字的方式，但是注意关键字参数要在位置参数之后</span></span><br><span class="line">cdef mycpx b = mycpx(real=<span class="number">1</span>, imag=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 声明之后，单独赋值</span></span><br><span class="line">cdef mycpx c</span><br><span class="line">c.real = <span class="number">1</span></span><br><span class="line">c.imag = <span class="number">2</span></span><br><span class="line"><span class="comment"># 这种方式会麻烦一些，但是可以更新单个字段</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 通过Python中的字典赋值</span></span><br><span class="line">cdef mycpx d = &#123;<span class="string">&quot;real&quot;</span>: <span class="number">1</span>, <span class="string">&quot;imag&quot;</span>: <span class="number">2</span>&#125;</span><br><span class="line"><span class="comment"># 显然这是使用Cython的自动转换完成此任务，它涉及更多的开销，不建议用此种方式。</span></span><br></pre></td></tr></table></figure><p>如果是嵌套结构体也是可以的，但是需要换种方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果是C中我们创建一个嵌套结构体，可以使用下面这种方式</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">struct girl&#123;</span></span><br><span class="line"><span class="string">    char *where;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    struct _info &#123;</span></span><br><span class="line"><span class="string">        char *name;</span></span><br><span class="line"><span class="string">        int age;</span></span><br><span class="line"><span class="string">        char *gender;</span></span><br><span class="line"><span class="string">    &#125; info;</span></span><br><span class="line"><span class="string">&#125;;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 但是Cython中不可以这样，需要把内部的结构体单独拿出来才行</span></span><br><span class="line">ctypedef struct _info:</span><br><span class="line">    char *name</span><br><span class="line">    <span class="built_in">int</span> age</span><br><span class="line">    char *gender</span><br><span class="line"></span><br><span class="line">ctypedef struct girl:</span><br><span class="line">    char *where</span><br><span class="line">    _info info  <span class="comment"># 创建一个info成员，类型是_info</span></span><br><span class="line"></span><br><span class="line">cdef girl g = girl(where=<span class="string">&quot;sakura sou&quot;</span>, info=_info(<span class="string">&quot;mashiro&quot;</span>, <span class="number">16</span>, <span class="string">&quot;female&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(g.where)</span><br><span class="line"><span class="built_in">print</span>(g.info.name)</span><br><span class="line"><span class="built_in">print</span>(g.info.age)</span><br><span class="line"><span class="built_in">print</span>(g.info.gender)</span><br></pre></td></tr></table></figure><p>注意：如果是定义结构体，那么类型必须是 C 中的类型才可以。</p><p>定义枚举也很简单，我们可以在多行中定义，也可以在单行中定义然后用逗号隔开。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cdef enum my_enum1:</span><br><span class="line">    RED = <span class="number">1</span></span><br><span class="line">    YELLOW = <span class="number">3</span></span><br><span class="line">    GREEN = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">cdef enum my_enum2:</span><br><span class="line">    PURPLE, BROWN</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：即使是不同枚举中的成员，但也不能重复</span></span><br><span class="line"><span class="comment"># 比如my_enum1中出现了RED，那么在my_enum2中就不可以出现了</span></span><br><span class="line"><span class="comment"># 当然声明枚举除了cdef之外，同样也可以使用cdef</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此外，如果我们不指定枚举名，那么它就是匿名枚举，匿名枚举用于声明全局整数常量</span></span><br></pre></td></tr></table></figure><h1 id="15、使用ctypedef给类型起别名"><a href="#15、使用ctypedef给类型起别名" class="headerlink" title="15、使用ctypedef给类型起别名"></a>15、使用ctypedef给类型起别名</h1><p>Cython 支持的另一个 C 特性就是可以使用 <strong><code>ctypedef</code>给类型起一个别名</strong>，和 C 中的 typedef 非常类似。主要用在和外部代码进行交互上面，我们还是将在后续系列中重点使用，目前可以先看一下用法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ctypedef <span class="built_in">list</span> LIST  <span class="comment"># 给list起一个别名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数是一个LIST类型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">LIST v</span>):</span><br><span class="line">    <span class="built_in">print</span>(v)</span><br><span class="line"> &gt;&gt;&gt; <span class="keyword">import</span> cython_test</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.f([])</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.f(())</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">TypeError: Argument <span class="string">&#x27;v&#x27;</span> has incorrect <span class="built_in">type</span> (expected <span class="built_in">list</span>, got <span class="built_in">tuple</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.f(<span class="literal">None</span>)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure><p>看到接收的是 list，但是传了一个 tuple 进去，因为 LIST 是 list 的别名，当然不管什么 Python 类型，None 都是满足的。</p><p>ctypedef 可以作用于 C 的类型也可以作用于 Python类型，起别名之后这个别名可以像上面那样作用于函数参数、也可以用于声明一个变量 <code>cdef LIST lst</code>，但是不可以像这样：<code>LIST(&quot;123&quot;)</code>。起的别名用于声明变量，但是不能当成类型本身来用，否则会报错：’LIST’ is not a constant, variable or function identifier。</p><p>ctypedef 对于 Cython 来说不是很常用，但是对于 C++ 来说则特别有用，使用 typedef 可以显著的缩短长模板类型，另外 ctypedef 必须出现在全局作用域中，不可以出现在函数内等局部作用域里。</p><h1 id="16、泛型编程"><a href="#16、泛型编程" class="headerlink" title="16、泛型编程"></a>16、泛型编程</h1><p>Cython 有一个新的类型特性，称为融合类型，它<strong>允许用一个类型来引用多个类型</strong>。</p><p>Cython 目前提供了三种我们可以直接使用的混合类型，<code>integral、floating、numeric</code>，它们都是通过 cython 命名空间来访问的，这个命名空间必须是通过 cimport 导入的。</p><ul><li><code>integral：代指C中的short、int、long</code></li><li><code>floating：代指C中的float、double</code></li><li><code>numeric：最通用的类型，包含上面的integral和floating以及复数</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cython cimport integral</span><br><span class="line"></span><br><span class="line">cpdef integral integral_max(integral a, integral b):</span><br><span class="line">    <span class="keyword">return</span> a <span class="keyword">if</span> a &gt;= b <span class="keyword">else</span> b </span><br></pre></td></tr></table></figure><p>上面这段代码，Cython 将会创建三个版本的函数：</p><ul><li>a 和 b 都是 short、</li><li>a 和 b 都是 int、 </li><li>a 和 b 都是 long。</li></ul><p>如果是在 Cython 内部使用的话，那么 Cython 在编译时会检查到底使用哪个版本；如果是从外部 Python 代码导入时，将使用 long 版本。</p><p>比如我们在 Cython 中调用一下，可以这么做。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cdef allowed():</span><br><span class="line">    <span class="built_in">print</span>(integral_max(&lt;short&gt; <span class="number">1</span>, &lt;short&gt; <span class="number">2</span>))</span><br><span class="line">    <span class="built_in">print</span>(integral_max(&lt;<span class="built_in">int</span>&gt; <span class="number">1</span>, &lt;<span class="built_in">int</span>&gt; <span class="number">2</span>))</span><br><span class="line">    <span class="built_in">print</span>(integral_max(&lt;long&gt; <span class="number">1</span>, &lt;long&gt; <span class="number">2</span>)   )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 但是下面的方式不可以</span></span><br><span class="line">cdef not_allowed():</span><br><span class="line">    <span class="built_in">print</span>(integral_max(&lt;short&gt; <span class="number">1</span>, &lt;<span class="built_in">int</span>&gt; <span class="number">2</span>))</span><br><span class="line">    <span class="built_in">print</span>(integral_max(&lt;<span class="built_in">int</span>&gt; <span class="number">1</span>, &lt;long&gt; <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 里面的类型不能混合，否则产生编译时错误</span></span><br><span class="line"><span class="comment"># 因为 Cython 没生成对应的版本的函数</span></span><br></pre></td></tr></table></figure><p>所以这里就要求了必须传递 integral，如果传递了其它类型，那么在 Cython 中会引发一个编译时错误，在 Python 中会引发一个 TypeError。</p><p>如果我们希望同时支持 integral 和 floating 呢？有人说可以使用 numeric，是的，但是它也支持复数，而我们不希望支持复数，所以可以<strong>定义一个混合类型。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">from</span> cython cimport <span class="built_in">int</span>, <span class="built_in">float</span>, short, long, double</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 ctypedef fused 类型 即可定义一个混合类型，支持的类型可以写在块里面</span></span><br><span class="line">ctypedef fused int_float:</span><br><span class="line">    <span class="built_in">int</span> </span><br><span class="line">    <span class="built_in">float</span></span><br><span class="line">    short</span><br><span class="line">    long</span><br><span class="line">    double</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不仅是C的类型，Python类型也是可以的    </span></span><br><span class="line">ctypedef fused list_tuple:</span><br><span class="line">    <span class="built_in">list</span></span><br><span class="line">    <span class="built_in">tuple</span> </span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f1</span>(<span class="params">int_float a</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f2</span>(<span class="params">list_tuple b</span>):</span><br><span class="line">    <span class="keyword">pass</span> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.f1(<span class="number">123</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.f2((<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.f1(<span class="string">&quot;xx&quot;</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">  File <span class="string">&quot;cython_test.pyx&quot;</span>, line <span class="number">15</span>, <span class="keyword">in</span> cython_test.__pyx_fused_cpdef</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">f1</span>(<span class="params">int_float a</span>):</span><br><span class="line">TypeError: No matching signature found</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cython_test.f2(<span class="string">&quot;xx&quot;</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">  File <span class="string">&quot;cython_test.pyx&quot;</span>, line <span class="number">18</span>, <span class="keyword">in</span> cython_test.__pyx_fused_cpdef</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">f2</span>(<span class="params">list_tuple b</span>):</span><br><span class="line">TypeError: No matching signature found</span><br></pre></td></tr></table></figure><p>传递的时候会对参数进行检测，不符合条件会抛出 TypeError。</p><h1 id="17、Cython中的for循环和while循环"><a href="#17、Cython中的for循环和while循环" class="headerlink" title="17、Cython中的for循环和while循环"></a>17、Cython中的for循环和while循环</h1><p>Python 中的 for 循环和 while 循环是灵活并且高级的，语法自然、读起来像伪代码。而 Cython 也是支持 for 和 while 的，无需修改，并且循环通常占据程序运行时的大部分时间，因此可以通过一些指针，确保 Cython 能够将 Python 中的循环转换为高效的 C 循环。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>上面是一个标准的 Python for 循环，如果这个 i 和 n 是静态类型，那么 Cython 就能生成更快的 C 代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cdef unsigned long i, n = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    ...</span><br><span class="line"><span class="comment"># 这段代码和下面的C代码是等效的</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">for (i=0; i&lt;n; ++i) &#123;</span></span><br><span class="line"><span class="string"> /* ... */</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>Cython 能够推断类型并自动生成快速的循环，但却并不总是这样。如果想加速循环，需要遵循以下原则。</p><p><strong>当通过 range 进行循环时，应该将 range 里面的参数换成 C 的整型。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cdef unsigned long n = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>在循环的时候，这里的 i 也会被当成是整型，前提是没有在循环体的表达式中使用 i 这个变量。但如果使用了，那么 Cython 无法确定是否会发生溢出，因此会保守的选择 Python 中的类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cdef unsigned n = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    <span class="built_in">print</span>(i + <span class="number">2</span> ** <span class="number">32</span>)</span><br></pre></td></tr></table></figure><p>看到在表达式中使用到了 i，如果这里的 <strong>i 是 C 中的整型</strong>，那么在和一个纯数字相加的时候，Cython 不知道是否会发生溢出，所以这里的 i 就不会变成 C 中的整型。</p><p>如果能保证表达式中一定不会发生溢出，那么可以显式地将 i 也声明为 C 中的整数类型。比如：<code>cdef unsigned long i, n = 100</code>。</p><p>当遍历一个容器（list、tuple、dict等等）的时候，对于容器的高效循环，可以考虑将容器转化为 C++ 的有效容器、或者使用类型化的内存视图。</p><p>目前只能在 range 中减少循环开销，将在后续系列中了解优化循环体的更多信息，包括 numpy 在 Cython 中的使用以及类型化内存视图。至于 while 循环的优化方式和 for 循环是类似的。</p><h1 id="18、Cython预处理器"><a href="#18、Cython预处理器" class="headerlink" title="18、Cython预处理器"></a>18、Cython预处理器</h1><p>在 C 中可以使用 <code>#define</code> 定义一个宏，在 Cython 中也是可以的，不过使用的是 <code>DEF</code> 关键字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DEF pi = <span class="number">3.14</span></span><br><span class="line"><span class="built_in">print</span>(pi * <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><strong>DEF 定义的宏在编译的时候就会被替换成指定的值</strong>，可以用于声明 C 的类型、也可以是 Python 的类型。比如这里的 pi，在编译的时候就会被换成 3.14，注意：这个宏只是简单的字符串替换，如果你了解 C 中的宏的话，那么 Cython 中的宏和 C 中的宏是类似的。</p><ul><li><code>UNAME_SYSNAME：操作系统的名称</code></li><li><code>UNAME_RELEASE：操作系统的发行版</code></li><li><code>UNAME_VERSION：操作系统的版本</code></li><li><code>UNAME_MACHINE：操作系统的机型、或者硬件名称</code></li><li><code>UNAME_NODENAME：网络名称</code></li></ul><p>除此之外，Cython 还允许我们像 C 一样使用 <code>IF ELIF ELSE</code> 进行条件编译。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">IF UNAME_SYSNAME == <span class="string">&quot;Windows&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;这是Windows系统&quot;</span>)</span><br><span class="line">ELIF UNAME_SYSNAME == <span class="string">&quot;Linux&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;这是Linux系统&quot;</span>)</span><br><span class="line">ELSE:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;这是其它系统&quot;</span>)</span><br><span class="line"> <span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">这是Windows系统</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>另外：操作系统这些内置的宏，需要搭配 <code>IF ELIF ELSE</code> 使用，单独使用是会报错的。</p><h1 id="19、Cython生态"><a href="#19、Cython生态" class="headerlink" title="19、Cython生态"></a>19、Cython生态</h1><p>Cython是一个辅助语言，它是建立在 Python 之上的，是为 Python 编写扩展模块的。所以很少有项目会完全使用 Cython 编写（uvloop 例外），但它确实是一个成熟的语言，有自己的语法（个人非常喜欢，觉得设计的真酷）。在 GitHub 上搜索，会发现大量的 Cython 源文件分布在众多的存储库中。</p><p>考虑到 numpy、pandas、scipy、sklearn 等知名模块内部都在使用，所以 Cython 也算是间接地被数百万的开发人员、分析师、工程师和科学家直接或者间接使用。</p><p>如果 Pareto 原理是可信的，程序中百分之 80 的运行时开销是由百分之 20 的代码引起的，那么对于一个 Python 项目来说，只需要将少部分 Python 代码转换成 Cython 代码即可。</p><p>一些用到 Cython 的顶尖项目都是与数据分析和科学计算有关的，这并非偶然。Cython 之所以会在这些领域大放异彩，有以下几个原因：</p><ul><li><code>1. Cython 可以高效且简便地封装现有的 C、C++、FORTRAN 库，从而对那些已经优化并调试过的功能进行访问。这里多提一句，FORTRAN算是一个上古的语言了，它的历史比 C 还要早，但是别看它出现的早、但速度是真的快，尤其是在数值计算方面甚至比 C 还要快。包括 numpy 使用的 blas 内部也用到了 FORTRAN，虽然 FORTRAN 编写代码异常的痛苦，但是它在一些学术界和工业界还是具有一席之地的。原因就是它内部的一些算法，都是经过大量的优化、并且久经考验的，直接拿来用就可以。而 Cython 也提供了相应的姿势来调用 FORTRAN 已经编写好的功能。</code></li><li><code>2. 当转化为静态类型语言时，内存和 CPU 密集的 Python 计算会有更好的执行性能。</code></li><li><code>3. 在处理大型的数据集时，与 Python 内置的数据结构相比，在低级别控制精确的数据类型和数据结构可以让存储更高效、执行性能更优秀。</code></li><li><code>4. Cython 可以和 C、C++、FORTRAN 库共享同类型的连续数组，并通过 numpy 中的数组直接暴露给Python。</code></li></ul><p>不过即便不是在数据分析和科学计算领域，Cython 也可以大放异彩，它也可以加速一般的 Python 代码，包括数据结构和密集型算法。例如：lxml 这个高性能的 xml 解析器内部就大量使用了 Cython。因此即使它不在科学计算和数据分析的保护伞下，也依旧有很大的用途。</p>]]></content>
      
      
      <categories>
          
          <category> PL </category>
          
          <category> Cython </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4.Cython中扩展类</title>
      <link href="/program_language/cython/4.Cython%E4%B8%AD%E6%89%A9%E5%B1%95%E7%B1%BB/"/>
      <url>/program_language/cython/4.Cython%E4%B8%AD%E6%89%A9%E5%B1%95%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="1、Python类和扩展类之间的差异"><a href="#1、Python类和扩展类之间的差异" class="headerlink" title="1、Python类和扩展类之间的差异"></a>1、Python类和扩展类之间的差异</h1><p>首先 Python 中 “一切皆对象”，怎么理解呢？首先在最基本的层次上，一个对象有三样东西：地址、值、类型，通过 id 函数可以获取地址并将每一个对象都区分开来，使用 type 获取类型。Python 中对象有很多属性，这些属性都放在自身的属性字典里面，这个字典可以通过 <code>__dict__</code> 获取。调用对象的某一个属性的时候，可以通过 <code>.</code> 的方式来调用，Python 也允许我们通过 class 关键字自定义一个类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A.__name__)  <span class="comment"># A</span></span><br><span class="line">A.__name__ = <span class="string">&quot;B&quot;</span></span><br><span class="line"><span class="built_in">print</span>(A.__name__)  <span class="comment"># B</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">int</span>.__name__ = <span class="string">&quot;INT&quot;</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># 内建类型 和 扩展类型 不允许修改属性</span></span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># can&#x27;t set attributes of built-in/extension type &#x27;int&#x27;</span></span><br></pre></td></tr></table></figure><p>除了在 Python 中定义类，还可以直接使用 Python/C API 在 C 级别创建自己的类型，这样的类型称之为扩展类、或者扩展类型（说白了在 C 中实现的类就叫做扩展类）。</p><blockquote><p>Python 解释器本来就是 C 写的，所以可以在 C 的层面上面实现 Python 的任何对象，类也是如此。Python 中自定义的类和内置的类在 C 一级的结构是一致的，所以只需要按照 Python/C API 提供的标准来编写即可。但还是那句话，使用 C 来编写会比较麻烦，因为本质上就是写 C 语言。</p></blockquote><p>当操作扩展类的时候，操作的是编译好的静态代码，因此在访问内部属性的时候，可以实现快速的 C 一级的访问，这种访问可以显著的提高性能。但是在扩展类的实现、以及处理相应的实例对象和在纯 Python 中定义类是完全不同的，需要有专业的 Python/C API 的知识，不适合新手。</p><p>这也是 Cython 要增强 Python 类的原因：Cython 使得创建和操作扩展类就像操作 Python 中的类一样。在Cython中定义一个扩展类通过 cdef class 的形式，和 Python 中的常规类保持了高度的相似性。</p><blockquote><p>尽管在语法上有着相似之处，但是 cdef class 定义的类对所有方法和数据都有快速的 C 级别的访问，这也是和扩展类和 Python 中的普通类之间的一个最显著的区别。而且扩展类和 int、str、list 等内置的类都属于静态类，它们的属性是不可修改的。</p></blockquote><h1 id="2、Cython中的扩展类"><a href="#2、Cython中的扩展类" class="headerlink" title="2、Cython中的扩展类"></a>2、Cython中的扩展类</h1><p>写一个 Python 中的类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Rectangle</span>:</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, width, height</span>):</span><br><span class="line">        self.width = width</span><br><span class="line">        self.height = height</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_area</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.width * self.height</span><br></pre></td></tr></table></figure><p>这个类是在 Python 级别定义的，可以被 CPython 编译的。定义了矩形的宽和高，并且提供了一个方法，计算面积。这个类是可以动态修改的，可以指定任意的属性。</p><p>如果对这个 Python 类编译的话，那么得到的类依旧是一个纯 Python 类，而不是扩展类。所有的操作，仍然是通过动态调度通用的 Python 对象来实现的。只不过由于解释器的开销省去了，因此效率上会提升一点点，但是它无法从静态类型上获益，因为此时的 Cython 代码仍然需要在运行时动态调度来解析类型。</p><p>改成扩展类的话，需要这么做。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pyx</span></span><br><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Rectangle</span>:</span><br><span class="line"></span><br><span class="line">    cdef long width, height</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, w, h</span>):</span><br><span class="line">        self.width = w</span><br><span class="line">        self.height = h</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_area</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.width * self.height</span><br></pre></td></tr></table></figure><p>此时的关键字使用的是<code>cdef class</code>，意思就是表示这个类不是一个普通的 Python 类，而是一个扩展类。内部代码，多了一个 <code>cdef long width, height</code>，这个是名称和 self 的属性是同名的，表示 self 中的 width、height 都必须是一个 long，或者说可以转为 C 中的 long 的 Python 对象。另外<strong>对于 cdef 来说，定义的类是可以被外部访问的，虽然函数不行、但类可以</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line">rect = cython_test.Rectangle(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(rect.get_area())  <span class="comment"># 12</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    rect = cython_test.Rectangle(<span class="string">&quot;3&quot;</span>, <span class="string">&quot;4&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> TypeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># an integer is required</span></span><br></pre></td></tr></table></figure><p>注意：在 <code>__init__</code> 中实例化的属性，都必须在类中使用 cdef 声明，举个栗子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Rectangle</span>:</span><br><span class="line"><span class="comment"># 这里我们只声明了width, 没有声明height, 那么是不是意味着这个height可以接收任意对象呢？</span></span><br><span class="line">    cdef long width</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, w, h</span>):</span><br><span class="line">        self.width = w</span><br><span class="line">        self.height = h</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_area</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.width * self.height</span><br><span class="line"> <span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">rect = cython_test.Rectangle(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">File &quot;cython_test.pyx&quot;, line 7, in cython_test.Rectangle.__init__</span></span><br><span class="line"><span class="string">    self.height = h</span></span><br><span class="line"><span class="string">AttributeError: &#x27;cython_test.Rectangle&#x27; object has no attribute &#x27;height&#x27;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>凡是在没有在 cdef 中声明的，都不可以赋值给 self</strong>，可能有人发现了这不是访问，而是添加呀。添加一个属性咋啦，没咋，无论是获取还是赋值，<strong>self 中的属性必须使用 cdef 在类中声明</strong>。举一个Python 内置类型的例子吧：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">1</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    a.xx = <span class="number">123</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># &#x27;int&#x27; object has no attribute &#x27;xx&#x27;</span></span><br></pre></td></tr></table></figure><p><strong>一样等价，扩展类和内建的类是同级别的，一个属性如果想通过 <code>self.</code> 的方式来调用，那么一定要在类里面通过 cdef 声明。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Rectangle</span>:</span><br><span class="line">    cdef long width, height</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, w, h</span>):</span><br><span class="line">        self.width = w</span><br><span class="line">        self.height = h</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_area</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.width * self.height</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">rect = cython_test.Rectangle(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    rect.a = <span class="string">&quot;xx&quot;</span></span><br><span class="line"><span class="keyword">except</span> AttributeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># &#x27;cython_test.Rectangle&#x27; object has no attribute &#x27;a&#x27;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">如果想动态修改、添加类型，那么需要解释器在解释的时候来动态操作</span></span><br><span class="line"><span class="string">但扩展类和内置的类是等价的，直接指向了C一级的结构，不需要解释器解释这一步，因此也失去了动态修改的能力</span></span><br><span class="line"><span class="string">也正因为如此，也能提高效率。因为很多时候，我们不需要动态修改。</span></span><br><span class="line"><span class="string">当一个类实例化之后，会给实例对象一个属性字典，通过__dict__获取，它的所有属性以及相关的值都会存储在这里</span></span><br><span class="line"><span class="string">其实获取一个实例对象的属性，本质上是从属性字典里面获取，instance.attr 等价于instance.__dict__[&quot;attr&quot;]，同理修改、创建也是。</span></span><br><span class="line"><span class="string">但是注意：这只是针对普通的 Python 类而言，但扩展类的实例对象内部是没有 __dict__ 的。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    rect.__dict__</span><br><span class="line"><span class="keyword">except</span> AttributeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># &#x27;cython_test.Rectangle&#x27; object has no attribute &#x27;__dict__&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不光 __dict__, 你连 self 本身的属性都无法访问</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    rect.width</span><br><span class="line"><span class="keyword">except</span> AttributeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># &#x27;cython_test.Rectangle&#x27; object has no attribute &#x27;width&#x27;</span></span><br><span class="line"><span class="comment"># 提示我们 self 没有 width 属性，所以我们实例化之后再想修改是不行的，连获取都获取不到</span></span><br><span class="line"><span class="comment"># 只能调用它的一些方法罢了。</span></span><br></pre></td></tr></table></figure><p>所以内建的类和扩展类是完全类似的，其实例对象都没有属性字典，至于类本身是有属性字典的，但是这个字典不可修改。因为虽然叫属性字典，但它的类型实际上是一个 mappingproxy。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">int</span>.__dict__[<span class="string">&quot;a&quot;</span>] = <span class="number">123</span></span><br><span class="line"><span class="keyword">except</span> TypeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># &#x27;mappingproxy&#x27; object does not support item assignment</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cython_test.Rectangle.__dict__[<span class="string">&quot;a&quot;</span>] = <span class="number">123</span></span><br><span class="line"><span class="keyword">except</span> TypeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># &#x27;mappingproxy&#x27; object does not support item assignment</span></span><br></pre></td></tr></table></figure><p>还是那句话，动态设置、修改、获取、删除属性，这些都是在解释器解释字节码的时候动态操作的，在解释的时候是允许你做一些这样的骚操作的。但是内置的类和扩展类是不需要解释这一步的，它们是彪悍的人生，直接指向了 C 一级的数据结构，因此也就丧失了这种动态的能力。</p><p>但是扩展类毕竟是自己指定的，如果想修改 self 的一些属性呢？答案是将其暴露给外界即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Rectangle</span>:</span><br><span class="line">    <span class="comment"># 通过cdef public的方式进行声明即可</span></span><br><span class="line">    <span class="comment"># 这样的话就会暴露给外界了</span></span><br><span class="line">    cdef public long width, height</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, w, h</span>):</span><br><span class="line">        self.width = w</span><br><span class="line">        self.height = h</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_area</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.width * self.height</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">rect = cython_test.Rectangle(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(rect.width)  <span class="comment"># 3</span></span><br><span class="line"><span class="built_in">print</span>(rect.get_area())  <span class="comment"># 12</span></span><br><span class="line"></span><br><span class="line">rect.width = <span class="number">123</span></span><br><span class="line"><span class="built_in">print</span>(rect.get_area())  <span class="comment"># 492</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    rect.__dict__</span><br><span class="line"><span class="keyword">except</span> AttributeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># &#x27;cython_test.Rectangle&#x27; object has no attribute &#x27;__dict__&#x27;</span></span><br><span class="line"><span class="comment"># 属性字典依旧是没有的</span></span><br></pre></td></tr></table></figure><p>通过 <code>cdef public</code>声明的属性，是可以被外界获取并修改的，除了<code>cdef public</code>之外还有 <code>cdef readonly</code>，同样会将属性暴露给外界，但是只能访问不能修改。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">rect = cython_test.Rectangle(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(rect.width)  <span class="comment"># 3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    rect.width = <span class="number">123</span></span><br><span class="line"><span class="keyword">except</span> AttributeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)  <span class="comment"># attribute &#x27;width&#x27; of &#x27;cython_test.Rectangle&#x27; objects is not writable</span></span><br></pre></td></tr></table></figure><ul><li><code>cdef readonly 类型 变量名：实例属性可以被访问，但是不可以被修改</code></li><li><code>cdef public 类型 变量名：实例属性可以被访问，也可以被修改</code></li><li><code>cdef 类型 变量名：实例属性既不可以被访问，更不可以被修改</code></li></ul><p>当然定义变量无论是使用 <code>cdef public</code>还是 <code>cdef readonly</code>，Cython 内部的方法都可以实行快速访问，因为扩展类的方法基本上忽略了 readonly 和 public 的声明，它们存在的目的只是为了控制来自外界的访问。</p><h1 id="3、C一级的构造函数和析构函数"><a href="#3、C一级的构造函数和析构函数" class="headerlink" title="3、C一级的构造函数和析构函数"></a>3、C一级的构造函数和析构函数</h1><p>每一个实例对象都对应了一个 C 结构体，其指针就是 Python 调用<code>__init__</code>函数里面的 self 参数。当<code>__init__</code>参数被调用时，会初始化 self 参数上的属性，而且<code>__init__</code>参数是自动调用的。但是在 <code>__init__</code>参数调用之前，会先调用<code>__new__</code>方法， <code>__new__</code>方法的作用就是为创建的实例对象开辟一份内存，然后返回其指针并交给 self。在 C 级别就是，在调用<code>__init__</code>之前，实例对象指向的结构体必须已经分配好内存，并且所有结构字段都处于可以接收初始值的有效状态。</p><p>Cython 扩充了一个名为<code>__cinit__</code>的特殊方法，用于执行 C 级别的内存分配和初始化。不过对于之前定义的 Rectangle 类的 <code>__init__</code>方法，因为内部的字段接收的值是两个 double，不需要 C 级别的内存分配。但如果需要 C 级别的内存分配，那么就不可以使用 <code>__init__</code>了，而是需要使用 <code>__cinit__</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入相关函数，malloc，free</span></span><br><span class="line"><span class="comment"># 如果不熟悉的话，建议去了解一下C语言</span></span><br><span class="line"><span class="keyword">from</span> libc.stdlib cimport malloc, free</span><br><span class="line"></span><br><span class="line">cdef <span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line">    cdef:</span><br><span class="line">        unsigned <span class="built_in">int</span> n</span><br><span class="line">        double *array  <span class="comment"># 一个数组，存储了double类型的变量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__cinit__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.n = n</span><br><span class="line">        <span class="comment"># 在C一级进行动态分配内存</span></span><br><span class="line">        self.array = &lt;double *&gt;malloc(n * sizeof(double))</span><br><span class="line">        <span class="keyword">if</span> self.array == NULL:</span><br><span class="line">            <span class="keyword">raise</span> MemoryError()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__dealloc__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;如果进行了动态内存分配，也就是定义了 __cinit__，那么必须要定义 __dealloc__</span></span><br><span class="line"><span class="string">        否则在编译的时候会抛出异常：Storing unsafe C derivative of temporary Python reference</span></span><br><span class="line"><span class="string">        然后我们释放掉指针指向的内存</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.array != NULL:</span><br><span class="line">            free(self.array)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_value</span>(<span class="params">self</span>):</span><br><span class="line">        cdef long i</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n):</span><br><span class="line">            self.array[i] = (i + <span class="number">1</span>) * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_value</span>(<span class="params">self</span>):</span><br><span class="line">        cdef long i</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n):</span><br><span class="line">            <span class="built_in">print</span>(self.array[i])</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">a = cython_test.A(<span class="number">5</span>)</span><br><span class="line">a.set_value()</span><br><span class="line">a.get_value()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">2.0</span></span><br><span class="line"><span class="string">4.0</span></span><br><span class="line"><span class="string">6.0</span></span><br><span class="line"><span class="string">8.0</span></span><br><span class="line"><span class="string">10.0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>所以<code>__cinit__</code>是用来进行 C 一级内存的动态分配的，另外如果在<code>__cinit__</code>通过 malloc 进行了内存分配，那么必须要定义<code>__dealloc__</code>函数将指针指向的内存释放掉。当然即使不释放也没关系，只不过可能发生内存泄露（雾），但是<code>__dealloc__</code>这个函数是必须要被定义，它会在实例对象回收时被调用。</p><p>这个时候可能有人好奇了，那么 <code>__cinit__</code>和<code>__init__</code> 函数有什么区别呢？</p><p>首先它们只能通过 def 来定义，另外在不涉及 malloc 动态分配内存的时候， <code>__cinit__</code> 和<code>__init__</code>是等价的。然而一旦涉及到 malloc，那么动态分配内存只能在 <code>__cinit__</code>中进行，如果这个过程写在了<code>__init__</code>函数中，比如将上面例子的<code>__cinit__</code>改为 <code>__init__</code>的话，你会发现 self 的所有变量都没有设置进去、或者说设置失败，并且其它的方法若是引用了 self.array，那么还会导致丑陋的段错误。</p><p>还有一点就是，<code>__cinit__</code> 函数会在 <code>__init__</code>函数之前调用，实例化一个扩展类的时候，参数会先传递给 <code>__cinit__</code>，然后 <code>__cinit__</code>再将接收到的参数原封不动的传递给<code>__init__</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line">    cdef public:</span><br><span class="line">        unsigned <span class="built_in">int</span> a, b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__cinit__</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;__cinit__&quot;</span>)</span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line">        <span class="built_in">print</span>(self.a, self.b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c, d</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;__cinit__ 中接收两个参数</span></span><br><span class="line"><span class="string">        然后会将参数原封不动的传递到这里，所以这里也要接收两个参数</span></span><br><span class="line"><span class="string">        参数名可以不一致，但是个数要匹配</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;__init__&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(c, d)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">a = cython_test.A(<span class="number">111</span>, <span class="number">222</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__cinit__</span></span><br><span class="line"><span class="string">111 222</span></span><br><span class="line"><span class="string">__init__</span></span><br><span class="line"><span class="string">111 222</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(a.a)  <span class="comment"># 111</span></span><br><span class="line"><span class="built_in">print</span>(a.b)  <span class="comment"># 222</span></span><br></pre></td></tr></table></figure><p>注意：<code>__cinit__</code> 只有在涉及 C 级别内存分配的时候才会出现，如果没有涉及那么使用 <code>__init__</code> 就可以，虽然在不涉及 malloc 的时候这两者是等价的，但是 <code>__cinit__</code>会比 <code>__init__</code> 的开销要大一些。而如果涉及 C 级别内存分配，那么建议 <code>__cinit__</code> 只负责内存的动态分配，<code>__init__</code>负责属性的创建。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> libc.stdlib cimport malloc, free</span><br><span class="line"></span><br><span class="line">cdef <span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    cdef public:</span><br><span class="line">        unsigned <span class="built_in">int</span> a, b, c</span><br><span class="line">    <span class="comment"># 这里的 array 不可以使用 public 或者 readonly</span></span><br><span class="line">    <span class="comment"># 原因很简单，因为一旦指定了 public 和 readonly，就意味着这些属性是可以被 Python 访问的</span></span><br><span class="line">    <span class="comment"># 所以需要其能够转化为 Python 中的对象，而 C 中的指针，除了 char *, 都是不能转化为 Python 对象的</span></span><br><span class="line">    <span class="comment"># 因此这里的 array 一定不能暴露给外界，否则编译出错，提示我们：double * 无法转为 Python 对象</span></span><br><span class="line">    cdef double *array</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__cinit__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 这里面只做内存分配，设置属性交给__init__</span></span><br><span class="line">        self.array = &lt;<span class="built_in">int</span> *&gt;malloc(<span class="number">3</span> * sizeof(<span class="built_in">int</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, a, b, c</span>):</span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line">        self.c = c </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__dealloc__</span>(<span class="params">self</span>):</span><br><span class="line">        free(self.array)</span><br></pre></td></tr></table></figure><h1 id="4、cdef和cpdef方法"><a href="#4、cdef和cpdef方法" class="headerlink" title="4、cdef和cpdef方法"></a>4、cdef和cpdef方法</h1><p>之前使用了 cdef 和 cpdef，<strong>cdef 可以定义变量和函数，但是不能被 Python 直接访问；可以定义一个类，能直接被外界访问</strong>。而 cpdef 专门用于定义函数，cpdef 定义的函数既可以在 Cython 内部访问，也可以被外界访问，因为它定义了两个版本的函数：一个是高性能的纯C版本（此时等价于 cdef，至于为什么高效，因为它是 C 一级的，直接指向了具体数据结构，当然还有其它原因，），另一个是 Python 包装器（相当于手动定义的 Python 函数），所以还要求使用cpdef定义的函数的参数和返回值类型必须是 Python 可以表示的，像 char * 之外的指针就不行。</p><p>那么同理它们也可以作用于方法，当然方法也是实例对象在获取函数的时候进行封装得到的，所以一样的道理。但是注意：<strong>cdef 和 cpdef 修饰的 cdef class 定义的静态类里面的方法，如果是 class 定义的纯 Python 类，那么内部是不可以出现 cdef 或者 cpdef 的。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    cdef public:</span><br><span class="line">        long a, b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line"></span><br><span class="line">    cdef long f1(self):</span><br><span class="line">        <span class="keyword">return</span> self.a * self.b</span><br><span class="line"></span><br><span class="line">    cpdef long f2(self):</span><br><span class="line">        <span class="keyword">return</span> self.a * self.b</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">a = cython_test.A(<span class="number">11</span>, <span class="number">22</span>)</span><br><span class="line"><span class="built_in">print</span>(a.f2())  <span class="comment"># 242</span></span><br><span class="line">a.f1()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    a.f1()</span></span><br><span class="line"><span class="string">AttributeError: &#x27;cython_test.A&#x27; object has no attribute &#x27;f1&#x27;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>cdef 和 cpdef 之间在函数上的差异，在方法中得到了同样的体现。</strong></p><p>此外，这个类的实例也可以作为函数的参数，这个是肯定的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    cdef public:</span><br><span class="line">        long a, b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line"></span><br><span class="line">    cpdef long f2(self):</span><br><span class="line">        <span class="keyword">return</span> self.a * self.b</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">self_lst</span>):</span><br><span class="line">    s = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> self <span class="keyword">in</span> self_lst:</span><br><span class="line">        s += self.f2()</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"> </span><br><span class="line">    </span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">a1 = cython_test.A(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">a2 = cython_test.A(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">a3 = cython_test.A(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(cython_test.func([a1, a2, a3]))  <span class="comment"># 16</span></span><br></pre></td></tr></table></figure><p>这是 Python 的特性，一切都是对象，尽管没有指明 self_lst 是什么类型，但只要它可以被 for 循环即可；尽管没有指明 self_lst 里面的元素是什么类型，只要它有 f2 方法即可。并且这里的 func 可以在 Cython 中定义，同样可以在 Python 中定义，这两者是没有差别的，因为都是 Python 中的函数。另外在遍历的时候仍然需要确定这个列表里面的元素是什么，意味着列表里面的元素仍然是 PyObject *，它需要获取类型、转化、属性查找，因为 Cython 不知道类型是什么、导致其无法优化。但如果规定了类型，那么再调用 f2 的时候，那么会直接指向 C 一级的数据结构，因此不需要那些无用的检测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    cdef public:</span><br><span class="line">        long a, b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line"></span><br><span class="line">    cpdef long f2(self):</span><br><span class="line">        <span class="keyword">return</span> self.a * self.b</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># 规定接收一个 list，返回一个 long, 它们都是静态的，总之静态类型定义越多速度会越快</span></span><br><span class="line">cpdef long func(<span class="built_in">list</span> self_lst):</span><br><span class="line">    <span class="comment"># 声明 long 类型的 s，A 类型的 self</span></span><br><span class="line">    <span class="comment"># 我们下面使用的是 s = s + self.f2(), 所以这里的s要赋一个初始值0</span></span><br><span class="line">    cdef long s = <span class="number">0</span></span><br><span class="line">    cdef A self</span><br><span class="line">    <span class="keyword">for</span> self <span class="keyword">in</span> self_lst:</span><br><span class="line">        s += self.f2()</span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><p>调用得到的结果是一样的。这样的话速度会变快很多，因为在循环的时候，规定了变量类型，并且求和也是一个只使用 C 的操作，因为 s 是一个 double。</p><p>这个版本的速度比之前快了 10 倍，这表明<strong>类型化比非类型化要快了 10 倍。</strong>如果删除了 <code>cdef A self</code>，也就是不规定其类型，而还是按照 Python 的语义来调用，那么速度仍然和之前一样，即便使用 cpdef 定义。所以重点在于指定类型为静态类型，只要规定好类型，那么就可以提升速度；而 Cython 是为 Python 服务的，肯定要经常使用 Python 的类型，那么提前规定好、让其指向 C 一级的数据结构，速度会提升很多。如果是 int 和 float，那么就使用 C 中的 long 和 double，这样速度就更加快速了，当然即便用 Python 的 int 和 float 依旧可以起到加速的效果，只不过没有C明显。因此重点是一定要静态定义类型，只要类型明确那么就能进行大量的优化。</p><p>Python 慢有很多原因，其中一个原因就是它无法对类型进行优化，以及对象分配在堆上。无法基于类型进行优化，就意味着每次都要进行大量的检测，当然这些前面已经说过了，如果规定好类型，那么就不用兜那么大圈子了；而对象分配在堆上这是无法避免的，只要你用 Python 的对象，都是分配在堆上，所以对于整型和浮点型，通过定义为 C 的类型使其分配在栈上，能够更加的提升速度。总之记住一句话：<strong>Cython 加速的关键就在于，类型的静态声明，以及对整数和浮点使用 C 中 long 和 double。</strong></p><blockquote><p>当然，虽说如此，但是该使用 Python 中对象就使用 Python 的对象，基于类型优化其实是可以获得相当可观的速度的。至于要不要通过C的类型（比如使用结构体、共同体这种复杂类型）进行更深一步的优化，就看你对 Cython 的掌握程度了。</p></blockquote><p>在上面的基础上，如果将 cpdef 改成 cdef 那么效率会再次提升，原因很简单，因为 def 和 cpdef 都是支持外部 Python 访问的；而 cdef 只支持内部 Cython 访问，那么它就只指向了一个 C 级的数据结构，但是 def 和 cpdef 都涉及到 Python 函数，而我们说 Python 函数比 C 函数开销要大的。当然 cdef 的缺点就是外部无法访问，而且函数调用需要的开销基本可以忽略不计的。</p><h2 id="4-1-方法中给参数指定类型"><a href="#4-1-方法中给参数指定类型" class="headerlink" title="4.1 方法中给参数指定类型"></a>4.1 方法中给参数指定类型</h2><p>无论是 def、cdef、cpdef，都可以给参数规定类型，如果类型传递的不对就会报错。比如：上面的 func 函数如果是普通的 Python 函数，那么内部的参数对于 Python 而言只要能够被 for 循环即可，所以它可以是列表、元组、集合。但是上面的 func 规定了类型，参数只能传递 list 对象或者其子类的实例对象，如果传递 tuple 对象就会报错。</p><p>然后我们来看看<code>__init__</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    cdef public:</span><br><span class="line">        long a, b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, <span class="built_in">float</span> a, <span class="built_in">float</span> b</span>):</span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br></pre></td></tr></table></figure><p>这里规定了类型，但是有没有发现什么问题呢？这里我们的参数 a 和 b 必须是一个 float，如果传递的是其它类型会报错，但是赋值的时候 self.a 和 self.b 又需要接收一个 long，所以这是一个自相矛盾的死结，在编译的时候就会报错。所以给<code>__init__</code>参数传递的值的类型要和类中 cdef 声明的类型保持一致。</p><p>然后为了更好地解释 Cython 带来的性能改进，需要了解关于继承、子类化、和扩展类型的多态性的基础知识。</p><h1 id="5、继承和子类化"><a href="#5、继承和子类化" class="headerlink" title="5、继承和子类化"></a>5、继承和子类化</h1><p>扩展类型<strong>只能继承单个基类，并且继承的基类必须是直接指向 C 实现的类型</strong>（可以是使用 cdef class 定义的扩展类型，也可以是内置类型，因为内置类型也是直接指向 C 一级的结构）。如果基类是常规的 Python 类（需要在运行时经过解释器动态解释才能指向 C 一级的结构），或者继承了多个基类，那么 Cython 在编译时会抛出异常。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Girl</span>:</span><br><span class="line">    cdef public:</span><br><span class="line">        <span class="built_in">str</span> name</span><br><span class="line">        long age</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age</span>):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line"></span><br><span class="line">    cpdef <span class="built_in">str</span> get_info(self):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;name: <span class="subst">&#123;self.name&#125;</span>, age: <span class="subst">&#123;self.age&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cdef <span class="keyword">class</span> <span class="title class_">CGirl</span>(<span class="title class_ inherited__">Girl</span>):</span><br><span class="line"></span><br><span class="line">    cdef public <span class="built_in">str</span> where</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age, where</span>):</span><br><span class="line">        self.where = where</span><br><span class="line">        <span class="built_in">super</span>().__init__(name, age)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PyGirl</span>(<span class="title class_ inherited__">Girl</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age, where</span>):</span><br><span class="line">        self.where = where</span><br><span class="line">        <span class="built_in">super</span>().__init__(name, age)</span><br></pre></td></tr></table></figure><p>定义了一个扩展类（Girl），然后让另一个扩展类（CGirl）和普通的 Python 类（PyGirl）都去继承它。<strong>扩展类不可以继承 Python 类，但 Python 类是可以继承扩展类的</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">c_girl = cython_test.CGirl(<span class="string">&quot;cython&quot;</span>, <span class="number">17</span>, <span class="string">&quot;python&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(c_girl.get_info())  <span class="comment"># name: cython, age: 17</span></span><br><span class="line"></span><br><span class="line">py_girl  = cython_test.PyGirl(<span class="string">&quot;cython&quot;</span>, <span class="number">17</span>, <span class="string">&quot;python&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(py_girl .get_info())  <span class="comment"># name: cython, age: 17</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c_girl.where)  <span class="comment"># python</span></span><br><span class="line"><span class="built_in">print</span>(py_girl.where)  <span class="comment"># python</span></span><br></pre></td></tr></table></figure><p>对于扩展类和普通的 Python 类，它们都是可以继承扩展类的。</p><p>继承的话，会有什么样的结果呢？cdef定义的方法和函数一样，无法被外部的Python访问，那么内部的Python类在继承的时候可不可以访问呢？以及私有属性呢？</p><p>先来看看 Python 中关于私有属性的例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.__name = <span class="string">&quot;xxx&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__foo</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.__name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>(<span class="title class_ inherited__">A</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.__name</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(e)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.__foo()</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(e)</span><br><span class="line"></span><br><span class="line">B().test()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#x27;B&#x27; object has no attribute &#x27;_B__name&#x27;</span></span><br><span class="line"><span class="string">&#x27;B&#x27; object has no attribute &#x27;_B__foo&#x27;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>定义的私有属性只能在当前类里面使用，一旦出去了就不能够再访问了。其实私有属性本质上只是 Python 给你改了个名字，在原来的名字前面加上一个 <code>_类名</code>，所以 <code>__name</code>和<code>__foo</code>其实相当于是 <code>_A__name</code>和 <code>_A__foo</code>。但是当在外部用实例属性去获取<code>__name</code> 和<code>__foo</code>的时候，获取的就是<code>__name</code>和<code>__foo</code>，而显然 A 里面没有这两个属性，因此报错。解决的办法就是通过调用<code>_A__name</code> 和<code>_A__foo</code>，但是不建议这么做，因为这是私有变量，如果非要访问的话，那就不要定义成私有的。如果是在 A 这个类里面调用的话，那么 Python 解释器也会自动为我们加上 <code>_类名</code> 这个前缀，在类里面调用 <code>self.__name</code>的时候，实际上调用的也是<code>self._A__name</code> 私有属性，但是在外部就不会了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> _A__name = <span class="string">&quot;cython&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.name = __name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是不是很神奇呢? 因为在类里面, __name 等价于 _A__name</span></span><br><span class="line"><span class="built_in">print</span>(A().name)  <span class="comment"># cython</span></span><br></pre></td></tr></table></figure><p>如果是继承的话，通过报错信息也知道原因。B也是一个类，那么在 B 里面调用私有属性，同样会加上 <code>_类名</code> 这个前缀，但是这个类名显然是 B 的类名，不是 A 的类名，因此找不到 <code>_B__name</code>和 <code>_B__foo</code>，当然我们强制通过<code>_A__name</code> 和<code>_A__foo</code>也是可以访问的，只是不建议这么做。</p><p>因此 Python 中不存在绝对的私有，只不过是解释器内部偷梁换柱将你的私有属性换了个名字罢了，但是可以认为它是私有的，因为按照原本的逻辑没有办法访问。同理继承的子类，也没有办法使用父类的私有属性。</p><p>但是在 Cython 中是不是这样子呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Person</span>:</span><br><span class="line">    cdef public:</span><br><span class="line">        long __age</span><br><span class="line">        <span class="built_in">str</span> __name</span><br><span class="line">        long length</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age, length</span>):</span><br><span class="line">        self.__age = age</span><br><span class="line">        self.__name = name</span><br><span class="line">        self.length = length</span><br><span class="line"></span><br><span class="line">    cdef <span class="built_in">str</span> __get_info(self):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;name: <span class="subst">&#123;self.__name&#125;</span>, age: <span class="subst">&#123;self.__age&#125;</span>, length: <span class="subst">&#123;self.length&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">    cdef <span class="built_in">str</span> get_info(self):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;name: <span class="subst">&#123;self.__name&#125;</span>, age: <span class="subst">&#123;self.__age&#125;</span>, length: <span class="subst">&#123;self.length&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cdef <span class="keyword">class</span> <span class="title class_">CGirl</span>(<span class="title class_ inherited__">Person</span>):</span><br><span class="line">    cpdef test1(self):</span><br><span class="line">        <span class="built_in">print</span>(self.__name, self.__age, self.length)</span><br><span class="line"></span><br><span class="line">    cpdef test2(self):</span><br><span class="line">        <span class="built_in">print</span>(self.__get_info())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PyGirl</span>(<span class="title class_ inherited__">Person</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test1</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(self.length)</span><br><span class="line">        <span class="built_in">print</span>(self.__name, self.__age)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test2</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(self.__get_info())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test3</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(self.get_info())</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c_g = cython_test.CGirl(<span class="string">&quot;古明地觉&quot;</span>, <span class="number">17</span>, <span class="number">156</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c_g.test1()</span><br><span class="line">古明地觉 <span class="number">17</span> <span class="number">156</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c_g.test2()</span><br><span class="line">name: 古明地觉, age: <span class="number">17</span>, length: <span class="number">156</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>py_g = cython_test.PyGirl(<span class="string">&quot;古明地觉&quot;</span>, <span class="number">17</span>, <span class="number">156</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>py_g.test1()</span><br><span class="line"><span class="number">156</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">  File <span class="string">&quot;cython_test.pyx&quot;</span>, line <span class="number">31</span>, <span class="keyword">in</span> cython_test.PyGirl.test1</span><br><span class="line">    <span class="built_in">print</span>(self.__name, self.__age)</span><br><span class="line">AttributeError: <span class="string">&#x27;PyGirl&#x27;</span> <span class="built_in">object</span> has no attribute <span class="string">&#x27;_PyGirl__name&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>py_g.test2()</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">  File <span class="string">&quot;cython_test.pyx&quot;</span>, line <span class="number">34</span>, <span class="keyword">in</span> cython_test.PyGirl.test2</span><br><span class="line">    <span class="built_in">print</span>(self.__get_info())</span><br><span class="line">AttributeError: <span class="string">&#x27;PyGirl&#x27;</span> <span class="built_in">object</span> has no attribute <span class="string">&#x27;_PyGirl__get_info&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>py_g.test3()</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">  File <span class="string">&quot;cython_test.pyx&quot;</span>, line <span class="number">37</span>, <span class="keyword">in</span> cython_test.PyGirl.test3</span><br><span class="line">    <span class="built_in">print</span>(self.get_info())</span><br><span class="line">AttributeError: <span class="string">&#x27;PyGirl&#x27;</span> <span class="built_in">object</span> has no attribute <span class="string">&#x27;get_info&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure><p>看到对于 Cython 定义的 C 一级的类而言，<strong>私有属性、私有方法可以一并使用</strong>。但是对于纯 Python 类就不行了，私有属性、私有方法 无法访问就算了，就连父类使用 cdef 定义的非私有方法也无法继承下来，原因就是 PyGirl 是一个 Python 类，不是使用 cdef class 定义的静态类。如果把父类的 cdef get_info 改成 def 或者 cpdef，那么Python子类是可以直接访问的。</p><p>我们说 cdef 定义的是 C 一级的方法，不是 Python 的方法、也不是 cpdef 定义的时候自带 Python 包装器，因此它无法被 Python 子类继承，因此它并没有跨越语言的边界。当然如果不熟悉 Cython 中的继承、并且有很想使用继承，那么就不要使用 cdef，直接使用 cpdef 定义吧（或者使用 def，只不过此时无法指定返回值类型）。虽说 cdef 只定义的 C 一级的函数调用比自带 Python 包装器的 cpdef 快，但是说实话那一点点快几乎没啥意义。Cython 加速的核心在于类型上的优化，如果我们能使用静态的方式声明，那么速度就会有明显的提升，不要为了加速反倒畏手畏脚地这不敢用那不敢用。</p><p>总之 Cython 加速记住两个原则：</p><ul><li>能使用静态声明的方式使用静态声明，不仅是变量，还有参数、返回值；</li><li>这是 Cython 默认的行为，int 和 float 使用的是 C 中 的int 和 float，但是为了支持更大的数字，我们直接使用 long 和 double 即可。</li></ul><p>但是问题来了，如果希望自定义的扩展类不可以被其它类继承的话该怎么做呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cimport cython</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 cython.final 进行装饰，那么这个类就不可被继承了</span></span><br><span class="line"><span class="meta">@cython.final</span></span><br><span class="line">cdef <span class="keyword">class</span> <span class="title class_">NotInheritable</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p><strong>通过 cython.final，那么被装饰的类就是一个不可继承类，不光是外界普通的 Python 类，内部的扩展类也是不可继承的。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>(cython_test.NotInheritable):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">TypeError: type &#x27;cython_test.NotInheritable&#x27; is not an acceptable base type</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>告诉我们 NotInheritable 不是一个合法的基类。</p><h1 id="6、类型转换"><a href="#6、类型转换" class="headerlink" title="6、类型转换"></a>6、类型转换</h1><p>Python 中类在继承扩展类的时候，无法继承其内部的 cdef 方法，但如果这个类是继承扩展类的，那么其实例对象可不可以转化为扩展类的类型呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    cdef funcA(self):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">123</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>(<span class="title class_ inherited__">A</span>):</span><br><span class="line">    <span class="comment"># 显然 func1 内部无法访问扩展类A的funcA</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">func1</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.funcA()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 但是我们在使用的时候将其类型转化一下</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">func2</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> (&lt;A&gt; self).funcA()</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = cython_test.B()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.func1()</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">  File <span class="string">&quot;cython_test.pyx&quot;</span>, line <span class="number">10</span>, <span class="keyword">in</span> cython_test.B.func1</span><br><span class="line">    <span class="keyword">return</span> self.funcA()</span><br><span class="line">AttributeError: <span class="string">&#x27;B&#x27;</span> <span class="built_in">object</span> has no attribute <span class="string">&#x27;funcA&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.func2()</span><br><span class="line"><span class="number">123</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure><p>看到 b.func2 是可以调用成功的，但知道使用 <code>&lt;&gt;</code> 这种方式如果转化不成功，那么也不会有任何影响，会保留原来值（C中的整型和浮点除外），这可能会有点危险。因此<strong>可以通过 <code>(&lt;A?&gt; self)</code>，这样 self 必须是 A 或者其子类的实例对象</strong>，否则报错。</p><h1 id="7、扩展类型对象和None"><a href="#7、扩展类型对象和None" class="headerlink" title="7、扩展类型对象和None"></a>7、扩展类型对象和None</h1><p>看一个简单的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Girl</span>:</span><br><span class="line">    cdef public:</span><br><span class="line">        <span class="built_in">str</span> name</span><br><span class="line">        long age</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age</span>):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line"></span><br><span class="line">cpdef <span class="built_in">tuple</span> dispatch(Girl g):</span><br><span class="line">    <span class="keyword">return</span> g.name, g.age</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(cython_test.dispatch(cython_test.Girl(<span class="string">&quot;python&quot;</span>, <span class="number">17</span>)))  <span class="comment"># (&#x27;python&#x27;, 17)</span></span><br><span class="line"><span class="built_in">print</span>(cython_test.dispatch(cython_test.Girl(<span class="string">&quot;cython&quot;</span>, <span class="number">16</span>)))  <span class="comment"># (&#x27;cython&#x27;, 16)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>(cython_test.Girl):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(cython_test.dispatch(B(<span class="string">&quot;mashiro&quot;</span>, <span class="number">16</span>)))  <span class="comment"># (&#x27;mashiro&#x27;, 16)</span></span><br><span class="line"></span><br><span class="line">cython_test.dispatch(<span class="built_in">object</span>())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    cython_test.dispatch(object())</span></span><br><span class="line"><span class="string">TypeError: Argument &#x27;g&#x27; has incorrect type (expected cython_test.Girl, got object)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>传递一个 Girl 或者其子类的实例对象的话是没有问题的，但是传递一个其它的则不行。</p><p>但是在 Cython 中 None 是一个例外，即使它不是 Girl 的实例对象，但也是可以传递的，除了 C 规定的类型之外，只要是 Python 的类型，不管什么，传递一个 None 都是可以的。这就类似于 C 中的空指针，任何指针都可以传递给空指针，但是没有办法做什么操作。</p><p>所以这里可以传递一个 None，但是执行逻辑的时候显然会报错。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line">cython_test.dispatch(<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>然而报错还是轻的，上面代码执行的时候会发生段错误，解释器直接异常退出了。原因就在于不安全地访问了 Girl 实例对象的成员属性，属性和方法都是 C 接口的一部分，而 Python 中 None 本质上没有 C 接口，因此访问属性或者调用方法都是无效的。为了确保这些操作的安全，最好加上一层检测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cpdef <span class="built_in">tuple</span> dispatch(Girl g):</span><br><span class="line">    <span class="keyword">if</span> g <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">&quot;...&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> g.name, g.age</span><br></pre></td></tr></table></figure><p>但是除了上面那种做法，Cython 还提供了一种特殊的语法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dispatch</span>(<span class="params">Girl g <span class="keyword">not</span> <span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> g.name, g.age</span><br></pre></td></tr></table></figure><p>此时如果我们传递了 None，那么就会报错。不过这个版本由于要预先进行类型检查，判断是否为 None，从而会牺牲一些效率。不过虽说如此，但是传递 None 所造成的段错误是非常致命的，因此非常有必要防范这一点的。当然还是那句话，虽然效率会牺牲一点点，但还是那句话，与 Cython 带来的效率提升相比，这点牺牲是非常小的，况且这也是必要的。但是注意：<strong>not None 只能出现在 def 定义的函数中，cdef 和 cpdef 是不合法的</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line">cython_test.dispatch(<span class="literal">None</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    cython_test.dispatch(None)</span></span><br><span class="line"><span class="string">TypeError: Argument &#x27;g&#x27; has incorrect type (expected cython_test.Girl, got NoneType)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>此时对 None 也是一视同仁的，传递一个 None 也是不符合类型的。这里我们设置的是 not None，但是除了 None 还能设置别的吗？答案是不行的，只能设置 None，因为 Cython 只有对 None 不会进行检测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> cpdef <span class="built_in">tuple</span> dispatch(Girl g <span class="keyword">not</span> <span class="number">123</span>):</span><br><span class="line">                       ^</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">cython_test.pyx:<span class="number">11</span>:<span class="number">24</span>: Expected <span class="string">&#x27;None&#x27;</span></span><br></pre></td></tr></table></figure><p>许多人认为需要 not None 字句是不方便的，这个特性经常被争论，但幸运的是，在函数的参数声明中使用 not None 是非常方便的。</p><p>为了更高的性能，Cython 还提供了一个默认的 nonecheck 编译器指令，可以对整个扩展模块不进行检查，通过在文件的开头加上一个注释：<code># cython: nonecheck=True</code>。</p><h1 id="8、Cython中扩展类的property"><a href="#8、Cython中扩展类的property" class="headerlink" title="8、Cython中扩展类的property"></a>8、Cython中扩展类的property</h1><p>Python 中的 property 非常的易用且强大，可以精确地控制某个属性的修改，而 Cython 也是支持 property 描述符的，但是方式有些不一样。不过在介绍 Cython 的 property 之前，来看看 Python 中的 property。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Girl</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.name = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">x</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 不需要我们对x进行调用，直接通过self.x即可获取返回值</span></span><br><span class="line">        <span class="comment"># 让函数像属性一样直接获取</span></span><br><span class="line">        <span class="keyword">return</span> self.name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @x.setter</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">x</span>(<span class="params">self, value</span>):</span><br><span class="line">        <span class="comment"># 当我们self.x = &quot;cython&quot;的时候，会调用这个函数</span></span><br><span class="line">        <span class="comment"># &quot;cython&quot;就会传递给这里的value</span></span><br><span class="line">        self.name = value</span><br><span class="line"></span><br><span class="line"><span class="meta">    @x.deleter</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">x</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 执行del self.x的时候，就会调用这个函数</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;被调用了&quot;</span>)</span><br><span class="line">        <span class="keyword">del</span> self.name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">girl = Girl()</span><br><span class="line"><span class="built_in">print</span>(girl.x)  <span class="comment"># None</span></span><br><span class="line">girl.x = <span class="string">&quot;cython&quot;</span></span><br><span class="line"><span class="built_in">print</span>(girl.x)  <span class="comment"># cython</span></span><br><span class="line"><span class="keyword">del</span> girl.x  <span class="comment"># 被调用了</span></span><br></pre></td></tr></table></figure><p>这里是通过装饰器的方式实现的，三个函数都是一样的名字，除了使用装饰器，还可以这么做。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Girl</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.name = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fget</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fset</span>(<span class="params">self, value</span>):</span><br><span class="line">        self.name = value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fdel</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;被调用了&quot;</span>)</span><br><span class="line">        <span class="keyword">del</span> self.name</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 传递三个函数即可，除此之外还有一个doc属性</span></span><br><span class="line">    x = <span class="built_in">property</span>(fget, fset, fdel, doc=<span class="string">&quot;这是property&quot;</span>)</span><br><span class="line"></span><br><span class="line">girl = Girl()</span><br><span class="line"><span class="built_in">print</span>(girl.x)  <span class="comment"># None</span></span><br><span class="line">girl.x = <span class="string">&quot;cython&quot;</span></span><br><span class="line"><span class="built_in">print</span>(girl.x)  <span class="comment"># cython</span></span><br><span class="line"><span class="keyword">del</span> girl.x  <span class="comment"># 被调用了</span></span><br></pre></td></tr></table></figure><p>所以 property 就是像访问属性一样访问函数，那么它内部是怎么做到的呢？不用想，肯定是通过描述符。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyProperty</span>:  <span class="comment"># 模仿类property，实现与其一样的功能</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, fget=<span class="literal">None</span>, fset=<span class="literal">None</span>, fdel=<span class="literal">None</span>, doc=<span class="literal">None</span></span>):</span><br><span class="line">        self.fget = fget</span><br><span class="line">        self.fset = fset</span><br><span class="line">        self.fdel = fdel</span><br><span class="line">        self.doc = doc</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__get__</span>(<span class="params">self, instance, owner</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fget(instance)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__set__</span>(<span class="params">self, instance, value</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fset(instance, value)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__delete__</span>(<span class="params">self, instance</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fdel(instance)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setter</span>(<span class="params">self, func</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>(self)(self.fget, func, self.fdel, self.doc)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">deleter</span>(<span class="params">self, func</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>(self)(self.fget, self.fset, func, self.doc)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Girl1</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.name = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @MyProperty</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">x</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @x.setter</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">x</span>(<span class="params">self, value</span>):</span><br><span class="line">        self.name = value</span><br><span class="line"></span><br><span class="line"><span class="meta">    @x.deleter</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">x</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;被调用了&quot;</span>)</span><br><span class="line">        <span class="keyword">del</span> self.name</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Girl2</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.name = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fget</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fset</span>(<span class="params">self, value</span>):</span><br><span class="line">        self.name = value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fdel</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;被调用了&quot;</span>)</span><br><span class="line">        <span class="keyword">del</span> self.name</span><br><span class="line"></span><br><span class="line">    x = MyProperty(fget, fset, fdel)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">girl1 = Girl1()</span><br><span class="line"><span class="built_in">print</span>(girl1.x)  <span class="comment"># None</span></span><br><span class="line">girl1.x = <span class="string">&quot;cython&quot;</span></span><br><span class="line"><span class="built_in">print</span>(girl1.x)  <span class="comment"># cython</span></span><br><span class="line"><span class="keyword">del</span> girl1.x  <span class="comment"># 被调用了</span></span><br><span class="line"></span><br><span class="line">girl2 = Girl2()</span><br><span class="line"><span class="built_in">print</span>(girl2.x)  <span class="comment"># None</span></span><br><span class="line">girl2.x = <span class="string">&quot;cython&quot;</span></span><br><span class="line"><span class="built_in">print</span>(girl2.x)  <span class="comment"># cython</span></span><br><span class="line"><span class="keyword">del</span> girl2.x  <span class="comment"># 被调用了</span></span><br></pre></td></tr></table></figure><p>通过描述符的方式手动实现了一个 property 的功能，描述符事实上在 Python 解释器的层面也用的非常多，实例调用方法的时候，第一个参数 self 会自动传递也是通过描述符实现的。所以描述符不光在 Python 的层面用，在解释器的层面上也大量使用描述符。同理字典也是如此，定义的类的实例对象的属性都是存在一个字典里面的，称之为属性字典，所以字典在 Python 中是经过高度优化的，原因就是不仅我们在用，底层也在大量使用。</p><p>下面来看看Cython中的property</p><p>针对扩展类的 property，Cython 有着不同的语法，但是实现了相同的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Girl</span>:</span><br><span class="line">    cdef <span class="built_in">str</span> name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.name = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">property</span> x:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__get__</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="keyword">return</span> self.name</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__set__</span>(<span class="params">self, value</span>):</span><br><span class="line">            self.name = value</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">g = cython_test.Girl()</span><br><span class="line"><span class="built_in">print</span>(g.x)  <span class="comment"># None</span></span><br><span class="line">g.x = <span class="string">&quot;cython&quot;</span></span><br><span class="line"><span class="built_in">print</span>(g.x)  <span class="comment"># cython</span></span><br></pre></td></tr></table></figure><p>看到 Cython 是将 property 和描述符结合在一起了，但是实现起来感觉更方便了。</p><p>不过最重要的还是魔法方法，魔法方法算是 Python 中非常强大的一个特性， Python 将每一个操作符都抽象成了对应的魔法方法，也正因为如此 numpy 也得以很好的实现。那么在 Cython 中，魔法方法是如何体现的呢？</p><h1 id="9、魔法方法在Cython中使用"><a href="#9、魔法方法在Cython中使用" class="headerlink" title="9、魔法方法在Cython中使用"></a>9、魔法方法在Cython中使用</h1><p>通过魔法方法可以对运算符进行重载，魔法方法的特点就是它的函数名<strong>以双下划线开头、并以双下划线结尾</strong>。之前讨论了<code>__cinit__</code>、<code>__init__</code>、<code>__dealloc__</code>，并了解了它们分别用于 C 一级的初始化、Python 一级的初始化、对象的释放（特指 C 中的指针）。除了那三个，Cython 中也支持其它的魔法方法，但是注意：Cython 不支持 <code>__del__</code>，<code>__del__</code>由 <code>__dealloc__</code>负责实现。</p><h2 id="9-1-算术魔法方法"><a href="#9-1-算术魔法方法" class="headerlink" title="9.1 算术魔法方法"></a>9.1 算术魔法方法</h2><p>假设在 Python 中定义了一个类 class A，如果希望 A 的实例对象可以进行加法运算，那么内部需要定义<code>__add__</code> 或<code>__radd__</code>方法。关于<code>__add__</code>和<code>__radd__</code>的区别就在于该实例对象是在加号的左边还是右边。以 <code>A() + B()</code> 为例，A 和 B 是我们自定义的类：</p><ul><li><code>首先尝试寻找 A 的 __add__ 方法, 如果有直接调用</code></li><li><code>如果 A 中不存在 __add__ 方法, 那么会去寻找 B 的 __radd__ 方法</code></li></ul><p>但如果是一个整数和自定义的类的实例对象相加呢？</p><ul><li><code>123 + A(): 先寻找 A 的 __radd__</code></li><li><code>A() + 123: 先寻找 A 的 __add__</code></li></ul><p>代码演示一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__add__</span>(<span class="params">self, other</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;A add&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__radd__</span>(<span class="params">self, other</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;A radd&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__add__</span>(<span class="params">self, other</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;B add&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__radd__</span>(<span class="params">self, other</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;B radd&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A() + B())  <span class="comment"># A add</span></span><br><span class="line"><span class="built_in">print</span>(B() + A())  <span class="comment"># B add</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">123</span> + B())  <span class="comment"># B radd</span></span><br><span class="line"><span class="built_in">print</span>(A() + <span class="number">123</span>)  <span class="comment"># A add</span></span><br></pre></td></tr></table></figure><p>除了类似于 <code>__add__</code>这种实例对象放在左边、<code>__radd__</code>这种实例对象放在右边，还有<code>__iadd__</code>，它用于 += 这种形式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iadd__</span>(<span class="params">self, other</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;__iadd__ is called&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> + other</span><br><span class="line"></span><br><span class="line">a = A()</span><br><span class="line">a += <span class="number">123</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__iadd__ is called</span></span><br><span class="line"><span class="string">124</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 如果没定义__iadd__，也是可以使用这种形式，会转化成a = a + 123，所以会调用__add__方法</span></span><br></pre></td></tr></table></figure><p>当然这都比较简单，其它的算数魔法方法也是类似的。并且里面的 self 就是对应类的实例对象，有人会觉得这不是废话吗？之所以要提这一点，是为了给下面的Cython做铺垫。</p><p>对于 Cython 中的扩展类来说，不使用类似于 <code>__radd__</code>这种实现方式，只需要定义一个 <code>__add__</code>即可同时实现 <code>__add__</code> 和 <code>__radd__</code>。对于 Cython 中的扩展类型 A，a 是 A 的实例对象，如果是 a + 123，那么会调用 <code>__add__</code>方法，然后第一个参数是 a、第二个参数是123；但如果是 123 + a，那么依旧会调用<code>__add__</code>，不过此时 <code>__add__</code>的第一个参数是 123、第二个参数才是 a。所以不像 Python 中的魔法方法，第一个参数 self 永远是实例本身，第一个参数是谁取决于谁在前面。所以将第一个参数叫做 self 容易产生误解，官方也不建议将第一个参数使用 self 作为参数名。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Girl</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__add__</span>(<span class="params">x, y</span>):</span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">g = cython_test.Girl()</span><br><span class="line"><span class="built_in">print</span>(g + <span class="number">123</span>)  <span class="comment"># (&lt;cython_test.Girl object at 0x0000028752477940&gt;, 123)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">123</span> + g)  <span class="comment"># (123, &lt;cython_test.Girl object at 0x0000028752477940&gt;)</span></span><br></pre></td></tr></table></figure><p>我们看到，<code>__add__</code>中的参数确实是由位置决定的，那么再来看一个例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Girl</span>:</span><br><span class="line">    cdef long a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, a</span>):</span><br><span class="line">        self.a = a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__add__</span>(<span class="params">x, y</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(x, Girl):</span><br><span class="line">            <span class="comment"># 这里为什么需要转化呢？直接 x.a + y 不行吗？</span></span><br><span class="line">            <span class="comment"># 答案是不行的，因为这个 x 是我们外部传过来的 Girl 对象</span></span><br><span class="line">            <span class="comment"># 但是我们这里的 a 不是一个 public 或者 readonly，直接访问是得不到的，所以需要转化一下才可以访问</span></span><br><span class="line">            <span class="keyword">return</span> (&lt;Girl&gt; x).a + y</span><br><span class="line">        <span class="keyword">return</span> (&lt;Girl&gt; y).a + x</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">g = cython_test.Girl(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(g + <span class="number">2</span>)  <span class="comment"># 5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">2</span> + g)  <span class="comment"># 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 和浮点数运算也是可以的</span></span><br><span class="line"><span class="built_in">print</span>(g + <span class="number">2.1</span>)  <span class="comment"># 5.1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">2.1</span> + g)  <span class="comment"># 5.1</span></span><br><span class="line"></span><br><span class="line">g += <span class="number">4</span></span><br><span class="line"><span class="built_in">print</span>(g)  <span class="comment"># 7</span></span><br></pre></td></tr></table></figure><p>除了 <code>__add__</code>，Cython 也是支持<code>__iadd__</code> 的，此时的第一个参数是 self，因为 += 这种形式，第一种参数永远是实例对象。</p><h2 id="9-2-富比较"><a href="#9-2-富比较" class="headerlink" title="9.2 富比较"></a>9.2 富比较</h2><p>Cython 的扩展类可以使用<code>__eq</code>、<code>__ne__</code>等等，和 Python 一致的富比较魔法方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line">    <span class="comment"># 这里比较操作符两边的值的位置依旧会影响这里的x、y</span></span><br><span class="line">    <span class="comment"># 但是对于Python中的比较来说则不会，self永远是实例对象</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__eq__</span>(<span class="params">self, other</span>):</span><br><span class="line">        <span class="built_in">print</span>(self, other)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;==&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">a = cython_test.A()</span><br><span class="line"><span class="built_in">print</span>(a == <span class="number">3</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;cython_test.A object at 0x0000015D792C7940&gt; 3</span></span><br><span class="line"><span class="string">==</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">3</span> == a)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;cython_test.A object at 0x0000015D792C7940&gt; 3</span></span><br><span class="line"><span class="string">==</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>和算术魔法方法不一样，比较操作没有<code>__req__</code>或者<code>__ieq</code>__，并且比较的时候第一个参数永远是实例对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__eq__</span>(<span class="params">self, other</span>):</span><br><span class="line">        <span class="built_in">print</span>(self, other)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;A ==&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__eq__</span>(<span class="params">self, other</span>):</span><br><span class="line">        <span class="built_in">print</span>(self, other)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;B ==&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">a = cython_test.A()</span><br><span class="line">b = cython_test.B()</span><br><span class="line"><span class="comment"># 调用 a 的 __eq__</span></span><br><span class="line"><span class="built_in">print</span>(a == <span class="number">123</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;cython_test.A object at 0x00000223641631C0&gt; 123</span></span><br><span class="line"><span class="string">A ==</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 调用 b 的 __eq__</span></span><br><span class="line"><span class="built_in">print</span>(b == <span class="number">123</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;cython_test.B object at 0x00000223641E71F0&gt; 123</span></span><br><span class="line"><span class="string">B ==</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 调用 a 的 __eq__, 第一个参数还是 a</span></span><br><span class="line"><span class="built_in">print</span>(a == <span class="number">123</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;cython_test.A object at 0x00000223641631C0&gt; 123</span></span><br><span class="line"><span class="string">A ==</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 调用 b 的 __eq__, 第一个参数还是 b</span></span><br><span class="line"><span class="built_in">print</span>(b == <span class="number">123</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;cython_test.B object at 0x00000223641E71F0&gt; 123</span></span><br><span class="line"><span class="string">B ==</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 调用 a 的 __eq__, 第一个参数是 a, 第二个参数是 b</span></span><br><span class="line"><span class="built_in">print</span>(a == b)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;cython_test.A object at 0x00000223641631C0&gt; &lt;cython_test.B object at 0x00000223641E71F0&gt;</span></span><br><span class="line"><span class="string">A ==</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 调用 b 的 __eq__, 第一个参数是 b, 第二个参数是 a</span></span><br><span class="line"><span class="built_in">print</span>(b == a)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;cython_test.B object at 0x00000223641E71F0&gt; &lt;cython_test.A object at 0x00000223641631C0&gt;</span></span><br><span class="line"><span class="string">B ==</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>链式比较也是可以的，比如：a == b == 123 等价于 a == b and b == 123。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">a = cython_test.A()</span><br><span class="line">b = cython_test.B()</span><br><span class="line"><span class="built_in">print</span>(a == b == <span class="number">123</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;cython_test.A object at 0x000001817F1D31C0&gt; &lt;cython_test.B object at 0x000001817630E0D0&gt;</span></span><br><span class="line"><span class="string">&lt;cython_test.B object at 0x000001817630E0D0&gt; 123</span></span><br><span class="line"><span class="string">B ==</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>先执行 a == b 返回 “A ==”，再执行 b == 3 返回 “B ==”，然后 “A ==” 和 “B ==” 进行 and，前面为真，所以返回后面的 “B ==”。</p><h2 id="9-3-迭代器支持"><a href="#9-3-迭代器支持" class="headerlink" title="9.3 迭代器支持"></a>9.3 迭代器支持</h2><p>Cython 中的扩展类也是支持迭代器协议的，而且定义的方法和纯 Python 之间是一样的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    cdef public:</span><br><span class="line">        <span class="built_in">list</span> values</span><br><span class="line">        long __index</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, values</span>):</span><br><span class="line">        self.values = values</span><br><span class="line">        self.__index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__next__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            ret = self.values[self.__index]</span><br><span class="line">            self.__index += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> ret</span><br><span class="line">        <span class="keyword">except</span> IndexError:</span><br><span class="line">            <span class="keyword">raise</span> StopIteration</span><br><span class="line"> <span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">a = cython_test.A([<span class="string">&#x27;椎名真白&#x27;</span>, <span class="string">&#x27;古明地觉&#x27;</span>, <span class="string">&#x27;雾雨魔理沙&#x27;</span>])</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> a:</span><br><span class="line">    <span class="built_in">print</span>(_)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">椎名真白</span></span><br><span class="line"><span class="string">古明地觉</span></span><br><span class="line"><span class="string">雾雨魔理沙</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>知道在 Python 中，for 循环会先去寻找<code>__iter__</code>，但如果找不到会退而求其次去找 <code>__getitem__</code>，那么在 Cython 中是不是也是如此呢。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="keyword">class</span> <span class="title class_">A</span>:</span><br><span class="line"></span><br><span class="line">    cdef public:</span><br><span class="line">        <span class="built_in">list</span> values</span><br><span class="line">        long __index</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, values</span>):</span><br><span class="line">        self.values = values</span><br><span class="line">        self.__index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>):</span><br><span class="line">        <span class="keyword">return</span> self.values[item]</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">a = cython_test.A([<span class="string">&#x27;椎名真白&#x27;</span>, <span class="string">&#x27;古明地觉&#x27;</span>, <span class="string">&#x27;雾雨魔理沙&#x27;</span>])</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> a:</span><br><span class="line">    <span class="built_in">print</span>(_)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">椎名真白</span></span><br><span class="line"><span class="string">古明地觉</span></span><br><span class="line"><span class="string">雾雨魔理沙</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>我们看到，也是一样的。</p><p>当然上面只是介绍了魔法方法的一部分，Python 中的魔法方法（比如<code>__getattr__</code>、<code>__call__</code>、<code>__hash__</code>等等）在 Cython 中基本上都支持，并且 Cython 还提供了一些 Python 所没有的魔法方法。当然这些我们就不说了，如果你熟悉 Python 的话，那么在 Cython 中也是按照相同的方式进行使用即可。总之，用久了就孰能生巧了。</p><blockquote><p><strong>注意</strong>：魔法方法只能用def定义，不可以使用cdef或者cpdef。</p></blockquote><p>还有上下文管理器，在Cython中也是一样的用法。Python中基本上所有的魔法方法在Cython都可以直接用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mport pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">a = cython_test.A()</span><br><span class="line"><span class="keyword">with</span> a:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__enter__</span></span><br><span class="line"><span class="string">__exit__</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>Cython 中的扩展类，它和 Python 中内置类是等价的，都是直接指向了 C 一级的数据结构，不需要字节码的翻译过程。也正因为如此，它失去一些动态特性，但同时也获得了效率，因为这两者本来就是不可兼得的。</p><p>Cython 的类有点复杂，还是需要多使用，不过它毕竟在各方面都和 Python 保持接近，因此学习来也不是那么费劲。</p><p>虽然创建扩展类的最简单的方式是通过 Cython，但是通过 Python/C API 直接在 C 中实现的话，则是最有用的练习，但还是那句话，它需要我们对 Python/C API 有一个很深的了解，而这是一个非常难得的事情，因此使用 Cython 就变成了我们最佳的选择。</p>]]></content>
      
      
      <categories>
          
          <category> PL </category>
          
          <category> Cython </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6.Cython使用C/C++外部库</title>
      <link href="/program_language/cython/6.Cython%E4%BD%BF%E7%94%A8C_C++%E5%A4%96%E9%83%A8%E5%BA%93/"/>
      <url>/program_language/cython/6.Cython%E4%BD%BF%E7%94%A8C_C++%E5%A4%96%E9%83%A8%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<h1 id="1、在Cython中声明外部的C代码"><a href="#1、在Cython中声明外部的C代码" class="headerlink" title="1、在Cython中声明外部的C代码"></a>1、在Cython中声明外部的C代码</h1><p>要用 Cython 包装 C 源文件，必须在 Cython 中声明使用的 C 组件的接口。为此，Cython 提供了一个 <code>extern</code>语句，它的目的就是告诉 Cython，希望从指定的 C 头文件中使用 C 结构。语法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;header_name&quot;</span>:</span><br><span class="line">    <span class="comment"># 相应的声明, 你希望使用哪些 C 结构, 那么就将其定义在这里</span></span><br><span class="line">    <span class="comment"># 如果不需要的话可以写上一个pass</span></span><br></pre></td></tr></table></figure><ul><li><code>1. Cython 编译器会在生成的源文件中写入 #include &quot;header_name&quot;</code></li><li><code>2. 在 extern 语句块中的类型、函数以及其它声明都可以在 Cython 中直接使用</code></li><li><code>3. Cython 会在编译时检查 C 的声明是否正确，如果不正确会编译错误。</code></li></ul><p>extern 语句块中的声明类似于 C，会用它来介绍之前说的结构体、共同体。另外 extern 关键字可以和 cdef 组合，一起添加到任意的 C 声明中。</p><p>extern 会在生成的源文件中写入一个 <code>#include</code> 语句，但如果不希望写入这个语句，但是又希望和外部代码进行交互，那么可以通过 from * 来禁止 Cython 生成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> *:</span><br><span class="line">   <span class="comment"># 声明</span></span><br></pre></td></tr></table></figure><blockquote><p>extern from 代码块内部写的是函数声明，这些声明要和 C 中的相匹配。</p></blockquote><p>下面就来详细介绍 extern 怎么用，不过在介绍之前，需要了解一下extern它不会做哪些事情。</p><h2 id="1-1-Cython不会自动包装"><a href="#1-1-Cython不会自动包装" class="headerlink" title="1.1 Cython不会自动包装"></a>1.1 Cython不会自动包装</h2><p>extern 语句块的目的很简单，但是乍一看可能会产生误导。在 Cython 中存在 extern 块（extern声明），确保能够以正确的类型调用声明的 C 函数、变量、结构体等等，但是<strong>它不会自动地为对象创建 Python 的包装器</strong>，仍然需要使用 def、或者 cpdef（可能还会使用 cdef）来调用 extern 块中声明的 C 函数。因为如果不这么做，则无法从 Python 代码中访问 extern 块中声明的外部 C 函数。因为 Cython 不会自动解析 C 文件、以及包装给外部 Python 访问，需要手动实现这一点。而这么做的原因也很好理解，因此 Cython 中包装器的实现已经非常简单了，完全可以自己自定制，自动实现的话反而会任务变得复杂。</p><h1 id="2、声明外部的C函数以及给类型起别名"><a href="#2、声明外部的C函数以及给类型起别名" class="headerlink" title="2、声明外部的C函数以及给类型起别名"></a>2、声明外部的C函数以及给类型起别名</h1><p>extern 块中最常见的声明是 C 函数和 <code>typedef</code>，这些声明几乎可以直接写在 Cython 中，只需要做一下修改：</p><ul><li><code>1. 将typedef变成ctypedef</code></li><li><code>2. 删除类似于restrict、volatile等不必要、以及不支持的关键字</code></li><li><code>3. 确保函数的返回值和对应类型的声明在同一行</code></li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//在C中，可以这么写，但是 Cython 中要在同一行</span></span><br><span class="line"><span class="type">int</span> </span><br><span class="line"><span class="title function_">foo</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">123</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>4. 删除行尾的分号</code></li></ul><p>此外，在 Cython 中声明函数时，参数可以写在多行，就像 Python 一样。</p><p>下面定义一个 C 的头文件：header.h，写上一些简单的 C 声明和宏。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> M_PI 3.1415926</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAX(a, b) ((a) &gt;= (b) ? (a) : (b))</span></span><br><span class="line"><span class="type">double</span> <span class="title function_">hypot</span><span class="params">(<span class="type">double</span>, <span class="type">double</span>)</span>;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">int</span> integral;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">double</span> real;</span><br><span class="line"><span class="type">void</span> <span class="title function_">func</span><span class="params">(integral, integral, real)</span>;</span><br><span class="line">real *<span class="title function_">func_arrays</span><span class="params">(integral[], integral[][<span class="number">10</span>], real **)</span>;</span><br></pre></td></tr></table></figure><p>如果想在 Cython 中使用的话，那么就把那些想用的写在 Cython 中，当然说不能直接照搬，因为 C 和 Cython 的声明还是有些略微的差异的，上面已经介绍过了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;header.h&quot;</span>:</span><br><span class="line">    double M_PI</span><br><span class="line">    <span class="built_in">float</span> MAX(<span class="built_in">float</span> a, <span class="built_in">float</span> b)</span><br><span class="line">    double hypot(double x, double y)</span><br><span class="line">    ctypedef <span class="built_in">int</span> integral</span><br><span class="line">    ctypedef double real</span><br><span class="line">    void func(integral a, integral b, real c)</span><br><span class="line">    real *func_arrays(integral[] i, integral[][<span class="number">10</span>] j, real **k)</span><br></pre></td></tr></table></figure><p>注意：我们在 Cython 中声明 C 中 M_PI 这个宏时，将其声明为 double 型的变量，同理对于 MAX 宏也是如此，就把它当成接收两个 float、返回一个 float 的名为 MAX 函数。</p><p>另外看到在 extern 块的声明中，为函数参数添加了一个名字。这是推荐的，但并不是强制的；如果有参数名的话，那么可以让通过关键字参数调用，对于接口的使用会更加明确。</p><p>Cython 支持 C 中的所有声明，甚至函数指针接收函数指针、返回包含函数指针的数组也是可以的。当然简单的类型声明：数值、字符串、数组、指针、void 等等已经构成了 C 声明的大多数，大多数时候可以直接将 C 中的声明复制粘贴过来，然后去掉分号就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;header2.h&quot;</span>:</span><br><span class="line">    ctypedef void (*void_int_fptr)(<span class="built_in">int</span>)</span><br><span class="line">    void_int_fptr signal(void_int_fptr)</span><br><span class="line">    <span class="comment"># 上面两行等价于 void (*signal(void(*)(int)))(int)</span></span><br></pre></td></tr></table></figure><p>所以可以进行非常复杂的声明，当然日常也很少会用到。</p><h1 id="3、声明并包装C结构体、共同体、枚举"><a href="#3、声明并包装C结构体、共同体、枚举" class="headerlink" title="3、声明并包装C结构体、共同体、枚举"></a>3、声明并包装C结构体、共同体、枚举</h1><p>如果声明一个结构体、共同体、枚举，那么可以使用如下方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;header_name&quot;</span>:</span><br><span class="line">    struct struct_name: </span><br><span class="line">        struct_members  <span class="comment"># 创建变量的时候通过 &quot;cdef struct_name 变量&quot; 的方式</span></span><br><span class="line">    </span><br><span class="line">    union struct_name:</span><br><span class="line">        union_members</span><br><span class="line">    </span><br><span class="line">    enum struct_name:</span><br><span class="line">        enum_members</span><br></pre></td></tr></table></figure><p>如果是在 C 中，等价于如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">struct_name</span> &#123;</span></span><br><span class="line">    struct_members</span><br><span class="line">&#125;;  <span class="comment">// 创建变量的时候通过 &quot;struct struct_name 变量&quot; 的方式</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">union</span> <span class="title">union_name</span> &#123;</span></span><br><span class="line">    union_members</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">enum_name</span> &#123;</span></span><br><span class="line">    enum_members</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>当然在 C 中还可以使用 typedef。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">struct_name</span> &#123;</span></span><br><span class="line">    struct_members</span><br><span class="line">&#125; struct_alias;   <span class="comment">// 然后创建变量的时候直接通过 &quot;struct_alisa 变量&quot; 即可，所以定义结构体的时候 struct_name 也可以不写</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">union</span> <span class="title">union_name</span> &#123;</span></span><br><span class="line">    union_members</span><br><span class="line">&#125; union_alias;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">enum</span> <span class="title">enum_name</span> &#123;</span></span><br><span class="line">    enum_members</span><br><span class="line">&#125; enum_alias;</span><br></pre></td></tr></table></figure><p>Cython 中的 typedef 则是使用 ctypedef。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;header_name&quot;</span>:</span><br><span class="line"></span><br><span class="line">    ctypedef struct struct_alias:</span><br><span class="line">        struct_members  </span><br><span class="line">        <span class="comment"># 创建变量的时候通过 &quot;cdef struct_name 变量&quot; 的方式</span></span><br><span class="line">        <span class="comment"># 所以无论是哪种方式，在 Cython 中创建结构体变量的时候是没有任何区别的</span></span><br><span class="line">    </span><br><span class="line">    ctypedef union struct_alias:</span><br><span class="line">        union_members</span><br><span class="line">    </span><br><span class="line">    ctypedef enum struct_alias:</span><br><span class="line">        enum_members</span><br></pre></td></tr></table></figure><p>Cython 中的 typedef 则是使用 ctypedef，此时就定义了一个类型别名。但是注意：如果结构体中没有字段，那么 Cython 中应该要给一个 pass 语句作为占位符。</p><h2 id="3-1-举例说明"><a href="#3-1-举例说明" class="headerlink" title="3.1 举例说明"></a>3.1 举例说明</h2><p>下面来实际演示一下，直接以结构体为例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// c_src/header.h</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Girl1</span> &#123;</span></span><br><span class="line">    <span class="type">char</span> * name;</span><br><span class="line">    <span class="type">int</span> age;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="type">char</span> *name;</span><br><span class="line">    <span class="type">int</span> age;</span><br><span class="line">&#125; Girl2;</span><br></pre></td></tr></table></figure><p>以上是一个 C 的头文件，我们在 Cython 中导入之后要怎么进行声明呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython/cython_test.pyx</span></span><br><span class="line"></span><br><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;header.h&quot;</span>:</span><br><span class="line">    struct Girl1:  </span><br><span class="line">        char *name</span><br><span class="line">        <span class="built_in">int</span> age</span><br><span class="line"></span><br><span class="line">    ctypedef struct Girl2: </span><br><span class="line">        char *name</span><br><span class="line">        <span class="built_in">int</span> age</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于结构体而言, 里面的成员只能用 C 中的类型</span></span><br><span class="line"><span class="comment"># 而且如何创建结构体的对应实例, 我们之前也说过了, 直接 &quot;cdef 结构体类型 变量名 =  &quot; 即可</span></span><br><span class="line">cdef Girl1 g1 = Girl1(<span class="string">&quot;komeiji satori&quot;</span>, <span class="number">16</span>)</span><br><span class="line">cdef Girl1 g2 = Girl1(<span class="string">&quot;komeiji koishi&quot;</span>, age=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以看到无论是 cdef struct 定义的, 还是通过 ctypedef 起的类型别名, 使用方式没有任何区别</span></span><br><span class="line"><span class="built_in">print</span>(g1)</span><br><span class="line"><span class="built_in">print</span>(g2)</span><br></pre></td></tr></table></figure><p>然后进行编译，测试一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># setup_exe.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> _locale</span><br><span class="line">_locale._getdefaultlocale = (<span class="keyword">lambda</span> *args: [<span class="string">&#x27;en_US&#x27;</span>, <span class="string">&#x27;utf8&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup, Extension</span><br><span class="line"><span class="keyword">from</span> Cython.Build <span class="keyword">import</span> cythonize</span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    ext_modules=cythonize([</span><br><span class="line">            Extension(</span><br><span class="line">                name=<span class="string">&#x27;cython_test&#x27;</span>,</span><br><span class="line">                sources=[<span class="string">&#x27;cython/*.pyx&#x27;</span>],</span><br><span class="line">                include_dirs=[<span class="string">&#x27;c_src&#x27;</span>])</span><br><span class="line">        ],</span><br><span class="line">        build_dir=<span class="string">&#x27;build&#x27;</span>,</span><br><span class="line">        compiler_directives=&#123;<span class="string">&#x27;language_level&#x27;</span>: <span class="number">3</span>&#125;</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>下面来进行导入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;&#x27;name&#x27;: b&#x27;komeiji satori&#x27;, &#x27;age&#x27;: 16&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;name&#x27;: b&#x27;komeiji koishi&#x27;, &#x27;age&#x27;: 16&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>因为里面有 print 所以导入的时候自动打印了，看到 C 的结构体到 Python 中会变成字典。</p><blockquote><p>有一点需要注意：使用 cdef extern from 导入头文件的时候，代码块里面的声明应该在 C 头文件里面存在。假设还想通过 ctypedef 给 int 起一个别名，而这个逻辑在 C 的头文件中是不存在的，而是自己想这么做，那么这个逻辑就不应该放在 cdef extern from 中，而是应该放在全局区域，否则是不起作用的。cdef extern from 里面的类型别名、声明什么的，都是根据头文件来的，将头文件中想要使用的放在 cdef extern from 中进行声明。而自己单独设置的声明、类型别名（头文件中不存在相应的逻辑）应该放在外面。</p><p>此外，除了 cdef extern from 之外，ctypedef 只能出现在全局区域（说白了就是没有缩进），像 if 语句、for 循环、while 循环、函数等等，内部都不能出现 ctypedef。</p></blockquote><h1 id="4、包装C函数"><a href="#4、包装C函数" class="headerlink" title="4、包装C函数"></a>4、包装C函数</h1><p>在最开始介绍斐波那契数列的时候，已经演示过这种方式了，再来感受一下。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// header.h</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="type">char</span> *name;</span><br><span class="line">    <span class="type">int</span> age;</span><br><span class="line">&#125; Girl;</span><br><span class="line"><span class="comment">// 里面定义一个结构体类型 和  一个函数声明</span></span><br><span class="line"><span class="type">char</span> *<span class="title function_">return_info</span> <span class="params">(Girl g)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// source.c</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;header.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">char</span> *<span class="title function_">return_info</span> <span class="params">(Girl g)</span> &#123;</span><br><span class="line">    <span class="comment">// 堆区申请一块内存</span></span><br><span class="line">    <span class="type">char</span> *info = (<span class="type">char</span> *)<span class="built_in">malloc</span>(<span class="number">20</span>);</span><br><span class="line">    <span class="comment">// 拷贝一个字符串进去</span></span><br><span class="line">    <span class="built_in">sprintf</span>(info, <span class="string">&quot;name: %s, age: %d&quot;</span>, g.name, g.age);</span><br><span class="line">    <span class="comment">// 返回指针</span></span><br><span class="line">    <span class="keyword">return</span> info;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">from libc.stdlib cimport <span class="built_in">free</span></span><br><span class="line"></span><br><span class="line">cdef <span class="keyword">extern</span> from <span class="string">&quot;header.h&quot;</span>:</span><br><span class="line">    # C 头文件中变量的声明 和 Cython 这里的声明是很类似的</span><br><span class="line">    ctypedef <span class="class"><span class="keyword">struct</span> <span class="title">Girl</span>:</span></span><br><span class="line">        <span class="type">char</span> *name</span><br><span class="line">        <span class="type">int</span> age</span><br><span class="line"></span><br><span class="line">    # 声明函数时不需要使用 cdef</span><br><span class="line">    <span class="type">char</span> *<span class="title function_">return_info</span><span class="params">(Girl)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 然后我们说如果想被 Python 访问, 还需要定义一个包装器</span><br><span class="line"># 我们通过 Python 无法直接调用 return_info, 因为它没有暴露给 Python</span><br><span class="line"># 我们需要在 Cython 内部定义一个可以暴露给 Python 的函数, 然后在这个函数中调用 return_info</span><br><span class="line">cpdef bytes <span class="title function_">info</span><span class="params">(dict d)</span>:</span><br><span class="line">    cdef:</span><br><span class="line">        # 接收一个字典</span><br><span class="line">        str name = d[<span class="string">&quot;name&quot;</span>]</span><br><span class="line">        <span class="type">int</span> age = d[<span class="string">&quot;age&quot;</span>]</span><br><span class="line"></span><br><span class="line">    # 根据对应的值创建结构体实例, 但 name 需要转成 bytes 对象, 因为 <span class="type">char</span> * 对应 Python 的 bytes 对象</span><br><span class="line">    cdef Girl g = Girl(name=bytes(name, encoding=<span class="string">&quot;utf-8&quot;</span>), age=age)</span><br><span class="line">    # 构造出结构体之后, 传到 C 的函数中, 得到返回值, 也就是字符串的首地址</span><br><span class="line">    cdef <span class="type">char</span> *p = return_info(g)</span><br><span class="line">    # 这里需要先拷贝给 Python, 此时会根据 p 这个 <span class="type">char</span> * 来创建一个 Python 的 bytes 对象, 然后让变量 res 指向</span><br><span class="line">    # 至于为什么不直接返回 p, 是因为 p 是在堆区申请的, 我们需要将它释放掉</span><br><span class="line">    res = p</span><br><span class="line">    <span class="built_in">free</span>(p)</span><br><span class="line">    # 返回 res</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>然后来进行编译：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup, Extension</span><br><span class="line"><span class="keyword">from</span> Cython.Build <span class="keyword">import</span> cythonize</span><br><span class="line"></span><br><span class="line">ext = [Extension(<span class="string">&quot;cython_test&quot;</span>,</span><br><span class="line">                 sources=[<span class="string">&quot;cython_test.pyx&quot;</span>, <span class="string">&quot;source.c&quot;</span>])]</span><br><span class="line"></span><br><span class="line">setup(ext_modules=cythonize(ext, language_level=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>最后调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="built_in">print</span>(cython_test.info(&#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;satori&quot;</span>, <span class="string">&quot;age&quot;</span>: <span class="number">16</span>&#125;))  <span class="comment"># b&#x27;name: satori, age: 16&#x27;</span></span><br></pre></td></tr></table></figure><p>看到整体没有任何问题，但是很明显这个例子有点刻意了，故意兜这么一个圈子。但这么做主要是想介绍 C 和 Cython 之间的交互方式，以及 Cython 调用 C 库是有多么的方便。</p><h1 id="5、常量、其它修饰符、以及控制-Cython-生成的内容"><a href="#5、常量、其它修饰符、以及控制-Cython-生成的内容" class="headerlink" title="5、常量、其它修饰符、以及控制 Cython 生成的内容"></a>5、常量、其它修饰符、以及控制 Cython 生成的内容</h1><p>正如之前提到的，Cython 理解 const 修饰符，但它在 cdef 声明中并不是有效的。它应该在 cdef extern from 语句块中使用，用来修饰一个函数的参数或者返回值。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="type">const</span> <span class="type">int</span> * const_int_ptr;</span><br><span class="line"><span class="type">const</span> <span class="type">double</span> *<span class="title function_">returns_ptr_to_const</span><span class="params">(const_int_ptr)</span>; </span><br></pre></td></tr></table></figure><p>如果在 Cython 中声明的话，应该这么做。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;header.h&quot;</span>:</span><br><span class="line">    ctypedef const <span class="built_in">int</span>* const_int_ptr</span><br><span class="line">    const double *returns_ptr_to_const(const_int_ptr)</span><br></pre></td></tr></table></figure><p>看到声明真的非常类似，基本上没太大改动，只需要将 typedef 换成 ctypedef、并将结尾的分号去掉即可，但事实上即使分号不去掉在 Cython 中也是合法的，只不过这不是符合 Cython 风格的代码。</p><blockquote><p>除了 const 还有 volatile、restrict，但这两个在 Cython 中是不合法的。</p></blockquote><p>另外在 Cython 中，偶尔为函数、结构体使用别名是很有用的，这允许在 Cython 中以不同的名称引用一个 C 级对象，怎么理解呢？举个栗子：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// header.h</span></span><br><span class="line"><span class="type">unsigned</span> <span class="type">long</span> __return_int(<span class="type">unsigned</span> <span class="type">long</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// source.c</span></span><br><span class="line"><span class="type">unsigned</span> <span class="type">long</span> __return_int(<span class="type">unsigned</span> <span class="type">long</span> n) &#123;</span><br><span class="line">    <span class="keyword">return</span> n;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> C 函数前面带了两个下划线，看着别扭，再或者它和 Python 中的某个内置函数的名称、或者关键字发生冲突等等，这个时候需要为其指定一个别名。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;header.h&quot;</span>:</span><br><span class="line">    <span class="comment"># 在 C 中定义的是 __return_int, 但是这里我们为其起了一个别名叫做 return_int</span></span><br><span class="line">    <span class="comment"># 再比如 ctypedef void klass &quot;class&quot;, C 中定义的是 class, 但这是 Python 的关键字, 所以将其起个别名叫 klass</span></span><br><span class="line">    unsigned long return_int <span class="string">&quot;__return_int&quot;</span>(unsigned long)</span><br><span class="line">    <span class="comment"># 这个过程就你就可以看做是: C 中定义的名称是 __return_int, 这里的声明是 unsigned long return_int (unsigned long)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后我们这里直接通过别名进行调用</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">py_return_int</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">return</span> return_int(n)  </span><br></pre></td></tr></table></figure><p>编译一下进行测试，这里编译的代码不变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="built_in">print</span>(cython_test.py_return_int(<span class="number">123</span>))  <span class="comment"># 123</span></span><br></pre></td></tr></table></figure><p>看到没有任何问题，Cython 做的还是比较周密的，为考虑到了方方面面。这里起别名不仅仅可以对函数、ctypedef 使用，还可以是结构体、枚举之类的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;header_file&quot;</span>:</span><br><span class="line">    <span class="comment"># C: struct del &#123;int a, b&#125;; 显然 del 是 Python 的关键字</span></span><br><span class="line">    struct _<span class="keyword">del</span> <span class="string">&quot;del&quot;</span>:</span><br><span class="line">        <span class="built_in">int</span> a, b</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># C: enum yield &#123;ALOT; SOME; ALTITLE;&#125;;</span></span><br><span class="line">    enum _<span class="keyword">yield</span> <span class="string">&quot;yield&quot;</span>:</span><br><span class="line">        ALOT</span><br><span class="line">        SOME</span><br><span class="line">        ALITTLE</span><br></pre></td></tr></table></figure><p>在任何情况下，引号中的字符串都是生成的 C 代码中的对象名，Cython 不会检查该字符串的内容，因此可以使用（滥用）这一特性来控制 C 一级的声明。</p><h2 id="5-1-错误检测和引发异常"><a href="#5-1-错误检测和引发异常" class="headerlink" title="5.1 错误检测和引发异常"></a>5.1 错误检测和引发异常</h2><p>对于外部 C 函数而言，如果出现了异常，那么一种常见的做法是返回一个错误的状态码或者错误标志。但这些异常是在 C 中出现的异常，不是在 Cython 中出现的，因此为了正确地表示 C 中出现的异常，必须要对其进行包装。当在 C 中出现异常时，显式地将其引发出来。如果不这么做、而只是单纯的异常捕获的话，那么是无效的，因为 <strong>Cython 不会对 C 中出现的异常进行检测，所以在 Python 中也是无法进行异常捕获的</strong>。</p><p>而如果想做到这一点，需要将 except 字句和 cdef 回调一起绑定起来。</p><h2 id="5-2-回调"><a href="#5-2-回调" class="headerlink" title="5.2 回调"></a>5.2 回调</h2><p><strong>Cython 支持 C 函数指针</strong>，通过这个特性，可以包装一个接收函数指针作为回调的 C 函数。回调函数可以是不调用 Python/C API 的纯 C 函数，也可以调用任意的 Python 代码，这取决于要实现的功能逻辑。因此这个强大的特性允许我们在运行时通过 cdef 创建一个函数来控制底层 C 函数的行为，如果能实现这个功能的话就好办了。</p><p>但涉及到跨语言边界的回调可能会变得很麻烦，因为直接调用 C 的函数会很简单，只不过 C 内部的逻辑与我们无关，只是单纯的调用。但如果说在运行时，还能对 C 内部的实现插上一脚就不是那么简单了，特别是涉及到合适的异常处理的时候。</p><p>下面举栗说明，首先在 C 的标准库 stdlib 中有一个 qsort 函数，希望对它进行包装，来对 Python 中的列表进行排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;stdlib.h&quot;</span>:</span><br><span class="line">    <span class="comment"># 这是 C 里面一个用于对数组进行排序的函数, 第一个参数是数组指针</span></span><br><span class="line">    <span class="comment"># 第二个元素是数组元素的个数, 因为数组在作为参数传递的时候会退化为指针, 所以无法通过 sizeof 计算出元素个数</span></span><br><span class="line">    <span class="comment"># 第三个参数是元素的大小</span></span><br><span class="line">    <span class="comment"># 第四个参数是一个回调函数, 显然是每两个元素之间进行比较的逻辑; a &gt; b 返回正数、a &lt; b 返回负数、a == b 返回 0</span></span><br><span class="line">    <span class="comment"># 而这第四个参数也是我们需要从外界(Cython)进行传递的, 此时就涉及到了 Cython 和 C 之间的交互</span></span><br><span class="line">    void qsort(</span><br><span class="line">            void *array,</span><br><span class="line">            size_t count,</span><br><span class="line">            size_t size,</span><br><span class="line">            <span class="built_in">int</span>(*compare)(const void *, const void *)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 从堆区申请内存、以及释放内存, 没错, 除了 from libc.stdlib cimport malloc, free 之外我们也可以使用这种方式</span></span><br><span class="line">    <span class="comment"># 因为这两个函数本身就在 stdlib 中</span></span><br><span class="line">    void *malloc(size_t size)</span><br><span class="line">    void free(void *ptr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义排序函数</span></span><br><span class="line">cdef <span class="built_in">int</span> int_compare(const void *a, const void *b):</span><br><span class="line">    cdef:</span><br><span class="line">        <span class="built_in">int</span> ia = (&lt;<span class="built_in">int</span> *&gt;a)[<span class="number">0</span>]</span><br><span class="line">        <span class="built_in">int</span> ib = (&lt;<span class="built_in">int</span> *&gt;b)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> ia - ib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为列表支持倒序排序, 所以需要再定义一个倒序排序函数</span></span><br><span class="line">cdef <span class="built_in">int</span> int_compare_reverse(const void *a, const void *b):</span><br><span class="line">    <span class="comment"># 直接在正序排序的基础上乘一个 -1 即可</span></span><br><span class="line">    <span class="keyword">return</span> -int_compare(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给一个函数指针起的类型别名</span></span><br><span class="line">ctypedef <span class="built_in">int</span>(*qsort_cmp)(const void *, const void *)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个包装器, 外界调用的是这个 pyqsort, 在 pyqsort 内部会调用 qsort</span></span><br><span class="line">cpdef pyqsort(<span class="built_in">list</span> x, bint reverse=<span class="literal">False</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将 Python 中的列表转成 C 的数组, 用于排序, 排序之后再将结果设置到列表中</span></span><br><span class="line"><span class="string">    :param x: 列表</span></span><br><span class="line"><span class="string">    :param reverse: 是否倒序排序 </span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    cdef:</span><br><span class="line">        <span class="built_in">int</span> *array</span><br><span class="line">        <span class="built_in">int</span> i, N</span><br><span class="line">    <span class="comment"># 计算列表长度, 并申请对应容量的内存</span></span><br><span class="line">    N = <span class="built_in">len</span>(x)</span><br><span class="line">    array = &lt;<span class="built_in">int</span> *&gt;malloc(sizeof(<span class="built_in">int</span>) * N)</span><br><span class="line">    <span class="keyword">if</span> array == NULL:</span><br><span class="line">        <span class="keyword">raise</span> MemoryError(<span class="string">&quot;内存不足, 申请失败&quot;</span>)</span><br><span class="line">    <span class="comment"># 将列表中的元素拷贝到数组中</span></span><br><span class="line">    <span class="keyword">for</span> i, val <span class="keyword">in</span> <span class="built_in">enumerate</span>(x):</span><br><span class="line">        array[i] = val</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取排序函数</span></span><br><span class="line">    cdef qsort_cmp cmp_callback</span><br><span class="line">    <span class="keyword">if</span> reverse:</span><br><span class="line">        cmp_callback = int_compare_reverse</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        cmp_callback = int_compare</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用 C 中的 qsort 函数进行排序</span></span><br><span class="line">    qsort(&lt;void *&gt; array, &lt;size_t&gt; N, sizeof(<span class="built_in">int</span>), cmp_callback)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用 qsort 结束之后, array 就排序好了, 然后再将排序好的结果设置在列表中</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="comment"># 注意: 此时不能对 array 使用 enumerate, 因为它是一个 int *</span></span><br><span class="line">        x[i] = array[i]</span><br><span class="line">    <span class="comment"># 此时 Python 中的列表就已经排序好了</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 别忘记最后将 array 释放掉</span></span><br><span class="line">    free(array)</span><br></pre></td></tr></table></figure><p>当导入自定义的 C 文件时，应该通过手动编译的方式，否则会找不到相应的文件。但这里导入的是标准库中的头文件，不是自己写的，所以可以不用编译，通过 pyximport 的方式即可实现导入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看到此时的 pyqsort 和 内置函数 一样, 都属于 built-in function 级别的, 是不是很有趣呢</span></span><br><span class="line"><span class="built_in">print</span>(cython_test.pyqsort)  <span class="comment"># &lt;built-in function pyqsort&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">max</span>)  <span class="comment"># &lt;built-in function max&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>)  <span class="comment"># &lt;built-in function isinstance&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">getattr</span>)  <span class="comment"># &lt;built-in function getattr&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后来看看结果如何吧, 是不是能起到排序的效果呢</span></span><br><span class="line">lst = [random.randint(<span class="number">10</span>, <span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line"><span class="built_in">print</span>(lst)  <span class="comment"># [65, 36, 12, 84, 97, 15, 19, 86, 11, 78]</span></span><br><span class="line"><span class="comment"># 排序</span></span><br><span class="line">cython_test.pyqsort(lst)</span><br><span class="line"><span class="comment"># 再次打印</span></span><br><span class="line"><span class="built_in">print</span>(lst)  <span class="comment"># [11, 12, 15, 19, 36, 65, 78, 84, 86, 97]</span></span><br><span class="line"><span class="comment"># 然后倒序排序</span></span><br><span class="line">cython_test.pyqsort(lst, reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(lst)  <span class="comment"># [97, 86, 84, 78, 65, 36, 19, 15, 12, 11]</span></span><br></pre></td></tr></table></figure><p>目前看起来一切顺利，没有任何障碍，而且在外部自己实现了一个内置函数，这是非常了不起的。</p><p>但是如果出现了异常呢？我们目前还没有对异常进行处理，现在我们将逻辑改一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="built_in">int</span> int_compare_reverse(const void *a, const void *b):</span><br><span class="line">   <span class="comment"># 其它地方完全不变, 只是在用于倒序排序的比较函数中加入一行 [][3], 故意引发一个索引越界</span></span><br><span class="line">   [][<span class="number">3</span>]</span><br><span class="line">   <span class="keyword">return</span> -int_compare(a, b)</span><br></pre></td></tr></table></figure><p>然后我们再调用它，看看会有什么现象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">cython_test.pyqsort([<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">IndexError: list index out of range</span></span><br><span class="line"><span class="string">Exception ignored in: &#x27;cython_test.int_compare_reverse&#x27;</span></span><br><span class="line"><span class="string">Traceback (most recent call last):</span></span><br><span class="line"><span class="string">  File &quot;D:/satori/1.py&quot;, line 7, in &lt;module&gt;</span></span><br><span class="line"><span class="string">    cython_test.pyqsort(lst, reverse=True)</span></span><br><span class="line"><span class="string">IndexError: list index out of range</span></span><br><span class="line"><span class="string">IndexError: list index out of range</span></span><br><span class="line"><span class="string">Exception ignored in: &#x27;cython_test.int_compare_reverse&#x27;</span></span><br><span class="line"><span class="string">Traceback (most recent call last):</span></span><br><span class="line"><span class="string">  File &quot;D:/satori/1.py&quot;, line 7, in &lt;module&gt;</span></span><br><span class="line"><span class="string">    cython_test.pyqsort(lst, reverse=True)</span></span><br><span class="line"><span class="string">IndexError: list index out of range</span></span><br><span class="line"><span class="string">IndexError: list index out of range</span></span><br><span class="line"><span class="string">Exception ignored in: &#x27;cython_test.int_compare_reverse&#x27;</span></span><br><span class="line"><span class="string">Traceback (most recent call last):</span></span><br><span class="line"><span class="string">  File &quot;D:/satori/1.py&quot;, line 7, in &lt;module&gt;</span></span><br><span class="line"><span class="string">    cython_test.pyqsort(lst, reverse=True)</span></span><br><span class="line"><span class="string">IndexError: list index out of range</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>明明出现了索引越界错误，但是程序居然没有立刻停下来，而是被忽略掉了。而每一次排序都需要调用这个函数，所以出现了多次 IndexError。虽然出现了异常，但不影响程序的执行，如果再最后加上一个 print 逻辑，会发现它依旧正常打印，这显然不是我们想要的。那么下面就来解决它。</p><h2 id="5-3-异常传递"><a href="#5-3-异常传递" class="headerlink" title="5.3 异常传递"></a>5.3 异常传递</h2><p>上面的索引越界是在 int_compare_reverse 中设置的，而它的调用是发生在什么地方呢？显然是发生在 C 中，因为它很明显是作为回调传递给了 qsort 这个 C 函数。所以 int_compare_reverse 中的索引越界是在执行 C 的 qsort 函数时候发生的，而不是在 Cython 中，如果发生的地点是在 Cython 中，那么会直接引发错误，当然也可以异常捕获，和 Python 没有任何区别。但不幸的是，这个函数是一个传递给 C 的回调函数，它是在 C 中被调用的。而为了解决这一点， Cython 提供了一个 except * 字句来帮更好的处理异常。举个栗子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="built_in">int</span> int_compare_reverse(const void *a, const void *b) <span class="keyword">except</span> *:</span><br><span class="line">    [][<span class="number">3</span>]</span><br><span class="line">    <span class="keyword">return</span> -int_compare(a, b)</span><br></pre></td></tr></table></figure><p><strong>只需要在结尾加上一个 except *，那么便可自动实现异常的传递</strong>。看到这个 except 是不是有点熟悉呢？之前在介绍 C 中的除法时，说如果返回值是 C 的类型、并且 C 中的整型发生除以 0 的情况，异常不会向上抛，需要在函数的结尾指定 <code>except ? -1</code>，来充当一个哨兵。这里也是与之类似的，通过指定 except *，使得它在作为回调函数的时候，如果内部发生了异常，能够转成传递给 Cython。但是这样还是不够的，因为这样会导致类型不匹配：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;stdlib.h&quot;</span>:</span><br><span class="line">    void qsort(</span><br><span class="line">            void *array,</span><br><span class="line">            size_t count,</span><br><span class="line">            size_t size,</span><br><span class="line">            <span class="comment"># 这里也需要加上 except *, 因为类型要一致</span></span><br><span class="line">            <span class="built_in">int</span>(*compare)(const void *, const void *) <span class="keyword">except</span> *</span><br><span class="line">    )</span><br><span class="line">    void *malloc(size_t size)</span><br><span class="line">    void free(void *ptr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显然这里也要加上 except *</span></span><br><span class="line">cdef <span class="built_in">int</span> int_compare(const void *a, const void *b) <span class="keyword">except</span> *:</span><br><span class="line">    cdef:</span><br><span class="line">        <span class="built_in">int</span> ia = (&lt;<span class="built_in">int</span> *&gt;a)[<span class="number">0</span>]</span><br><span class="line">        <span class="built_in">int</span> ib = (&lt;<span class="built_in">int</span> *&gt;b)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> ia - ib</span><br><span class="line"></span><br><span class="line">cdef <span class="built_in">int</span> int_compare_reverse(const void *a, const void *b) <span class="keyword">except</span> *:</span><br><span class="line">    [][<span class="number">3</span>]</span><br><span class="line">    <span class="keyword">return</span> -int_compare(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里也是如此, 否则类型不匹配</span></span><br><span class="line">ctypedef <span class="built_in">int</span>(*qsort_cmp)(const void *, const void *) <span class="keyword">except</span> *</span><br></pre></td></tr></table></figure><p>然后再来调用一下试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">lst = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">cython_test.pyqsort(lst, reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Traceback (most recent call last):</span></span><br><span class="line"><span class="string">  File &quot;cython_test.pyx&quot;, line 21, in cython_test.int_compare_reverse</span></span><br><span class="line"><span class="string">    [][3]</span></span><br><span class="line"><span class="string">IndexError: list index out of range</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The above exception was the direct cause of the following exception:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Traceback (most recent call last):</span></span><br><span class="line"><span class="string">  File &quot;D:/satori/1.py&quot;, line 7, in &lt;module&gt;</span></span><br><span class="line"><span class="string">    cython_test.pyqsort(lst, reverse=True)</span></span><br><span class="line"><span class="string">SystemError: &lt;built-in function pyqsort&gt; returned a result with an error set</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>看到此时程序就直接终止了，因为虽然错误在 C 中出现的，但是它传递给了 Cython，所以程序终止了。而且 Cython 在接收到这个异常时，并没有原原本本的直接输出，而是又引发了一个 SystemError，因为它是在 C 中出现的。</p><blockquote><p>总结一下：Python 在调用 Cython 时（可以是 pyx、pyd），如果发生了异常，那么就看这个异常是在哪里发生的。如果是在 Cython 中，那么和纯 Python 中发生异常时的表现是一样的，可以使用 try except 进行异常捕获。但如果是在 C 中发生的（出现这种情况的可能性非常有限，基本上都是作为 C 函数的一个回调函数，在 C 函数中调用这个回调函数引发异常），那么异常不会导致程序停止、也无法进行异常捕获（因为异常会被忽略掉），需要在回调函数的结尾加上 except *，来使得在 C 中发生异常时能够传递给 Cython。</p><p>如果这里的索引越界是在 pyqsort 中出现的，那么直接就会出现 IndexError，程序终止。因为，异常在 Cython 中的表现和 Python 是一模一样的。</p></blockquote><p>异常的传递真的是非常的不容易，通过 except * 这种方式，使得 Cython 中即可以定义一个 C 函数的回调函数、还能在出现异常的时候传递给 Cython，这个过程真的是走了很长的一段路。</p><h1 id="6、在Cython中引入C"><a href="#6、在Cython中引入C" class="headerlink" title="6、在Cython中引入C++"></a>6、在Cython中引入C++</h1><p>下面来看看如何在 Cython 中引入 C++，这里主要介绍如何编译。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># distutils: language=c++</span></span><br><span class="line"><span class="keyword">from</span> libcpp.vector cimport vector</span><br><span class="line"><span class="keyword">from</span> libcpp.<span class="built_in">map</span> cimport <span class="built_in">map</span></span><br><span class="line"></span><br><span class="line">cdef vector[<span class="built_in">int</span>] vect</span><br><span class="line"></span><br><span class="line">cdef <span class="built_in">int</span> i</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    vect.push_back(i)  <span class="comment"># 类似于 Python 列表的 append</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    <span class="built_in">print</span>(vect[i])</span><br><span class="line"></span><br><span class="line">cdef <span class="built_in">map</span>[<span class="built_in">int</span>, <span class="built_in">int</span>] m</span><br><span class="line">m[<span class="number">123</span>] = <span class="number">456</span></span><br><span class="line"><span class="built_in">print</span>(m[<span class="number">123</span>])</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：看一下最上面的注释，如果想要编译成功，那么必须要在开头加上 <code># distutils: language=c++</code>。并且一定要通过 setup 进行编译，采用 pyximport 是会失败的。</p><p>然后来测试一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0</span></span><br><span class="line"><span class="string">1</span></span><br><span class="line"><span class="string">2</span></span><br><span class="line"><span class="string">456</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h2 id="6-1-C-中的异常"><a href="#6-1-C-中的异常" class="headerlink" title="6.1 C++中的异常"></a>6.1 C++中的异常</h2><p>然后是 C++ 中的异常，Cython 无法抛出 C++ 中的异常，并且也无法使用 try-except 语句进行捕获。但是可以进行特殊的声明，当 C++ 函数中引发异常的时候能够转化成 Python 的异常。先来看看 C++ 的异常和 Python 的异常之间的对应关系：</p><p><img src="./image/6_1.jpg" alt="c++"></p><p>假设在 C++ 的函数中可能引发 bad_cast，那么我们在声明函数时就可以这么做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">&quot;some_file.h&quot;</span>:</span><br><span class="line">    cdef <span class="built_in">int</span> foo() <span class="keyword">except</span> +TypeError</span><br></pre></td></tr></table></figure><p>然后在调用 C++ 函数的时候，就可以进行异常捕获了，但如果不确定会引发什么错误，那么声明的时候通过 except +* 的方式即可，相当于 Python 的 Exception。</p><h1 id="7、引入静态库和动态库"><a href="#7、引入静态库和动态库" class="headerlink" title="7、引入静态库和动态库"></a>7、引入静态库和动态库</h1><p>在 Windows 上静态库是以 .lib 结尾、动态库是以 .dll 结尾；在 Linux 上静态库则以 .a 结尾、动态库以 .so 结尾。</p><p><img src="./image/6_2.jpg" alt="lib"></p><h2 id="7-1-Cython-静态库-动态库"><a href="#7-1-Cython-静态库-动态库" class="headerlink" title="7.1 Cython 静态库 动态库"></a>7.1 Cython 静态库 动态库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> _locale</span><br><span class="line">_locale._getdefaultlocale = (<span class="keyword">lambda</span> *args: [<span class="string">&#x27;en_US&#x27;</span>, <span class="string">&#x27;utf8&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> distutils.extension <span class="keyword">import</span> Extension</span><br><span class="line"><span class="keyword">from</span> Cython.Build <span class="keyword">import</span> cythonize</span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">&#x27;databus.python&#x27;</span>, </span><br><span class="line">    version=<span class="string">&#x27;v1.0&#x27;</span>,</span><br><span class="line">    author=<span class="string">&#x27;wdn&#x27;</span>,</span><br><span class="line">    author_email=<span class="string">&#x27;dongnian.wang@outlook.com&#x27;</span>,</span><br><span class="line">    description=<span class="string">&#x27;databus python wrapper&#x27;</span>,</span><br><span class="line">    keywords=<span class="string">&#x27;databus zmq&#x27;</span>,</span><br><span class="line">    package_dir=&#123;<span class="string">&#x27;&#x27;</span>: <span class="string">&#x27;.&#x27;</span>&#125;,</span><br><span class="line">    packages=[<span class="string">&#x27;databus&#x27;</span>],</span><br><span class="line">    ext_modules=cythonize([</span><br><span class="line">            Extension(</span><br><span class="line">                <span class="string">&#x27;*&#x27;</span>,</span><br><span class="line">                [<span class="string">&#x27;databus/*.pyx&#x27;</span>],</span><br><span class="line">                include_dirs=[<span class="string">&#x27;databus_c_lib/include&#x27;</span>],</span><br><span class="line">                library_dirs=[<span class="string">&#x27;databus_c_lib/lib&#x27;</span>],</span><br><span class="line">                libraries=[<span class="string">&#x27;databus&#x27;</span>])</span><br><span class="line">        ],</span><br><span class="line">        build_dir=<span class="string">&#x27;build/cython&#x27;</span>,</span><br><span class="line">        compiler_directives=&#123;<span class="string">&#x27;language_level&#x27;</span>: <span class="number">3</span>&#125;</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> PL </category>
          
          <category> Cython </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5.Cython模块导入</title>
      <link href="/program_language/cython/5.Cython%E6%A8%A1%E5%9D%97%E5%AF%BC%E5%85%A5/"/>
      <url>/program_language/cython/5.Cython%E6%A8%A1%E5%9D%97%E5%AF%BC%E5%85%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>Python 提供了 modules 和 packages 来组织项目，这允许函数、类、变量等，按照各自的功能或者实现的业务，分组到各自的逻辑单元中，从而使项目更容易理解和定位。并且模块和包也使得代码重用变得容易，在 Python 中我们使用 import 语句访问其它 module 和 package 中的函数。</p><p>而 Cython 也支持将项目分成多个模块，首先它完全支持 import 语句，并且含义与 Python 中的含义完全相同。这就允许我们在运行时访问外部纯 Python 模块中定义的 Python 对象，或者其它扩展模块中定义的可以被访问的 Python 对象。</p><p>只有 import 的话，Cython 是不允许两个模块访问彼此的 cdef、cpdef 定义的函数、ctypedef、struct 等等，也不允许访问对其它的扩展类型进行 C 一级的访问。比如：cython_test1.pyx 和 cython_test2.pyx ，这两个文件之间无法通过 import 互相访问，当然 cimport 也无法实现这一点。</p><p>而为了解决这一问题，Cython 提供了相应类型的文件来组织 Cython 文件以及 C 文件。到目前为止，一直使用扩展名为 .pyx 的 Cython 源文件，它是包含代码的逻辑的实现文件，但是除了它还有扩展名为 <strong>.pxd</strong> 的文件。</p><blockquote><p>pxd 文件你可以想象成类似于 C 中的头文件，用于存放一些声明之类的，而 Cython 的 cimport 就是从 pxd 文件中进行属性导入。</p></blockquote><p>这一节来介绍 cimport 语句的详细信息，以及 .pyx、.pxd 文件之间的相互联系，如何使用它们来构建更大的 Cython 项目。有了 cimport 和这两种类型的文件，就可以有效地组织 Cython 项目，而不会影响性能。</p><h1 id="2、Cython的实现文件和声明文件"><a href="#2、Cython的实现文件和声明文件" class="headerlink" title="2、Cython的实现文件和声明文件"></a>2、Cython的实现文件和声明文件</h1><p>如果的 Cython 项目非常小，并且没有其它代码需要访问里面的 C 级结构，那么一个 .pyx 文件足够了。但如果我们想要共享 pyx 文件中的 C 级结构，那么就需要 .pxd 文件了。</p><p>假设我们的文件还叫 cython_test.pyx。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> libc.stdlib cimport malloc, free</span><br><span class="line"></span><br><span class="line">ctypedef double real</span><br><span class="line"></span><br><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Girl</span>:</span><br><span class="line"></span><br><span class="line">    cdef public :</span><br><span class="line">        <span class="built_in">str</span> name  <span class="comment"># 姓名</span></span><br><span class="line">        long age  <span class="comment"># 年龄</span></span><br><span class="line">        <span class="built_in">str</span> gender  <span class="comment"># 性别</span></span><br><span class="line">    cdef real *scores  <span class="comment"># 分数, 这里我们的 double 数组长度为3, 但是 real * 不能被访问，所以它不可以使用 public</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__cinit__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        self.scores = &lt;real *&gt; malloc(<span class="number">3</span> * sizeof(real))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age, gender</span>):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.gender = gender</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__dealloc__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.scores != NULL:</span><br><span class="line">            free(self.scores)</span><br><span class="line"></span><br><span class="line">    cpdef <span class="built_in">str</span> get_info(self):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;name: <span class="subst">&#123;self.name&#125;</span>, age: <span class="subst">&#123;self.age&#125;</span>, gender: <span class="subst">&#123;self.gender&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">    cpdef set_score(self, <span class="built_in">list</span> scores):  </span><br><span class="line">        <span class="comment"># 虽然 not None 也可以写在参数后面, 但是它只适用于 Python 函数, 也就是 def 定义的函数</span></span><br><span class="line">        <span class="keyword">assert</span> scores <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">len</span>(scores) == <span class="number">3</span></span><br><span class="line">        cdef real score</span><br><span class="line">        cdef long idx</span><br><span class="line">        <span class="keyword">for</span> idx, score <span class="keyword">in</span> <span class="built_in">enumerate</span>(scores):</span><br><span class="line">            self.scores[idx] = score</span><br><span class="line"></span><br><span class="line">    cpdef <span class="built_in">list</span> get_score(self):</span><br><span class="line">        cpdef <span class="built_in">list</span> res = [self.scores[<span class="number">0</span>], self.scores[<span class="number">1</span>], self.scores[<span class="number">2</span>]]</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>目前来讲，由于所有内容都在一个 pyx 文件里面，因此任何 C 级属性都可以自由访问。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"></span><br><span class="line">g = cython_test.Girl(<span class="string">&#x27;python&#x27;</span>, <span class="number">16</span>, <span class="string">&#x27;female&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(g)  <span class="comment"># &lt;cython_test.Girl object at 0x000001C49D81B330&gt;</span></span><br><span class="line"></span><br><span class="line">g.get_info()</span><br><span class="line">g.set_score([<span class="number">90.4</span>, <span class="number">97.3</span>, <span class="number">97.6</span>])</span><br><span class="line"><span class="built_in">print</span>(g.get_score())  <span class="comment"># [90.4, 97.3, 97.6]</span></span><br></pre></td></tr></table></figure><p>访问非常地自由，没有任何限制，但是随着 Girl 这个类的功能越来越多的话，该怎么办呢？</p><p>所以需要创建一个 pxd 文件： cython_test.pxd，然后把我们希望暴露给外界访问的 C 级结构放在里面。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pxd</span></span><br><span class="line"></span><br><span class="line">ctypedef double real</span><br><span class="line"></span><br><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Girl</span>:</span><br><span class="line"></span><br><span class="line">    cdef public :</span><br><span class="line">        <span class="built_in">str</span> name  </span><br><span class="line">        long age  </span><br><span class="line">        <span class="built_in">str</span> gender  </span><br><span class="line">    cdef real *scores  </span><br><span class="line">    </span><br><span class="line">    cpdef <span class="built_in">str</span> get_info(self)</span><br><span class="line">    cpdef set_score(self, <span class="built_in">list</span> scores)  <span class="comment"># 如果参数有默认值，那么在声明的时候让其等于 * 即可，比如：arg=*，表示该函数的 arg 参数有默认值</span></span><br><span class="line">    cpdef <span class="built_in">list</span> get_score(self)</span><br></pre></td></tr></table></figure><p>看到在 pxd 文件中，只存放了 C 级结构的声明，像 ctypedef、cdef、cpdef 等等，并且函数的话只是存放了定义，函数体并没有写在里面，同理后面也不可以有冒号。另外，pxd 文件是在编译时访问的，而且不可以在里面放类似于 def 这样的纯 Python 声明，否则会发生编译错误，因为纯 Python 的数据结构直接定义就好，不存在什么声明。</p><p>所以 <strong>pxd 文件只放相应的声明，而它们的具体实现是在 pyx 文件中</strong>，因此有人发现了，这个 pxd 文件不就是 C 中的头文件吗？答案确实如此。</p><p>然后对应的 cython_test.pyx 文件也需要修改，cython_test.pyx 和 cython_test.pxd 具有相同的基名称，Cython 会将它们视为一个命名空间。另外，如果<strong>在 pxd 文件中声明了一个函数或者变量，那么在 pyx 文件中不可以再次声明</strong>，否则会发生编译错误。怎么理解呢？</p><blockquote><p>类似于 <code>cpdef func(): pass</code> 这种形式，它是一个函数（有定义）；但是 <code>cpdef func()</code> 这种形式，它只是一个函数声明。所以 Cython 的函数声明和 C 的函数声明也是类似的，函数在 Cython 中没有冒号、以及函数体的话，那么就是函数声明。而在 Cython 的 pyx 文件中也可以进行函数声明，就像 C 源文件中也是可以声明函数一样，但是一般都会把声明写在 h 头文件中，在 Cython 中也是如此，会把 C 级结构、一些声明写在 pxd 文件中。</p><p>而一旦声明了，就不可再次声明。比如 cdef public 那些成员变量，它们在 pxd 文件中已经声明了，那么 pyx 中就不可以再有了，否则就会出现变量的重复声明。</p></blockquote><p>重新修改pyx 文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> libc.stdlib cimport malloc, free</span><br><span class="line"></span><br><span class="line">cdef <span class="keyword">class</span> <span class="title class_">Girl</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__cinit__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        self.scores = &lt;real *&gt; malloc(<span class="number">3</span> * sizeof(real))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age, gender</span>):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.gender = gender</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__dealloc__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.scores != NULL:</span><br><span class="line">            free(self.scores)</span><br><span class="line"></span><br><span class="line">    cpdef <span class="built_in">str</span> get_info(self):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;name: <span class="subst">&#123;self.name&#125;</span>, age: <span class="subst">&#123;self.age&#125;</span>, gender: <span class="subst">&#123;self.gender&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">    cpdef set_score(self, <span class="built_in">list</span> scores):</span><br><span class="line">        <span class="keyword">assert</span> scores <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">len</span>(scores) == <span class="number">3</span></span><br><span class="line">        cdef real score</span><br><span class="line">        cdef long idx</span><br><span class="line">        <span class="keyword">for</span> idx, score <span class="keyword">in</span> <span class="built_in">enumerate</span>(scores):</span><br><span class="line">            self.scores[idx] = score</span><br><span class="line"></span><br><span class="line">    cpdef <span class="built_in">list</span> get_score(self):</span><br><span class="line">        cpdef <span class="built_in">list</span> res = [self.scores[<span class="number">0</span>], self.scores[<span class="number">1</span>], self.scores[<span class="number">2</span>]]</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>虽然结构没有什么变化，但是把一些 C 级数据拿到 pxd 文件中了，所以 pyx 文件中的可以直接删掉了，会自动到对应的 pxd 文件中找，因为它们有相同的基名称，Cython 会将其整体看成一个命名空间。所以：这里的 pyx 文件和 pxd 文件一定要有相同的基名称，只有这样才能够找得到，否则你会发现代码中 real 是没有被定义的，当然还有 self 的一些属性，因为它们必须要使用 cdef 在类里面进行声明。</p><p>然后调用方式还是和之前一样，也是没有任何问题的。</p><p>但是哪些东西我们才应该写在 pxd 文件中呢？本质上讲，任何在 C 级别上，需要对其它模块（pyx）公开的，才需要写在 pxd 文件中，比如：</p><ul><li><code>C类型声明--ctypedef、结构体、共同体、枚举(后续系列中介绍)</code></li><li><code>外部的C、C++库的声明(后续系列中介绍)</code></li><li><code>cdef、cpdef模块级函数的声明</code></li><li><code>cdef class扩展类的声明</code></li><li><code>扩展类的cdef属性</code></li><li><code>使用cdef、cpdef方法的声明</code></li><li><code>C级内联函数或者方法的实现</code></li></ul><p>但是，一个 pxd 文件不可以包含如下内容：</p><ul><li><code>Python函数和非内联C级函数、方法的实现</code></li><li><code>Python类的定义</code></li><li><code>IF或者DEF宏的外部Python可执行代码</code></li></ul><p>那么 pxd 文件都带来了哪些功能呢？那就是 cython_test.pyx 文件可以被其它的 pyx 文件导入了，这几个 pyx 文件作为一个整体为 Python 提供更强大的功能，否则的话其它的 pyx 文件是无法导入的。所以应该将需要其它 pyx 文件导入的内容在对应的 pxd 文件中进行声明，然后在导入的时候会去找 pxd 文件，根据里面声明去（和当前 pxd 文件具有相同基名称的 pyx 文件中）寻找对应的实现逻辑，而导入方式是使用 cimport。</p><blockquote><p>cimport 和 import 语法一致，只不过前者多了一个 c，但是 cimport 是用来导入 pxd 文件中声明的静态数据。</p></blockquote><h1 id="3、多文件相互导入"><a href="#3、多文件相互导入" class="headerlink" title="3、多文件相互导入"></a>3、多文件相互导入</h1><p>然后在另一个 pyx 文件中导入这个 cython_test.pyx，当然导入的话其实寻找的是 cython_test.pxd，然后调用的是 cython_test.pyx 里面的具体实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件名: caller.pyx</span></span><br><span class="line"><span class="keyword">from</span> cython_test cimport Girl</span><br><span class="line"></span><br><span class="line">cdef <span class="keyword">class</span> <span class="title class_">NewGirl</span>(<span class="title class_ inherited__">Girl</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>这里由于涉及到了多个 pyx 文件，所以先来介绍一下通过编译的方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> _locale</span><br><span class="line">_locale._getdefaultlocale = (<span class="keyword">lambda</span> *args: [<span class="string">&#x27;en_US&#x27;</span>, <span class="string">&#x27;utf8&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup, Extension</span><br><span class="line"><span class="keyword">from</span> Cython.Build <span class="keyword">import</span> cythonize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不用管 pxd, 会自动包含, 因为它们具有相同的基名称, cython 在编译的时候会自动寻找</span></span><br><span class="line">ext = [Extension(<span class="string">&quot;caller&quot;</span>, [<span class="string">&quot;caller.pyx&quot;</span>]),  </span><br><span class="line">       Extension(<span class="string">&quot;cython_test&quot;</span>, [<span class="string">&quot;cython_test.pyx&quot;</span>])]</span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    ext_modules=cythonize([</span><br><span class="line">            Extension(</span><br><span class="line">                <span class="string">&#x27;*&#x27;</span>,</span><br><span class="line">                [<span class="string">&#x27;*.pyx&#x27;</span>])</span><br><span class="line">        ],</span><br><span class="line">        build_dir=<span class="string">&#x27;build&#x27;</span>,</span><br><span class="line">        compiler_directives=&#123;<span class="string">&#x27;language_level&#x27;</span>: <span class="number">3</span>&#125;</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>编译的命令和之前一样，编译之后会发现原来的目录中有两个 pyd 文件了。</p><p><img src="./image/5_1.jpg" alt="img"></p><p>将这两个文件拷贝出来，首先在 caller.pyx 中是直接导入的 cython_test.pyx，因此这两个 pyd 文件要也在一个目录中。所以编译之后，还要注意它们之间的层级关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> caller</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(caller)  <span class="comment"># &lt;module &#x27;caller&#x27; from &#x27;D:\\satori\\caller.cp38-win_amd64.pyd&#x27;&gt;</span></span><br><span class="line">g = caller.NewGirl(<span class="string">&quot;古明地觉&quot;</span>, <span class="number">17</span>, <span class="string">&quot;female&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(g.get_info())  <span class="comment"># name: 古明地觉, age: 17, gender: female</span></span><br><span class="line"></span><br><span class="line">g.set_score([<span class="number">99.9</span>, <span class="number">90.4</span>, <span class="number">97.6</span>])</span><br><span class="line"><span class="built_in">print</span>(g.get_score())  <span class="comment"># [99.9, 90.4, 97.6]</span></span><br></pre></td></tr></table></figure><p>看到完全没有问题，而且我们还可以将 caller.pyx 写更复杂一些。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cython_test cimport Girl</span><br><span class="line"></span><br><span class="line">cdef <span class="keyword">class</span> <span class="title class_">NewGirl</span>(<span class="title class_ inherited__">Girl</span>):</span><br><span class="line"></span><br><span class="line">    cdef public <span class="built_in">str</span> where</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age, gender, where</span>):</span><br><span class="line">        self.where = where</span><br><span class="line">        <span class="built_in">super</span>().__init__(name, age, gender)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">new_get_info</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>(NewGirl, self).get_info() + <span class="string">f&quot;, where: <span class="subst">&#123;self.where&#125;</span>&quot;</span></span><br></pre></td></tr></table></figure><p><strong>重新编译之后，再次导入。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;build/lib.win-amd64-3.7&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> caller</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自己定义了 __init__, 传递 4 个参数, 前面 3 个会交给父类处理</span></span><br><span class="line">g = caller.NewGirl(<span class="string">&quot;cython&quot;</span>, <span class="number">17</span>, <span class="string">&quot;female&quot;</span>, <span class="string">&quot;hoem&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 父类的方法</span></span><br><span class="line"><span class="built_in">print</span>(g.get_info())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在父类的方法返回的结果之上，进行添加</span></span><br><span class="line"><span class="built_in">print</span>(g.new_get_info()) </span><br></pre></td></tr></table></figure><blockquote><p>看起来基本上和 Python 之间是没有区别，主要就是如果涉及到多个 pyx，那么这些 pyx 都要进行编译。并且想被导入，那么该 pyx 文件一定要有相同基名称的 pxd 文件。导入的时候使用 cimport，会去 pxd 文件中找，然后具体实现则是去 pyx 文件中寻找。</p></blockquote><p>另外，可能有人发现了，这里是绝对导入。但实际上，一些 pyd 文件会放在单独工程目录中，这时候应该采用相对导入，况且它无法作为启动文件，只能被导入。所以我们可以在 pyx 文件中进行相对导入，因为编译之后的 pyd 文件和之前的 pyx 文件之间的关系是对应的。</p><p>然后将之前的 cython_test.pxd、cython_test.pyx、caller.pyx 放在一个单独目录中。</p><p><img src="./image/5_2.jpg" alt="img"></p><p>然后将 caller.pyx 中的绝对导入改成相对导入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .cython_test cimport Girl</span><br></pre></td></tr></table></figure><p>然后编译扩展模块的时候可以用之前的方式编译，只不过 Extension 中文件路径要指定对。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> _locale</span><br><span class="line">_locale._getdefaultlocale = (<span class="keyword">lambda</span> *args: [<span class="string">&#x27;en_US&#x27;</span>, <span class="string">&#x27;utf8&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup, Extension</span><br><span class="line"><span class="keyword">from</span> Cython.Build <span class="keyword">import</span> cythonize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意: Extension 的第一个参数, 首先我们这个文件叫做 build_ext.py, 当前的目录层级如下</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">当前目录:</span></span><br><span class="line"><span class="string">    cython_relative_demo:</span></span><br><span class="line"><span class="string">        caller.pyx</span></span><br><span class="line"><span class="string">        cython_test.pxd</span></span><br><span class="line"><span class="string">        cython_test.pyx</span></span><br><span class="line"><span class="string">    build_ext.py  </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 所以我们的 build_ext.py 和 cython_relative_demo 是同级的</span></span><br><span class="line"><span class="comment"># 然后我们在编译的时候, name(Extension 的第一个参数) 不可以指定为 caller、cython_test</span></span><br><span class="line"><span class="comment"># 如果这么做的话, 当代码中涉及到相对导入的时候, 在编译时就会报错: relative cimport beyond main package is not allowed</span></span><br><span class="line"><span class="comment"># cython 编译器要求, 你在编译 pyx 文件、指定模块名的时候, 也需要把该 pyx 文件所在的目录也带上</span></span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    ext_modules=cythonize([</span><br><span class="line">            Extension(</span><br><span class="line">                <span class="string">&#x27;*&#x27;</span>,</span><br><span class="line">                [<span class="string">&#x27;cython_relative/*.pyx&#x27;</span>])</span><br><span class="line">        ],</span><br><span class="line">        build_dir=<span class="string">&#x27;build&#x27;</span>,</span><br><span class="line">        compiler_directives=&#123;<span class="string">&#x27;language_level&#x27;</span>: <span class="number">3</span>&#125;</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这样编译就没有问题了，执行 python build_ext.py build 编译成功，然后我们来看一下编译之后的目录：</p><p><img src="./image/5_3.jpg" alt="img"></p><p>看到多了之前指定的目录，其实个人觉得 cython_relative.caller 这种形式完全可以写成 caller，因为文件路径都已经指定了。但是 cython 编译器要求，在执行相对导入的时候不可以只指定模块名，也没得办法。</p><p>将这两个文件拷贝出来，移动到下面的 cython_relative 目录中，因为我们的 pyx 文件就是在那里定义的，所以编译之后也应该放在原来的位置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里不需要 pyximport 了, 因为导入的是已经编译好的 pyd 文件</span></span><br><span class="line"><span class="comment"># 当然即使有 pyximport, 也会优先导入 pyd 文件</span></span><br><span class="line"><span class="keyword">from</span> cython_relative_demo <span class="keyword">import</span> caller</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自己定义了 __init__, 传递 4 个参数, 前面 3 个会交给父类处理</span></span><br><span class="line">g = caller.NewGirl(<span class="string">&quot;cython&quot;</span>, <span class="number">17</span>, <span class="string">&quot;female&quot;</span>, <span class="string">&quot;hoem&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 父类的方法</span></span><br><span class="line"><span class="built_in">print</span>(g.get_info())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在父类的方法返回的结果之上，进行添加</span></span><br><span class="line"><span class="built_in">print</span>(g.new_get_info()) </span><br></pre></td></tr></table></figure><h1 id="4、预定义的-pxd文件"><a href="#4、预定义的-pxd文件" class="headerlink" title="4、预定义的.pxd文件"></a>4、预定义的.pxd文件</h1><p>之前的 <code>from libc.stdlib cimport malloc, free</code> ，显然这是 Cython 提供的，没错它就在 Cython 模块主目录下的 Includes 目录中，libc 这个包下面除了 stdlib 之外，还有 stdio、math、string 等 pxd 文件。除此之外，Includes 目录还有一个 libcpp 包对应的 pxd 文件，里面包含了 C++ 标准模板库（STL）容器的声明，如：string、vector、list、map、pair、set 等等。当然它们都是声明，但是在编译的时候会自动寻找相关实现，只不过实现逻辑需要借助编译器，而看不到罢了。</p><p>当然除了 libc、libcpp 之外，Includes 目录中还有其它的好东西，比如 cpython 目录，里面提供了大量的 pxd 文件，通过它们可以方便地访问 Python/C API。当然还有一个最重要的包就是 numpy，Cython 也是支持的，当然这些会在后面系列中介绍了。</p><h2 id="4-1-使用cimport导入一个C模块"><a href="#4-1-使用cimport导入一个C模块" class="headerlink" title="4.1 使用cimport导入一个C模块"></a>4.1 使用cimport导入一个C模块</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cython_test.pyx</span></span><br><span class="line"><span class="keyword">from</span> libc cimport math</span><br><span class="line"><span class="built_in">print</span>(math.sin(math.pi / <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> libc.math cimport sin, pi  </span><br><span class="line"><span class="built_in">print</span>(sin(pi / <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1.0</span></span><br><span class="line"><span class="string">1.0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>cimport 的使用方式和 import 是一致的，但只不过上面导入的是更快的 C 版本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> libcpp.vector cimport vector</span><br><span class="line"></span><br><span class="line">cdef vector[<span class="built_in">int</span>] *v1 = new vector[<span class="built_in">int</span>](<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>Cython 也支持从 C++ STL 中导入 C++ 类。另外，如果使用 import、cimport 导入了具有相同名称的不同函数，Cython 将引发一个编译错误。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> libc.math cimport sin</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sin</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Error compiling Cython file:</span></span><br><span class="line"><span class="string">------------------------------------------------------------</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">from libc.math cimport sin</span></span><br><span class="line"><span class="string">from math import sin                ^</span></span><br><span class="line"><span class="string">------------------------------------------------------------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">cython_test.pyx:2:17: Assignment to non-lvalue &#x27;sin&#x27;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>为了修复这一点，只需要这么做。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">from</span> libc.math cimport sin <span class="keyword">as</span> c_sin</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sin <span class="keyword">as</span> py_sin</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c_sin(<span class="number">3.1415926</span> / <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(py_sin(<span class="number">3.1415926</span> / <span class="number">2</span>))</span><br><span class="line"> <span class="keyword">import</span> pyximport</span><br><span class="line">pyximport.install(language_level=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cython_test</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0.9999999999999997</span></span><br><span class="line"><span class="string">0.9999999999999997</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>此时就没有任何问题了。但如果导入的是模块的话，那么是可以重名的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> libc cimport math</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(math.sin(math.pi / <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>尽管 import math 是在下面，但是调用的时候会从 C 标准库中进行调用，但是不管怎么样，这种做法总归是不好的。应该修改一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">from</span> libc cimport math <span class="keyword">as</span> c_math</span><br><span class="line"><span class="keyword">import</span> math <span class="keyword">as</span> py_math</span><br></pre></td></tr></table></figure><p>因此这些预定义的 pxd 文件就类似于 C、C++ 中的头文件。</p><ul><li><code>它们都声明了 C 一级的数据结构供外界调用</code></li><li><code>它们都允许我们对功能进行拆分, 分别通过不同的模块实现</code></li><li><code>它们都实现了公共的 C 级接口</code></li></ul><p>C、C++ 头文件通过 #include 命令进行访问，该命令会对相应的头文件进行包含。而 Cython 的 cimport 更智能，也更不容易出错，可以把它看做是一个使用命名空间的编译时导入语句。</p><p>而 Cython 的早期没有 cimport 语句，而是有一个 include 语句，之前说过了，它是在源码级对文件进行包含。而 include 则类似于 Python 的import，如果你觉得这个文件内容太多了，那么可以单独进行拆分，然后再使用 include 包含进去。</p>]]></content>
      
      
      <categories>
          
          <category> PL </category>
          
          <category> Cython </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PL </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
