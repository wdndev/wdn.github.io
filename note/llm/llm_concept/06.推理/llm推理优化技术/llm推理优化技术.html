<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>37.2° Blog | 37.2° Blog</title><meta name="author" content="Dongnian"><meta name="copyright" content="Dongnian"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="llm推理优化技术 - wolai 笔记原文链接：Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog堆叠Transformer层以创建大型模型可以获得更好的准确性、few-shot学习能力，甚至在各种语言任务中具有接近人类的涌现能力。这些基础模型的训练成本很高，而且在推理过程中可能需要大量的内存和计算（">
<meta property="og:type" content="website">
<meta property="og:title" content="37.2° Blog">
<meta property="og:url" content="https://wdndev.github.io/note/llm/llm_concept/06.%E6%8E%A8%E7%90%86/llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="llm推理优化技术 - wolai 笔记原文链接：Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog堆叠Transformer层以创建大型模型可以获得更好的准确性、few-shot学习能力，甚至在各种语言任务中具有接近人类的涌现能力。这些基础模型的训练成本很高，而且在推理过程中可能需要大量的内存和计算（">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2024-12-08T03:56:28.134Z">
<meta property="article:modified_time" content="2024-12-08T03:56:28.134Z">
<meta property="article:author" content="Dongnian">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/note/llm/llm_concept/06.%E6%8E%A8%E7%90%86/llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Dongnian","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '37.2° Blog',
  isPost: false,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-08 11:56:28'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-solid fa-magnifying-glass"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/note"><i class="fa-fw fa-regular fa-bookmark"></i><span> Note</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-solid fa-magnifying-glass"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/note"><i class="fa-fw fa-regular fa-bookmark"></i><span> Note</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="page"><h1 class="page-title"></h1><div id="article-container"><!DOCTYPE html>
<html lang="zh-Hans-CN"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=Edge"/><link rel="stylesheet" type="text/css" href="../../css/modern-norm.min.css"/><link rel="stylesheet" type="text/css" href="../../css/prism.min.css"/><link rel="stylesheet" type="text/css" href="../../css/katex.min.css"/><link rel="stylesheet" type="text/css" href="../../css/wolai.css"/><title>llm推理优化技术 - wolai 笔记</title><link rel="shortcut icon" href="data:image/svg+xml,%3Csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 800 800&apos;%3E%3Cdefs%3E%3Cstyle%3E.cls-1%7Bfill:%23fff;%7D%3C/style%3E%3C/defs%3E%3Cg%3E%3Cpath class=&apos;cls-1&apos; d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Z&apos;/%3E%3Cpath d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Zm4.72,88.9H185.2L172.42,89c-32.78.62-43.68,3.24-54.71,9.14a45.84,45.84,0,0,0-19.54,19.54c-6.61,12.36-9.11,24.55-9.27,67.49V614.8L89,627.58c.62,32.78,3.24,43.68,9.14,54.71a45.84,45.84,0,0,0,19.54,19.54c12.36,6.61,24.55,9.11,67.49,9.27H610.08c46.79,0,59.41-2.44,72.21-9.28a45.84,45.84,0,0,0,19.54-19.54c6.61-12.36,9.11-24.55,9.27-67.49V189.92c0-46.79-2.44-59.41-9.28-72.21a45.84,45.84,0,0,0-19.54-19.54C669.93,91.56,657.74,89.06,614.8,88.9ZM233.33,493.33A73.34,73.34,0,1,1,160,566.67,73.35,73.35,0,0,1,233.33,493.33Z&apos;/%3E%3C/g%3E%3C/svg%3E"></link></head><body><header><div class="image"></div><div class="title"><div class="banner"><div class="icon"></div></div><div data-title="llm推理优化技术" class="main-title"></div></div></header><article><blockquote id="6sAm4miD5PYLQYqZwEYWX3" class="wolai-block"><span class="inline-wrap">原文链接：</span><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/"><span>Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog</span></a></span></blockquote><div id="asHV8UPZwx36zzURiUjSdU" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image.png" style="width: 100%"/></figure></div><div id="eKHztgbRSBiGqtju1p1LPZ" class="wolai-block wolai-text"><div><span class="inline-wrap">堆叠<span class="jill"></span>Transformer<span class="jill"></span>层以创建大型模型可以获得更好的准确性、few-shot<span class="jill"></span>学习能力，甚至在各种语言任务中具有接近人类的涌现能力。这些基础模型的训练成本很高，而且在推理过程中可能需要大量的内存和计算（经常性成本）。当今最流行的大型语言模型（LLM）的大小可以达到数百亿到数千亿个参数，并且根据用例的不同，可能需要摄入长输入（或上下文），这也会增加开销。
</span></div></div><div id="5GPiLkewNjH5QNUP3X37ZL" class="wolai-block wolai-text"><div><span class="inline-wrap">这篇文章讨论了<span class="jill"></span>LLM<span class="jill"></span>推理中最紧迫的挑战，以及一些实用的解决方案。读者应该对<span class="jill"></span>transformer<span class="jill"></span>架构和注意力机制有一个基本的了解。</span></div></div><h1 id="q5RMS1i4SLNMBN9K1EKAnB" class="wolai-block"><span class="inline-wrap">1.理解<span class="jill"></span>LLM<span class="jill"></span>推理</span></h1><div id="84TNjZkSBLkv3tLR9npAhS" class="wolai-block wolai-text"><div><span class="inline-wrap">大多数流行的<span class="jill"></span>only-decode LLM（例如 GPT-3）都是针对因果建模目标进行预训练的，本质上是作为下一个词预测器。这些 LLM 将一系列<span class="jill"></span>tokens<span class="jill"></span>作为输入，并自回归生成后续<span class="jill"></span>tokens，直到满足停止条件（例如，生成<span class="jill"></span>tokens<span class="jill"></span>数量的限制或遇到停止词）或直到生成特殊的 </span><span class="inline-wrap"><code>&lt;end&gt;</code></span><span class="inline-wrap"> 标记生成结束的<span class="jill"></span>tokens。该过程涉及两个阶段：预填充阶段和解码阶段。

请注意，tokens<span class="jill"></span>是模型处理的语言的原子部分。一个<span class="jill"></span>tokens<span class="jill"></span>大约是四个英文字符。所有自然语言在输入模型之前都会转换为<span class="jill"></span>toikens。</span></div></div><h2 id="vQ6Uvy3fjUXpeHQiVP3woB" class="wolai-block"><span class="inline-wrap">1.1 预填充阶段或处理输入</span></h2><div id="mXvBD3qALMLUdWiY5THpwE" class="wolai-block wolai-text"><div><span class="inline-wrap">在预填充阶段，LLM<span class="jill"></span>处理输入<span class="jill"></span>token<span class="jill"></span>以计算中间状态（keys<span class="jill"></span>和<span class="jill"></span>value），用于生成“第一个”token。每个新的<span class="jill"></span>token<span class="jill"></span>都依赖于所有先前的<span class="jill"></span>token，但由于输入的全部已知，因此在运算上，都是高度并行化矩阵运算，可以有效地使用<span class="jill"></span>GPU。</span></div></div><h2 id="68fV8aNsjpMbm8V8wXwk6K" class="wolai-block"><span class="inline-wrap">1.2 解码阶段或生成输出</span></h2><div id="tyXcWGNoccKquHTYrnPaq" class="wolai-block wolai-text"><div><span class="inline-wrap">在解码阶段，LLM<span class="jill"></span>一次自回归生成一个输出<span class="jill"></span>token，直到满足停止条件。每个输出<span class="jill"></span>tokens<span class="jill"></span>都需要直到之前迭代的所有输出状态（keys<span class="jill"></span>和<span class="jill"></span>values）。这与预填充输入处理相比，就像矩阵向量运算未充分利用<span class="jill"></span>GPU<span class="jill"></span>计算能力。数据（weights, keys, values, activations） 从内存传输到<span class="jill"></span>GPU<span class="jill"></span>的速度决定了延迟，而不是计算实际时间消耗。即，这是一个内存限制操作。</span></div></div><div id="6Lpha5AEz1TAdNGogPE1Fu" class="wolai-block wolai-text"><div><span class="inline-wrap">本文中的许多推理挑战和相应的解决方案都涉及此解码阶段的优化：高效的注意力模块、有效管理键和值等。</span></div></div><div id="tBz2DyGSYAHBMGPwcbc6DA" class="wolai-block wolai-text"><div><span class="inline-wrap">不同的<span class="jill"></span>LLMs<span class="jill"></span>可能使用不同的<span class="jill"></span>tokenizers，因此比较它们之间的输出<span class="jill"></span>tokens<span class="jill"></span>可能并不简单。在比较推理吞吐量时，即使两个 LLMs<span class="jill"></span>每秒输出的<span class="jill"></span>tokens<span class="jill"></span>相似，如果它们使用不同的<span class="jill"></span>tokenizers，也可能不相等。这是因为相应的<span class="jill"></span>tokens<span class="jill"></span>可能代表不同数量的字符。</span></div></div><h2 id="u8U6zYZyRXtMadGutqsVob" class="wolai-block"><span class="inline-wrap">1.3 批处理（Batching）</span></h2><div id="9pMgiuC76aQJcPhS3LiixL" class="wolai-block wolai-text"><div><span class="inline-wrap">提高 GPU 利用率和有效吞吐量的最简单方法是通过</span><span class="red inline-wrap"><b>批处理</b></span><span class="inline-wrap">。由于多个请求使用相同的模型，因此权重的内存成本被分散。大批量数据传输到 GPU 一次处理，将提高<span class="jill"></span>GPU<span class="jill"></span>资源的利用率。</span></div></div><div id="5js6HrLd98sVd9XK5mH6m" class="wolai-block wolai-text"><div><span class="inline-wrap">然而，批量大小只能增加到一定限制，此时可能会导致内存溢出。为了防止这种情况发生，需要查看键值 (KV) 缓存和 LLM 内存要求。</span></div></div><div id="uGfdunYoY67kPxPq2oudaA" class="wolai-block wolai-text"><div><span class="inline-wrap">传统批处理（也称为静态批处理， static batching）不是最佳的。这是因为</span><span class="red inline-wrap"><b>对于批次中的每个请求，LLM 可能会生成不同数量的<span class="jill"></span>tokens，并且不同<span class="jill"></span>tokens<span class="jill"></span>有不同的执行时间</b></span><span class="inline-wrap">。因此，批次中的所有请求都必须等待最长<span class="jill"></span>token<span class="jill"></span>的处理完成，而生成长度的巨大差异可能会加剧这种情况。有一些方法可以缓解这种情况，例如稍动态批处理。</span></div></div><h2 id="mMzXqRDMad13M6MSLTVJSS" class="wolai-block"><span class="inline-wrap">1.4 KV<span class="jill"></span>缓存</span></h2><div id="7AMAzeS8kxsQEUcSvJiCjn" class="wolai-block wolai-text"><div><span class="inline-wrap">解码阶段的一种常见优化是 KV 缓存。解码阶段在每个时间步生成单个<span class="jill"></span>token，但每个<span class="jill"></span>token<span class="jill"></span>依赖于之前<span class="jill"></span>token<span class="jill"></span>的键和值张量（包括预填充时计算的输入<span class="jill"></span>tokens<span class="jill"></span>的 KV 张量，以及当前时间步之前计算的任何新 KV 张量） 。</span></div></div><div id="81Yt17uNn4WhBXgAodGxow" class="wolai-block wolai-text"><div><span class="inline-wrap">为了避免在每个时间步重新计算所有<span class="jill"></span>tokens<span class="jill"></span>的这些张量，</span><span class="red inline-wrap"><b>可以将它们缓存在 GPU 内存中</b></span><span class="inline-wrap">。每次迭代，当需要计算新<span class="jill"></span>token<span class="jill"></span>时，它们都会被添加到正在运行的缓存中，以便在下一次迭代中使用。在一些实现中，模型的每一层都有一个<span class="jill"></span>KV<span class="jill"></span>缓存。</span></div></div><div id="poW4ncWYfPCeTt3tnimymm" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_1.png" style="width: 619px"/></figure></div><blockquote id="aXtUPSHPmB3syHKjoPNFTt" class="wolai-block"><span class="inline-wrap">图<span class="jill"></span>1 KV<span class="jill"></span>缓存机制</span></blockquote><h2 id="rhRuUkrr2N75Eu7z83eXbw" class="wolai-block"><span class="inline-wrap">1.5 LLM<span class="jill"></span>内存需求</span></h2><div id="2LqwXWia6cwXdrk95zTbDF" class="wolai-block wolai-text"><div><span class="inline-wrap">实际上，LLM<span class="jill"></span>对<span class="jill"></span>GPU<span class="jill"></span>显存的需求主要是模型权重和<span class="jill"></span>KV<span class="jill"></span>缓存：</span></div></div><ul class="wolai-block"><li id="3cSadeByG7VUmecRTBFy8s"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>模型权重</b></span><span class="inline-wrap">：模型参数占用内存。例如，具有 70 亿个参数的模型（例如 Llama2-7B），以 16 位精度（FP16 或 BF16）加载，将占用大约 </span><span class="inline-wrap"><code>7B * sizeof(FP16) ~= 14 GB</code></span><span class="inline-wrap"> 的内存。</span></li><li id="syMv1JqQ125hD2DMHtxrAS"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>KV<span class="jill"></span>缓存</b></span><span class="inline-wrap">：自注意力张量的缓存占用内存，避免冗余计算。</span></li></ul><div id="7ocnP9DiXGARpngXnzEMwC" class="wolai-block wolai-text"><div><span class="inline-wrap">使用批处理时，批处理中每个请求的 KV 缓存仍然必须单独分配，并且可能会占用大量内存。下面的公式描述了 KV 缓存的大小，适用于当今最常见的 LLM 架构。</span></div></div><div id="nTduHLtFU96eUyVbnmxZs3" class="wolai-block wolai-text"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>每个</mtext><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi><mtext>的</mtext><mi>K</mi><mi>V</mi><mtext>缓存大小</mtext><mo stretchy="false">(</mo><mtext>字节</mtext><mo stretchy="false">)</mo><mo>=</mo><mn>2</mn><mo>∗</mo><mo stretchy="false">(</mo><mi>n</mi><mi>u</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>s</mi><mo stretchy="false">)</mo><mo>∗</mo><mo stretchy="false">(</mo><mi>n</mi><mi>u</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mo>∗</mo><mi>d</mi><mi>i</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>b</mi><mi>y</mi><mi>t</mi><mi>e</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">每个token的KV缓存大小(字节) = 2 * (num\_layers) * (num\_heads * dim\_head) *  precision\_in\_bytes

</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord cjk_fallback">每个</span><span class="mord mathnormal">t</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord cjk_fallback">的</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord cjk_fallback">缓存大小</span><span class="mopen">(</span><span class="mord cjk_fallback">字节</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal">yers</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">im</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">rec</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">in</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">b</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">t</span><span class="mord mathnormal">es</span></span></span></span></span></div><div id="w35znzRP1c3sAHwkAVbW1a" class="wolai-block wolai-text"><div><span class="inline-wrap">第一个因子 2 代表 K 和 V 矩阵。通常，</span><span class="inline-wrap"><code>(num_heads * dim_head)</code></span><span class="inline-wrap">的值与<span class="jill"></span>Transformer<span class="jill"></span>的</span><span class="inline-wrap"><code>hidden_​​size</code></span><span class="inline-wrap">（或模型的维度，</span><span class="inline-wrap"><code>d_model</code></span><span class="inline-wrap">）相同。这些模型属性通常可以在配置文件中找到。</span></div></div><div id="dfGTmMstcw4rSHovKNGs7W" class="wolai-block wolai-text"><div><span class="inline-wrap">输入批次中输入序列中的每个<span class="jill"></span>tokens<span class="jill"></span>都需要此内存大小。假设半精度，KV<span class="jill"></span>缓存的总大小由以下公式给出:</span></div></div><div id="77iy4XVyB8XoXqRA6x9jWF" class="wolai-block wolai-text"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>总</mtext><mi>K</mi><mi>V</mi><mtext>缓存大小</mtext><mo stretchy="false">(</mo><mtext>字节</mtext><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo stretchy="false">)</mo><mo>∗</mo><mo stretchy="false">(</mo><mi>s</mi><mi>e</mi><mi>q</mi><mi>u</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo stretchy="false">)</mo><mo>∗</mo><mn>2</mn><mo>∗</mo><mo stretchy="false">(</mo><mi>n</mi><mi>u</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>s</mi><mo stretchy="false">)</mo><mo>∗</mo><mo stretchy="false">(</mo><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mi>o</mi><mi>f</mi><mo stretchy="false">(</mo><mi>F</mi><mi>P</mi><mn>16</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">总KV缓存大小(字节)=(batch\_size) * (sequence\_length) * 2 * (num\_layers) * (hidden\_size) *  sizeof(FP16)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord cjk_fallback">总</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord cjk_fallback">缓存大小</span><span class="mopen">(</span><span class="mord cjk_fallback">字节</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord mathnormal">ba</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord mathnormal">se</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mord mathnormal">u</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">ce</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal">yers</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">(</span><span class="mord mathnormal">hi</span><span class="mord mathnormal">dd</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">zeo</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">FP</span><span class="mord">16</span><span class="mclose">)</span></span></span></span></span></div><div id="pTZr4CRpSRnWxTiESGDtHT" class="wolai-block wolai-text"><div><span class="inline-wrap">例如，对于 16 位精度的 Llama 2 7B 模型，批量大小为 </span><span class="inline-wrap"><code>1</code></span><span class="inline-wrap">，KV 缓存的大小将为 </span><span class="inline-wrap"><code>1 * 4096 * 2 * 32 * 4096 * 2</code></span><span class="inline-wrap"> 字节，即约 </span><span class="inline-wrap"><code>2 GB</code></span><span class="inline-wrap">。</span></div></div><div id="f3hmcCCXUsJmzQFTkNssdG" class="wolai-block wolai-text"><div><span class="inline-wrap">高效的管理 KV 缓存是一项具有挑战性的工作。</span><span class="red inline-wrap">内存需求随着批量大小和序列长度线性增长</span><span class="inline-wrap">，可以快速扩展。因此，它限制了可服务的吞吐量，并对长上下文输入提出了挑战。这就是本文中介绍的多项优化背后的动机。</span></div></div><h1 id="rzmH1QA46nKkmM5F8gs6hp" class="wolai-block"><span class="inline-wrap">2.模型并行化扩展<span class="jill"></span>LLM</span></h1><div id="ty96Rz2CYdxHT6h545CbuQ" class="wolai-block wolai-text"><div><span class="inline-wrap">减少模型权重在每设备的显存占用的一种方法是</span><span class="red inline-wrap"><b>将模型分布在多个 GPU 上</b></span><span class="inline-wrap">。分散内存和计算可以运行更大的模型或更大批量的输入。模型并行化是训练或推理模型所必需的，模型并行化需要比单个设备更多的内存，用来训练和推理（延迟或吞吐量）。根据模型权重的划分方式，有多种方法可以并行化模型。</span></div></div><div id="feSYas79ZX8uRFWE1DuuTB" class="wolai-block wolai-text"><div><span class="inline-wrap">请注意，</span><span class="red inline-wrap">数据并行性</span><span class="inline-wrap">也是一种经常在与下面列出的其他技术相同的的技术。在这种情况下，</span><span class="red inline-wrap">模型的权重被复制到多个设备上，并且输入的（全局）批量大小在每个设备上被分成微批次</span><span class="inline-wrap">。它通过处理较大的批次来减少总体执行时间。然而，这是一种训练时间优化，在推理过程中不太相关。</span></div></div><h2 id="iabGUbyVfuA7F1JjaSnJcR" class="wolai-block"><span class="inline-wrap">2.1 Pipeline<span class="jill"></span>并行</span></h2><div id="6Nv6b7mXMQk4nVphw8GKiY" class="wolai-block wolai-text"><div><span class="inline-wrap">Pipeline<span class="jill"></span>并行化</span><span class="red inline-wrap"><b>将模型（垂直）分片为块，其中每个块包含在单独设备上执行的层的子集</b></span><span class="inline-wrap">。图 2a 说明了四路<span class="jill"></span>Pipeline，其中模型按顺序分区，并且所有层的四分之一子集在每个设备上执行。一个设备上的一组操作的输出被传递到下一个设备，后者继续执行后续块。</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">F_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">和 </span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">B_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">分别表示设备 </span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span><span class="inline-wrap"> 上的前向传播和后向传播。每个设备上存储模型权重的内存需求被分成四份。</span></div></div><div id="sdHDVmEPjst5DSfgb7dsez" class="wolai-block wolai-text"><div><span class="inline-wrap">该方法的缺点是，由于处理的顺序性质，</span><span class="red inline-wrap"><b>某些设备或层在等待前一层的输出（激活、梯度）时可能保持空闲状态</b></span><span class="inline-wrap">。这会导致前向和后向传递效率低下或出现“Pipeline bubbles”。在图 2b 中，白色空白区域是<span class="jill"></span>Pipeline<span class="jill"></span>并行性产生的<span class="jill"></span>Pipeline bubbles，其中设备闲置且未得到充分利用。</span></div></div><div id="tFsesJ1e3fAVtPmMwfEmLA" class="wolai-block wolai-text"><div><span class="red inline-wrap"><b>微批处理可以在一定程度上缓解这种情况</b></span><span class="inline-wrap">，如图 2c 所示。输入的全局批次大小被分成子批次，这些子批次被一一处理，最后累积梯度。请注意，</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mi>n</mi><mo separator="true">,</mo><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">F_{n,m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap"> 和 </span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mrow><mi>n</mi><mo separator="true">,</mo><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">B_{n,m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap"> 分别表示设备</span><span class="inline-wrap"><code>n</code></span><span class="inline-wrap">上</span><span class="inline-wrap"><code>m</code></span><span class="inline-wrap">批次的前向和后向传递。</span><span class="red inline-wrap"><b>这种方法缩小了管道气泡的尺寸，但并没有完全消除它们</b></span><span class="inline-wrap">。</span></div></div><div id="rP3xHXHLAeNnWVUPoCZwgB" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_2.png" style="width: 100%"/></figure></div><blockquote id="gCNtXjcFkewBT5joCkKo6a" class="wolai-block"><span class="inline-wrap">图<span class="jill"></span>2 Pipeline<span class="jill"></span>并行，</span></blockquote><h2 id="ttKP6SfrxC2P9Wzv3bicpW" class="wolai-block"><span class="inline-wrap">2.2 Tensor<span class="jill"></span>并行</span></h2><div id="6bpD2i7GTvsCcw1xb7KyiS" class="wolai-block wolai-text"><div><span class="inline-wrap">Tensor<span class="jill"></span>并行化</span><span class="red inline-wrap"><b>将模型的各个层（水平）分片为更小的、独立的计算块，这些计算块可以在不同的设备上执行</b></span><span class="inline-wrap">。Transformer<span class="jill"></span>的主要组成部分，注意力块和多层感知器（MLP）层是可以利用<span class="jill"></span>Tensor<span class="jill"></span>并行化的。在多头注意力块中，每个头或一组头可以分配给不同的设备，以便它们可以独立且并行地计算。</span></div></div><div id="dxJftmviKdHghUWetdCw2u" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_3.png" style="width: 563px"/></figure></div><blockquote id="rJqyuAo8yX7NhecWDe9nWL" class="wolai-block"><span class="inline-wrap">图<span class="jill"></span>3 Tensor<span class="jill"></span>并行化<span class="jill"></span>MLP<span class="jill"></span>和自注意力</span></blockquote><div id="wdsX1YfNQjLToLfb9GR9ZT" class="wolai-block wolai-text"><div><span class="inline-wrap">图 3a 显示了两层 MLP Tensor<span class="jill"></span>并行的示例，每一层都由一个圆角框表示。在第一层中，权重矩阵</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span></span><span class="inline-wrap">分为</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">A_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">和</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">A_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap"> 。对于输入<span class="jill"></span>X，可以在同一批次不同设备上计算</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><msub><mi>A</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">XA_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap"> 和</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><msub><mi>A</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">XA_2 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">，其中，f<span class="jill"></span>是</span><span class="inline-wrap">identity 操作</span><span class="inline-wrap">。这将每个设备上存储权重的内存需求减半。归约操作</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="inline-wrap">组合了第二层的输出。</span></div></div><div id="kkUwns4Y9SnChwjTRVmXcq" class="wolai-block wolai-text"><div><span class="inline-wrap">图 3b 是自注意力层中<span class="jill"></span>Tensor<span class="jill"></span>并行的示例。多个注意力头本质上是并行的，并且可以跨设备分割。</span></div></div><h2 id="61SHu5XqEnkis83J1FprRU" class="wolai-block"><span class="inline-wrap">2.3 Sequence<span class="jill"></span>并行</span></h2><div id="3V6twDCtv2aQwJdJtm6hv6" class="wolai-block wolai-text"><div><span class="inline-wrap">Tensor<span class="jill"></span>并行化是有局限性，它需要</span><span class="red inline-wrap">将层划分为独立的、可管理的块，</span><span class="inline-wrap">不适用于 </span><span class="inline-wrap"><code>LayerNorm </code></span><span class="inline-wrap">和 </span><span class="inline-wrap"><code>Dropout </code></span><span class="inline-wrap">等操作，而是在<span class="jill"></span>tensor<span class="jill"></span>并行中复制。虽然 </span><span class="inline-wrap"><code>LayerNorm </code></span><span class="inline-wrap">和 </span><span class="inline-wrap"><code>Dropout </code></span><span class="inline-wrap">的计算成本较低，但它们确实需要大量内存来存储（冗余）激活。</span></div></div><div id="uAyVBWBspaAuh3m78ypV6W" class="wolai-block wolai-text"><div><span class="inline-wrap">如</span><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.05198.pdf"><span>Reducing Activation Recomputation in Large Transformer Models</span></a></span><span class="inline-wrap">所示，这些操作在输入序列中是独立的，并且这些操作</span><span class="red inline-wrap"><b>可以沿着“序列维度”进行分区</b></span><span class="inline-wrap">，从而提高内存效率。这称为序列并行性。</span></div></div><div id="ch2VBrzN7CPQcrUgLjkfvW" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_4.png" style="width: 100%"/></figure></div><blockquote id="uZfe5buTBMpXYV4UaC3Aiu" class="wolai-block"><span class="inline-wrap">图<span class="jill"></span>4，transformer<span class="jill"></span>层的<span class="jill"></span>tensor<span class="jill"></span>并行化和<span class="jill"></span>sequence<span class="jill"></span>并行化</span></blockquote><div id="tQ1LbieanrDoypzweHskpK" class="wolai-block wolai-text"><div><span class="inline-wrap">模型并行技术不是唯一的，可以结合使用。它们可以帮助扩展和减少 LLM 的每 GPU 内存占用量，但也有专门针对注意力模块的优化技术。</span></div></div><h1 id="f38FHBB7zYXPB7bomv7HLt" class="wolai-block"><span class="inline-wrap">3.注意力机制优化</span></h1><div id="atKR1bXCC6bUe33SxtVzKg" class="wolai-block wolai-text"><div><span class="inline-wrap">缩放点积注意力 (SDPA， scaled dot-product attention) 操作将</span><span class="inline-wrap"><code>query</code></span><span class="inline-wrap">和</span><span class="inline-wrap"><code>key</code></span><span class="inline-wrap">对映射到输出，如论文</span><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf"><span>Attention Is All You Need</span></a></span><span class="inline-wrap">所述。</span></div></div><h2 id="5yjivuEBr4r9Xe1cnD3waU" class="wolai-block"><span class="inline-wrap">3.1 多头注意力（MHA）</span></h2><div id="asivrqTk94WnjjFkrWy66u" class="wolai-block wolai-text"><div><span class="inline-wrap">作为 SDPA 的增强，三个变换张量对<span class="jill"></span>Q，K，V<span class="jill"></span>分别进行线性变换，</span><span class="inline-wrap"><b>这些变换不会改变原有张量的尺寸</b></span><span class="inline-wrap">，使模型能够共同关注来自不同位置的不同表示子空间的信息。这些子空间是独立学习的，使模型能够更丰富地理解输入中的不同位置。</span></div></div><div id="rrw7H8WGUxMxHsdJJ1sgXe" class="wolai-block wolai-text"><div><span class="inline-wrap">如图 5 所示，多个并行注意力操作的输出被拼接后线性投影以组合起来。每个并行注意力层称为“头”，这种方法称为多头注意力（MHA）。</span></div></div><div id="cGb6VLUohMP4jpzY8gpeBx" class="wolai-block wolai-text"><div><span class="inline-wrap">当使用八个并行注意力头时，每个注意力头的维度都会减少（例如 </span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi mathvariant="normal">_</mi><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mi mathvariant="normal">/</mi><mn>8</mn></mrow><annotation encoding="application/x-tex">d\_model/8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal">d</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord">/8</span></span></span></span></span><span class="inline-wrap">）。这使得计算成本与单头注意力相似。</span></div></div><div id="3U1xjMcJrq2FMqAHxeRiu1" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_5.png" style="width: 100%"/></figure></div><blockquote id="oMEYSQk3UFiy9rubSvUn8y" class="wolai-block"><span class="inline-wrap">图<span class="jill"></span>5 缩放点积注意力（左）和多头注意力（右）的图示，并行的多个 SDPA 头</span></blockquote><h2 id="cXnyMTez9Xaj7vzz5pZisY" class="wolai-block"><span class="inline-wrap">3.2 多查询注意力（MQA）</span></h2><div id="8LUKBPujjHXSnc5hHmnUMa" class="wolai-block wolai-text"><div><span class="inline-wrap">MHA 的推理优化之一称为多查询注意力 (MQA)，如 Fast Transformer Decoding 中提出的，</span><span class="red inline-wrap"><b>在多个注意力头之间共享键和值</b></span><span class="inline-wrap">。与以前一样，查询向量仍然被投影多次。</span></div></div><div id="oqnvXstxudVyVGuVRBNfgo" class="wolai-block wolai-text"><div><span class="inline-wrap">虽然 MQA 中完成的计算量与 MHA 相同，但从内存读取的数据量（键、值）只是以前的一小部分。当受内存带宽限制时，这可以实现更好的计算利用率。它还减少了内存中 KV 缓存的大小，为更大的批量大小留出了空间。</span></div></div><div id="sy5JQrruzqYc7GmmuNzmty" class="wolai-block wolai-text"><div><span class="inline-wrap">key<span class="jill"></span>头的减少会带来潜在的准确性下降。此外，需要在推理时利用这种优化的模型需要在启用 MQA 的情况下进行训练（或至少使用大约 5% 的训练量进行微调）。</span></div></div><h2 id="tzPepHStTMD9cWz6CPNtVm" class="wolai-block"><span class="inline-wrap">3.3 分组注意力（GQA）</span></h2><div id="gqr4bcg9CVGCzsRdEP2vQV" class="wolai-block wolai-text"><div><span class="inline-wrap">分组查询注意力 (GQA) 通过将键和值投影到几组查询头，在 MHA 和 MQA 之间取得平衡（图 6）。在每个组中，它的行为类似于多查询注意力。</span></div></div><div id="mLphq7ffJA27BvD7e5xnJg" class="wolai-block wolai-text"><div><span class="inline-wrap">图 6 显示多头注意力有多个键值头（左）。分组查询注意力（中心）的键值头多于一个，但少于查询头的数量，这是内存需求和模型质量之间的平衡。多查询注意力（右）具有单个键值头，有助于节省内存。</span></div></div><div id="jJp9Jbfrwd1LLnqAAkHaMY" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_6.png" style="width: 100%"/></figure></div><div id="r2pydKLCMzru7UygQN9MAb" class="wolai-block wolai-text"><div><span class="inline-wrap">最初使用 MHA 训练的模型可以使用原始训练计算的一小部分通过 GQA 进行“升级训练”。它们获得接近 MHA 的质量，同时保持接近 MQA 的计算效率。 Llama 2 70B 是利用 GQA 的模型示例。</span></div></div><div id="bGgcYqnJdqp75BXaJrLmn2" class="wolai-block wolai-text"><div><span class="red inline-wrap"><b>MQA 和 GQA 等优化通过减少存储的<span class="jill"></span>key<span class="jill"></span>头和<span class="jill"></span>value<span class="jill"></span>头的数量来帮助减少 KV 缓存所需的内存</b></span><span class="inline-wrap">。 KV 缓存的管理方式可能仍然效率低下。与优化注意力模块本身不同，下一节将介绍一种更高效的 KV 缓存管理技术。</span></div></div><h2 id="7foqe5vJ4Cuu8L8qjBPx51" class="wolai-block"><span class="inline-wrap">3.4 Flash attention</span></h2><div id="7gMTTHaJafbkEgbaq5A5MY" class="wolai-block wolai-text"><div><span class="inline-wrap">优化注意力机制的另一种方法是</span><span class="red inline-wrap"><b>修改某些计算的顺序，以更好地利用 GPU 的内存层次结构</b></span><span class="inline-wrap">。神经网络通常用层来描述，大多数实现也以这种方式布局，每次按顺序对输入数据进行一种计算。这并不总是能带来最佳性能，因为对已经进入内存层次结构的更高、性能更高级别的值进行更多计算可能是有益的。</span></div></div><div id="8SAK5RpQ1izE86KET2Rnjq" class="wolai-block wolai-text"><div><span class="inline-wrap">在实际计算过程中将多个层融合在一起可以最大限度地减少 GPU 需要读取和写入内存的次数，并将需要相同数据的计算分组在一起，即使它们是神经网络中不同层的一部分。</span></div></div><div id="izg4qaNoRFguxg1EcHQiCt" class="wolai-block wolai-text"><div><span class="inline-wrap">一种非常流行的融合是 FlashAttention，这是一种 I/O 感知精确注意算法，详细信息请参阅 </span><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.14135"><span>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</span></a></span><span class="inline-wrap">。精确注意力意味着它在数学上与标准多头注意力相同（具有可用于多查询和分组查询注意力的变体），因此可以无需修改即可交换到现有的模型架构，甚至是已经训练的模型。</span></div></div><div id="2ASowSXLkyQwJDePE5MmZy" class="wolai-block wolai-text"><div><span class="inline-wrap">I/O 感知意味着在将操作融合在一起时，它会考虑前面讨论的一些内存移动成本。特别是，FlashAttention 使用“平铺”一次性完全计算并写出最终矩阵的一小部分，而不是分步对整个矩阵进行部分计算，写出中间的中间值。</span></div></div><div id="s5qdhUENXWN42KqVYRSeJn" class="wolai-block wolai-text"><div><span class="inline-wrap">图 7 显示了 40 GB GPU 上的平铺 FlashAttention 计算模式和内存层次结构。右图显示了对注意力机制的不同组件进行融合和重新排序所带来的相对加速。</span></div></div><div id="noJ1HGPNnbrgnDjpTMCVoV" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_7.png" style="width: 100%"/></figure></div><blockquote id="nQ27FeDW9RubbcpoAy3wWZ" class="wolai-block"><span class="inline-wrap">图<span class="jill"></span>7 40 GB GPU 上的平铺 FlashAttention 计算模式和内存层次结构</span></blockquote><h1 id="ai43QzDgWe7jgByqXUNvaL" class="wolai-block"><span class="inline-wrap">4.KV<span class="jill"></span>缓存的分页高效管理</span></h1><div id="6eECCjZn5bMRoPLMK5X9qb" class="wolai-block wolai-text"><div><span class="inline-wrap">有时，KV 缓存会静态地“过度配置”(over-provisioned)，以考虑最大可能的输入（支持的序列长度），因为输入的大小是不可预测的。例如，如果模型支持的最大序列长度为 2,048，则</span><span class="red inline-wrap"><b>无论请求中输入和生成的输出的大小如何，都将在内存中保留大小为 2,048 的数据。该空间可以是连续分配的，并且通常其中大部分未被使用，从而导致内存浪费或碎片</b></span><span class="inline-wrap">。该保留空间在请求的生命周期内被占用。</span></div></div><div id="qVWx9pxJRYS1gxbQKaFsbg" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_8.png" style="width: 100%"/></figure></div><blockquote id="gNXAMCbtCLPFk3u3K9fnYM" class="wolai-block"><span class="inline-wrap">图<span class="jill"></span>8 由于过度配置和低效的 KV 缓存管理而导致的内存浪费和碎片</span></blockquote><div id="nEnFJj3JGnHnPfGPVeQSFX" class="wolai-block wolai-text"><div><span class="red inline-wrap">受操作系统分页的启发</span><span class="inline-wrap">，PagedAttention 算法能够</span><span class="red inline-wrap"><b>将连续的键和值存储在内存中的不连续空间中</b></span><span class="inline-wrap">。它将每个请求的 KV 缓存划分为代表固定数量<span class="jill"></span>token<span class="jill"></span>的块，这些块可以不连续存储。</span></div></div><div id="wVLPYtyaNCmNkbzjSRmCuU" class="wolai-block wolai-text"><div><span class="inline-wrap">在注意力计算期间，使用根据记录索引获取这些块。当新的<span class="jill"></span>token<span class="jill"></span>产生时，就会进行新的区块分配。这些块的大小是固定的，消除了因不同请求需要不同分配等挑战而产生的低效率。这极大地限制了内存浪费，从而实现了更大的批量大小（从而提高了吞吐量）。</span></div></div><h1 id="6G448RZm72uNUSWYKSrT5a" class="wolai-block"><span class="inline-wrap">5.模型优化技术</span></h1><div id="phxpqBaaV3NWKN6NJzzXHD" class="wolai-block wolai-text"><div><span class="inline-wrap">到目前为止，我们已经讨论了 LLM 消耗内存的不同方式、跨多个不同 GPU 分配内存的一些方式，以及优化注意力机制和 KV 缓存。还有多种模型优化技术可以通过修改模型权重本身来减少每个 GPU 上的内存使用。 GPU 还具有专用硬件来加速这些修改值的运算，从而为模型提供更多加速。</span></div></div><h2 id="kxM2kyxgaQofitLHnaRpSb" class="wolai-block"><span class="inline-wrap">5.1 量化（</span><span class="inline-wrap">Quantization</span><span class="inline-wrap">）</span></h2><div id="rPjhku213HadwvzuJg2VdX" class="wolai-block wolai-text"><div><span class="red inline-wrap"><b>量化是降低模型权重和激活精度的过程</b></span><span class="inline-wrap">。大多数模型都以 32 或 16 位精度进行训练，其中每个参数和激活元素占用 32 或 16 位内存（单精度浮点）。然而，大多数深度学习模型可以用每个值八个甚至更少的位来有效表示。</span></div></div><div id="7Fi6NitZf7bBcaB63aq1RH" class="wolai-block wolai-text"><div><span class="inline-wrap">图 9 显示了一种可能的量化方法之前和之后的值分布。在这种情况下，舍入会丢失一些精度，并且剪裁会丢失一些动态范围，从而允许以更小的格式表示值。</span></div></div><div id="5KhoZ9AiDZhUHy16jWtWqn" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_9.png" style="width: 100%"/></figure></div><blockquote id="nTTNnt1HgsBQhVzKgyJeoX" class="wolai-block"><span class="inline-wrap">图<span class="jill"></span>9 一种可能的量化方法之前和之后的值分布</span></blockquote><div id="r6Qnz5Un1gnyXNx6NTSL2N" class="wolai-block wolai-text"><div><span class="inline-wrap">降低模型的精度可以带来多种好处。如果模型占用的内存空间较少，则可以在相同数量的硬件上安运行更大的模型。量化还意味着可以在相同的带宽上传输更多参数，这有助于加速带宽有限的模型。</span></div></div><div id="urfg4jwUB7GxngBqKjzhi7" class="wolai-block wolai-text"><div><span class="inline-wrap">LLM 有许多不同的量化技术，涉及降低激活、权重或两者的精度。量化权重要简单得多，因为它们在训练后是固定的。然而，这可能会留下一些性能问题，因为激活仍然保持在更高的精度。 GPU 没有用于乘以 INT8 和 FP16 数字的专用硬件，因此必须将权重转换回更高精度以进行实际运算。
</span></div></div><div id="e1mdZUecLBsWoooGtvsRAF" class="wolai-block wolai-text"><div><span class="inline-wrap">还可以量化激活、Transformer<span class="jill"></span>块和网络层的输入，但这也有其自身的挑战。激活向量通常包含异常值，有效地增加了它们的动态范围，并使以比权重更低的精度表示这些值变得更具挑战性。</span></div></div><div id="kgnohetEkwg5nz7NUJspJx" class="wolai-block wolai-text"><div><span class="inline-wrap">一种选择是通过模型传递代表性数据集并选择以比其他激活更高的精度表示某些激活来找出这些异常值可能出现的位置 (</span><span class="inline-wrap"><code>LLM.int8()</code></span><span class="inline-wrap">)。另一种选择是借用易于量化的权重的动态范围，并在激活中重用该范围。</span></div></div><h2 id="65w8Hgo63MmJQ29cMRYeoU" class="wolai-block"><span class="inline-wrap">5.2 稀疏（Sparsity）</span></h2><div id="sSGneE5nrzUP3GGatnpwoV" class="wolai-block wolai-text"><div><span class="inline-wrap">与量化类似，事实证明，许多深度学习模型对于修剪或用 </span><span class="inline-wrap"><code>0</code></span><span class="inline-wrap"> 本身替换某些接近 </span><span class="inline-wrap"><code>0</code></span><span class="inline-wrap"> 的值具有鲁棒性。稀疏矩阵是许多元素为 0 的矩阵。这些矩阵可以用压缩形式表示，比完整的稠密矩阵占用的空间更少。</span></div></div><div id="Kk9KqKPgCeGNHZA5JYLac" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_10.png" style="width: 100%"/></figure></div><blockquote id="dQwnunMNrejF1mYsSpvrru" class="wolai-block"><span class="inline-wrap">图<span class="jill"></span>10，以压缩格式表示的稀疏矩阵，由非零数据值及其相应的两位索引组成</span></blockquote><div id="8qAru9Zk8rU5jnpBcFt3kP" class="wolai-block wolai-text"><div><span class="inline-wrap">GPU 尤其具有针对某种结构化稀疏性的硬件加速，其中每四个值中有两个由零表示。稀疏表示还可以与量化相结合，以实现更大的执行速度。寻找以稀疏格式表示大型语言模型的最佳方法仍然是一个活跃的研究领域，并为未来提高推理速度提供了一个有希望的方向。</span></div></div><h2 id="itjYtfZzzF9NszZEBW2GvV" class="wolai-block"><span class="inline-wrap">5.3 蒸馏（Distillation）</span></h2><div id="hRHU9P6Nocm1EYQTAeDa7t" class="wolai-block wolai-text"><div><span class="inline-wrap">缩小模型大小的另一种方法是通过称为蒸馏的过程</span><span class="red inline-wrap"><b>将其知识转移到较小的模型</b></span><span class="inline-wrap">。此过程涉及训练较小的模型（称为学生）来模仿较大模型（教师）的行为。</span></div></div><div id="8aUsEyQL32CyxLzJFvVpCm" class="wolai-block wolai-text"><div><span class="inline-wrap">蒸馏模型的成功例子包括 </span><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.01108"><span>DistilBERT</span></a></span><span class="inline-wrap">，它将 BERT 模型压缩了 40%，同时保留了 97% 的语言理解能力，速度提高了 60%。
</span></div></div><div id="2GzK9zNzPJbTXzQAqZKpr4" class="wolai-block wolai-text"><div><span class="inline-wrap">虽然<span class="jill"></span>LLMs<span class="jill"></span>中的蒸馏是一个活跃的研究领域，但神经网络的一般方法首次在</span><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.02531"><span>Distilling the Knowledge in a Neural Network</span></a></span><span class="inline-wrap">中提出：</span></div></div><ul class="wolai-block"><li id="8JzHjXkUA2vhVjh6e9PMta"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">学生网络经过训练，可以反映较大教师网络的性能，使用损失函数来测量其输出之间的差异。该目标还可能包括将学生的输出与真实标签进行匹配的原始损失函数。
</span></li><li id="9jck2DY4KmDkuULSAF9tkX"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">匹配的教师输出可以是最后一层（称为 </span><span class="inline-wrap"><code>logits</code></span><span class="inline-wrap">）或中间层激活。</span></li></ul><div id="7qb1Fsai2SLSLuFS9ptgMN" class="wolai-block wolai-text"><div><span class="inline-wrap">图 11 显示了知识蒸馏的总体框架。教师的 </span><span class="inline-wrap"><code>logits </code></span><span class="inline-wrap">是学生使用蒸馏损失进行优化的软目标。其他蒸馏方法可能会使用其他损失措施来从老师那里“蒸馏”知识。</span></div></div><div id="9mPfP6qT9Rs2wU2Zdkk1kn" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_11.png" style="width: 100%"/></figure></div><blockquote id="nw85xEgftQb57AH8ZMQoVj" class="wolai-block"><span class="inline-wrap">图<span class="jill"></span>11，知识蒸馏的通用框架</span></blockquote><div id="vZcCftvkf6ABD5x22sMTZ5" class="wolai-block wolai-text"><div><span class="inline-wrap">蒸馏的另一种方法是使用教师合成的数据对<span class="jill"></span>LLMs<span class="jill"></span>学生进行监督培训，这在人工注释稀缺或不可用时特别有用。一步一步蒸馏！更进一步，除了作为基本事实的标签之外，还从<span class="jill"></span>LLMs<span class="jill"></span>教师那里提取基本原理。这些基本原理作为中间推理步骤，以数据有效的方式培训规模较小的<span class="jill"></span>LLMs。</span></div></div><div id="8g78JZtNLqWrw6C4CyUtA3" class="wolai-block wolai-text"><div><span class="inline-wrap">值得注意的是，当今许多最先进的<span class="jill"></span>LLMs<span class="jill"></span>都拥有限制性许可证，禁止使用他们的成果来训练其他<span class="jill"></span>LLMs，这使得找到合适的教师模型具有挑战性。</span></div></div><h1 id="wegwRLM1UeTHccbJQ1X1JA" class="wolai-block"><span class="inline-wrap">6.模型服务技术</span></h1><div id="e7KTKfXvPkhUSijiHPtb7r" class="wolai-block wolai-text"><div><span class="inline-wrap">模型执行通常受内存带宽限制，特别是权重中的带宽限制。即使在应用了前面描述的所有模型优化之后，它仍然很可能受到内存限制。因此，在加载模型权重时尽可能多地处理它们。换句话说，尝试并行。可以采取两种方法：</span></div></div><ul class="wolai-block"><li id="66QiUvuXtbsWS2cwe2nsNe"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">动态批处理(</span><span class="inline-wrap"><b>In-flight batching</b></span><span class="inline-wrap">) ：同时执行多个不同的请求。</span></li><li id="cY9perw42DdhunrQMpxYjv"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">预测</span><span class="inline-wrap">推理(</span><span class="inline-wrap"><b>Speculative inference</b></span><span class="inline-wrap">) ：并行执行序列的多个不同步骤以尝试节省时间。</span></li></ul><h2 id="tTZ1x9M8zGEXfAExFPRB6X" class="wolai-block"><span class="inline-wrap">6.1 </span><span class="inline-wrap">动态批处理</span><span class="inline-wrap">（</span><span class="inline-wrap"><b>In-flight batching</b></span><span class="inline-wrap">）</span></h2><div id="eNFaTugFKeJeQA5pAyQxW3" class="wolai-block wolai-text"><div><span class="inline-wrap">LLMs 具有一些独特的执行特征，这些特征可能导致在实践中难以有效地处理批量请求。一个模型可以同时用于多种不同的任务。从聊天机器人中的简单问答响应到文档摘要或代码块的生成，工作负载是高度动态的，输出大小变化几个数量级。</span></div></div><div id="ei83cVRjUt8g13djYRrLYj" class="wolai-block wolai-text"><div><span class="inline-wrap">这种多功能性使得批处理请求并有效地并行执行它们变得具有挑战性，这是服务神经网络的常见优化。这可能会导致某些请求比其他请求更早完成。</span></div></div><div id="e8hU2yZhEJfkpGxhBPoEom" class="wolai-block wolai-text"><div><span class="inline-wrap">为了管理这些动态负载，许多<span class="jill"></span>LLMs 服务解决方案包括一种称</span><span class="red inline-wrap"><b>为连续或动态批处理的优化调度技术</b></span><span class="inline-wrap">。这利用了这样一个事实：</span><span class="red inline-wrap"><b>LLMs<span class="jill"></span>的整个文本生成过程可以分解为模型上的多次执行迭代</b></span><span class="inline-wrap">。</span></div></div><div id="w1V5z57t2xbofEskQEnEy6" class="wolai-block wolai-text"><div><span class="inline-wrap">通过动态批处理，服务器运行时会</span><span class="yellow inline-wrap"><b>立即从批处理中剔除已完成的序列，而不是等待整个批处理完成后再继续处理下一组请求</b></span><span class="inline-wrap">。然后，它开始执行新请求，而其他请求仍在进行中。因此，动态批处理可以极大地提高实际用例中 GPU 的整体利用率。</span></div></div><h2 id="fnP38wY7cW1BhfAB4GxGbU" class="wolai-block"><span class="inline-wrap">6.2 预测推理（</span><span class="inline-wrap"><b>Speculative inference</b></span><span class="inline-wrap">）</span></h2><div id="9BMe9wqYLmqwgZ6gUDhWvv" class="wolai-block wolai-text"><div><span class="inline-wrap">预测</span><span class="inline-wrap">推理也称为推测采样、辅助生成或分块并行解码，是并行执行 LLM 的另一种方式。通常，GPT 风格的大语言模型是自回归模型，逐个生成文本标记。
</span></div></div><div id="2sFgK2GVXQsPVow1ZkMLYz" class="wolai-block wolai-text"><div><span class="inline-wrap">生成的每个标记都依赖于它之前的所有标记来提供上下文。这意味着在常规执行中，</span><span class="yellow inline-wrap"><b>不可能从同一个序列并行生成多个<span class="jill"></span>token，必须等待第 n 个<span class="jill"></span>token<span class="jill"></span>生成后才能生成 n+1 个<span class="jill"></span>token</b></span><span class="inline-wrap">。</span></div></div><div id="69y8wGX6jWmXWj5i9t7oWA" class="wolai-block wolai-text"><div><span class="inline-wrap">图 12 显示了</span><span class="inline-wrap">预测</span><span class="inline-wrap">推理的示例，其中临时模型临时预测并行验证或拒绝的多个未来步骤。在这种情况下，临时模型中的前两个预测<span class="jill"></span>token<span class="jill"></span>被接受，而最后一个在继续生成之前被拒绝并删除。</span></div></div><div id="van8EQSv3S6tuK3UW7iEf7" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_12.png" style="width: 100%"/></figure></div><blockquote id="6keFWFVgMBRaoq7gb9mMwz" class="wolai-block"><span class="inline-wrap">图<span class="jill"></span>12， 预测推理示例</span></blockquote><div id="dhXia5vnQKj8M3xhTg7Bjc" class="wolai-block wolai-text"><div><span class="inline-wrap">预测</span><span class="inline-wrap">性抽样提供了一种解决方法。这种方法的基本思想是使用一些“更便宜”的过程来生成几个<span class="jill"></span>token<span class="jill"></span>长的临时序列。然后，并行执行多个步骤的主要“验证”模型，使用廉价</span><span class="inline-wrap">临时序列</span><span class="inline-wrap">作为需要的执行步骤的“</span><span class="inline-wrap">预测</span><span class="inline-wrap">”上下文。</span></div></div><div id="afeByisQm1SBBzan3qBUuE" class="wolai-block wolai-text"><div><span class="inline-wrap">如果验证模型生成与临时序列相同的<span class="jill"></span>token，那么就知道接受这些<span class="jill"></span>token<span class="jill"></span>作为输出。否则，可以丢弃第一个不匹配标记之后的所有内容，并使用新的</span><span class="inline-wrap">临时序列</span><span class="inline-wrap">重复该过程。</span></div></div><div id="uovhfh4tkKEoSCdYzN6eRq" class="wolai-block wolai-text"><div><span class="inline-wrap">如何生成临时<span class="jill"></span>token<span class="jill"></span>有许多不同的选项，每个选项都有不同的权衡。可以训练多个模型，或在单个预训练模型上微调多个头，以预测未来多个步骤的标记。或者，可以使用小型模型作为临时模型，使用更大、功能更强大的模型作为验证器。</span></div></div><h1 id="cJsUhvZUV9Qy2oZXofXvuY" class="wolai-block"><span class="inline-wrap">7.结论</span></h1><div id="gDDC4iFif2sAP6hnTouCCu" class="wolai-block wolai-text"><div><span class="inline-wrap">这篇文章概述了许多最流行的解决方案，以帮助高效地优化和服务<span class="jill"></span>LLMs，无论是在数据中心还是在 PC 边缘。其中许多技术都经过优化并通过 NVIDIA TensorRT-LLM 提供，这是一个开源库，由 TensorRT 深度学习编译器以及优化的内核、预处理和后处理步骤以及多 GPU/多节点通信原语组成，可在 NVIDIA 上实现突破性的性能<span class="jill"></span>GPU。</span></div></div><div id="9aMbx1sgphn59o7ia81AdQ" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div><div id="9LhjrTnj5mozAJjrFs8ccy" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div><div id="o2g12nEHwQoJwutDYhjiai" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div></article><footer></footer></body></html></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/wdn_icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Dongnian</div><div class="author-info__description">A salty fish swimming in the sea of deep learning!</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/wdndev"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wdndev" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:dongnian.wang@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #24292e;"></i></a><a class="social-icon" href="https://blog.csdn.net/wdnshadow" target="_blank" title="CSDN"><i class="fas fa-rss" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to My Personal Blog! <br /> If Not, Please Visit <a target="_blank" rel="noopener" href="https://wdndev.gitee.io/"> <font color=#00BFFF>Gitee Mirror</font></a>.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#q5RMS1i4SLNMBN9K1EKAnB"><span class="toc-text">1.理解LLM推理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#vQ6Uvy3fjUXpeHQiVP3woB"><span class="toc-text">1.1 预填充阶段或处理输入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#68fV8aNsjpMbm8V8wXwk6K"><span class="toc-text">1.2 解码阶段或生成输出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#u8U6zYZyRXtMadGutqsVob"><span class="toc-text">1.3 批处理（Batching）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mMzXqRDMad13M6MSLTVJSS"><span class="toc-text">1.4 KV缓存</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rhRuUkrr2N75Eu7z83eXbw"><span class="toc-text">1.5 LLM内存需求</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#rzmH1QA46nKkmM5F8gs6hp"><span class="toc-text">2.模型并行化扩展LLM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#iabGUbyVfuA7F1JjaSnJcR"><span class="toc-text">2.1 Pipeline并行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ttKP6SfrxC2P9Wzv3bicpW"><span class="toc-text">2.2 Tensor并行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#61SHu5XqEnkis83J1FprRU"><span class="toc-text">2.3 Sequence并行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#f38FHBB7zYXPB7bomv7HLt"><span class="toc-text">3.注意力机制优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5yjivuEBr4r9Xe1cnD3waU"><span class="toc-text">3.1 多头注意力（MHA）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cXnyMTez9Xaj7vzz5pZisY"><span class="toc-text">3.2 多查询注意力（MQA）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tzPepHStTMD9cWz6CPNtVm"><span class="toc-text">3.3 分组注意力（GQA）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7foqe5vJ4Cuu8L8qjBPx51"><span class="toc-text">3.4 Flash attention</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ai43QzDgWe7jgByqXUNvaL"><span class="toc-text">4.KV缓存的分页高效管理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6G448RZm72uNUSWYKSrT5a"><span class="toc-text">5.模型优化技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#kxM2kyxgaQofitLHnaRpSb"><span class="toc-text">5.1 量化（Quantization）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#65w8Hgo63MmJQ29cMRYeoU"><span class="toc-text">5.2 稀疏（Sparsity）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#itjYtfZzzF9NszZEBW2GvV"><span class="toc-text">5.3 蒸馏（Distillation）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#wegwRLM1UeTHccbJQ1X1JA"><span class="toc-text">6.模型服务技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tTZ1x9M8zGEXfAExFPRB6X"><span class="toc-text">6.1 动态批处理（In-flight batching）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#fnP38wY7cW1BhfAB4GxGbU"><span class="toc-text">6.2 预测推理（Speculative inference）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cJsUhvZUV9Qy2oZXofXvuY"><span class="toc-text">7.结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_article/9.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BALLM/" title="检索增强LLM">检索增强LLM</a><time datetime="2024-01-12T16:00:00.000Z" title="Created 2024-01-13 00:00:00">2024-01-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/6.%E6%96%87%E6%9C%AC%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/" title="LLMs公开课 - 6.文本理解和生成大模型">LLMs公开课 - 6.文本理解和生成大模型</a><time datetime="2024-01-09T16:00:00.000Z" title="Created 2024-01-10 00:00:00">2024-01-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/5.%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/" title="LLMs公开课 - 5.高效训练&amp;模型压缩">LLMs公开课 - 5.高效训练&amp;模型压缩</a><time datetime="2024-01-06T16:00:00.000Z" title="Created 2024-01-07 00:00:00">2024-01-07</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DSA/"><span class="card-category-list-name">DSA</span><span class="card-category-list-count">24</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLMs/"><span class="card-category-list-name">LLMs</span><span class="card-category-list-count">16</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/PL/"><span class="card-category-list-name">PL</span><span class="card-category-list-count">7</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/PL/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">6</span></a></li></ul></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/DSA/" style="font-size: 1.42em; color: rgb(91, 145, 17)">DSA</a><a href="/tags/RL/" style="font-size: 1.28em; color: rgb(176, 79, 19)">RL</a><a href="/tags/Transformer/" style="font-size: 1.45em; color: rgb(127, 170, 70)">Transformer</a><a href="/tags/LLMs/" style="font-size: 1.32em; color: rgb(112, 148, 55)">LLMs</a><a href="/tags/PaperReading/" style="font-size: 1.38em; color: rgb(110, 146, 60)">PaperReading</a><a href="/tags/DeepLearning/" style="font-size: 1.25em; color: rgb(90, 48, 1)">DeepLearning</a><a href="/tags/CV/" style="font-size: 1.15em; color: rgb(82, 200, 174)">CV</a><a href="/tags/GPT/" style="font-size: 1.18em; color: rgb(7, 16, 91)">GPT</a><a href="/tags/PL/" style="font-size: 1.22em; color: rgb(17, 30, 26)">PL</a><a href="/tags/leetcode/" style="font-size: 1.35em; color: rgb(106, 126, 145)">leetcode</a><a href="/tags/algo/" style="font-size: 1.15em; color: rgb(181, 144, 151)">algo</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">January 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">December 2023</span><span class="card-archive-list-count">14</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">November 2023</span><span class="card-archive-list-count">26</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">October 2023</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/09/"><span class="card-archive-list-date">September 2023</span><span class="card-archive-list-count">4</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">87</div></div><div class="webinfo-item"><div class="item-name">Run time :</div><div class="item-count" id="runtimeshow" data-publishDate="2023-05-31T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Total Count :</div><div class="item-count">411.2k</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2024-12-08T03:57:10.055Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By Dongnian</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'oe7vzWxH80qwJJjWslYTCViT-gzGzoHsz',
      appKey: 'k89nSbK0BTbmzmpQottRHvNI',
      avatar: 'monsterid',
      serverURLs: 'https://oe7vzwxh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>