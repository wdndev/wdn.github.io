<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>37.2° Blog | 37.2° Blog</title><meta name="author" content="Dongnian"><meta name="copyright" content="Dongnian"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1.attention - wolai 笔记1.Attention1.1 讲讲对Attention的理解？Attention机制是一种在处理时序相关问题的时候常用的技术，主要用于处理序列数据。核心思想是在处理序列数据时，网络应该更关注输入中的重要部分，而忽略不重要的部分，它通过学习不同部分的权重，将输入的序列中的重要部分显式地加权，从而使得模型可以更好地关注与输出有关的信息。在序列建模任务中，比">
<meta property="og:type" content="website">
<meta property="og:title" content="37.2° Blog">
<meta property="og:url" content="https://wdndev.github.io/note/llm/llm_concept/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.attention/1.attention.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="1.attention - wolai 笔记1.Attention1.1 讲讲对Attention的理解？Attention机制是一种在处理时序相关问题的时候常用的技术，主要用于处理序列数据。核心思想是在处理序列数据时，网络应该更关注输入中的重要部分，而忽略不重要的部分，它通过学习不同部分的权重，将输入的序列中的重要部分显式地加权，从而使得模型可以更好地关注与输出有关的信息。在序列建模任务中，比">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2024-12-08T03:56:27.902Z">
<meta property="article:modified_time" content="2024-12-08T03:56:27.902Z">
<meta property="article:author" content="Dongnian">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/note/llm/llm_concept/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.attention/1.attention.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Dongnian","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '37.2° Blog',
  isPost: false,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-08 11:56:27'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-solid fa-magnifying-glass"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/note"><i class="fa-fw fa-regular fa-bookmark"></i><span> Note</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-solid fa-magnifying-glass"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/note"><i class="fa-fw fa-regular fa-bookmark"></i><span> Note</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="page"><h1 class="page-title"></h1><div id="article-container"><!DOCTYPE html>
<html lang="zh-Hans-CN"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=Edge"/><link rel="stylesheet" type="text/css" href="../../css/modern-norm.min.css"/><link rel="stylesheet" type="text/css" href="../../css/prism.min.css"/><link rel="stylesheet" type="text/css" href="../../css/katex.min.css"/><link rel="stylesheet" type="text/css" href="../../css/wolai.css"/><title>1.attention - wolai 笔记</title><link rel="shortcut icon" href="data:image/svg+xml,%3Csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 800 800&apos;%3E%3Cdefs%3E%3Cstyle%3E.cls-1%7Bfill:%23fff;%7D%3C/style%3E%3C/defs%3E%3Cg%3E%3Cpath class=&apos;cls-1&apos; d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Z&apos;/%3E%3Cpath d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Zm4.72,88.9H185.2L172.42,89c-32.78.62-43.68,3.24-54.71,9.14a45.84,45.84,0,0,0-19.54,19.54c-6.61,12.36-9.11,24.55-9.27,67.49V614.8L89,627.58c.62,32.78,3.24,43.68,9.14,54.71a45.84,45.84,0,0,0,19.54,19.54c12.36,6.61,24.55,9.11,67.49,9.27H610.08c46.79,0,59.41-2.44,72.21-9.28a45.84,45.84,0,0,0,19.54-19.54c6.61-12.36,9.11-24.55,9.27-67.49V189.92c0-46.79-2.44-59.41-9.28-72.21a45.84,45.84,0,0,0-19.54-19.54C669.93,91.56,657.74,89.06,614.8,88.9ZM233.33,493.33A73.34,73.34,0,1,1,160,566.67,73.35,73.35,0,0,1,233.33,493.33Z&apos;/%3E%3C/g%3E%3C/svg%3E"></link></head><body><header><div class="image"></div><div class="title"><div class="banner"><div class="icon"></div></div><div data-title="1.attention" class="main-title"></div></div></header><article><h3 id="vMhftaG8ZreffVQSubhT92" class="wolai-block"><span class="inline-wrap">1.Attention</span></h3><h4 id="ejSHqGSiHA1RdEVG3VSwRH" class="wolai-block"><span class="inline-wrap"><b>1.1 讲讲对<span class="jill"></span>Attention<span class="jill"></span>的理解？</b></span></h4><div id="2SHUxnRU29KFCwUeTUNDPf" class="wolai-block wolai-text"><div><span class="inline-wrap">Attention<span class="jill"></span>机制是一种在处理时序相关问题的时候常用的技术，主要用于处理序列数据。</span></div></div><div id="7pDi2WcdHAfzAWSvSFtLJa" class="wolai-block wolai-text"><div><span class="inline-wrap">核心思想是在处理序列数据时，</span><span class="red inline-wrap">网络应该更关注输入中的重要部分，而忽略不重要的部分，它通过学习不同部分的权重，将输入的序列中的重要部分显式地加权，从而使得模型可以更好地关注与输出有关的信息</span><span class="inline-wrap">。</span></div></div><div id="pekSFoV2s3iz7zm8Yt9ZMb" class="wolai-block wolai-text"><div><span class="inline-wrap">在序列建模任务中，比如机器翻译、文本摘要、语言理解等，输入序列的不同部分可能具有不同的重要性。传统的循环神经网络（RNN）或卷积神经网络（CNN）在处理整个序列时，难以捕捉到序列中不同位置的重要程度，可能导致信息传递不够高效，特别是在处理长序列时表现更明显。</span></div></div><div id="pEDtaqUcAA5K8bgovV3WiM" class="wolai-block wolai-text"><div><span class="red inline-wrap">Attention<span class="jill"></span>机制的关键是引入一种机制来动态地计算输入序列中各个位置的权重，从而在每个时间步上，对输入序列的不同部分进行加权求和，得到当前时间步的输出</span><span class="inline-wrap">。这样就实现了模型对输入中不同部分的关注度的自适应调整。</span></div></div><div id="mXJFaDmgqH1LNJ1LzK71f3" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>1.2 Attention<span class="jill"></span>的计算步骤是什么？</b></span></div></div><div id="hDJML7poomwvho6vDQYPMw" class="wolai-block wolai-text"><div><span class="inline-wrap">具体的计算步骤如下：</span></div></div><ul class="wolai-block"><li id="7wQo6CntQVZLjaKJBKWHzQ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>计算查询（Query）</b></span><span class="inline-wrap">：查询是当前时间步的输入，用于和序列中其他位置的信息进行比较。</span></li><li id="6q4bSyfDaW1LEh1xD16Dmv"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>计算键（Key）和值（Value）</b></span><span class="inline-wrap">：键表示序列中其他位置的信息，值是对应位置的表示。键和值用来和查询进行比较。</span></li><li id="4Phy8x9AJ5W8MLna7f8wNR"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>计算注意力权重</b></span><span class="inline-wrap">：通过将查询和键进行内积运算，然后应用<span class="jill"></span>softmax<span class="jill"></span>函数，得到注意力权重。这些权重表示了在当前时间步，模型应该关注序列中其他位置的重要程度。</span></li><li id="rEcbNxrYvTqhVKk76Grm1z"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>加权求和</b></span><span class="inline-wrap">：根据注意力权重将值进行加权求和，得到当前时间步的输出。</span></li></ul><div id="anio2RpTsz79xrgvFy8Men" class="wolai-block wolai-text"><div><span class="inline-wrap">在<span class="jill"></span>Transformer<span class="jill"></span>中，Self-Attention 被称为&quot;Scaled Dot-Product Attention&quot;，其计算过程如下：</span></div></div><ol class="wolai-block"><li id="iT4pwUbiLfh5CCdHpGmG8X"><div class="marker"></div><span class="inline-wrap">对于输入序列中的每个位置，通过计算其与所有其他位置之间的相似度得分（通常通过点积计算）。</span></li><li id="nHF1ovodsbnArd5TTfcGn"><div class="marker"></div><span class="inline-wrap">对得分进行缩放处理，以防止梯度爆炸。</span></li><li id="a2SH3eqWD1TRdXguLhozBd"><div class="marker"></div><span class="inline-wrap">将得分用<span class="jill"></span>softmax<span class="jill"></span>函数转换为注意力权重，以便计算每个位置的加权和。</span></li><li id="upEo2RfmmKykTMG5Hk4M8A"><div class="marker"></div><span class="inline-wrap">使用注意力权重对输入序列中的所有位置进行加权求和，得到每个位置的自注意输出。</span></li></ol><div id="9iA8ByYMtehYZZC86hnn7C" class="wolai-block wolai-text"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.5033em;vertical-align:-0.985em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.1978em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8772em;"><span class="svg-align" style="top:-3.01em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8272em;"><span class="pstrut" style="height:3.01em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.09em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.09em' viewBox='0 0 400000 1090' preserveAspectRatio='xMinYMin slice'><path d='M95,712
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l4.819277108433735 -10.000000000000002
c5.3,-9.3,12,-14,20,-14
H400000v50H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M844 80h400000v50h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.985em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></div><h4 id="byMTBmocQmws9e7JAWZDKC" class="wolai-block"><span class="inline-wrap"><b>1.3 Attention<span class="jill"></span>机制和传统的<span class="jill"></span>Seq2Seq<span class="jill"></span>模型有什么区别？</b></span></h4><div id="nYMCZCeeeGpeCd3mXtHbca" class="wolai-block wolai-text"><div><span class="inline-wrap">Seq2Seq<span class="jill"></span>模型是一种基于编码器-解码器结构的模型，主要用于处理序列到序列的任务，例如机器翻译、语音识别等。</span></div></div><div id="2eXsQg9jwfZmf7m2JKEA5i" class="wolai-block wolai-text"><div><span class="inline-wrap">传统的<span class="jill"></span>Seq2Seq<span class="jill"></span>模型</span><span class="yellow inline-wrap">只使用编码器来捕捉输入序列的信息，而解码器只从编码器的最后状态中获取信息，并将其用于生成输出序列</span><span class="inline-wrap">。</span></div></div><div id="nfwbrg3x3xZkDRXatsjbrJ" class="wolai-block wolai-text"><div><span class="inline-wrap">而<span class="jill"></span>Attention<span class="jill"></span>机制则允许</span><span class="green inline-wrap">解码器在生成每个输出时，根据输入序列的不同部分给予不同的注意力，从而使得模型更好地关注到输入序列中的重要信息</span><span class="inline-wrap">。</span></div></div><h4 id="jeBJoE22HG3UcXfSnYv6GB" class="wolai-block"><span class="inline-wrap"><b>1.4 self-attention 和 target-attention<span class="jill"></span>的区别？</b></span></h4><div id="4bTo16oinnT3M2TztXskuq" class="wolai-block wolai-text"><div><span class="inline-wrap">self-attention<span class="jill"></span>是指在序列数据中，</span><span class="red inline-wrap"><b>将当前位置与其他位置之间的关系建模</b></span><span class="inline-wrap">。它通过计算每个位置与其他所有位置之间的相关性得分，从而为每个位置分配一个权重。这使得模型能够根据输入序列的不同部分的重要性，自适应地选择要关注的信息。</span></div></div><div id="by9eBPQafFFJ1vbFeC23Gz" class="wolai-block wolai-text"><div><span class="inline-wrap">target-attention<span class="jill"></span>则是指将</span><span class="red inline-wrap"><b>注意力机制应用于目标（或查询）和一组相关对象之间的关系</b></span><span class="inline-wrap">。它用于将目标与其他相关对象进行比较，并将注意力分配给与目标最相关的对象。这种类型的注意力通常用于任务如机器翻译中的编码-解码模型，其中需要将源语言的信息对齐到目标语言。</span></div></div><div id="jk8Peud6QgouPmKdGHiXHn" class="wolai-block wolai-text"><div><span class="inline-wrap">因此，</span><span class="red inline-wrap"><b>自注意力主要关注序列内部的关系，而目标注意力则关注目标与其他对象之间的关系</b></span><span class="inline-wrap">。这两种注意力机制在不同的上下文中起着重要的作用，帮助模型有效地处理序列数据和相关任务。</span></div></div><h4 id="3ZBm6o98dTjFaYpADSnMxN" class="wolai-block"><span class="inline-wrap"><b>1.5 在常规<span class="jill"></span>attention<span class="jill"></span>中，一般有<span class="jill"></span>k=v，那<span class="jill"></span>self-attention 可以吗?</b></span></h4><div id="gcag8tvnAC81b3yW16JzpW" class="wolai-block wolai-text"><div><span class="inline-wrap">self-attention<span class="jill"></span>实际只是<span class="jill"></span>attention<span class="jill"></span>中的一种特殊情况，因此<span class="jill"></span>k=v<span class="jill"></span>是没有问题的，也即<span class="jill"></span>K，V<span class="jill"></span>参数矩阵相同。实际上，在<span class="jill"></span>Transformer<span class="jill"></span>模型中，Self-Attention<span class="jill"></span>的典型实现就是<span class="jill"></span>k<span class="jill"></span>等于<span class="jill"></span>v<span class="jill"></span>的情况。Transformer<span class="jill"></span>中的<span class="jill"></span>Self-Attention<span class="jill"></span>被称为&quot;Scaled Dot-Product Attention&quot;，其中通过将词向量进行线性变换来得到<span class="jill"></span>Q、K、V，并且这三者是相等的。</span></div></div><h4 id="aBtRwnUKC3w99Y1ReuN85b" class="wolai-block"><span class="inline-wrap"><b>1.6 目前主流的<span class="jill"></span>attention<span class="jill"></span>方法有哪些？</b></span></h4><div id="5QfLrbW7jauktbZB5CFnsn" class="wolai-block wolai-text"><div><span class="inline-wrap">讲自己熟悉的就可：</span></div></div><ul class="wolai-block"><li id="2Lqd9RVk2d4Ry27hbLCzPZ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Scaled Dot-Product Attention</b></span><span class="inline-wrap">: 这是<span class="jill"></span>Transformer<span class="jill"></span>模型中最常用的<span class="jill"></span>Attention<span class="jill"></span>机制，用于计算查询向量（Q）与键向量（K）之间的相似度得分，然后使用注意力权重对值向量（V）进行加权求和。</span></li><li id="xA6cocpioFvnQAF878LZZB"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Multi-Head Attention</b></span><span class="inline-wrap">: 这是<span class="jill"></span>Transformer<span class="jill"></span>中的一个改进，通过同时使用多组独立的注意力头（多个<span class="jill"></span>QKV<span class="jill"></span>三元组），并在输出时将它们拼接在一起。这样的做法允许模型在不同的表示空间上学习不同类型的注意力模式。</span></li><li id="9tLnfD4WkMijT9j2gBv5mk"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Relative Positional Encoding</b></span><span class="inline-wrap">: 传统的<span class="jill"></span>Self-Attention<span class="jill"></span>机制在处理序列时并未直接考虑位置信息，而相对位置编码引入了位置信息，使得模型能够更好地处理序列中不同位置之间的关系。</span></li><li id="fBn7e9nexiyNNiUdhLoJtp"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Transformer-XL</b></span><span class="inline-wrap">: 一种改进的<span class="jill"></span>Transformer<span class="jill"></span>模型，通过使用循环机制来扩展<span class="jill"></span>Self-Attention<span class="jill"></span>的上下文窗口，从而处理更长的序列依赖性。</span></li></ul><h4 id="quxVnSY2jR75A8FyGdY6K1" class="wolai-block"><span class="inline-wrap"><b>1.7 self-attention 在计算的过程中，如何对<span class="jill"></span>padding<span class="jill"></span>位做<span class="jill"></span>mask？</b></span></h4><div id="r7KtsKBpJS4u9uZH8v18iy" class="wolai-block wolai-text"><div><span class="inline-wrap">在 Attention 机制中，同样需要忽略 padding 部分的影响，这里以<span class="jill"></span>transformer encoder<span class="jill"></span>中的<span class="jill"></span>self-attention<span class="jill"></span>为例：self-attention<span class="jill"></span>中，Q<span class="jill"></span>和<span class="jill"></span>K<span class="jill"></span>在点积之后，需要先经过<span class="jill"></span>mask<span class="jill"></span>再进行<span class="jill"></span>softmax，因此，</span><span class="red inline-wrap"><b>对于要屏蔽的部分，mask<span class="jill"></span>之后的输出需要为负无穷</b></span><span class="inline-wrap">，这样<span class="jill"></span>softmax<span class="jill"></span>之后输出才为<span class="jill"></span>0。</span></div></div><h4 id="kJSHnKH1ZHTvTNwVPbguvo" class="wolai-block"><span class="inline-wrap"><b>1.8 深度学习中<span class="jill"></span>attention<span class="jill"></span>与全连接层的区别何在？</b></span></h4><div id="vzR91w6NM4NLdARPsjeJs2" class="wolai-block wolai-text"><div><span class="inline-wrap">这是个非常有意思的问题，要回答这个问题，我们必须重新定义一下<span class="jill"></span>Attention。</span></div></div><div id="aMnZbBcuykRuKbG5MX4J38" class="wolai-block wolai-text"><div><span class="inline-wrap">Transformer Paper<span class="jill"></span>里重新用<span class="jill"></span>QKV<span class="jill"></span>定义了<span class="jill"></span>Attention。所谓的<span class="jill"></span>QKV<span class="jill"></span>就是<span class="jill"></span>Query，Key，Value。如果我们用这个机制来研究传统的<span class="jill"></span>RNN attention，就会发现这个过程其实是这样的：RNN<span class="jill"></span>最后一步的<span class="jill"></span>output<span class="jill"></span>是<span class="jill"></span>Q，这个<span class="jill"></span>Q query<span class="jill"></span>了每一个中间步骤的<span class="jill"></span>K。Q<span class="jill"></span>和<span class="jill"></span>K<span class="jill"></span>共同产生了<span class="jill"></span>Attention Score，最后<span class="jill"></span>Attention Score<span class="jill"></span>乘以<span class="jill"></span>V<span class="jill"></span>加权求和得到<span class="jill"></span>context。那如果我们不用<span class="jill"></span>Attention，单纯用全连接层呢？很简单，全链接层可没有什么<span class="jill"></span>Query<span class="jill"></span>和<span class="jill"></span>Key<span class="jill"></span>的概念，只有一个<span class="jill"></span>Value，也就是说给每个<span class="jill"></span>V<span class="jill"></span>加一个权重再加到一起（如果是<span class="jill"></span>Self Attention，加权这个过程都免了，因为<span class="jill"></span>V<span class="jill"></span>就直接是从<span class="jill"></span>raw input<span class="jill"></span>加权得到的。）</span></div></div><div id="or5nNuiWniQGcvuFqoRg3" class="wolai-block wolai-text"><div><span class="red inline-wrap"><b>可见<span class="jill"></span>Attention<span class="jill"></span>和全连接最大的区别就是<span class="jill"></span>Query<span class="jill"></span>和<span class="jill"></span>Key</b></span><span class="inline-wrap">，而这两者也恰好产生了<span class="jill"></span>Attention Score<span class="jill"></span>这个<span class="jill"></span>Attention<span class="jill"></span>中最核心的机制。</span><span class="red inline-wrap"><b>而在<span class="jill"></span>Query<span class="jill"></span>和<span class="jill"></span>Key<span class="jill"></span>中，我认为<span class="jill"></span>Query<span class="jill"></span>又相对更重要，因为<span class="jill"></span>Query<span class="jill"></span>是一个锚点，Attention Score<span class="jill"></span>便是从过计算与这个锚点的距离算出来的</b></span><span class="inline-wrap">。任何<span class="jill"></span>Attention based algorithm<span class="jill"></span>里都会有<span class="jill"></span>Query<span class="jill"></span>这个概念，但全连接显然没有。</span></div></div><div id="2BmtH89gmPPTE8C96ehUxN" class="wolai-block wolai-text"><div><span class="inline-wrap">最后来一个比较形象的比喻吧。如果一个神经网络的任务是从一堆白色小球中找到一个略微发灰的，那么全连接就是在里面随便乱抓然后凭记忆和感觉找，而<span class="jill"></span>attention<span class="jill"></span>则是左手拿一个白色小球，右手从袋子里一个一个抓出来，两两对比颜色，你左手抓的那个白色小球就是<span class="jill"></span>Query。</span></div></div><h3 id="u4XPT61bZsaseApzLSj7oi" class="wolai-block"><span class="inline-wrap">2.Transformer</span></h3><div id="rjerjoRBN8H2PYDhebmHPt" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>2.1 transformer<span class="jill"></span>中<span class="jill"></span>multi-head attention<span class="jill"></span>中每个<span class="jill"></span>head<span class="jill"></span>为什么要进行降维？</b></span></div></div><div id="xp6bKtHiR4Q82jVGqCbZtA" class="wolai-block wolai-text"><div><span class="inline-wrap">在<span class="jill"></span>Transformer<span class="jill"></span>的<span class="jill"></span>Multi-Head Attention<span class="jill"></span>中，对每个<span class="jill"></span>head<span class="jill"></span>进行降维是</span><span class="red inline-wrap"><b>为了增加模型的表达能力和效率。</b></span></div></div><div id="63GRtarP4wrC5pBwrTJh39" class="wolai-block wolai-text"><div><span class="yellow inline-wrap">每个<span class="jill"></span>head<span class="jill"></span>是独立的注意力机制，它们可以学习不同类型的特征和关系</span><span class="inline-wrap">。通过使用多个注意力头，Transformer<span class="jill"></span>可以并行地学习多种不同的特征表示，从而增强了模型的表示能力。</span></div></div><div id="aakQG2g8ccfd2eGoy9ypoG" class="wolai-block wolai-text"><div><span class="inline-wrap">然而，在使用多个注意力头的同时，注意力机制的计算复杂度也会增加。原始的<span class="jill"></span>Scaled Dot-Product Attention<span class="jill"></span>的计算复杂度为</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(d^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><span class="inline-wrap">，其中<span class="jill"></span>d<span class="jill"></span>是输入向量的维度。如果使用<span class="jill"></span>h<span class="jill"></span>个注意力头，计算复杂度将增加到</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>h</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(hd^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><span class="inline-wrap">。这可能会导致<span class="jill"></span>Transformer<span class="jill"></span>在处理大规模输入时变得非常耗时。</span></div></div><div id="3JMZwtx9LfmRvomgxCAPQ4" class="wolai-block wolai-text"><div><span class="inline-wrap">为了缓解计算复杂度的问题，Transformer<span class="jill"></span>中在每个<span class="jill"></span>head<span class="jill"></span>上进行降维。在每个注意力头中，输入向量通过线性变换被映射到一个较低维度的空间。这个降维过程使用两个矩阵：一个是查询（Q）和键（K）的降维矩阵</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">W_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">和</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">W_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">，另一个是值（V）的降维矩阵</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">W_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">。</span></div></div><div id="pvZQsKRFazqH3T75Mv7D8S" class="wolai-block wolai-text"><div><span class="inline-wrap">通过降低每个<span class="jill"></span>head<span class="jill"></span>的维度，Transformer<span class="jill"></span>可以在</span><span class="red inline-wrap"><b>保持较高的表达能力的同时，大大减少计算复杂度</b></span><span class="inline-wrap">。降维后的计算复杂度为</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>h</mi><msup><mover accent="true"><mi>d</mi><mo>^</mo></mover><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(h\hat d ^ 2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2079em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">d</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><span class="inline-wrap">，其中</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>d</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9579em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">d</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">是降维后的维度。通常情况下，</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>d</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9579em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">d</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">会远小于原始维度<span class="jill"></span>d，这样就可以显著提高模型的计算效率。</span></div></div><div id="aMAMaeCZjHriios3WYt4x2" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>2.2 transformer<span class="jill"></span>在哪里做了权重共享，为什么可以做权重共享？</b></span></div></div><div id="7RTvop9fpZAv2HCSf3nBYQ" class="wolai-block wolai-text"><div><span class="inline-wrap">Transformer<span class="jill"></span>在<span class="jill"></span>Encoder<span class="jill"></span>和<span class="jill"></span>Decoder<span class="jill"></span>中都进行了权重共享。</span></div></div><div id="vWWDbqRxMEaP11X8Zb2YZH" class="wolai-block wolai-text"><div><span class="inline-wrap">在<span class="jill"></span>Transformer<span class="jill"></span>中，Encoder<span class="jill"></span>和<span class="jill"></span>Decoder<span class="jill"></span>是由多层的<span class="jill"></span>Self-Attention Layer<span class="jill"></span>和前馈神经网络层交叉堆叠而成。</span><span class="red inline-wrap"><b>权重共享是指在这些堆叠的层中，相同位置的层共用相同的参数</b></span><span class="inline-wrap">。</span></div></div><div id="geKnd4p1NywD9NKTivMb1V" class="wolai-block wolai-text"><div><span class="inline-wrap">在<span class="jill"></span>Encoder<span class="jill"></span>中，所有的自注意力层和前馈神经网络层都共享相同的参数。这意味着每一层的自注意力机制和前馈神经网络都使用相同的权重矩阵来进行计算。</span><span class="green inline-wrap">这种共享保证了每一层都执行相同的计算过程，使得模型能够更好地捕捉输入序列的不同位置之间的关联性</span><span class="inline-wrap">。</span></div></div><div id="5xFNegE9p81YUfs1sRVDLL" class="wolai-block wolai-text"><div><span class="inline-wrap">在<span class="jill"></span>Decoder<span class="jill"></span>中，除了和<span class="jill"></span>Encoder<span class="jill"></span>相同的权重共享方式外，还存在另一种特殊的权重共享：</span><span class="red inline-wrap"><b>Decoder<span class="jill"></span>的自注意力层和<span class="jill"></span>Encoder<span class="jill"></span>的自注意力层之间也进行了共享</b></span><span class="inline-wrap">。这种共享方式被称为&quot;masked self-attention&quot;，因为在解码过程中，当前位置的注意力不能关注到未来的位置（后续位置），以避免信息泄漏。通过这种共享方式，Decoder<span class="jill"></span>可以利用<span class="jill"></span>Encoder<span class="jill"></span>的表示来理解输入序列并生成输出序列。权重共享的好处是大大减少了模型的参数数量，使得<span class="jill"></span>Transformer<span class="jill"></span>可以更有效地训练，并且更容易进行推理。此外，共享参数还有助于加快训练速度和提高模型的泛化能力，因为模型可以在不同位置共享并学习通用的特征表示。</span></div></div><h4 id="6abBjaTV6L2H7dwaGdX5Tf" class="wolai-block"><span class="inline-wrap"><b>2.3 transformer<span class="jill"></span>的点积模型做缩放的原因是什么？</b></span></h4><div id="7RX6Crr7pYFzRd6X4pCqTi" class="wolai-block wolai-text"><div><span class="inline-wrap">使用缩放的原因是为了控制注意力权重的尺度，以避免在计算过程中出现梯度爆炸的问题。</span></div></div><div id="f7uYqvfpYFcVrBHWGpR5MA" class="wolai-block wolai-text"><div><span class="inline-wrap">Attention<span class="jill"></span>的计算是在内积之后进行<span class="jill"></span>softmax，主要涉及的运算是</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mi>q</mi><mo>⋅</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">e^{q \cdot k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mbin mtight">⋅</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">，可以大致认为内积之后、softmax<span class="jill"></span>之前的数值在</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>3</mn><msqrt><mi>d</mi></msqrt></mrow><annotation encoding="application/x-tex">-3\sqrt{d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.1078em;"></span><span class="mord">−</span><span class="mord">3</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9522em;"><span class="svg-align" style="top:-3.01em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">d</span></span></span><span style="top:-2.9022em;"><span class="pstrut" style="height:3.01em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.09em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.09em' viewBox='0 0 400000 1090' preserveAspectRatio='xMinYMin slice'><path d='M95,712
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l4.819277108433735 -10.000000000000002
c5.3,-9.3,12,-14,20,-14
H400000v50H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M844 80h400000v50h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1078em;"><span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">到</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><msqrt><mi>d</mi></msqrt></mrow><annotation encoding="application/x-tex">3\sqrt{d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.1078em;"></span><span class="mord">3</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9522em;"><span class="svg-align" style="top:-3.01em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">d</span></span></span><span style="top:-2.9022em;"><span class="pstrut" style="height:3.01em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.09em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.09em' viewBox='0 0 400000 1090' preserveAspectRatio='xMinYMin slice'><path d='M95,712
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l4.819277108433735 -10.000000000000002
c5.3,-9.3,12,-14,20,-14
H400000v50H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M844 80h400000v50h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1078em;"><span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">这个范围内，由于<span class="jill"></span>d<span class="jill"></span>通常都至少是<span class="jill"></span>64，所以</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mn>3</mn><msqrt><mi>d</mi></msqrt></mrow></msup></mrow><annotation encoding="application/x-tex">e^{3\sqrt{d}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0335em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0335em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9578em;"><span class="svg-align" style="top:-3.01em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mathnormal mtight">d</span></span></span><span style="top:-2.9078em;"><span class="pstrut" style="height:3.01em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.09em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.09em' viewBox='0 0 400000 1090' preserveAspectRatio='xMinYMin slice'><path d='M95,712
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l4.819277108433735 -10.000000000000002
c5.3,-9.3,12,-14,20,-14
H400000v50H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M844 80h400000v50h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1022em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">比较大而 </span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mo>−</mo><mn>3</mn><msqrt><mi>d</mi></msqrt></mrow></msup></mrow><annotation encoding="application/x-tex">e^{-3\sqrt{d}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0335em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0335em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">3</span><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9578em;"><span class="svg-align" style="top:-3.01em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mathnormal mtight">d</span></span></span><span style="top:-2.9078em;"><span class="pstrut" style="height:3.01em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.09em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.09em' viewBox='0 0 400000 1090' preserveAspectRatio='xMinYMin slice'><path d='M95,712
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l4.819277108433735 -10.000000000000002
c5.3,-9.3,12,-14,20,-14
H400000v50H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M844 80h400000v50h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1022em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">比较小，因此经过<span class="jill"></span>softmax<span class="jill"></span>之后，Attention<span class="jill"></span>的分布非常接近一个<span class="jill"></span>one hot<span class="jill"></span>分布了，这带来严重的梯度消失问题，导致训练效果差。（例如<span class="jill"></span>y=softmax(x)在<span class="jill"></span>|x|<span class="jill"></span>较大时进入了饱和区，x<span class="jill"></span>继续变化<span class="jill"></span>y<span class="jill"></span>值也几乎不变，即饱和区梯度消失）
</span></div></div><div id="krDnuufg9S1RQm7TH53Zq6" class="wolai-block wolai-text"><div><span class="inline-wrap">相应地，解决方法就有两个:
</span></div></div><ol class="wolai-block"><li id="p9NsocQa1osH9GCdsvQtXR"><div class="marker"></div><span class="inline-wrap">像<span class="jill"></span>NTK<span class="jill"></span>参数化那样，在内积之后除以 </span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mi>d</mi></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.1078em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9522em;"><span class="svg-align" style="top:-3.01em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">d</span></span></span><span style="top:-2.9022em;"><span class="pstrut" style="height:3.01em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.09em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.09em' viewBox='0 0 400000 1090' preserveAspectRatio='xMinYMin slice'><path d='M95,712
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l4.819277108433735 -10.000000000000002
c5.3,-9.3,12,-14,20,-14
H400000v50H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M844 80h400000v50h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1078em;"><span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">，使<span class="jill"></span>q⋅k<span class="jill"></span>的方差变为<span class="jill"></span>1，对应</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mn>3</mn></msup><mo separator="true">,</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">e^3,e^{−3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">都不至于过大过小，这样<span class="jill"></span>softmax<span class="jill"></span>之后也不至于变成<span class="jill"></span>one hot<span class="jill"></span>而梯度消失了，这也是常规的<span class="jill"></span>Transformer<span class="jill"></span>如<span class="jill"></span>BERT<span class="jill"></span>里边的<span class="jill"></span>Self Attention<span class="jill"></span>的做法
</span></li><li id="3ucpq6uPZJNTgDzz4kaZJq"><div class="marker"></div><span class="inline-wrap">另外就是不除以 </span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mi>d</mi></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.1078em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9522em;"><span class="svg-align" style="top:-3.01em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">d</span></span></span><span style="top:-2.9022em;"><span class="pstrut" style="height:3.01em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.09em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.09em' viewBox='0 0 400000 1090' preserveAspectRatio='xMinYMin slice'><path d='M95,712
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l4.819277108433735 -10.000000000000002
c5.3,-9.3,12,-14,20,-14
H400000v50H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M844 80h400000v50h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1078em;"><span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">，但是初始化<span class="jill"></span>q,k<span class="jill"></span>的全连接层的时候，其初始化方差要多除以一个<span class="jill"></span>d，这同样能使得使<span class="jill"></span>q⋅k<span class="jill"></span>的初始方差变为<span class="jill"></span>1，T5<span class="jill"></span>采用了这样的做法。</span></li></ol><h3 id="iTpLLSVagKvqUekt86wA9p" class="wolai-block"><span class="inline-wrap">3.BERT</span></h3><h4 id="4om84ZFWQAkAJBGS9KjX41" class="wolai-block"><span class="inline-wrap"><b>3.1 BERT<span class="jill"></span>用字粒度和词粒度的优缺点有哪些？</b></span></h4><div id="GjwZzdr6AtBviBfUTFqpH" class="wolai-block wolai-text"><div><span class="inline-wrap">BERT<span class="jill"></span>可以使用字粒度（character-level）和词粒度（word-level）两种方式来进行文本表示，它们各自有优缺点：</span></div></div><div id="s1krertawnAQHeT7WjMM39" class="wolai-block wolai-text"><div><span class="red inline-wrap">字粒度（Character-level）</span><span class="inline-wrap">：</span></div></div><ul class="wolai-block"><li id="qFEuWPXcNBr51vU7LKJHH8"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>优点</b></span><span class="inline-wrap">：处理未登录词（Out-of-Vocabulary，OOV）：</span><span class="red inline-wrap">字粒度可以处理任意字符串，包括未登录词</span><span class="inline-wrap">，不需要像词粒度那样遇到未登录词就忽略或使用特殊标记。对于少见词和低频词，字粒度可以学习更丰富的字符级别表示，使得模型能够更好地捕捉词汇的细粒度信息。</span></li><li id="amzUVLXMorYYVvAGETeDXU"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>缺点</b></span><span class="inline-wrap">：</span><span class="red inline-wrap">计算复杂度高</span><span class="inline-wrap">：使用字粒度会导致输入序列的长度大大增加，进而增加模型的计算复杂度和内存消耗。</span><span class="red inline-wrap">需要更多的训练数据</span><span class="inline-wrap">：字粒度模型对于少见词和低频词需要更多的训练数据来学习有效的字符级别表示，否则可能会导致过拟合。</span></li></ul><div id="iCHaaChzgtrj98ZrAvFr38" class="wolai-block wolai-text"><div><span class="red inline-wrap">词粒度（Word-level）</span><span class="inline-wrap">：</span></div></div><ul class="wolai-block"><li id="iZeKfHhk75pakDENU2ksWo"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>优点</b></span><span class="inline-wrap">：</span><span class="red inline-wrap">计算效率高</span><span class="inline-wrap">：使用词粒度可以大大减少输入序列的长度，从而降低模型的计算复杂度和内存消耗。</span><span class="red inline-wrap">学习到更加稳定的词级别表示</span><span class="inline-wrap">：词粒度模型可以学习到更加稳定的词级别表示，特别是对于高频词和常见词，有更好的表示能力。</span></li><li id="iUcPYgtbo7z4koWYMj3g4T"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>缺点</b></span><span class="inline-wrap">：</span><span class="red inline-wrap">处理未登录词（OOV）</span><span class="inline-wrap">：词粒度模型无法处理未登录词，遇到未登录词时需要采用特殊处理（如使用未登录词的特殊标记或直接忽略）。对于多音字等形态复杂的词汇，可能无法准确捕捉其细粒度的信息。</span></li></ul><h4 id="tzkG2q4W7bizWGTSi4hGc9" class="wolai-block"><span class="inline-wrap"><b>3.2 BERT<span class="jill"></span>的<span class="jill"></span>Encoder<span class="jill"></span>与<span class="jill"></span>Decoder<span class="jill"></span>掩码有什么区别？</b></span></h4><div id="7qRdCwcx7g8WBD6AvWvLG8" class="wolai-block wolai-text"><div><span class="inline-wrap">Encoder<span class="jill"></span>主要使用</span><span class="red inline-wrap">自注意力掩码和填充掩码</span><span class="inline-wrap">，而<span class="jill"></span>Decoder<span class="jill"></span>除了自注意力掩码外，还需要使用编码器-解码器注意力</span><span class="red inline-wrap">掩码来避免未来位置信息的泄露</span><span class="inline-wrap">。这些掩码操作保证了<span class="jill"></span>Transformer<span class="jill"></span>在处理自然语言序列时能够准确、有效地进行计算，从而获得更好的表现。</span></div></div><h4 id="sU3GEmJcEUeaBBKYw4dEDP" class="wolai-block"><span class="inline-wrap"><b>3.3 BERT<span class="jill"></span>用的是<span class="jill"></span>transformer<span class="jill"></span>里面的<span class="jill"></span>encoder<span class="jill"></span>还是<span class="jill"></span>decoder？</b></span></h4><div id="f3jkdRvkb4V8tXZjEcjRrf" class="wolai-block wolai-text"><div><span class="inline-wrap">BERT<span class="jill"></span>使用的是<span class="jill"></span>Transformer<span class="jill"></span>中的</span><span class="red inline-wrap"><b>Encoder<span class="jill"></span>部分</b></span><span class="inline-wrap">，而不是<span class="jill"></span>Decoder<span class="jill"></span>部分。</span></div></div><div id="9Tc2EoUNMK7YHy5J5mCo6o" class="wolai-block wolai-text"><div><span class="inline-wrap">Transformer<span class="jill"></span>模型由<span class="jill"></span>Encoder<span class="jill"></span>和<span class="jill"></span>Decoder<span class="jill"></span>两个部分组成。Encoder<span class="jill"></span>用于将输入序列编码为一系列高级表示，而<span class="jill"></span>Decoder<span class="jill"></span>用于基于这些表示生成输出序列。</span></div></div><div id="jdqb1CF1y9JQJswaHvfAnv" class="wolai-block wolai-text"><div><span class="inline-wrap">在<span class="jill"></span>BERT<span class="jill"></span>模型中，只使用了<span class="jill"></span>Transformer<span class="jill"></span>的<span class="jill"></span>Encoder<span class="jill"></span>部分，并且对其进行了一些修改和自定义的预训练任务，而没有使用<span class="jill"></span>Transformer<span class="jill"></span>的<span class="jill"></span>Decoder<span class="jill"></span>部分。</span></div></div><h4 id="6rR3tvAYp3VUnHQNBNSpny" class="wolai-block"><span class="inline-wrap"><b>3.4 为什么<span class="jill"></span>BERT<span class="jill"></span>选择<span class="jill"></span>mask<span class="jill"></span>掉<span class="jill"></span>15%<span class="jill"></span>这个比例的词，可以是其他的比例吗？</b></span></h4><div id="gRi5EfrrEWEbyucFimzXAm" class="wolai-block wolai-text"><div><span class="inline-wrap">BERT<span class="jill"></span>选择<span class="jill"></span>mask<span class="jill"></span>掉<span class="jill"></span>15%<span class="jill"></span>的词是一种经验性的选择，是原论文中的一种选择，并没有一个固定的理论依据，实际中当然可以尝试不同的比例，15%<span class="jill"></span>的比例是由<span class="jill"></span>BERT<span class="jill"></span>的作者在原始论文中提出，并在实验中发现对于<span class="jill"></span>BERT<span class="jill"></span>的训练效果是有效的。</span></div></div><h4 id="xpJ2kbFzfUgevYbcvvxX3V" class="wolai-block"><span class="inline-wrap"><b>3.5 为什么<span class="jill"></span>BERT<span class="jill"></span>在第一句前会加一个[CLS] 标志?</b></span></h4><div id="woUoDRT3KUUNUv8UNWjWVD" class="wolai-block wolai-text"><div><span class="inline-wrap">BERT<span class="jill"></span>在第一句前会加一个 [CLS] 标志，</span><span class="red inline-wrap"><b>最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等</b></span><span class="inline-wrap">。为什么选它？因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。</span></div></div><div id="3ZWaK2PtNBy8Zxfuu5krpw" class="wolai-block wolai-text"><div><span class="inline-wrap">具体来说，self-attention<span class="jill"></span>是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过<span class="jill"></span>BERT<span class="jill"></span>的<span class="jill"></span>12<span class="jill"></span>层，每次词的<span class="jill"></span>embedding<span class="jill"></span>融合了所有词的信息，可以去更好的表示自己的语义。而 [CLS] 位本身没有语义，经过<span class="jill"></span>12<span class="jill"></span>层，得到的是<span class="jill"></span>attention<span class="jill"></span>后所有词的加权平均，相比其他正常词，可以更好的表征句子语义。</span></div></div><h4 id="5wkR2wJBDpxCjTeqh1tJw2" class="wolai-block"><span class="inline-wrap"><b>3.6 BERT<span class="jill"></span>非线性的来源在哪里？</b></span></h4><div id="rNJn5FkTcWDn3kdzrD3xG5" class="wolai-block wolai-text"><div><span class="inline-wrap">主要来自两个地方：</span><span class="red inline-wrap"><b>前馈层的<span class="jill"></span>gelu<span class="jill"></span>激活函数</b></span><span class="inline-wrap">和</span><span class="red inline-wrap"><b>self-attention</b></span><span class="inline-wrap">。</span></div></div><div id="3MC1pSvfz9q8qNwgbs9BrT" class="wolai-block wolai-text"><div><span class="green inline-wrap"><b>前馈神经网络层</b></span><span class="inline-wrap">：在<span class="jill"></span>BERT<span class="jill"></span>的<span class="jill"></span>Encoder<span class="jill"></span>中，每个自注意力层之后都跟着一个前馈神经网络层。前馈神经网络层是全连接的神经网络，通常包括一个线性变换和一个非线性的激活函数，如<span class="jill"></span>gelu。这样的非线性激活函数引入了非线性变换，使得模型能够学习更加复杂的特征表示。</span></div></div><div id="kzNDV5ZNb8q5pr9GZ3wLy9" class="wolai-block wolai-text"><div><span class="green inline-wrap"><b>self-attention layer</b></span><span class="inline-wrap">：在自注意力层中，查询（Query）、键（Key）、值（Value）之间的点积得分会经过<span class="jill"></span>softmax<span class="jill"></span>操作，形成注意力权重，然后将这些权重与值向量相乘得到每个位置的自注意输出。这个过程中涉及了<span class="jill"></span>softmax<span class="jill"></span>操作，使得模型的计算是非线性的。</span></div></div><h4 id="pea1FAmWQ1eHxijb7AyKig" class="wolai-block"><span class="inline-wrap"><b>3.7 BERT<span class="jill"></span>训练时使用的学习率 warm-up 策略是怎样的？为什么要这么做？</b></span></h4><div id="rNsY4q96mLR5vvcgRGcTJ9" class="wolai-block wolai-text"><div><span class="inline-wrap">在<span class="jill"></span>BERT<span class="jill"></span>的训练中，使用了学习率<span class="jill"></span>warm-up<span class="jill"></span>策略，这是</span><span class="red inline-wrap"><b>为了在训练的早期阶段增加学习率，以提高训练的稳定性和加快模型收敛</b></span><span class="inline-wrap">。</span></div></div><div id="6i4CDscvJgG6J77H3NXA6k" class="wolai-block wolai-text"><div><span class="inline-wrap">学习率<span class="jill"></span>warm-up<span class="jill"></span>策略的具体做法是，在训练开始的若干个步骤（通常是一小部分训练数据的迭代次数）内，</span><span class="red inline-wrap"><b>将学习率逐渐从一个较小的初始值增加到预定的最大学习率</b></span><span class="inline-wrap">。在这个过程中，学习率的变化是线性的，即学习率在<span class="jill"></span>warm-up<span class="jill"></span>阶段的每个步骤按固定的步幅逐渐增加。学习率<span class="jill"></span>warm-up<span class="jill"></span>的目的是为了解决<span class="jill"></span>BERT<span class="jill"></span>在训练初期的两个问题：</span></div></div><ul class="wolai-block"><li id="swHwDiTqmN6pXtGvf5HY5D"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>不稳定性</b></span><span class="inline-wrap">：在训练初期，由于模型参数的随机初始化以及模型的复杂性，模型可能处于一个较不稳定的状态。此时使用较大的学习率可能导致模型的参数变动太大，使得模型很难收敛，学习率<span class="jill"></span>warm-up<span class="jill"></span>可以在这个阶段将学习率保持较小，提高模型训练的稳定性。</span></li><li id="hd8Propxhm8RTPSWMBwUYY"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>避免过拟合</b></span><span class="inline-wrap">：BERT<span class="jill"></span>模型往往需要较长的训练时间来获得高质量的表示。如果在训练的早期阶段就使用较大的学习率，可能会导致模型在训练初期就过度拟合训练数据，降低模型的泛化能力。通过学习率<span class="jill"></span>warm-up，在训练初期使用较小的学习率，可以避免过度拟合，等模型逐渐稳定后再使用较大的学习率进行更快的收敛。</span></li></ul><h4 id="rjRVNdTi1rUehMvjbw58uU" class="wolai-block"><span class="inline-wrap"><b>3.8 在<span class="jill"></span>BERT<span class="jill"></span>应用中，如何解决长文本问题？</b></span></h4><div id="nrCU9M2Y2WYQ6xHJGzivjZ" class="wolai-block wolai-text"><div><span class="inline-wrap">在<span class="jill"></span>BERT<span class="jill"></span>应用中，处理长文本问题有以下几种常见的解决方案：</span></div></div><ul class="wolai-block"><li id="jF9FYJpHqX4L7MHDXGgLTt"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>截断与填充</b></span><span class="inline-wrap">：将长文本截断为固定长度或者进行填充。BERT<span class="jill"></span>模型的输入是一个固定长度的序列，因此当输入的文本长度超过模型的最大输入长度时，需要进行截断或者填充。通常，可以根据任务的要求，选择适当的最大长度，并对文本进行截断或者填充，使其满足模型输入的要求。</span></li><li id="eV1uZr6xZXb2RNreiM2gHi"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Sliding Window</b></span><span class="inline-wrap">：将长文本分成多个短文本，然后分别输入<span class="jill"></span>BERT<span class="jill"></span>模型。这种方法被称为<span class="jill"></span>Sliding Window<span class="jill"></span>技术。具体来说，将长文本按照固定的步长切分成多个片段，然后分别输入<span class="jill"></span>BERT<span class="jill"></span>模型进行处理。每个片段的输出可以进行进一步的汇总或者融合，得到最终的表示。</span></li><li id="khQwN1ee3wb6aR4KwxGvXC"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Hierarchical Model</b></span><span class="inline-wrap">：使用分层模型来处理长文本，其中底层模型用于处理短文本片段，然后将不同片段的表示进行汇总或者融合得到整个长文本的表示。这样的分层模型可以充分利用<span class="jill"></span>BERT<span class="jill"></span>模型的表示能力，同时处理长文本。</span></li><li id="wfCC5FNa95TTSMUQCZxjXc"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Longformer、BigBird<span class="jill"></span>等模型</b></span><span class="inline-wrap">：使用专门针对长文本的模型，如<span class="jill"></span>Longformer<span class="jill"></span>和<span class="jill"></span>BigBird。这些模型采用了不同的注意力机制，以处理超长序列，并且通常在处理长文本时具有更高的效率。</span></li><li id="c5MX4fhHrNzQpK1dER7DTC"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Document-Level Model</b></span><span class="inline-wrap">：将文本看作是一个整体，而不是将其拆分成句子或段落，然后输入<span class="jill"></span>BERT<span class="jill"></span>模型进行处理。这样的文档级模型可以更好地捕捉整个文档的上下文信息，但需要更多的计算资源。</span></li></ul><h3 id="dGg172iwdWk1XrxTaCbxxy" class="wolai-block"><span class="inline-wrap">4.MHA &amp; MQA &amp; MGA</span></h3><h4 id="f1XQzmKo8rPvohiUjQ1aqm" class="wolai-block"><span class="inline-wrap">（1）MHA</span></h4><div id="6K9XP8VZghfoYrSAUda6cU" class="wolai-block wolai-text"><div><span class="inline-wrap">从多头注意力的结构图中，貌似这个所谓的</span><span class="yellow inline-wrap"><b>多个头就是指多组线性变换层</b></span><span class="inline-wrap">，其实并不是，只有使用了一组线性变化层，即三个变换张量对<span class="jill"></span>Q，K，V<span class="jill"></span>分别进行线性变换，</span><span class="red inline-wrap"><b>这些变换不会改变原有张量的尺寸</b></span><span class="inline-wrap">，因此每个变换矩阵都是方阵，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组<span class="jill"></span>Q，K，V<span class="jill"></span>进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量。这就是所谓的多头，将每个头的获得的输入送到注意力机制中, 就形成多头注意力机制.</span></div></div><div id="wJeMSSxYBcpYonZZabpNAA" class="wolai-block wolai-text"><div><span class="inline-wrap">Multi-head attention<span class="jill"></span>允许模型</span><span class="yellow inline-wrap"><b>共同关注来自不同位置的不同表示子空间的信息</b></span><span class="inline-wrap">，如果只有一个<span class="jill"></span>attention head，它的平均值会削弱这个信息。</span></div></div><div id="3wVvWtHD5YCk8r3mDdM58z" class="wolai-block wolai-text"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>M</mi><mi>u</mi><mi>l</mi><mi>t</mi><mi>i</mi><mi>H</mi><mi>e</mi><mi>a</mi><mi>d</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi><mo stretchy="false">(</mo><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mi>h</mi></msub><mo stretchy="false">)</mo><msup><mi>W</mi><mi>O</mi></msup><mspace linebreak="newline"></mspace><mi>w</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>e</mi><mtext> </mtext><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><mi>K</mi><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><mi>V</mi><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O \\
where ~ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">u</span><span class="mord mathnormal">lt</span><span class="mord mathnormal">i</span><span class="mord mathnormal">He</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">h</span><span class="mord mathnormal">ere</span><span class="mspace nobreak"> </span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div><div id="wyp1PiGRMzBBue5jnmj4Ut" class="wolai-block wolai-text"><div><span class="inline-wrap">其中映射由权重矩阵完成：</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W^Q_i \in \mathbb{R}^{d_ \times d_k}
 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">, </span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">, </span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>×</mo><msub><mi>d</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">和</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>O</mi></msubsup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>h</mi><msub><mi>d</mi><mi>v</mi></msub><mo>×</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W^O_i \in \mathbb{R}^{hd_v \times d_{\text{model}} }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap">。</span></div></div><div id="6QuBdcp56b7bR6VYJVyiPx" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image.png" style="width: 463px"/></figure></div><div id="k3JEzQfwhuM3SuTkV76woU" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_1.png" style="width: 100%"/></figure></div><div id="mLbi5bW9L21QeeqTHwYjgZ" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>多头注意力作用</b></span></div></div><div id="v66KxsvheqMg6oDieQ3vEu" class="wolai-block wolai-text"><div><span class="inline-wrap">这种结构设计能</span><span class="red inline-wrap"><b>让每个注意力机制去优化每个词汇的不同特征部分</b></span><span class="inline-wrap">，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</span></div></div><div id="PNnZzNubgxiU7Ah2RT4na" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>为什么要做多头注意力机制呢</b></span><span class="inline-wrap">？</span></div></div><ul class="wolai-block"><li id="rxxDPHFrrYygH6QykSHBPW"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">一个 dot product 的注意力里面，没有什么可以学的参数。具体函数就是内积，为了识别不一样的模式，希望有不一样的计算相似度的办法。加性 attention 有一个权重可学，也许能学到一些内容。</span></li><li id="3srJ8T63Wz4yfGV3oCHkm9"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">multi-head attention 给 h 次机会去学习 不一样的投影的方法，使得在投影进去的度量空间里面能够去匹配不同模式需要的一些相似函数，然后把 h 个 heads 拼接起来，最后再做一次投影。</span></li><li id="pjpHvZrKQoCS8GAs6oF8D2"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">每一个头 hi 是把 Q,K,V 通过 可以学习的 Wq, Wk, Wv 投影到 dv 上，再通过注意力函数，得到 headi。 </span></li></ul><h4 id="oBDe5xv5iUv2fzedZNDG5n" class="wolai-block"><span class="inline-wrap">（2）MQA</span></h4><div id="sK8nskamGVaL2TFkUgXq3P" class="wolai-block wolai-text"><div><span class="inline-wrap">MQA（Multi Query Attention）最早是出现在<span class="jill"></span>2019<span class="jill"></span>年谷歌的一篇论文 《Fast Transformer Decoding: One Write-Head is All You Need》。</span></div></div><div id="qawHHgsncwHCdVJqRu16co" class="wolai-block wolai-text"><div><span class="inline-wrap">MQA<span class="jill"></span>的思想其实比较简单，MQA 与 MHA 不同的是，</span><span class="red inline-wrap"><b>MQA 让所有的头之间共享同一份 Key 和 Value 矩阵，每个头正常的只单独保留了一份 Query 参数，从而大大减少 Key 和 Value 矩阵的参数量</b></span><span class="inline-wrap">。</span></div></div><blockquote id="gmu1CjqmmpyGZrCVJHDz2W" class="wolai-block"><span class="inline-wrap">Multi-query attention is identical except that the different heads share a single set of keys and values.</span></blockquote><div id="pAWdiT71isX8UXEQTjY1q5" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_2.png" style="width: 100%"/></figure></div><div id="aA1rckaxL9CaRmUnh7bcd8" class="wolai-block wolai-text"><div><span class="inline-wrap">在 Multi-Query Attention 方法中只会保留一个单独的<span class="jill"></span>key-value<span class="jill"></span>头，这样</span><span class="red inline-wrap"><b>虽然可以提升推理的速度，但是会带来精度上的损失</b></span><span class="inline-wrap">。《Multi-Head Attention:Collaborate Instead of Concatenate 》这篇论文的第一个思路是</span><span class="green inline-wrap"><b>基于多个 MQA 的 checkpoint 进行 finetuning，来得到了一个质量更高的 MQA 模型</b></span><span class="inline-wrap">。这个过程也被称为 Uptraining。</span></div></div><div id="qZpPfg3sKAQG5Rpp2UoMPt" class="wolai-block wolai-text"><div><span class="inline-wrap">具体分为两步：</span></div></div><ol class="wolai-block"><li id="jfzpu3jvnRNYKQdTXGzcrp"><div class="marker"></div><span class="inline-wrap">对多个 MQA 的 checkpoint 文件进行融合，融合的方法是: 通过对 key 和 value 的 head 头进行 mean pooling 操作，如下图。
</span></li><li id="aA5HorvLjXqogFq5y3FwDm"><div class="marker"></div><span class="inline-wrap">对融合后的模型使用少量数据进行 finetune 训练，重训后的模型大小跟之前一样，但是效果会更好</span></li></ol><div id="dqscyhJF1DNL1sF682TvG2" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_3.png" style="width: 590px"/></figure></div><h4 id="p28PAWFqoLTyfTacWKMVzu" class="wolai-block"><span class="inline-wrap">（3）GQA</span></h4><div id="chSvQv24VAbySFuYCaxe7C" class="wolai-block wolai-text"><div><span class="inline-wrap">Google 在 2023 年发表的一篇 </span><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.13245.pdf"><span>《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》</span></a></span><span class="inline-wrap">的论文</span></div></div><div id="qNB9sbs1Xiw4okj4GQ2DRJ" class="wolai-block wolai-text"><div><span class="inline-wrap">如下图所示，</span></div></div><ul class="wolai-block"><li id="s7Ma682MB3Qaq4yooyGYXw"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">在 </span><span class="yellow inline-wrap"><b>MHA（Multi Head Attention）</b></span><span class="inline-wrap">中，每个头有自己单独的 key-value 对；</span></li><li id="vmr613ETBuuJRPCNuxXqpk"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">在 </span><span class="yellow inline-wrap"><b>MQA（Multi Query Attention）</b></span><span class="inline-wrap">中只会有一组 key-value 对；</span></li><li id="cmGBsdyGhDXqMHAyrDEggX"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">在 </span><span class="yellow inline-wrap"><b>GQA（Grouped Query Attention）</b></span><span class="inline-wrap">中，会对 attention 进行分组操作，query 被分为 N 组，每个组共享一个 Key 和 Value 矩阵。</span></li></ul><div id="mmB9ANRYTPA6NrNHeBbimb" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_4.png" style="width: 100%"/></figure></div><div id="fw1G2cBuUnQY7tyK1JoqDH" class="wolai-block wolai-text"><div><span class="inline-wrap">GQA-N 是指具有 N 组的 Grouped Query Attention。GQA-1<span class="jill"></span>具有单个组，因此具有单个<span class="jill"></span>Key 和 Value，等效于<span class="jill"></span>MQA。而<span class="jill"></span>GQA-H<span class="jill"></span>具有与头数相等的组，等效于<span class="jill"></span>MHA。</span></div></div><div id="29poMDBo6ho2vCe7AGaRfE" class="wolai-block wolai-text"><div><span class="inline-wrap">在基于 Multi-head 多头结构变为 Grouped-query 分组结构的时候，也是采用跟上图一样的方法，对每一组的 key-value 对进行 mean pool 的操作进行参数融合。</span><span class="red inline-wrap"><b>融合后的模型能力更综合，精度比 Multi-query 好，同时速度比 Multi-head 快</b></span><span class="inline-wrap">。</span></div></div><div id="a37YHB1MSLvTSt7HCtFEWk" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_5.png" style="width: 100%"/></figure></div><h4 id="dreiWQ7CdkvpZm8DxjF9Q9" class="wolai-block"><span class="inline-wrap">（4）总结</span></h4><div id="9yCJ1SX9cN1AcML7NXo1FZ" class="wolai-block wolai-text"><div><span class="inline-wrap">MHA（Multi-head Attention）是标准的多头注意力机制，h<span class="jill"></span>个<span class="jill"></span>Query、Key 和 Value 矩阵。</span></div></div><div id="aQh5TJ3WPBzxLcjfj1Fi8y" class="wolai-block wolai-text"><div><span class="inline-wrap">MQA（Multi-Query Attention）是多查询注意力的一种变体，也是用于自回归解码的一种注意力机制。与<span class="jill"></span>MHA<span class="jill"></span>不同的是，</span><span class="yellow inline-wrap"><b>MQA 让所有的头之间共享同一份 Key 和 Value 矩阵，每个头只单独保留了一份 Query 参数，从而大大减少 Key 和 Value 矩阵的参数量</b></span><span class="inline-wrap">。</span></div></div><div id="hACDfkSnQb26mWgKUATPyE" class="wolai-block wolai-text"><div><span class="inline-wrap">GQA（Grouped-Query Attention）是分组查询注意力，</span><span class="yellow inline-wrap"><b>GQA<span class="jill"></span>将查询头分成<span class="jill"></span>G<span class="jill"></span>组，每个组共享一个<span class="jill"></span>Key 和 Value 矩阵</b></span><span class="inline-wrap">。GQA-G<span class="jill"></span>是指具有<span class="jill"></span>G<span class="jill"></span>组的<span class="jill"></span>grouped-query attention。GQA-1<span class="jill"></span>具有单个组，因此具有单个<span class="jill"></span>Key 和 Value，等效于<span class="jill"></span>MQA。而<span class="jill"></span>GQA-H<span class="jill"></span>具有与头数相等的组，等效于<span class="jill"></span>MHA。</span></div></div><div id="gU14ZV31ndSwXyzZLXHNdu" class="wolai-block wolai-text"><div><span class="inline-wrap">GQA<span class="jill"></span>介于<span class="jill"></span>MHA<span class="jill"></span>和<span class="jill"></span>MQA<span class="jill"></span>之间。GQA 综合 MHA 和 MQA ，既不损失太多性能，又能利用 MQA 的推理加速。不是所有 Q 头共享一组 KV，而是分组一定头数 Q 共享一组 KV，比如上图中就是两组 Q 共享一组 KV。</span></div></div><div id="hSHSfGvtFupxs1pKfK3Aa7" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_6.png" style="width: 540px"/></figure></div><h3 id="cdXnKCp1ainFQ1wWQYvKTD" class="wolai-block"><span class="inline-wrap">5.Flash Attention </span></h3><div id="twWMb57Jcm442YtX96HUPv" class="wolai-block wolai-text"><div><span class="inline-wrap">论文名称：</span><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.14135"><span>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</span></a></span></div></div><div id="fioMF1shPGHNRUCrWC8x5y" class="wolai-block wolai-text"><div><span class="inline-wrap">Flash Attention<span class="jill"></span>的主要目的是加速和节省内存，主要贡献包括：  </span></div></div><ul class="wolai-block"><li id="vd94fmYVQUZTjmYus2Pu8F"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">计算<span class="jill"></span>softmax<span class="jill"></span>时候不需要全量<span class="jill"></span>input<span class="jill"></span>数据，可以分段计算；  </span></li><li id="ePwq5cTC7xY8jUzHtDvKqi"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">反向传播的时候，不存储<span class="jill"></span>attention matrix (N^2<span class="jill"></span>的矩阵)，而是只存储<span class="jill"></span>softmax<span class="jill"></span>归一化的系数。</span></li></ul><h4 id="xiHRKKSZda5db1JrZT12zJ" class="wolai-block"><span class="inline-wrap">5.1 动机</span></h4><div id="6NuJsxd4JHkss3mfgNcL9y" class="wolai-block wolai-text"><div><span class="yellow inline-wrap">不同硬件模块之间的带宽和存储空间有明显差异</span><span class="inline-wrap">，例如下图中左边的三角图，最顶端的是<span class="jill"></span>GPU<span class="jill"></span>种的</span><span class="inline-wrap"><code>SRAM</code></span><span class="inline-wrap">，它的容量非常小但是带宽非常大，以<span class="jill"></span>A100 GPU<span class="jill"></span>为例，它有<span class="jill"></span>108<span class="jill"></span>个流式多核处理器，每个处理器上的片上<span class="jill"></span>SRAM<span class="jill"></span>大小只有<span class="jill"></span>192KB，因此<span class="jill"></span>A100<span class="jill"></span>总共的<span class="jill"></span>SRAM<span class="jill"></span>大小是</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>192</mn><mi>K</mi><mi>B</mi><mo>×</mo><mn>108</mn><mo>=</mo><mn>20</mn><mi>M</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">192KB\times 108 = 20MB</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord">192</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">108</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">20</span><span class="mord mathnormal" style="margin-right:0.05017em;">MB</span></span></span></span></span><span class="inline-wrap">，但是其吞吐量能高达<span class="jill"></span>19TB/s。而<span class="jill"></span>A100 GPU </span><span class="inline-wrap"><code>HBM</code></span><span class="inline-wrap">（High Bandwidth Memory<span class="jill"></span>也就是我们常说的<span class="jill"></span>GPU<span class="jill"></span>显存大小）大小在<span class="jill"></span>40GB~80GB<span class="jill"></span>左右，但是带宽只与<span class="jill"></span>1.5TB/s。</span></div></div><div id="oWDwe8TQtH1DxAckZ1ZHVt" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_7.png" style="width: 100%"/></figure></div><div id="xmPXWnrFtVA2Jowfey85u2" class="wolai-block wolai-text"><div><span class="inline-wrap">下图给出了标准的注意力机制的实现流程，可以看到因为</span><span class="inline-wrap"><code>HBM</code></span><span class="inline-wrap">的大小更大，</span><span class="yellow inline-wrap"><b>我们平时写<span class="jill"></span>pytorch<span class="jill"></span>代码的时候最常用到的就是<span class="jill"></span>HBM，所以对于<span class="jill"></span>HBM<span class="jill"></span>的读写操作非常频繁，而<span class="jill"></span>SRAM<span class="jill"></span>利用率反而不高</b></span><span class="inline-wrap">。</span></div></div><div id="pzvrd1DRfiBuK1qXaABJsf" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_8.png" style="width: 100%"/></figure></div><div id="rdgr3JVnjHmUHigkaG2wo3" class="wolai-block wolai-text"><div><span class="inline-wrap">FlashAttention<span class="jill"></span>的主要动机就是</span><span class="red inline-wrap"><b>希望把<span class="jill"></span>SRAM<span class="jill"></span>利用起来</b></span><span class="inline-wrap">，但是难点就在于<span class="jill"></span>SRAM<span class="jill"></span>太小了，一个普通的矩阵乘法都放不下去。FlashAttention<span class="jill"></span>的解决思路就是将计算模块进行分解，拆成一个个小的计算任务。</span></div></div><h4 id="2QyastS4BvjwqWBq8eLnMi" class="wolai-block"><span class="inline-wrap">5.2 Softmax Tiling</span></h4><div id="mnjiWNkgPqmtdYzH5xg5hY" class="wolai-block wolai-text"><div><span class="inline-wrap">在介绍具体的计算算法前，我们首先需要了解一下<span class="jill"></span>Softmax Tiling。</span></div></div><div id="3noLJEwdw4bJaPZDLCEwVG" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>（1）数值稳定</b></span></div></div><div id="2a6RZm6kpECczkL1fjdnJY" class="wolai-block wolai-text"><div><span class="inline-wrap"> Softmax<span class="jill"></span>包含指数函数，所以</span><span class="yellow inline-wrap">为了避免数值溢出问题，可以将每个元素都减去最大值</span><span class="inline-wrap">，如下图示，最后计算结果和原来的<span class="jill"></span>Softmax<span class="jill"></span>是一致的。</span></div></div><div id="9VXh153Zu69DP5exC1ZLXE" class="wolai-block wolai-text"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>i</mi></munder><mtext> </mtext><msub><mi>x</mi><mi>i</mi></msub><mspace linebreak="newline"></mspace><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="left left left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msup><mi>e</mi><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">…</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>B</mi></msub><mo>−</mo><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mspace linebreak="newline"></mspace><mi mathvariant="normal">ℓ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mspace linebreak="newline"></mspace><mi mathvariant="normal">softmax</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mfrac><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">m(x):=\max _{i} ~ x_{i} \\ 
f(x):=\left[\begin{array}{llll}e^{x_{1}-m(x)} &amp; \ldots &amp; e^{x_{B}-m(x)}\end{array}\right] \\ 
\ell(x):=\sum_{i} f(x)_{i} \\ 
\operatorname{softmax}(x):=\frac{f(x)}{\ell(x)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1582em;vertical-align:-0.7277em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.3723em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7277em;"><span></span></span></span></span></span><span class="mspace nobreak"> </span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.248em;vertical-align:-0.374em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">[</span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.874em;"><span style="top:-2.986em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">m</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.374em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.874em;"><span style="top:-2.986em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="minner">…</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.374em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.874em;"><span style="top:-2.986em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">m</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.374em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">]</span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">ℓ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3277em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop"><span class="mord mathrm">softmax</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">ℓ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><div id="eEPED36a9j75m8vobhj3Wm" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>（2）分块计算<span class="jill"></span>softmax</b></span></div></div><div id="kkhkai1uWRTfVuFS2Yf27v" class="wolai-block wolai-text"><div><span class="inline-wrap">因为<span class="jill"></span>Softmax<span class="jill"></span>都是按行计算的，所以我们考虑一行切分成两部分的情况，即原本的一行数据</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>2</mn><mi>B</mi></mrow></msup><mo>=</mo><mrow><mo fence="true">[</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^{2 B}=\left[x^{(1)}, x^{(2)}\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.238em;vertical-align:-0.35em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">[</span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">]</span></span></span></span></span></span></span></div></div><div id="5PfRR8PrrsqmAZ2EH3DkKg" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_9.png" style="width: 100%"/></figure></div><div id="sCEZY2WrHBtTQ8iFCaEBdR" class="wolai-block wolai-text"><div><span class="inline-wrap">可以看到计算不同块的</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span><span class="inline-wrap">值时，乘上的系数是不同的，但是最后化简后的结果都是指数函数减去了整行的最大值。以</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">x^{(1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="inline-wrap"> 为例，</span></div></div><div id="tueeGkjF3oGRrqncyNuwB6" class="wolai-block wolai-text"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi>m</mi><mrow><mi>m</mi><mrow><mo fence="true">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo fence="true">)</mo></mrow><mo>−</mo><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup><mi>f</mi><mrow><mo fence="true">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo fence="true">)</mo></mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msup><mi>e</mi><mrow><mi>m</mi><mrow><mo fence="true">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo fence="true">)</mo></mrow><mo>−</mo><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup><mrow><mo fence="true">[</mo><msup><mi>e</mi><mrow><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><mi>m</mi><mrow><mo fence="true">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo fence="true">)</mo></mrow></mrow></msup><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>e</mi><mrow><msubsup><mi>x</mi><mi>B</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><mi>m</mi><mrow><mo fence="true">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo fence="true">)</mo></mrow></mrow></msup><mo fence="true">]</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mrow><mo fence="true">[</mo><msup><mi>e</mi><mrow><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>e</mi><mrow><msubsup><mi>x</mi><mi>B</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup><mo fence="true">]</mo></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} m^{m\left(x^{(1)}\right)-m(x)} f\left(x^{(1)}\right) &amp; =e^{m\left(x^{(1)}\right)-m(x)}\left[e^{x_{1}^{(1)}-m\left(x^{(1)}\right)}, \ldots, e^{x_{B}^{(1)}-m\left(x^{(1)}\right)}\right] \\ &amp; =\left[e^{x_{1}^{(1)}-m(x)}, \ldots, e^{x_{B}^{(1)}-m(x)}\right]\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.2088em;vertical-align:-1.8544em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.3544em;"><span style="top:-4.3544em;"><span class="pstrut" style="height:3.1544em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1105em;"><span style="top:-3.413em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9667em;"><span style="top:-2.9667em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class="mtight">)</span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">m</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.1544em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.8544em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.3544em;"><span style="top:-4.3544em;"><span class="pstrut" style="height:3.1544em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1105em;"><span style="top:-3.413em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9667em;"><span style="top:-2.9667em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class="mtight">)</span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">m</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">[</span></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1544em;"><span style="top:-3.413em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0591em;"><span style="top:-2.2242em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span style="top:-3.0591em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">m</span><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9667em;"><span style="top:-2.9667em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class="mtight">)</span></span></span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1544em;"><span style="top:-3.413em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0591em;"><span style="top:-2.1964em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.0591em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3393em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">m</span><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9667em;"><span style="top:-2.9667em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class="mtight">)</span></span></span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">]</span></span></span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.1544em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">[</span></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1544em;"><span style="top:-3.1544em;margin-right:0.05em;"><span class="pstrut" style="height:2.7414em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0591em;"><span style="top:-2.2242em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span style="top:-3.0591em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">m</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1544em;"><span style="top:-3.1544em;margin-right:0.05em;"><span class="pstrut" style="height:2.7414em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0591em;"><span style="top:-2.1964em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.0591em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3393em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">m</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">]</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.8544em;"><span></span></span></span></span></span></span></span></span></span></span></span></div><h4 id="6L1DoZnLrAjS5tHog1429R" class="wolai-block"><span class="inline-wrap">5.3 算法流程</span></h4><div id="vUFk7bADQTxcenpeg4PuX9" class="wolai-block wolai-text"><div><span class="inline-wrap">FlashAttention<span class="jill"></span>旨在避免从 HBM（High Bandwidth Memory）中读取和写入注意力矩阵，这需要做到：</span></div></div><ol class="wolai-block"><li id="j5JssPFS9ACqcZ9zmfdydT"><div class="marker"></div><span class="inline-wrap">目标一：在不访问整个输入的情况下计算<span class="jill"></span>softmax<span class="jill"></span>函数的缩减；</span><span class="yellow inline-wrap"><b>将输入分割成块，并在输入块上进行多次传递，从而以增量方式执行<span class="jill"></span>softmax<span class="jill"></span>缩减</b></span><span class="inline-wrap">。</span></li><li id="jSW6pvYuzLFbsxFbHqxBJP"><div class="marker"></div><span class="inline-wrap">目标二：在后向传播中不能存储中间注意力矩阵。标准<span class="jill"></span>Attention<span class="jill"></span>算法的实现需要将计算过程中的<span class="jill"></span>S、P<span class="jill"></span>写入到<span class="jill"></span>HBM<span class="jill"></span>中，而这些中间矩阵的大小与输入的序列长度有关且为二次型，因此</span><span class="yellow inline-wrap"><b>Flash Attention<span class="jill"></span>就提出了不使用中间注意力矩阵，通过存储归一化因子来减少<span class="jill"></span>HBM<span class="jill"></span>内存的消耗</b></span><span class="inline-wrap"><b>。</b></span></li></ol><div id="jsCkLQ8BZhu2wCb4gzQ7gG" class="wolai-block wolai-text"><div><span class="inline-wrap">FlashAttention<span class="jill"></span>算法流程如下图所示：</span></div></div><div id="2qebcXihVKGUSBWySgBmpF" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_10.png" style="width: 100%"/></figure></div><div id="rJEM1fffRCbjgGVpRQLrFd" class="wolai-block wolai-text"><div><span class="inline-wrap">为方便理解，下图将<span class="jill"></span>FlashAttention<span class="jill"></span>的计算流程可视化出来了，简单理解就是每一次只计算一个<span class="jill"></span>block<span class="jill"></span>的值，通过多轮的双<span class="jill"></span>for<span class="jill"></span>循环完成整个注意力的计算。</span></div></div><div id="d7QKq2jAxRzRXEpttT2Yob" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_11.png" style="width: 100%"/></figure></div><h3 id="d7VpuusLETCEoL2ZYiZADH" class="wolai-block"><span class="inline-wrap">6.Transformer<span class="jill"></span>常见问题</span></h3><h4 id="um4jhDYj9qWgP6XjWEt8xM" class="wolai-block"><span class="inline-wrap">6.1 Transformer<span class="jill"></span>和<span class="jill"></span>RNN</span></h4><div id="nFb2xhjYSvHjdjTq5CkjLV" class="wolai-block wolai-text"><div><span class="inline-wrap">最简单情况：没有残差连接、没有 layernorm、 attention 单头、没有投影。看和 RNN 区别</span></div></div><ul class="wolai-block"><li id="72BEoygRsyyGa3qfojCA5e"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">attention 对输入做一个加权和，加权和 进入 point-wise MLP。（画了多个红色方块 MLP， 是一个权重相同的 MLP）</span></li><li id="hMp673R7riQLN4U9SuZQcP"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">point-wise MLP 对 每个输入的点 做计算，得到输出。</span></li><li id="ttdZM6L4CLqYnqrtatW8dZ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">attention 作用：把整个序列里面的信息抓取出来，做一次汇聚 aggregation</span></li></ul><div id="5g3iBLkCkhWkEB5SGSu58P" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_12.png" style="width: 533px"/></figure></div><div id="6D5WSMWXKuenENRJ5Wozkj" class="wolai-block wolai-text"><div><span class="inline-wrap">RNN 跟 transformer </span><span class="inline-wrap"><b>异：如何传递序列的信</b></span><span class="inline-wrap">息</span></div></div><div id="svYuxvmGPvfVH8fghyRsGV" class="wolai-block wolai-text"><div><span class="inline-wrap">RNN 是把上一个时刻的信息输出传入下一个时候做输入。Transformer 通过一个 attention 层，去全局的拿到整个序列里面信息，再用 MLP 做语义的转换。</span></div></div><div id="roi22Epzr16AzPE3HrXaAM" class="wolai-block wolai-text"><div><span class="inline-wrap">RNN 跟 transformer </span><span class="inline-wrap"><b>同：语义空间的转换 + 关注点</b></span></div></div><div id="voiiEqhZPMt67y49czBFbj" class="wolai-block wolai-text"><div><span class="inline-wrap">用一个线性层 or 一个 MLP 来做语义空间的转换。</span></div></div><div id="gt2YcJMU1pKhfeTVX5Eosj" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>关注点</b></span><span class="inline-wrap">：</span><span class="red inline-wrap">怎么有效的去使用序列的信息</span><span class="inline-wrap">。</span></div></div><h4 id="sPpbB6VUNLrLSEap8acGt5" class="wolai-block"><span class="inline-wrap">6.2 一些细节</span></h4><div id="6mmqSvia9FTzu34hDw8h6q" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>Transformer<span class="jill"></span>为何使用多头注意力机制？</b></span><span class="inline-wrap">（为什么不使用一个头）</span></div></div><ul class="wolai-block"><li id="nkmDwMo4qYJuZkdByGep27"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">多头保证了<span class="jill"></span>transformer<span class="jill"></span>可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。可以类比<span class="jill"></span>CNN<span class="jill"></span>中同时使用</span><span class="inline-wrap"><b>多个滤波器</b></span><span class="inline-wrap">的作用，直观上讲，</span><span class="yellow inline-wrap">多头的注意力</span><span class="yellow inline-wrap"><b>有助于网络捕捉到更丰富的特征/信息。</b></span></li></ul><div id="oj4W4tJf3rtwNYYDbfWsxz" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>Transformer<span class="jill"></span>为什么<span class="jill"></span>Q<span class="jill"></span>和<span class="jill"></span>K<span class="jill"></span>使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</b></span><span class="inline-wrap"> （注意和第一个问题的区别）</span></div></div><ul class="wolai-block"><li id="pnB57A28sVqgVSsgi8VzJU"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">使用<span class="jill"></span>Q/K/V<span class="jill"></span>不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。</span></li><li id="jMNuHqWmjS2JWJMpUc7gb8"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">同时，由<span class="jill"></span>softmax<span class="jill"></span>函数的性质决定，实质做的是一个<span class="jill"></span>soft<span class="jill"></span>版本的<span class="jill"></span>arg max<span class="jill"></span>操作，得到的向量接近一个<span class="jill"></span>one-hot<span class="jill"></span>向量（接近程度根据这组数的数量级有所不同）。如果令<span class="jill"></span>Q=K，那么得到的模型大概率会得到一个类似单位矩阵的<span class="jill"></span>attention<span class="jill"></span>矩阵，</span><span class="inline-wrap"><b>这样<span class="jill"></span>self-attention<span class="jill"></span>就退化成一个<span class="jill"></span>point-wise<span class="jill"></span>线性映射</b></span><span class="inline-wrap">。这样至少是违反了设计的初衷。</span></li></ul><div id="7mXcv5GneC7hprfuV8X2h" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>Transformer<span class="jill"></span>计算<span class="jill"></span>attention<span class="jill"></span>的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</b></span></div></div><ul class="wolai-block"><li id="8UTbuTTBQtrePGNfPmab59"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">K<span class="jill"></span>和<span class="jill"></span>Q<span class="jill"></span>的点乘是为了得到一个<span class="jill"></span>attention score 矩阵，用来对<span class="jill"></span>V<span class="jill"></span>进行提纯。K<span class="jill"></span>和<span class="jill"></span>Q<span class="jill"></span>使用了不同的<span class="jill"></span>W_k, W_Q<span class="jill"></span>来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的<span class="jill"></span>attention score<span class="jill"></span>矩阵的泛化能力更高。</span></li><li id="hXFqsgt9KYcL5sYh8Z1kZV"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">为了计算更快。矩阵加法在加法这一块的计算量确实简单，但是作为一个整体计算<span class="jill"></span>attention<span class="jill"></span>的时候相当于一个隐层，整体计算量和点积相似。在效果上来说，从实验分析，两者的效果和<span class="jill"></span>dk<span class="jill"></span>相关，dk<span class="jill"></span>越大，加法的效果越显著。</span></li></ul><div id="3DR7xCMtHuZvRuYfnjcaEy" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>为什么在进行<span class="jill"></span>softmax<span class="jill"></span>之前需要对<span class="jill"></span>attention<span class="jill"></span>进行<span class="jill"></span>scaled（为什么除以<span class="jill"></span>dk<span class="jill"></span>的平方根）</b></span><span class="inline-wrap">，并使用公式推导进行讲解</span></div></div><ul class="wolai-block"><li id="r4qPYPKYDe4xBV8D4MNbYw"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">这取决于<span class="jill"></span>softmax<span class="jill"></span>函数的特性，如果<span class="jill"></span>softmax<span class="jill"></span>内计算的数数量级太大，会输出近似<span class="jill"></span>one-hot<span class="jill"></span>编码的形式，导致梯度消失的问题，所以需要<span class="jill"></span>scale</span></li><li id="72C6WykNUcY5mtXFj6WAXb"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">那么至于为什么需要用维度开根号，假设向量<span class="jill"></span>q，k<span class="jill"></span>满足各分量独立同分布，均值为<span class="jill"></span>0，方差为<span class="jill"></span>1，那么<span class="jill"></span>qk<span class="jill"></span>点积均值为<span class="jill"></span>0，方差为<span class="jill"></span>dk，从统计学计算，若果让<span class="jill"></span>qk<span class="jill"></span>点积的方差控制在<span class="jill"></span>1，需要将其除以<span class="jill"></span>dk<span class="jill"></span>的平方根，是的<span class="jill"></span>softmax<span class="jill"></span>更加平滑</span></li></ul><div id="qBG6ub7hYnTPhcdfNDgYwE" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>在计算<span class="jill"></span>attention score<span class="jill"></span>的时候如何对<span class="jill"></span>padding<span class="jill"></span>做<span class="jill"></span>mask<span class="jill"></span>操作？</b></span></div></div><ul class="wolai-block"><li id="wx23pTS2eVV72nRw8uptUE"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">padding<span class="jill"></span>位置置为负无穷(一般来说-1000<span class="jill"></span>就可以)，再对<span class="jill"></span>attention score<span class="jill"></span>进行相加。对于这一点，涉及到<span class="jill"></span>batch_size<span class="jill"></span>之类的，具体的大家可以看一下实现的源代码，位置在这里：</span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720"><span>https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720</span></a></span></li><li id="6ozgrVVRnz5FgDtEaPTgpb"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">padding<span class="jill"></span>位置置为负无穷而不是<span class="jill"></span>0，是因为后续在<span class="jill"></span>softmax<span class="jill"></span>时，</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mn>0</mn></msup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">e^0=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span><span class="inline-wrap">，不是<span class="jill"></span>0，计算会出现错误；而</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></msup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">e^{-\infty} = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7713em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">∞</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></span><span class="inline-wrap">，所以取负无穷</span></li></ul><div id="vLyEJjNPUWCznuWeshKojr" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>为什么在进行多头注意力的时候需要对每个<span class="jill"></span>head<span class="jill"></span>进行降维？</b></span><span class="inline-wrap">（可以参考上面一个问题）</span></div></div><ul class="wolai-block"><li id="iTd5GzN19mbpbQEzkwkHyq"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">将原有的</span><span class="inline-wrap"><b>高维空间转化为多个低维空间</b></span><span class="inline-wrap">并再最后进行拼接，形成同样维度的输出，借此丰富特性信息</span><ul class="wolai-block"><li id="qWTixP31ABpDPxFkjwGaSE"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">基本结构：Embedding + Position Embedding，Self-Attention，Add + LN，FN，Add + LN</span></li></ul></li></ul><div id="gCB6Ck35Gdoi7TkWzCVV77" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>为何在获取输入词向量之后需要对矩阵乘以<span class="jill"></span>embedding size<span class="jill"></span>的开方？意义是什么？</b></span></div></div><ul class="wolai-block"><li id="gxC51gpmXEVjccrh3iLWGs"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">embedding matrix<span class="jill"></span>的初始化方式是<span class="jill"></span>xavier init，这种方式的方差是<span class="jill"></span>1/embedding size，因此乘以<span class="jill"></span>embedding size<span class="jill"></span>的开方使得<span class="jill"></span>embedding matrix<span class="jill"></span>的方差是<span class="jill"></span>1，在这个<span class="jill"></span>scale<span class="jill"></span>下可能更有利于<span class="jill"></span>embedding matrix<span class="jill"></span>的收敛。</span></li></ul><div id="3PNUHAAWcDN54HETVZG6E6" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>简单介绍一下<span class="jill"></span>Transformer<span class="jill"></span>的位置编码？有什么意义和优缺点？</b></span></div></div><ul class="wolai-block"><li id="srfLnZwtPV3FKCmmJAovWA"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">因为<span class="jill"></span>self-attention<span class="jill"></span>是位置无关的，无论句子的顺序是什么样的，通过<span class="jill"></span>self-attention<span class="jill"></span>计算的<span class="jill"></span>token<span class="jill"></span>的<span class="jill"></span>hidden embedding<span class="jill"></span>都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个<span class="jill"></span>token<span class="jill"></span>的位置信息，transformer<span class="jill"></span>使用了固定的<span class="jill"></span>positional encoding<span class="jill"></span>来表示<span class="jill"></span>token<span class="jill"></span>在句子中的绝对位置信息。</span></li></ul><div id="bbmiX964wdLRZsVgXHfb3B" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</b></span><span class="inline-wrap">（参考上一题）</span></div></div><ul class="wolai-block"><li id="4W3htVS3ZqHdsW38yhsu6a"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">相对位置编码（RPE）1.在计算<span class="jill"></span>attention score<span class="jill"></span>和<span class="jill"></span>weighted value<span class="jill"></span>时各加入一个可训练的表示相对位置的参数。2.在生成多头注意力时，把对<span class="jill"></span>key<span class="jill"></span>来说将绝对位置转换为相对<span class="jill"></span>query<span class="jill"></span>的位置<span class="jill"></span>3.复数域函数，已知一个词在某个位置的词向量表示，可以计算出它在任何位置的词向量表示。前两个方法是词向量<span class="jill"></span>+<span class="jill"></span>位置编码，属于亡羊补牢，复数域是生成词向量的时候即生成对应的位置信息。</span></li></ul><div id="9N3JL8866s4wr7PgKWqpdf" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>简单讲一下<span class="jill"></span>Transformer<span class="jill"></span>中的残差结构以及意义。</b></span></div></div><ul class="wolai-block"><li id="j6XAw5BE4ekdS7B5qHiVfZ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">就是<span class="jill"></span>ResNet<span class="jill"></span>的优点，解决梯度消失</span></li></ul><div id="uie5LyzCS8mxTMu4yt46KT" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>为什么<span class="jill"></span>transformer<span class="jill"></span>块使用<span class="jill"></span>LayerNorm<span class="jill"></span>而不是<span class="jill"></span>BatchNorm？LayerNorm 在<span class="jill"></span>Transformer<span class="jill"></span>的位置是哪里？</b></span></div></div><ul class="wolai-block"><li id="gww5Kx33qpEbHoA3f4kTr7"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">LN：针对每个样本序列进行<span class="jill"></span>Norm，没有样本间的依赖。对一个序列的不同特征维度进行<span class="jill"></span>Norm</span></li><li id="rYP4S5d9Ahjhp2aGhcpLH"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">CV<span class="jill"></span>使用<span class="jill"></span>BN<span class="jill"></span>是认为<span class="jill"></span>channel<span class="jill"></span>维度的信息对<span class="jill"></span>cv<span class="jill"></span>方面有重要意义，如果对<span class="jill"></span>channel<span class="jill"></span>维度也归一化会造成不同通道信息一定的损失。而同理<span class="jill"></span>nlp<span class="jill"></span>领域认为句子长度不一致，并且各个<span class="jill"></span>batch<span class="jill"></span>的信息没什么关系，因此只考虑句子内信息的归一化，也就是<span class="jill"></span>LN。</span></li></ul><div id="5BkQDWfNZvMtQTDYdXxM2z" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>简答讲一下<span class="jill"></span>BatchNorm<span class="jill"></span>技术，以及它的优缺点。</b></span></div></div><ul class="wolai-block"><li id="4SgEb6Bwci8x5MPzj9dtHF"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">优点：</span><ul class="wolai-block"><li id="ox7WzVoeyQ3GiP7GCHLuK8"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">第一个就是可以解决内部协变量偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN<span class="jill"></span>缓解了这个问题。当然后来也有论文证明<span class="jill"></span>BN<span class="jill"></span>有作用和这个没关系，而是可以使</span><span class="inline-wrap"><b>损失平面更加的平滑</b></span><span class="inline-wrap">，从而加快的收敛速度。</span></li><li id="jD1bpg1H65fEyNWPqfyWcK"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">第二个优点就是缓解了</span><span class="inline-wrap"><b>梯度饱和问题</b></span><span class="inline-wrap">（如果使用<span class="jill"></span>sigmoid<span class="jill"></span>激活函数的话），加快收敛。</span></li></ul></li><li id="8c27bq41dasCwxDQbzH8cA"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">缺点：</span><ul class="wolai-block"><li id="33Xu5SFg6pVax94qPVg9GM"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">第一个，batch_size<span class="jill"></span>较小的时候，效果差。这一点很容易理解。BN<span class="jill"></span>的过程，使用 整个<span class="jill"></span>batch<span class="jill"></span>中样本的均值和方差来模拟全部数据的均值和方差，在<span class="jill"></span>batch_size 较小的时候，效果肯定不好。</span></li><li id="5Qv34Jf6CJTz8jjRxcVQB"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">第二个缺点就是 BN 在<span class="jill"></span>RNN<span class="jill"></span>中效果比较差。</span></li></ul></li></ul><div id="trcV6xCzJctWd5ZS19Wvr2" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>简单描述一下<span class="jill"></span>Transformer<span class="jill"></span>中的前馈神经网络？使用了什么激活函数？相关优缺点？</b></span></div></div><ul class="wolai-block"><li id="2eP4U5sHSkcZVYgMt38Csd"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">ReLU</span></li></ul><div id="3bYz1po47UznPHihC96yGX" class="wolai-block wolai-text"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mi>F</mi><mi>N</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mtext> </mtext><mi>x</mi><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy="false">)</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">FFN(x)=max(0,~ xW_1+b_1)W_2+b_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">FFN</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace nobreak"> </span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></div><div id="wKbkMApjbs8STLNBEtKjbB" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>Encoder<span class="jill"></span>端和<span class="jill"></span>Decoder<span class="jill"></span>端是如何进行交互的？</b></span><span class="inline-wrap">（在这里可以问一下关于<span class="jill"></span>seq2seq<span class="jill"></span>的<span class="jill"></span>attention<span class="jill"></span>知识）</span></div></div><ul class="wolai-block"><li id="7EReBhAZF9LkcsMNyEtfmq"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">Cross Self-Attention，Decoder<span class="jill"></span>提供<span class="jill"></span>Q，Encoder<span class="jill"></span>提供<span class="jill"></span>K，V</span></li></ul><div id="tsGv3pxqa8bPc7EGVAA5wQ" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>Decoder<span class="jill"></span>阶段的多头自注意力和<span class="jill"></span>encoder<span class="jill"></span>的多头自注意力有什么区别？</b></span><span class="inline-wrap">（为什么需要<span class="jill"></span>decoder<span class="jill"></span>自注意力需要进行 sequence mask)</span></div></div><ul class="wolai-block"><li id="avXbfZBYorJ2wmzSAhE44e"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">让输入序列只看到过去的信息，不能让他看到未来的信息</span></li></ul><div id="ta52pJRLcpwL6MKs3PZXaR" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>Transformer<span class="jill"></span>的并行化提现在哪个地方？Decoder<span class="jill"></span>端可以做并行化吗？</b></span></div></div><ul class="wolai-block"><li id="vf66oMnpV1DDzu1uRGbz4j"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">Encoder<span class="jill"></span>侧：模块之间是串行的，一个模块计算的结果做为下一个模块的输入，互相之前有依赖关系。从每个模块的角度来说，注意力层和前馈神经层这两个子模块单独来看都是可以并行的，不同单词之间是没有依赖关系的。</span></li><li id="4Kyjxd6Kvjqfm4QMr72dt2"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">Decode<span class="jill"></span>引入<span class="jill"></span>sequence mask<span class="jill"></span>就是为了并行化训练，Decoder<span class="jill"></span>推理过程没有并行，只能一个一个的解码，很类似于<span class="jill"></span>RNN，这个时刻的输入依赖于上一个时刻的输出。</span></li></ul><div id="tgWuNHsqQ9XfJ68YtVc2tX" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>简单描述一下<span class="jill"></span>wordpiece model 和 byte pair encoding，有实际应用过吗？</b></span></div></div><ul class="wolai-block"><li id="xxGaVLUS55hy6thboUhAeq"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">传统词表示方法无法很好的处理未知或罕见的词汇（OOV<span class="jill"></span>问题），传统词<span class="jill"></span>tokenization<span class="jill"></span>方法不利于模型学习词缀之间的关系”</span></li><li id="3AQZr7pxfqxfPqgMYFUeFF"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">BPE（字节对编码）或二元编码是一种简单的数据压缩形式，其中最常见的一对连续字节数据被替换为该数据中不存在的字节。后期使用时需要一个替换表来重建原始数据。</span></li><li id="o8i2ccYVPhmFaDyZnduG62"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">优点：可以有效地平衡词汇表大小和步数（编码句子所需的<span class="jill"></span>token<span class="jill"></span>次数）。</span></li><li id="snggbMtkKt5tShhSfm4F9M"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">缺点：基于贪婪和确定的符号替换，不能提供带概率的多个分片结果。</span></li></ul><div id="som9kZsvnoPzPrHKdtzqg7" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>Transformer<span class="jill"></span>训练的时候学习率是如何设定的？Dropout<span class="jill"></span>是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</b></span></div></div><ul class="wolai-block"><li id="5w7CGRMKkktspnuvJYkt9p"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">Dropout<span class="jill"></span>测试的时候记得对输入整体呈上<span class="jill"></span>dropout<span class="jill"></span>的比率</span></li></ul><div id="2JQe24VnqnCpRN6QhiBHgP" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>引申一个关于<span class="jill"></span>bert<span class="jill"></span>问题，bert<span class="jill"></span>的<span class="jill"></span>mask<span class="jill"></span>为何不学习<span class="jill"></span>transformer<span class="jill"></span>在<span class="jill"></span>attention<span class="jill"></span>处进行屏蔽<span class="jill"></span>score<span class="jill"></span>的技巧？</b></span></div></div><ul class="wolai-block"><li id="woXHuAWsaHFBibNoN6uorA"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">BERT<span class="jill"></span>和<span class="jill"></span>transformer<span class="jill"></span>的目标不一致，bert<span class="jill"></span>是语言的预训练模型，需要充分考虑上下文的关系，而<span class="jill"></span>transformer<span class="jill"></span>主要考虑句子中第<span class="jill"></span>i<span class="jill"></span>个元素与前<span class="jill"></span>i-1<span class="jill"></span>个元素的关系。</span></li></ul><div id="eK84DL5wfJv6GFFP8dfe1G" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div><div id="n6xP6cpM2WdMG14sLATRJu" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div><div id="keY5RyjhgmJozPpUFi7kWZ" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div><div id="cRdenKFe6W4oa6L9iNVLYW" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div><div id="XmNTgYd6Zy8qA1b5VwAWQ" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div><div id="nfkoSGpauUUd4SGo3rcxEv" class="wolai-block wolai-text"><div></div></div></article><footer></footer></body></html></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/wdn_icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Dongnian</div><div class="author-info__description">A salty fish swimming in the sea of deep learning!</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/wdndev"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wdndev" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:dongnian.wang@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #24292e;"></i></a><a class="social-icon" href="https://blog.csdn.net/wdnshadow" target="_blank" title="CSDN"><i class="fas fa-rss" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to My Personal Blog! <br /> If Not, Please Visit <a target="_blank" rel="noopener" href="https://wdndev.gitee.io/"> <font color=#00BFFF>Gitee Mirror</font></a>.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#vMhftaG8ZreffVQSubhT92"><span class="toc-text">1.Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ejSHqGSiHA1RdEVG3VSwRH"><span class="toc-text">1.1 讲讲对Attention的理解？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#byMTBmocQmws9e7JAWZDKC"><span class="toc-text">1.3 Attention机制和传统的Seq2Seq模型有什么区别？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#jeBJoE22HG3UcXfSnYv6GB"><span class="toc-text">1.4 self-attention 和 target-attention的区别？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3ZBm6o98dTjFaYpADSnMxN"><span class="toc-text">1.5 在常规attention中，一般有k&#x3D;v，那self-attention 可以吗?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#aBtRwnUKC3w99Y1ReuN85b"><span class="toc-text">1.6 目前主流的attention方法有哪些？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#quxVnSY2jR75A8FyGdY6K1"><span class="toc-text">1.7 self-attention 在计算的过程中，如何对padding位做mask？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#kJSHnKH1ZHTvTNwVPbguvo"><span class="toc-text">1.8 深度学习中attention与全连接层的区别何在？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#u4XPT61bZsaseApzLSj7oi"><span class="toc-text">2.Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6abBjaTV6L2H7dwaGdX5Tf"><span class="toc-text">2.3 transformer的点积模型做缩放的原因是什么？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#iTpLLSVagKvqUekt86wA9p"><span class="toc-text">3.BERT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4om84ZFWQAkAJBGS9KjX41"><span class="toc-text">3.1 BERT用字粒度和词粒度的优缺点有哪些？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tzkG2q4W7bizWGTSi4hGc9"><span class="toc-text">3.2 BERT的Encoder与Decoder掩码有什么区别？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sU3GEmJcEUeaBBKYw4dEDP"><span class="toc-text">3.3 BERT用的是transformer里面的encoder还是decoder？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6rR3tvAYp3VUnHQNBNSpny"><span class="toc-text">3.4 为什么BERT选择mask掉15%这个比例的词，可以是其他的比例吗？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#xpJ2kbFzfUgevYbcvvxX3V"><span class="toc-text">3.5 为什么BERT在第一句前会加一个[CLS] 标志?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5wkR2wJBDpxCjTeqh1tJw2"><span class="toc-text">3.6 BERT非线性的来源在哪里？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pea1FAmWQ1eHxijb7AyKig"><span class="toc-text">3.7 BERT训练时使用的学习率 warm-up 策略是怎样的？为什么要这么做？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#rjRVNdTi1rUehMvjbw58uU"><span class="toc-text">3.8 在BERT应用中，如何解决长文本问题？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dGg172iwdWk1XrxTaCbxxy"><span class="toc-text">4.MHA &amp; MQA &amp; MGA</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#f1XQzmKo8rPvohiUjQ1aqm"><span class="toc-text">（1）MHA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#oBDe5xv5iUv2fzedZNDG5n"><span class="toc-text">（2）MQA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#p28PAWFqoLTyfTacWKMVzu"><span class="toc-text">（3）GQA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#dreiWQ7CdkvpZm8DxjF9Q9"><span class="toc-text">（4）总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cdXnKCp1ainFQ1wWQYvKTD"><span class="toc-text">5.Flash Attention </span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#xiHRKKSZda5db1JrZT12zJ"><span class="toc-text">5.1 动机</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2QyastS4BvjwqWBq8eLnMi"><span class="toc-text">5.2 Softmax Tiling</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6L1DoZnLrAjS5tHog1429R"><span class="toc-text">5.3 算法流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#d7VpuusLETCEoL2ZYiZADH"><span class="toc-text">6.Transformer常见问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#um4jhDYj9qWgP6XjWEt8xM"><span class="toc-text">6.1 Transformer和RNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sPpbB6VUNLrLSEap8acGt5"><span class="toc-text">6.2 一些细节</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_article/9.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BALLM/" title="检索增强LLM">检索增强LLM</a><time datetime="2024-01-12T16:00:00.000Z" title="Created 2024-01-13 00:00:00">2024-01-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/6.%E6%96%87%E6%9C%AC%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/" title="LLMs公开课 - 6.文本理解和生成大模型">LLMs公开课 - 6.文本理解和生成大模型</a><time datetime="2024-01-09T16:00:00.000Z" title="Created 2024-01-10 00:00:00">2024-01-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/5.%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/" title="LLMs公开课 - 5.高效训练&amp;模型压缩">LLMs公开课 - 5.高效训练&amp;模型压缩</a><time datetime="2024-01-06T16:00:00.000Z" title="Created 2024-01-07 00:00:00">2024-01-07</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DSA/"><span class="card-category-list-name">DSA</span><span class="card-category-list-count">24</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLMs/"><span class="card-category-list-name">LLMs</span><span class="card-category-list-count">16</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/PL/"><span class="card-category-list-name">PL</span><span class="card-category-list-count">7</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/PL/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">6</span></a></li></ul></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/DSA/" style="font-size: 1.42em; color: rgb(91, 145, 17)">DSA</a><a href="/tags/RL/" style="font-size: 1.28em; color: rgb(176, 79, 19)">RL</a><a href="/tags/Transformer/" style="font-size: 1.45em; color: rgb(127, 170, 70)">Transformer</a><a href="/tags/LLMs/" style="font-size: 1.32em; color: rgb(112, 148, 55)">LLMs</a><a href="/tags/PaperReading/" style="font-size: 1.38em; color: rgb(110, 146, 60)">PaperReading</a><a href="/tags/DeepLearning/" style="font-size: 1.25em; color: rgb(90, 48, 1)">DeepLearning</a><a href="/tags/CV/" style="font-size: 1.15em; color: rgb(82, 200, 174)">CV</a><a href="/tags/GPT/" style="font-size: 1.18em; color: rgb(7, 16, 91)">GPT</a><a href="/tags/PL/" style="font-size: 1.22em; color: rgb(17, 30, 26)">PL</a><a href="/tags/leetcode/" style="font-size: 1.35em; color: rgb(106, 126, 145)">leetcode</a><a href="/tags/algo/" style="font-size: 1.15em; color: rgb(181, 144, 151)">algo</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">January 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">December 2023</span><span class="card-archive-list-count">14</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">November 2023</span><span class="card-archive-list-count">26</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">October 2023</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/09/"><span class="card-archive-list-date">September 2023</span><span class="card-archive-list-count">4</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">87</div></div><div class="webinfo-item"><div class="item-name">Run time :</div><div class="item-count" id="runtimeshow" data-publishDate="2023-05-31T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Total Count :</div><div class="item-count">411.2k</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2024-12-08T03:57:10.055Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By Dongnian</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'oe7vzWxH80qwJJjWslYTCViT-gzGzoHsz',
      appKey: 'k89nSbK0BTbmzmpQottRHvNI',
      avatar: 'monsterid',
      serverURLs: 'https://oe7vzwxh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>