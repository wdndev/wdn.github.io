<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>37.2° Blog | 37.2° Blog</title><meta name="author" content="Dongnian"><meta name="copyright" content="Dongnian"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="2.数据并行 - wolai 笔记1.简述所谓数据并行，就是由于训练数据集太大；因此，将数据集分为N份，每一份分别装载到N个GPU节点中，同时，每个GPU节点持有一个完整的模型副本，分别基于每个GPU中的数据去进行梯度求导。然后，在GPU0上对每个GPU中的梯度进行累加，最后，再将GPU0聚合后的结果广播到其他GPU节点。注意：这里是以GPU0作为参数服务器，除此之外，还可以使用CPU作为参数服">
<meta property="og:type" content="website">
<meta property="og:title" content="37.2° Blog">
<meta property="og:url" content="https://wdndev.github.io/note/llm/llm_concept/04.%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/2.%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C/2.%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="2.数据并行 - wolai 笔记1.简述所谓数据并行，就是由于训练数据集太大；因此，将数据集分为N份，每一份分别装载到N个GPU节点中，同时，每个GPU节点持有一个完整的模型副本，分别基于每个GPU中的数据去进行梯度求导。然后，在GPU0上对每个GPU中的梯度进行累加，最后，再将GPU0聚合后的结果广播到其他GPU节点。注意：这里是以GPU0作为参数服务器，除此之外，还可以使用CPU作为参数服">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2024-12-08T03:56:27.930Z">
<meta property="article:modified_time" content="2024-12-08T03:56:27.930Z">
<meta property="article:author" content="Dongnian">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/note/llm/llm_concept/04.%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/2.%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C/2.%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Dongnian","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '37.2° Blog',
  isPost: false,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-08 11:56:27'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-solid fa-magnifying-glass"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/note"><i class="fa-fw fa-regular fa-bookmark"></i><span> Note</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-solid fa-magnifying-glass"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/note"><i class="fa-fw fa-regular fa-bookmark"></i><span> Note</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="page"><h1 class="page-title"></h1><div id="article-container"><!DOCTYPE html>
<html lang="zh-Hans-CN"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=Edge"/><link rel="stylesheet" type="text/css" href="../../css/modern-norm.min.css"/><link rel="stylesheet" type="text/css" href="../../css/prism.min.css"/><link rel="stylesheet" type="text/css" href="../../css/katex.min.css"/><link rel="stylesheet" type="text/css" href="../../css/wolai.css"/><title>2.数据并行 - wolai 笔记</title><link rel="shortcut icon" href="data:image/svg+xml,%3Csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 800 800&apos;%3E%3Cdefs%3E%3Cstyle%3E.cls-1%7Bfill:%23fff;%7D%3C/style%3E%3C/defs%3E%3Cg%3E%3Cpath class=&apos;cls-1&apos; d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Z&apos;/%3E%3Cpath d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Zm4.72,88.9H185.2L172.42,89c-32.78.62-43.68,3.24-54.71,9.14a45.84,45.84,0,0,0-19.54,19.54c-6.61,12.36-9.11,24.55-9.27,67.49V614.8L89,627.58c.62,32.78,3.24,43.68,9.14,54.71a45.84,45.84,0,0,0,19.54,19.54c12.36,6.61,24.55,9.11,67.49,9.27H610.08c46.79,0,59.41-2.44,72.21-9.28a45.84,45.84,0,0,0,19.54-19.54c6.61-12.36,9.11-24.55,9.27-67.49V189.92c0-46.79-2.44-59.41-9.28-72.21a45.84,45.84,0,0,0-19.54-19.54C669.93,91.56,657.74,89.06,614.8,88.9ZM233.33,493.33A73.34,73.34,0,1,1,160,566.67,73.35,73.35,0,0,1,233.33,493.33Z&apos;/%3E%3C/g%3E%3C/svg%3E"></link></head><body><header><div class="image"></div><div class="title"><div class="banner"><div class="icon"></div></div><div data-title="2.数据并行" class="main-title"></div></div></header><article><h3 id="mPV8nomWh3Mabr25we2W3s" class="wolai-block"><span class="inline-wrap">1.简述</span></h3><div id="tmAonW4CHRXBJofQRLfLGJ" class="wolai-block wolai-text"><div><span class="inline-wrap">所谓数据并行，就是由于训练数据集太大；因此，</span><span class="red inline-wrap"><b>将数据集分为<span class="jill"></span>N<span class="jill"></span>份，每一份分别装载到<span class="jill"></span>N<span class="jill"></span>个<span class="jill"></span>GPU<span class="jill"></span>节点中，同时，每个<span class="jill"></span>GPU<span class="jill"></span>节点持有一个完整的模型副本</b></span><span class="inline-wrap">，分别基于每个<span class="jill"></span>GPU<span class="jill"></span>中的数据去进行梯度求导。然后，在<span class="jill"></span>GPU0<span class="jill"></span>上对每个<span class="jill"></span>GPU<span class="jill"></span>中的梯度进行累加，最后，再将<span class="jill"></span>GPU0<span class="jill"></span>聚合后的结果广播到其他<span class="jill"></span>GPU<span class="jill"></span>节点。</span></div></div><div id="hMaSwjQk9cVyoz8LJXZtAU" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image.png" style="width: 318px"/></figure></div><div id="qqdsRsC7g9NKbDLPyd3wmh" class="wolai-block wolai-text"><div><span class="inline-wrap">注意：这里是以<span class="jill"></span>GPU0<span class="jill"></span>作为参数服务器，除此之外，还可以使用<span class="jill"></span>CPU<span class="jill"></span>作为参数服务器。但是这种场景的训练速度通常会慢于使用<span class="jill"></span>GPU0<span class="jill"></span>作为参数服务器（通常情况下，GPU<span class="jill"></span>与<span class="jill"></span>CPU<span class="jill"></span>之间通信使用<span class="jill"></span>PCIe，而<span class="jill"></span>GPU<span class="jill"></span>与<span class="jill"></span>GPU<span class="jill"></span>之间通信使用<span class="jill"></span>Nvlink）。</span></div></div><div id="5qDT2mfKX5S4iLCcPKifQ4" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_1.png" style="width: 402px"/></figure></div><div id="aTTnUCbVhGFJsme7f8eXso" class="wolai-block wolai-text"><div><span class="inline-wrap">当然，还可以将参数服务器分布在所有<span class="jill"></span>GPU<span class="jill"></span>节点上面，每个<span class="jill"></span>GPU<span class="jill"></span>只更新其中一部分梯度。</span></div></div><div id="uogw1obYy2jW88QKuq53jq" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_2.png" style="width: 315px"/></figure></div><div id="uUR99FAcK5gnPsTvB7Qmqa" class="wolai-block wolai-text"><div><span class="inline-wrap">当然，</span><span class="red inline-wrap">数据并行不仅仅指对训练的数据并行操作，还可以对网络模型梯度、权重参数、优化器状态等数据进行并行</span><span class="inline-wrap">。</span></div></div><div id="52szCERFrLvwDb6e9NbyDD" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_3.png" style="width: 669px"/></figure></div><div id="o4ePVNa1BHrGpUVyWZEHSG" class="wolai-block wolai-text"><div><span class="inline-wrap">下面主要以<span class="jill"></span>PyTorch<span class="jill"></span>中数据并行的发展为主线讲述现有一些数据并行方法。</span></div></div><h3 id="gtRTPisArMHkfdQ6TRun6j" class="wolai-block"><span class="inline-wrap">2.数据并行（PyTorch DP）</span></h3><div id="rZMy54cTzri4ZUgUzwpqpW" class="wolai-block wolai-text"><div><span class="inline-wrap">数据并行(</span><span class="inline-wrap"><code>torch.nn.DataParallel</code></span><span class="inline-wrap">)，这是<span class="jill"></span>Pytorch<span class="jill"></span>最早提供的一种数据并行方式，它基于单进程多线程进行实现的，它使用</span><span class="red inline-wrap"><b>一个进程来计算模型权重</b></span><span class="inline-wrap">，在每个批处理期间将数据分发到每个<span class="jill"></span>GPU。</span></div></div><div id="dSJ45h7teyrRZVxMNdAyiL" class="wolai-block wolai-text"><div><span class="inline-wrap">DataParallel 的计算过程如下所示：</span></div></div><ul class="wolai-block"><li id="tPVcwdyXXcpAhZYEQh2b7L"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">将 inputs 从主 GPU 分发到所有 GPU 上。</span></li><li id="m4NP6db3jFWPcgUeGkSSHz"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">将 model 从主 GPU 分发到所有 GPU 上。</span></li><li id="qbiUnWNwn5Nw1p2DRQR9EY"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">每个 GPU 分别独立进行前向传播，得到 outputs。</span></li><li id="tFEq18akDNyH76yB9ZGXgV"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">将每个 GPU 的 outputs 发回主 GPU。</span></li><li id="tJXFdXnjxVGEVMD8EJH5ec"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">在主 GPU 上，通过 loss function 计算出 loss，对 loss function 求导，求出损失梯度。</span></li><li id="tBagpcn3FBKEWNFwokKK6m"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">计算得到的梯度分发到所有 GPU 上。</span></li><li id="mMSj2BgSP1moUkhhXjkpZY"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">反向传播计算参数梯度。</span></li><li id="hMrqdD8p8BvJi7qDGYq65y"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">将所有梯度回传到主 GPU，通过梯度更新模型权重。</span></li><li id="kqXQTpRZP6n181fjwZczF7"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">不断重复上面的过程。</span></li></ul><div id="dFbUCz5v7imPSwMSpjSyJ4" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_4.png" style="width: 499px"/></figure></div><div id="SoegEJqPFWKT4yaxCZpMW" class="wolai-block wolai-text"><div><span class="inline-wrap">它使用非常简单，仅需一行代码即可实现。</span></div></div><code-block id="2ZWzHvPnJZso4KSDJLLZ7G" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all">net <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> net<span class="token punctuation">(</span>input_var<span class="token punctuation">)</span>  <span class="token comment"># input_var can be on any device, including CPU</span></pre></div></code-block><div id="kwr7mWyVsVGarggDe3RPtU" class="wolai-block wolai-text"><div><span class="inline-wrap">但是它的缺点也很明显：</span></div></div><ul class="wolai-block"><li id="thk7HDVYyUdwGsNE3kwYkk"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>单进程多线程带来的问题</b></span><span class="inline-wrap">：DataParallel<span class="jill"></span>使用单进程多线程进行实现的，方便了信息的交换，但受困于 GIL，会带来性能开销，速度很慢。而且，只能在单台服务器（单机多卡）上使用（不支持分布式）。同时，不能使用 Apex 进行混合精度训练。</span></li><li id="sW3KjpGAHZZvpbdSA95ee3"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>效率问题，主卡性能和通信开销容易成为瓶颈，GPU 利用率通常很低</b></span><span class="inline-wrap">：数据集需要先拷贝到主进程，然后再分片（split）到每个设备上；权重参数只在主卡（GPU0）上更新，需要每次迭代前向所有设备做一次同步；每次迭代的网络输出需要聚集到主卡（GPU0）上。因此，通信很快成为一个瓶颈。除此之外，这将导致主卡和其他卡之间，</span><span class="inline-wrap"><b>GPU<span class="jill"></span>利用率严重不均衡</b></span><span class="inline-wrap">（比如：主卡使用了<span class="jill"></span>10G<span class="jill"></span>显存，而其他卡只使用了<span class="jill"></span>2G<span class="jill"></span>显存，batch size<span class="jill"></span>稍微设置大一点主卡的显存就<span class="jill"></span>OOM<span class="jill"></span>了）。</span></li><li id="22JWixSaT2S2nUA1i2maXn"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>不支持模型并行</b></span><span class="inline-wrap">，由于其本身的局限性，没办法与模型并行组合使用。</span></li></ul><div id="3YVCamaap7eFsD67wUKEMn" class="wolai-block wolai-text"><div><span class="inline-wrap">当然，目前<span class="jill"></span>PyTorch<span class="jill"></span>官方建议使用<span class="jill"></span>DistributedDataParallel，而不是<span class="jill"></span>DataParallel<span class="jill"></span>类来进行多 GPU 训练，即使在单机多卡的情况下。那么下面我们来看看<span class="jill"></span>PyTorch DDP。</span></div></div><h3 id="qc9hQb38rRuiykiNfzjqbw" class="wolai-block"><span class="inline-wrap">3.分布式数据并行</span><span class="inline-wrap"><b>（PyTorch DDP）</b></span></h3><div id="dL8g9ZhqoncgNkpWvtn6Jy" class="wolai-block wolai-text"><div><span class="inline-wrap">分布式数据并行(</span><span class="inline-wrap"><code>torch.nn.DistributedDataParallel</code></span><span class="inline-wrap">)，基于多进程进行实现的，每个进程都有独立的优化器，执行自己的更新过程。每个进程都执行相同的任务，并且每个进程都与所有其他进程通信。进程（GPU）之间只传递梯度，这样网络通信就不再是瓶颈。</span></div></div><div id="anzKfHix6xW8ZTCh3CsBpg" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_5.png" style="width: 531px"/></figure></div><div id="iZTiBC9cgoSSHjMnCPCxdw" class="wolai-block wolai-text"><div><span class="inline-wrap">具体流程如下：</span></div></div><ul class="wolai-block"><li id="p9z6t8Neris3Vu5sr4YzYS"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">首先将 rank=0 进程中的模型参数广播到进程组中的其他进程；</span></li><li id="8DVHFt5LZwj7KTpyQb13Yj"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">然后，每个 DDP 进程都会创建一个 </span><span class="inline-wrap"><b>local Reducer</b></span><span class="inline-wrap"> 来负责梯度同步。</span></li><li id="vtTw2GP54YWTgzUoTRS7Pv"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">在训练过程中，每个进程从磁盘加载 batch 数据，并将它们传递到其 GPU。每个 GPU 都有自己的前向过程，完成前向传播后，</span><span class="inline-wrap"><b>梯度在各个 GPUs 间进行 All-Reduce</b></span><span class="inline-wrap">，每个 GPU 都收到其他 GPU 的梯度，从而可以独自进行反向传播和参数更新。</span></li><li id="nZwxgw6Kasqf19x5yiPjsV"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">同时，每一层的梯度不依赖于前一层，所以</span><span class="inline-wrap"><b>梯度的 All-Reduce 和后向过程同时计算</b></span><span class="inline-wrap">，以进一步缓解网络瓶颈。</span></li><li id="herBzNfGZHGGFVhbbVU1XN"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">在后向过程的最后，每个节点都得到了平均梯度，这样各个 GPU 中的模型参数保持同步 。</span></li></ul><div id="jfHKguzDKJBo6gSrug2W71" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_6.png" style="width: 100%"/></figure></div><div id="5ZBSP3fXsiW6MtbmgVwGu4" class="wolai-block wolai-text"><div><span class="inline-wrap">而</span><span class="inline-wrap"><b>DataParallel</b></span><span class="inline-wrap"> 是将梯度 reduce 到主卡，在主卡上更新参数，再将参数 broadcast 给其他 GPU，这样</span><span class="inline-wrap"><b>无论是主卡的负载还是通信开销都比 DDP 大很多</b></span><span class="inline-wrap">)，相比于<span class="jill"></span>DataParallel，DistributedDataParallel<span class="jill"></span>方式可以更好地进行多机多卡运算，更好的进行负载均衡，运行效率也更高，虽然使用起来较为麻烦，但对于追求性能来讲是一个更好的选择。</span></div></div><div id="t6xCznAAJoTTLssWU7trQP" class="wolai-block wolai-text"><div><span class="inline-wrap">以下为<span class="jill"></span>DistributedDataParallel<span class="jill"></span>的简单示例，使用 </span><span class="inline-wrap"><code>torch.nn.Linear </code></span><span class="inline-wrap">作为本地模型，用 DDP 对其进行包装，然后在 DDP 模型上运行一次前向传播、一次反向传播和更新优化器参数步骤。 之后，本地模型上的参数将被更新，并且不同进程上的所有模型完全相同。</span></div></div><code-block id="gVsiQPqJbtCjk6yYzaMEri" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> t dist
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>multiprocessing <span class="token keyword">as</span> mp
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel <span class="token keyword">import</span> DistributedDataParallel <span class="token keyword">as</span> DDP


<span class="token keyword">def</span> <span class="token function">example</span><span class="token punctuation">(</span>rank<span class="token punctuation">,</span> world_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># create default process group</span>
    dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span><span class="token string">"gloo"</span><span class="token punctuation">,</span> rank<span class="token operator">=</span>rank<span class="token punctuation">,</span> world_size<span class="token operator">=</span>world_size<span class="token punctuation">)</span>
    <span class="token comment"># create local model</span>
    model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>rank<span class="token punctuation">)</span>
    <span class="token comment"># construct DDP model</span>
    ddp_model <span class="token operator">=</span> DDP<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>rank<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># define loss function and optimizer</span>
    loss_fn <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>ddp_model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>

    <span class="token comment"># forward pass</span>
    outputs <span class="token operator">=</span> ddp_model<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>rank<span class="token punctuation">)</span><span class="token punctuation">)</span>
    labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>rank<span class="token punctuation">)</span>
    <span class="token comment"># backward pass</span>
    loss_fn<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># update parameters</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    world_size <span class="token operator">=</span> <span class="token number">2</span>
    mp<span class="token punctuation">.</span>spawn<span class="token punctuation">(</span>example<span class="token punctuation">,</span>
        args<span class="token operator">=</span><span class="token punctuation">(</span>world_size<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nprocs<span class="token operator">=</span>world_size<span class="token punctuation">,</span>
        join<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__<span class="token operator">==</span><span class="token string">"__main__"</span><span class="token punctuation">:</span>
    <span class="token comment"># Environment variables which need to be</span>
    <span class="token comment"># set when using c10d's default "env"</span>
    <span class="token comment"># initialization mode.</span>
    os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">"MASTER_ADDR"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"localhost"</span>
    os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">"MASTER_PORT"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"29500"</span>
    main<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></div></code-block><h4 id="jqVCYGMdnX1iMmzP8V43XC" class="wolai-block"><span class="inline-wrap">3.1 DP<span class="jill"></span>和<span class="jill"></span>DDP<span class="jill"></span>的区别</span></h4><div id="562p2tf9aY7JBCGDchuxQw" class="wolai-block wolai-text"><div><span class="inline-wrap">DP 和 DDP 的主要差异有以下几点：</span></div></div><ul class="wolai-block"><li id="fTU1ZABJ29u1MFEcCdEk2e"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">DP 是基于单进程多线程的实现，只用于单机情况，而 DDP 是多进程实现的，每个 GPU 对应一个进程，适用于单机和多机情况，真正实现分布式训练，并且因为每个进程都是独立的 Python 解释器，DDP 避免了 GIL 带来的性能开销。</span></li><li id="EMXQjMvHgf3gT2HwhPSXy"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">参数更新的方式不同。</span><span class="yellow inline-wrap">DDP<span class="jill"></span>在各进程梯度计算完成之后，各进程需要将梯度进行汇总平均，然后再由 rank=0 的进程，将其广播到所有进程后，各进程用该梯度来独立的更新参数（而 DP<span class="jill"></span>是梯度汇总到 GPU0，反向传播更新参数，再广播参数给其他剩余的 GPU）</span><span class="inline-wrap">。由于<span class="jill"></span>DDP<span class="jill"></span>各进程中的模型，初始参数一致 (初始时刻进行一次广播)，而每次用于更新参数的梯度也一致；因此，各进程的模型参数始终保持一致（而在<span class="jill"></span>DP<span class="jill"></span>中，全程维护一个 optimizer，对各个<span class="jill"></span>GPU<span class="jill"></span>上梯度进行求平均，而在主卡进行参数更新，之后再将模型参数广播到其他<span class="jill"></span>GPU）。相较于<span class="jill"></span>DP，DDP<span class="jill"></span>传输的数据量更少，训练更高效，不存在 DP 中负载不均衡的问题。目前，基本上 DP 已经被弃用。</span></li><li id="mc9X8uGuRjQvTa3xALHWpx"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">DDP 支持模型并行，而 DP 并不支持，这意味如果模型太大单卡显存不足时，只能使用<span class="jill"></span>DDP。</span></li></ul><h4 id="fog2mJtFZ3YmteSNGqpAYR" class="wolai-block"><span class="inline-wrap">3.2 补充说明</span></h4><div id="9s4u33fpRCaW6dUQ2ynqN" class="wolai-block wolai-text"><div><span class="inline-wrap">DP<span class="jill"></span>数据传输过程：</span></div></div><ol class="wolai-block"><li id="toG8CnfL4BMBdbUcTKWV9E"><div class="marker"></div><span class="inline-wrap">前向传播得到的输出结果<span class="jill"></span>gather<span class="jill"></span>到主<span class="jill"></span>cuda<span class="jill"></span>计算<span class="jill"></span>loss</span></li><li id="paVj3Pt6bfCpEoJZtY7Qqk"><div class="marker"></div><span class="inline-wrap">scatter<span class="jill"></span>上述<span class="jill"></span>loss<span class="jill"></span>到各个<span class="jill"></span>cuda</span></li><li id="dRe29Tfpw7VratbcKYWMwQ"><div class="marker"></div><span class="inline-wrap">各个<span class="jill"></span>cuda<span class="jill"></span>反向传播计算得到梯度后<span class="jill"></span>gather<span class="jill"></span>到主<span class="jill"></span>cuda<span class="jill"></span>后，主<span class="jill"></span>cuda<span class="jill"></span>的模型参数被更新。</span></li><li id="moj1tKSyHHU7FH9KmBA5AZ"><div class="marker"></div><span class="inline-wrap">主<span class="jill"></span>cuda<span class="jill"></span>将模型参数<span class="jill"></span>broadcast<span class="jill"></span>到其它<span class="jill"></span>cuda<span class="jill"></span>设备上，至此，完成权重参数值的同步。</span></li></ol><div id="rhgiHz4fnFVFQggeKSpsrC" class="wolai-block wolai-text"><div><span class="inline-wrap">综上，DP<span class="jill"></span>大概是有<span class="jill"></span>4<span class="jill"></span>次输出传输。</span></div></div><div id="jCeUNFP5twVK6eJHi8tGA1" class="wolai-block wolai-text"><div><span class="inline-wrap">DDP<span class="jill"></span>数据传输过程：</span></div></div><ol class="wolai-block"><li id="9yzb5sWwKNFMxjWtY3e2kB"><div class="marker"></div><span class="inline-wrap">前向传播的输出和<span class="jill"></span>loss<span class="jill"></span>的计算都是在每个<span class="jill"></span>cuda<span class="jill"></span>独立计算的，梯度<span class="jill"></span>all-reduce<span class="jill"></span>到所有的<span class="jill"></span>CUDA(传输梯度)，这样初始参数相同，para.grad<span class="jill"></span>也相同，反向传播后参数就还是保持一致的，其他没有数据传输了。</span></li></ol><h3 id="cuEG6ntsuVo7CrcMCTSWr9" class="wolai-block"><span class="inline-wrap">4.完全分片数据并行</span><span class="inline-wrap"><b>(PyTorch FSDP)</b></span></h3><div id="49XhZo2DRrCjj1WC2CXS7B" class="wolai-block wolai-text"><div><span class="inline-wrap">由于 PyTorch FSDP 受 DeepSpeed ZeRO 启发而获得灵感，因此，下面先简要介绍下 ZeRO。</span></div></div><h4 id="dHFSo5u5pDT38UBXdQZdAd" class="wolai-block"><span class="inline-wrap">4.1 补充说明：ZeRO</span></h4><div id="cotXCaHVJUGKgZhbGMmjiT" class="wolai-block wolai-text"><div><span class="inline-wrap">通常来说，在模型训练的过程中，GPU<span class="jill"></span>上需要进行存储的参数包括了模型本身的参数、优化器状态、激活函数的输出值、梯度以及一些零时的<span class="jill"></span>Buffer。各种数据的占比如下图所示：</span></div></div><div id="7Hnom5q5b3pjxEWUqvCzcQ" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_7.png" style="width: 100%"/></figure></div><div id="aDQuuYdYgYmmAawfBdDrWT" class="wolai-block wolai-text"><div><span class="inline-wrap">可以看到模型参数仅占模型训练过程中所有数据的一部分，当进行混合精度运算时，其中模型状态参数(优化器状态 + 梯度<span class="jill"></span>+ 模型参数）占到了一大半以上。因此，我们需要想办法去除模型训练过程中的冗余数据。</span></div></div><div id="j4htFBqwBQBU9sqroKgCwx" class="wolai-block wolai-text"><div><span class="inline-wrap">针对模型状态的存储优化（去除冗余），DeepSpeed 提出了 </span><span class="yellow inline-wrap"><b>ZeRO</b></span><span class="inline-wrap">，</span><span class="red inline-wrap"><b>ZeRO 使用的方法是分片，即每张卡只存 1/N 的模型状态量，这样系统内只维护一份模型状态参数</b></span><span class="inline-wrap">。</span></div></div><div id="6D1PQCLqmAFm4Tsy9xz2Ft" class="wolai-block wolai-text"><div><span class="inline-wrap">ZeRO<span class="jill"></span>对 模型状态（Model States）参数进行不同程度的分割，主要有三个不同级别：</span></div></div><ul class="wolai-block"><li id="vCjhJELmKNJVngwyxvXLS"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>ZeRO-1 </b></span><span class="inline-wrap">: 对优化器状态分片（Optimizer States Sharding）</span></li><li id="pHA3wPKhFF5RqaRQt2AM79"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>ZeRO-2</b></span><span class="inline-wrap"> : 对优化器状态和梯度分片（Optimizer States &amp; Gradients Sharding）</span></li><li id="gxjpcG9BhSPRz1zoWNAH2c"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="red inline-wrap"><b>ZeRO-3</b></span><span class="inline-wrap"> : 对优化器状态、梯度分片以及模型权重参数分片（Optimizer States &amp; Gradients &amp; Parameters Sharding）</span></li></ul><div id="9EZGC45yZ9xP5cMDhwn6Us" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_8.png" style="width: 100%"/></figure></div><div id="tS1P9wpsUJFHBjibSxQDVg" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>ZeRO-1</b></span><span class="inline-wrap">：</span></div></div><div id="trSoQrdS2eybNBooj7j1mk" class="wolai-block wolai-text"><div><span class="inline-wrap">ZeRO-1<span class="jill"></span>没有将模型本身进行分片，也</span><span class="red inline-wrap"><b>没有将<span class="jill"></span>Gradient<span class="jill"></span>进行分片，而是只将优化器进行分片</b></span><span class="inline-wrap">。训练过程与<span class="jill"></span>DDP<span class="jill"></span>类似。</span></div></div><ol class="wolai-block"><li id="fjETuyGXNcV1e8XVWDx6iH"><div class="marker"></div><span class="inline-wrap">forward<span class="jill"></span>过程由每个<span class="jill"></span>rank<span class="jill"></span>的<span class="jill"></span>GPU<span class="jill"></span>独自完整的完成，然后进行<span class="jill"></span>backward<span class="jill"></span>过程。在<span class="jill"></span>backward<span class="jill"></span>过程中，梯度通过<span class="jill"></span>allReduce<span class="jill"></span>进行同步。</span></li><li id="s99tB5Runo723NgYWhwjZt"><div class="marker"></div><span class="inline-wrap">Optimizer state 使用贪心策略基于参数量进行分片，以此确保每个<span class="jill"></span>rank<span class="jill"></span>几乎拥有相同大小的优化器内存。</span></li><li id="6qjVYkyi7LoC33gFtaiUQW"><div class="marker"></div><span class="inline-wrap">每个<span class="jill"></span>rank<span class="jill"></span>只负责更新当前优化器分片的部分，由于每个<span class="jill"></span>rank<span class="jill"></span>只有分片的优化器<span class="jill"></span>state，所以当前<span class="jill"></span>rank<span class="jill"></span>忽略其余的<span class="jill"></span>state。</span></li><li id="cB2WowHwhs2TWaXqXi69a"><div class="marker"></div><span class="inline-wrap">在更新过后，通过广播或者<span class="jill"></span>allGather<span class="jill"></span>的方式确保所有的<span class="jill"></span>rank<span class="jill"></span>都收到最新更新过后的模型参数。</span></li></ol><div id="9wLvwfMaB3e6bG8Vti7CQN" class="wolai-block wolai-text"><div><span class="inline-wrap">ZeRO-1 </span><span class="green inline-wrap"><b>非常适合使用类似<span class="jill"></span>Adam<span class="jill"></span>进行优化的模型训练</b></span><span class="inline-wrap">，因为<span class="jill"></span>Adam<span class="jill"></span>拥有额外的参数<span class="jill"></span>m（momentum）与<span class="jill"></span>v（variance），特别是<span class="jill"></span>FP16<span class="jill"></span>混合精度训练。ZeRO-1 不适合使用<span class="jill"></span>SGD<span class="jill"></span>类似的优化器进行模型训练，因为<span class="jill"></span>SGD<span class="jill"></span>只有较少的参数内存，并且由于需要更新模型参数，导致额外的通讯成本。ZeRO-1<span class="jill"></span>只是解决了<span class="jill"></span>Optimizer state<span class="jill"></span>的冗余。</span></div></div><div id="o9TEvVPYskjkK6Ub38nSYJ" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>ZeRO-2</b></span><span class="inline-wrap">：</span></div></div><div id="ntNkeVWcXhfgsPuc8aySA4" class="wolai-block wolai-text"><div><span class="inline-wrap">相比于<span class="jill"></span>ZeRO-1，</span><span class="red inline-wrap"><b>ZeRO-2<span class="jill"></span>除了对<span class="jill"></span>optimizer state<span class="jill"></span>进行切分，还对<span class="jill"></span>Gradient<span class="jill"></span>进行了切分</b></span><span class="inline-wrap">。</span></div></div><div id="g1zGDutuuCYsFM2dsPdaat" class="wolai-block wolai-text"><div><span class="inline-wrap">像<span class="jill"></span>ZeRO-1<span class="jill"></span>一样将<span class="jill"></span>optimizer<span class="jill"></span>的参数进行分片，并安排在不同的<span class="jill"></span>rank<span class="jill"></span>上。在<span class="jill"></span>backward<span class="jill"></span>过程中，</span><span class="green inline-wrap"><b>gradients<span class="jill"></span>被<span class="jill"></span>reduce<span class="jill"></span>操作到对应的<span class="jill"></span>rank<span class="jill"></span>上，取代了<span class="jill"></span>all-reduce</b></span><span class="inline-wrap">，以此减少了通讯开销。 每个<span class="jill"></span>rank<span class="jill"></span>独自更新各自负责的参数。在更新操作之后，广播或<span class="jill"></span>allGather<span class="jill"></span>保证所有的<span class="jill"></span>ranks<span class="jill"></span>接收到更新后的参数。</span></div></div><div id="tB8cSrhaNXqkpTQWu6fkMS" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>ZeRO-3</b></span><span class="inline-wrap">：</span></div></div><div id="oGb7tvDLaPdb1u595QC7Sy" class="wolai-block wolai-text"><div><span class="inline-wrap">为了进一步节省更多的内存，</span><span class="red inline-wrap"><b>ZeRO-3<span class="jill"></span>提出进行模型参数的分片</b></span><span class="inline-wrap">。类似以上两种分片方式，ranks<span class="jill"></span>负责模型参数的切片。可以进行参数切片的原因主要有以下两点：</span></div></div><ol class="wolai-block"><li id="4SxShR1Wj992fMhvQmLYLN"><div class="marker"></div><span class="inline-wrap">All-Reduce<span class="jill"></span>操作可以被拆分为<span class="jill"></span>Reduce<span class="jill"></span>与<span class="jill"></span>allgather<span class="jill"></span>操作的结合。</span></li><li id="boDSi2vPX6v6RPxovdTRZQ"><div class="marker"></div><span class="inline-wrap">模型的每一层拥有该层的完整参数，并且整个层能够直接被一个<span class="jill"></span>GPU<span class="jill"></span>装下。所以计算前向的时候，除了当前<span class="jill"></span>rank<span class="jill"></span>需要的层之外，其余的层的参数可以抛弃。从这个层面上来说，Zero<span class="jill"></span>相当于数据并行<span class="jill"></span>+<span class="jill"></span>模型并行。</span></li></ol><h4 id="nhjoogNXPU1WdQwFhX6N1p" class="wolai-block"><span class="inline-wrap">4.2 FSDP</span></h4><div id="gf9rxm13GzDhUXNPKqjw73" class="wolai-block wolai-text"><div><span class="inline-wrap">完全分片数据并行(</span><span class="inline-wrap"><code>torch.distributed.fsdp.FullyShardedDataParallel</code></span><span class="inline-wrap">)，是<span class="jill"></span>Pytorch<span class="jill"></span>最新的数据并行方案，在<span class="jill"></span>1.11<span class="jill"></span>版本引入的新特性，目的主要是用于训练大模型。我们都知道<span class="jill"></span>Pytorch DDP<span class="jill"></span>用起来简单方便，但是要求整个模型加载到一个<span class="jill"></span>GPU<span class="jill"></span>上，这使得大模型的训练需要使用额外复杂的设置进行模型分片。因此，为了打破模型分片的障碍（</span><span class="inline-wrap"><b>包括模型参数，梯度，优化器状态</b></span><span class="inline-wrap">）；同时，仍然保持了数据并行的简单性，该新特性应运而生。</span></div></div><div id="p2wHRTR6ZKxZnbqswm1KxE" class="wolai-block wolai-text"><div><span class="inline-wrap">FSDP 是一种新型数据并行训练方法，但与传统的数据并行不同，传统的数据并行维护模型参数、梯度和优化器状态的每个 GPU 副本，而 </span><span class="red inline-wrap"><b>FSDP 将所有这些状态跨数据并行工作线程进行分片，并且可以选择将模型参数分片卸载到 CPU</b></span><span class="inline-wrap">。</span></div></div><div id="QryAQ9KYjBguPSsxwcDHy" class="wolai-block wolai-text"><div><span class="inline-wrap">下图显示了 FSDP 如何在 2 个数据并行进程中工作流程：</span></div></div><div id="dJLKsUQguBvYS3seXGzjXN" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_9.png" style="width: 100%"/></figure></div><div id="8yDDantT3xjtKfWTXSBZz6" class="wolai-block wolai-text"><div><span class="inline-wrap">通常，模型层以嵌套方式用 FSDP 包装，因此，只有</span><span class="inline-wrap"><b>单个 FSDP 实例</b></span><span class="inline-wrap">中的层需要在前向或后向计算期间将完整参数收集到</span><span class="inline-wrap"><b>单个设备</b></span><span class="inline-wrap">。 计算完成后，收集到的完整参数将立即释放，释放的内存可用于下一层的计算。 通过这种方式，可以节省峰值 GPU 内存，从而可以扩展训练以使用更大的模型大小或更大的批量大小。 为了进一步最大化内存效率，当实例在计算中不活动时，FSDP 可以将参数、梯度和优化器状态卸载到 CPU。</span></div></div><div id="dg8aFvAZvRhFYJ6k4aXSob" class="wolai-block wolai-text"><div><span class="inline-wrap">解锁<span class="jill"></span>ZeRO/FSDP<span class="jill"></span>的关键是我们可以把<span class="jill"></span>DDP<span class="jill"></span>之中的<span class="jill"></span>All-Reduce<span class="jill"></span>操作分解为独立的 Reduce-Scatter 和 All-Gather 操作。</span></div></div><div id="ev2LruZfQ6TEojRQhZk7Xk" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_10.png" style="width: 100%"/></figure></div><div id="p932kFVCNL2HwvP54nqcwe" class="wolai-block wolai-text"><div><span class="inline-wrap">All-Reduce 是 Reduce-Scatter 和 All-Gather 的组合。聚合梯度的标准 All-Reduce 操作可以分解为两个单独的阶段。</span></div></div><ul class="wolai-block"><li id="bFrXymQEDrVPXpUt8mToqW"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">Reduce-Scatter 阶段，在每个<span class="jill"></span>GPU<span class="jill"></span>上，会基于 rank 索引对 rank 之间相等的块进行求和。</span></li><li id="o26Pu3hw9bUqVf8xGhdgmw"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">All-Gather 阶段，每个<span class="jill"></span>GPU<span class="jill"></span>上的聚合梯度分片可供所有<span class="jill"></span>GPU<span class="jill"></span>使用。</span></li></ul><div id="jVaab4URYCUEDoTkLLDYHP" class="wolai-block wolai-text"><div><span class="inline-wrap">通过重新整理 Reduce-Scatter 和 All-Gather，每个 DDP worker<span class="jill"></span>只需要存储一个参数分片和优化器状态。</span></div></div><div id="8yJ8CpT3v9Kpe4VyEcBGPU" class="wolai-block wolai-text"><div><span class="inline-wrap">在 PyTorch 中使用 FSDP 包装模型有两种方法。</span></div></div><ul class="wolai-block"><li id="75xfCWUB7iiyYjcdVQYoz6"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">自动包装（Auto Wrapping）是 DDP 的直接替代品；</span></li><li id="3ChamKNLK8iiCiqpuo4uG4"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">手动包装（Manual Wrapping）需要对模型定义代码进行少量的更改，并且能够探索复杂的分片策略。</span></li></ul><div id="ffwD8yLcWS97e1G3zUJWMN" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>自动包装（Auto Wrapping）</b></span></div></div><div id="r7HKy8E15LjbuqDaj1CYyy" class="wolai-block wolai-text"><div><span class="inline-wrap">模型层应以嵌套方式包装在 FSDP 中，以节省峰值内存并实现通信和计算重叠。 最简单的方法是自动包装，它可以作为 DDP 的直接替代品，而无需更改其余代码。</span></div></div><div id="gedzgKLq6MjCD63shFqH33" class="wolai-block wolai-text"><div><span class="inline-wrap"><code>fsdp_auto_wrap_policy</code></span><span class="inline-wrap">参数允许指定可调用函数以使用 FSDP 递归地包裹层。 PyTorch FSDP<span class="jill"></span>提供的</span><span class="inline-wrap"><code>default_auto_wrap_policy</code></span><span class="inline-wrap">函数递归地包裹参数数量大于<span class="jill"></span>100M<span class="jill"></span>的层。当然，您也可以根据需要提供自己的包装策略。</span></div></div><div id="rbXGK5i1m6x8WWfTuTgzBH" class="wolai-block wolai-text"><div><span class="inline-wrap">此外，可以选择配置 </span><span class="inline-wrap"><code>cpu_offload</code></span><span class="inline-wrap">，以便在计算中不使用包装参数时将这些参数卸载到 CPU。 这可以进一步提高内存效率，但代价是主机和设备之间的数据传输开销。</span></div></div><div id="ncBy1TBTepNMVLAhfsvrCx" class="wolai-block wolai-text"><div><span class="inline-wrap">下面的示例展示了如何使用自动包装（Auto Wrapping）来包装 FSDP。</span></div></div><code-block id="aBujh1RhD298UWS2DKGgQx" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>fsdp <span class="token keyword">import</span> <span class="token punctuation">(</span>
   FullyShardedDataParallel<span class="token punctuation">,</span>
   CPUOffload<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>fsdp<span class="token punctuation">.</span>wrap <span class="token keyword">import</span> <span class="token punctuation">(</span>
   default_auto_wrap_policy<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
 
<span class="token keyword">class</span> <span class="token class-name">model</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
   <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
       <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
       self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
       self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">)</span>
       self<span class="token punctuation">.</span>layer3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
 
model <span class="token operator">=</span> DistributedDataParallel<span class="token punctuation">(</span>model<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
fsdp_model <span class="token operator">=</span> FullyShardedDataParallel<span class="token punctuation">(</span>
   model<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
   fsdp_auto_wrap_policy<span class="token operator">=</span>default_auto_wrap_policy<span class="token punctuation">,</span>
   cpu_offload<span class="token operator">=</span>CPUOffload<span class="token punctuation">(</span>offload_params<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span></pre></div></code-block><div id="sgXxyfvvsJxRf5XapjQAKb" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>手动包装（Manual Wrapping）</b></span></div></div><div id="djc6LUcBXMutsTztSVoXXC" class="wolai-block wolai-text"><div><span class="inline-wrap">通过有选择地对模型的某些部分应用包装，手动包装对于探索复杂的分片策略非常有用。 总体设置可以传递给<span class="jill"></span>enable_wrap()上下文管理器。</span></div></div><code-block id="6ws4tgxbiSdxX8ptAdfZYD" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>fsdp <span class="token keyword">import</span> <span class="token punctuation">(</span>
   FullyShardedDataParallel<span class="token punctuation">,</span>
   CPUOffload<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>fsdp<span class="token punctuation">.</span>wrap <span class="token keyword">import</span> <span class="token punctuation">(</span>
   enable_wrap<span class="token punctuation">,</span>
   wrap<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> typing <span class="token keyword">import</span> Dict
 
 
<span class="token keyword">class</span> <span class="token class-name">model</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
   <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
       <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
       self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> wrap<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
       self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">)</span>
       self<span class="token punctuation">.</span>layer3 <span class="token operator">=</span> wrap<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 
wrapper_kwargs <span class="token operator">=</span> Dict<span class="token punctuation">(</span>cpu_offload<span class="token operator">=</span>CPUOffload<span class="token punctuation">(</span>offload_params<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> enable_wrap<span class="token punctuation">(</span>wrapper_cls<span class="token operator">=</span>FullyShardedDataParallel<span class="token punctuation">,</span> <span class="token operator">**</span>wrapper_kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
   fsdp_model <span class="token operator">=</span> wrap<span class="token punctuation">(</span>model<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></div></code-block><div id="mdxM64P7zPDKYmuYBTjWrK" class="wolai-block wolai-text"><div><span class="inline-wrap">使用上述两种方法之一，用 FSDP 包装模型后，可以采用与本地训练类似的方式训练模型，具体如下所示：</span></div></div><code-block id="pwmaGYLPWdQ4TNBQYdxq4" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all">optim <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>fsdp_model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> sample<span class="token punctuation">,</span> label <span class="token keyword">in</span> next_batch<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  out <span class="token operator">=</span> fsdp_model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
  loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>out<span class="token punctuation">,</span> label<span class="token punctuation">)</span>
  loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
  optim<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></div></code-block><h4 id="9uDtL8oFBgpMzXoCkGrR4X" class="wolai-block"><span class="inline-wrap">4.3 DDP<span class="jill"></span>和<span class="jill"></span>FSDP<span class="jill"></span>的区别</span></h4><div id="rU1H8UTicA218qjJEeULaP" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_11.png" style="width: 100%"/></figure></div><div id="4cPnou7xpZQeebfmUnCUMa" class="wolai-block wolai-text"><div><span class="inline-wrap">在标准的数据并行（DistributedDataParallel）训练方法中，</span><span class="red inline-wrap"><b>每个<span class="jill"></span>GPU<span class="jill"></span>上都有一个模型副本，向前和向后传递的序列只在自己的数据分片上进行运行</b></span><span class="inline-wrap">。在这些局部计算之后，每个局部过程的参数和优化器与其他<span class="jill"></span>GPU<span class="jill"></span>共享，以便计算全局权重更新。</span></div></div><div id="dYMkgXGvhpEzTSW7w2DvBA" class="wolai-block wolai-text"><div><span class="inline-wrap">而在<span class="jill"></span>FullyShardedDataParallel<span class="jill"></span>训练方法中：</span></div></div><ul class="wolai-block"><li id="66L5U78TCzRoLwmyxpFagU"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Model shard</b></span><span class="inline-wrap">：每个<span class="jill"></span>GPU<span class="jill"></span>上仅存在</span><span class="inline-wrap"><b>模型的分片</b></span><span class="inline-wrap">。</span></li><li id="mumn6FCeVnCtrxzuyM1QpG"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>All-gather</b></span><span class="inline-wrap">：每个<span class="jill"></span>GPU<span class="jill"></span>通过<span class="jill"></span>all-gather<span class="jill"></span>从其他<span class="jill"></span>GPU<span class="jill"></span>收集所有</span><span class="inline-wrap"><b>权重</b></span><span class="inline-wrap">，以在本地计算前向传播。</span></li><li id="rZMcPzgVVAahPCwAMrMzKV"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Forward（local）</b></span><span class="inline-wrap">：在本地进行前向操作。前向计算和后向计算都是利用完整模型。</span></li><li id="mdsuYTcrAggmiC49PacnNm"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>All-gather</b></span><span class="inline-wrap">：然后在后向传播之前再次执行此</span><span class="inline-wrap"><b>权重</b></span><span class="inline-wrap">收集。</span></li><li id="VtYCLVBGrJQrG6JybNcPq"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Backward（local）</b></span><span class="inline-wrap">：本地进行后向操作。前向计算和后向计算都是利用完整模型，此时每个<span class="jill"></span>GPU<span class="jill"></span>上也都是</span><span class="inline-wrap"><b>全部梯度</b></span><span class="inline-wrap">。</span></li><li id="tsPzrx5LuLi5f6ji6PKHXk"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Reduce-Scatter</b></span><span class="inline-wrap">：在向后传播之后，局部</span><span class="inline-wrap"><b>梯度</b></span><span class="inline-wrap">被聚合并且通过 Reduce-Scatter 在各个<span class="jill"></span>GPU<span class="jill"></span>上分片，每个分片上的梯度是聚合之后本分片对应的那部分。</span></li><li id="pAHhMVMHGVFwWYLQgZRpVZ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="yellow inline-wrap"><b>Update Weight（local）</b></span><span class="inline-wrap">：每个<span class="jill"></span>GPU<span class="jill"></span>更新其局部</span><span class="inline-wrap"><b>权重</b></span><span class="inline-wrap">分片。</span></li></ul><div id="gD8D3xbHWEhDHNSaJTAMAB" class="wolai-block wolai-text"><div><span class="inline-wrap">同时，为了最大限度地提高内存效率，我们可以在每层前向传播后丢弃全部权重，为后续层节省内存。这可以通过将 FSDP 包装应用于网络中的每一层来实现（通过设置</span><span class="inline-wrap"><code>reshard_after_forward=True</code></span><span class="inline-wrap">）。</span></div></div><h3 id="cMr3fyMhohHDScZpbjarAf" class="wolai-block"><span class="inline-wrap">5.总结</span></h3><div id="56fcsQAt6wdgD5w3MAgvaR" class="wolai-block wolai-text"><div><span class="inline-wrap">本文主要讲解了大模型分布式训练并行技术的数据并行，并以<span class="jill"></span>Pytorch<span class="jill"></span>为主线讲解了<span class="jill"></span>DP、DDP、FSDP<span class="jill"></span>三种不同的数据并行方案。</span></div></div><div id="dh6ChYKuZLnKzWbp1RBG3J" class="wolai-block wolai-text"><div><span class="inline-wrap">DP 主要存在如下问题：</span></div></div><ol class="wolai-block"><li id="3Q3SeCRDFe57FsBduCfvw"><div class="marker"></div><span class="inline-wrap">单进程多线程模式，由于锁的机制导致线程间同步存在瓶颈。</span></li><li id="mCDofca3FzNd647uGS2Gia"><div class="marker"></div><span class="inline-wrap">使用普通的<span class="jill"></span>All-Reduce<span class="jill"></span>机制，所有的卡需要将梯度同步给<span class="jill"></span>0<span class="jill"></span>号节点，并由<span class="jill"></span>0<span class="jill"></span>号节点平均梯度后反向传播，再分发给所有其他节点，意味着<span class="jill"></span>0<span class="jill"></span>号节点负载很重。</span></li><li id="ruJK2Cw72kecScv3HHjGoY"><div class="marker"></div><span class="inline-wrap">由于第二点的原因，导致<span class="jill"></span>0<span class="jill"></span>号<span class="jill"></span>GPU<span class="jill"></span>通讯成本是随着<span class="jill"></span>GPU<span class="jill"></span>数量的上升而线性上升的。</span></li><li id="wczrtEFE8avKUgRZ8wgKkt"><div class="marker"></div><span class="inline-wrap">不支持多机多卡。</span></li></ol><div id="iPhbdmf5uKUUrWc8aaRrQC" class="wolai-block wolai-text"><div><span class="inline-wrap">目前，由于性能问题，DP<span class="jill"></span>基本不用了。</span></div></div><div id="qkFGScxcXRdBKEmJPZyUa9" class="wolai-block wolai-text"><div><span class="inline-wrap">而 DDP 是多进程实现的，每个 GPU 对应一个进程，适用于单机和多机情况，真正实现分布式训练，并且因为每个进程都是独立的 Python 解释器，DDP 避免了 GIL 带来的性能开销。</span></div></div><div id="4MyxKJMwtdHnDk8G82b6RA" class="wolai-block wolai-text"><div><span class="inline-wrap">DDP<span class="jill"></span>在各进程梯度计算完成之后，各进程需要将梯度进行汇总平均，然后再由 rank=0 的进程，将其广播到所有进程后，各进程用该梯度来独立的更新参数。由于<span class="jill"></span>DDP<span class="jill"></span>各进程中的模型，初始参数一致 (初始时刻进行一次广播)，而每次用于更新参数的梯度也一致；因此，各进程的模型参数始终保持一致。相较于<span class="jill"></span>DP，DDP<span class="jill"></span>传输的数据量更少，训练更高效，不存在 DP 中负载不均衡的问题。</span></div></div><div id="hdY28rdyP9LJXzyfHfe3vp" class="wolai-block wolai-text"><div><span class="inline-wrap">虽然<span class="jill"></span>Pytorch DDP<span class="jill"></span>实现了真正的分布式训练，同时，避免了<span class="jill"></span>DP 中负载不均衡的问题，但是，要求整个模型加载到一个<span class="jill"></span>GPU<span class="jill"></span>上，这使得大模型的训练需要使用额外复杂的设置进行模型分片。因此，为了打破模型分片的障碍（</span><span class="inline-wrap"><b>包括模型参数，梯度，优化器状态</b></span><span class="inline-wrap">），同时仍然保持了数据并行的简单性，FSDP<span class="jill"></span>应运而生。</span></div></div><div id="hDexAgPZT6yUH8osvSKzxG" class="wolai-block wolai-text"><div><span class="inline-wrap">FSDP 是一种新型数据并行训练方法，但与传统的数据并行不同，传统的数据并行维护模型参数、梯度和优化器状态的每个 GPU 副本，而 FSDP 将所有这些状态跨数据并行工作线程进行分片，并且可以选择将模型参数分片卸载到 CPU。</span></div></div></article><footer></footer></body></html></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/wdn_icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Dongnian</div><div class="author-info__description">A salty fish swimming in the sea of deep learning!</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/wdndev"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wdndev" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:dongnian.wang@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #24292e;"></i></a><a class="social-icon" href="https://blog.csdn.net/wdnshadow" target="_blank" title="CSDN"><i class="fas fa-rss" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to My Personal Blog! <br /> If Not, Please Visit <a target="_blank" rel="noopener" href="https://wdndev.gitee.io/"> <font color=#00BFFF>Gitee Mirror</font></a>.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#mPV8nomWh3Mabr25we2W3s"><span class="toc-text">1.简述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gtRTPisArMHkfdQ6TRun6j"><span class="toc-text">2.数据并行（PyTorch DP）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#qc9hQb38rRuiykiNfzjqbw"><span class="toc-text">3.分布式数据并行（PyTorch DDP）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#jqVCYGMdnX1iMmzP8V43XC"><span class="toc-text">3.1 DP和DDP的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#fog2mJtFZ3YmteSNGqpAYR"><span class="toc-text">3.2 补充说明</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cuEG6ntsuVo7CrcMCTSWr9"><span class="toc-text">4.完全分片数据并行(PyTorch FSDP)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#dHFSo5u5pDT38UBXdQZdAd"><span class="toc-text">4.1 补充说明：ZeRO</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nhjoogNXPU1WdQwFhX6N1p"><span class="toc-text">4.2 FSDP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9uDtL8oFBgpMzXoCkGrR4X"><span class="toc-text">4.3 DDP和FSDP的区别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cMr3fyMhohHDScZpbjarAf"><span class="toc-text">5.总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_article/9.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BALLM/" title="检索增强LLM">检索增强LLM</a><time datetime="2024-01-12T16:00:00.000Z" title="Created 2024-01-13 00:00:00">2024-01-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/6.%E6%96%87%E6%9C%AC%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/" title="LLMs公开课 - 6.文本理解和生成大模型">LLMs公开课 - 6.文本理解和生成大模型</a><time datetime="2024-01-09T16:00:00.000Z" title="Created 2024-01-10 00:00:00">2024-01-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/5.%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/" title="LLMs公开课 - 5.高效训练&amp;模型压缩">LLMs公开课 - 5.高效训练&amp;模型压缩</a><time datetime="2024-01-06T16:00:00.000Z" title="Created 2024-01-07 00:00:00">2024-01-07</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DSA/"><span class="card-category-list-name">DSA</span><span class="card-category-list-count">24</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLMs/"><span class="card-category-list-name">LLMs</span><span class="card-category-list-count">16</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/PL/"><span class="card-category-list-name">PL</span><span class="card-category-list-count">7</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/PL/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">6</span></a></li></ul></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/DSA/" style="font-size: 1.42em; color: rgb(91, 145, 17)">DSA</a><a href="/tags/RL/" style="font-size: 1.28em; color: rgb(176, 79, 19)">RL</a><a href="/tags/Transformer/" style="font-size: 1.45em; color: rgb(127, 170, 70)">Transformer</a><a href="/tags/LLMs/" style="font-size: 1.32em; color: rgb(112, 148, 55)">LLMs</a><a href="/tags/PaperReading/" style="font-size: 1.38em; color: rgb(110, 146, 60)">PaperReading</a><a href="/tags/DeepLearning/" style="font-size: 1.25em; color: rgb(90, 48, 1)">DeepLearning</a><a href="/tags/CV/" style="font-size: 1.15em; color: rgb(82, 200, 174)">CV</a><a href="/tags/GPT/" style="font-size: 1.18em; color: rgb(7, 16, 91)">GPT</a><a href="/tags/PL/" style="font-size: 1.22em; color: rgb(17, 30, 26)">PL</a><a href="/tags/leetcode/" style="font-size: 1.35em; color: rgb(106, 126, 145)">leetcode</a><a href="/tags/algo/" style="font-size: 1.15em; color: rgb(181, 144, 151)">algo</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">January 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">December 2023</span><span class="card-archive-list-count">14</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">November 2023</span><span class="card-archive-list-count">26</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">October 2023</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/09/"><span class="card-archive-list-date">September 2023</span><span class="card-archive-list-count">4</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">87</div></div><div class="webinfo-item"><div class="item-name">Run time :</div><div class="item-count" id="runtimeshow" data-publishDate="2023-05-31T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Total Count :</div><div class="item-count">411.2k</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2024-12-08T03:57:10.055Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By Dongnian</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'oe7vzWxH80qwJJjWslYTCViT-gzGzoHsz',
      appKey: 'k89nSbK0BTbmzmpQottRHvNI',
      avatar: 'monsterid',
      serverURLs: 'https://oe7vzwxh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>