<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>37.2° Blog | 37.2° Blog</title><meta name="author" content="Dongnian"><meta name="copyright" content="Dongnian"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="分布式训练题目 - wolai 笔记1. 理论篇1.1 训练 大语言模型 存在问题？计算资源需求： 训练大型语言模型需要大量的计算资源，包括高端 GPU、大量的内存和高速存储器。这可能限制了许多研究人员和组织的训练能力，因为这些资源通常很昂贵。数据需求： 训练大型语言模型需要大规模的数据集，这些数据集通常需要大量的标注和清洗工作。获取高质量的数据可能是一项困难和昂贵的任务。长时间训练： 训练大型">
<meta property="og:type" content="website">
<meta property="og:title" content="37.2° Blog">
<meta property="og:url" content="https://wdndev.github.io/note/llm/llm_concept/04.%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E9%A2%98%E7%9B%AE/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E9%A2%98%E7%9B%AE.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="分布式训练题目 - wolai 笔记1. 理论篇1.1 训练 大语言模型 存在问题？计算资源需求： 训练大型语言模型需要大量的计算资源，包括高端 GPU、大量的内存和高速存储器。这可能限制了许多研究人员和组织的训练能力，因为这些资源通常很昂贵。数据需求： 训练大型语言模型需要大规模的数据集，这些数据集通常需要大量的标注和清洗工作。获取高质量的数据可能是一项困难和昂贵的任务。长时间训练： 训练大型">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2024-12-08T03:56:28.050Z">
<meta property="article:modified_time" content="2024-12-08T03:56:28.050Z">
<meta property="article:author" content="Dongnian">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/note/llm/llm_concept/04.%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E9%A2%98%E7%9B%AE/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E9%A2%98%E7%9B%AE.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Dongnian","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '37.2° Blog',
  isPost: false,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-08 11:56:28'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-solid fa-magnifying-glass"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/note"><i class="fa-fw fa-regular fa-bookmark"></i><span> Note</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-solid fa-magnifying-glass"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/note"><i class="fa-fw fa-regular fa-bookmark"></i><span> Note</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="page"><h1 class="page-title"></h1><div id="article-container"><!DOCTYPE html>
<html lang="zh-Hans-CN"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=Edge"/><link rel="stylesheet" type="text/css" href="../../css/modern-norm.min.css"/><link rel="stylesheet" type="text/css" href="../../css/prism.min.css"/><link rel="stylesheet" type="text/css" href="../../css/katex.min.css"/><link rel="stylesheet" type="text/css" href="../../css/wolai.css"/><title>分布式训练题目 - wolai 笔记</title><link rel="shortcut icon" href="data:image/svg+xml,%3Csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 800 800&apos;%3E%3Cdefs%3E%3Cstyle%3E.cls-1%7Bfill:%23fff;%7D%3C/style%3E%3C/defs%3E%3Cg%3E%3Cpath class=&apos;cls-1&apos; d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Z&apos;/%3E%3Cpath d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Zm4.72,88.9H185.2L172.42,89c-32.78.62-43.68,3.24-54.71,9.14a45.84,45.84,0,0,0-19.54,19.54c-6.61,12.36-9.11,24.55-9.27,67.49V614.8L89,627.58c.62,32.78,3.24,43.68,9.14,54.71a45.84,45.84,0,0,0,19.54,19.54c12.36,6.61,24.55,9.11,67.49,9.27H610.08c46.79,0,59.41-2.44,72.21-9.28a45.84,45.84,0,0,0,19.54-19.54c6.61-12.36,9.11-24.55,9.27-67.49V189.92c0-46.79-2.44-59.41-9.28-72.21a45.84,45.84,0,0,0-19.54-19.54C669.93,91.56,657.74,89.06,614.8,88.9ZM233.33,493.33A73.34,73.34,0,1,1,160,566.67,73.35,73.35,0,0,1,233.33,493.33Z&apos;/%3E%3C/g%3E%3C/svg%3E"></link></head><body><header><div class="image"></div><div class="title"><div class="banner"><div class="icon"></div></div><div data-title="分布式训练题目" class="main-title"></div></div></header><article><h3 id="2jxMTmbiEQtHJ3ZNP6NMqH" class="wolai-block"><span class="inline-wrap">1. 理论篇</span></h3><h4 id="nyWsZgxwB9nxSrYEYG1QoV" class="wolai-block"><span class="inline-wrap">1.1 训练 大语言模型 存在问题？</span></h4><ol class="wolai-block"><li id="qRmBSWVqzbzmw2qNFPk63x"><div class="marker"></div><span class="yellow inline-wrap"><b>计算资源需求</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 训练大型语言模型需要大量的计算资源，包括高端 GPU、大量的内存和高速存储器。这可能限制了许多研究人员和组织的训练能力，因为这些资源通常很昂贵。</span></li><li id="FaznbUmnicWVwAFm7NvxL"><div class="marker"></div><span class="yellow inline-wrap"><b>数据需求</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 训练大型语言模型需要大规模的数据集，这些数据集通常需要大量的标注和清洗工作。获取高质量的数据可能是一项困难和昂贵的任务。</span></li><li id="bYge1w4U2hEbtPxKnHawHy"><div class="marker"></div><span class="yellow inline-wrap"><b>长时间训练</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 训练大型语言模型需要大量的时间。特别是对于巨型模型，训练可能需要数周甚至数月的时间，这增加了实验的时间和成本。</span></li><li id="pt3FDEjrc3mwTiBaSyiMWZ"><div class="marker"></div><span class="yellow inline-wrap"><b>环境影响</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 大规模模型的训练需要大量的能源和计算资源，可能对环境造成影响。这引发了对训练模型的可持续性和能源效率的关注。</span></li><li id="iH3RRfn73Cf5w8JV7ySwiY"><div class="marker"></div><span class="yellow inline-wrap"><b>过拟合和泛化</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 训练大型模型可能导致过拟合问题，特别是当训练数据集不能充分覆盖所有可能的语言情况和使用场景时。此外，对于大型模型，泛化能力可能会受到一定程度的影响。</span></li><li id="6b6bGn4ufbyRtf9gifbaVG"><div class="marker"></div><span class="yellow inline-wrap"><b>认知偏差和歧视性</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 如果训练数据集存在偏差或歧视性，大型语言模型可能会继承这些问题，并在生成文本时表现出类似的偏见。</span></li></ol><h4 id="u2XWVo5oGqBzn3ZzqJnYKk" class="wolai-block"><span class="inline-wrap">1.2 什么是 点对点通信？</span></h4><div id="vNKr12TbooPv8VYn6XtfX7" class="wolai-block wolai-text"><div><span class="inline-wrap">点对点通信（Peer-to-Peer Communication）是一种网络通信模式，其中</span><span class="red inline-wrap"><b>两个或多个计算机或设备之间直接进行通信，而不需要通过中央服务器或集中式系统</b></span><span class="inline-wrap">。在点对点通信中，每个参与者都可以充当客户端和服务器，能够直接与其他节点通信、交换信息或共享资源。</span></div></div><div id="kxZRBiUCPnB1Y6ZpdEo9N2" class="wolai-block wolai-text"><div><span class="inline-wrap">这种通信模式与传统的客户端-服务器模型不同，后者在网络中有一个中心服务器负责处理和转发所有请求和数据。而点对点通信模式中，参与者之间能够直接建立连接，相互传输信息或资源，使得网络更为分散和去中心化。</span></div></div><h4 id="gNoJreub76M4QUAGvDviw" class="wolai-block"><span class="inline-wrap">1.3 什么是 集体通信？</span></h4><div id="fPjP9qtiVip6gsBrgR16pK" class="wolai-block wolai-text"><div><span class="inline-wrap">集体通信（Collective Communication）是指</span><span class="red inline-wrap"><b>一组计算节点或处理单元之间进行协作、交换信息或执行通信操作的过程</b></span><span class="inline-wrap">。这种通信形式</span><span class="yellow inline-wrap"><b>涉及到多个节点之间的集体参与，而不仅仅是点对点的通信</b></span><span class="inline-wrap">。它可以用于并行计算、分布式系统和群集计算等领域，以便在多个节点之间协调和管理数据的传输、处理和同步操作。</span></div></div><div id="rK1TqTT1WWoAmpEbpoobiu" class="wolai-block wolai-text"><div><span class="inline-wrap">集体通信常见的操作包括广播、散射、汇总、规约和全局同步等。</span></div></div><h4 id="o31hwvYZfq9Hro2zHCmBmn" class="wolai-block"><span class="inline-wrap">1.4 什么是 数据并行？</span></h4><div id="6JAxhsPGx3xEYmy6GFgGBh" class="wolai-block wolai-text"><div><span class="inline-wrap">所谓数据并行，就是由于训练数据集太大；因此，</span><span class="red inline-wrap"><b>将数据集分为<span class="jill"></span>N<span class="jill"></span>份，每一份分别装载到<span class="jill"></span>N<span class="jill"></span>个<span class="jill"></span>GPU<span class="jill"></span>节点中，同时，每个<span class="jill"></span>GPU<span class="jill"></span>节点持有一个完整的模型副本</b></span><span class="inline-wrap">，分别基于每个<span class="jill"></span>GPU<span class="jill"></span>中的数据去进行梯度求导。然后，在<span class="jill"></span>GPU0<span class="jill"></span>上对每个<span class="jill"></span>GPU<span class="jill"></span>中的梯度进行累加，最后，再将<span class="jill"></span>GPU0<span class="jill"></span>聚合后的结果广播到其他<span class="jill"></span>GPU<span class="jill"></span>节点。</span></div></div><h4 id="nXkZPFtizzjiUKAwjXnxVQ" class="wolai-block"><span class="inline-wrap">1.5 数据并行 如何 提升效率？</span></h4><div id="2aU5qSPHfbo61oYMXyZfQc" class="wolai-block wolai-text"><div><span class="inline-wrap">数据并行是在多个处理单元上同时处理数据的策略，它可以通过一些方法来提高效率：</span></div></div><ol class="wolai-block"><li id="dNqVNvygWJsvV4W9EcPBSm"><div class="marker"></div><span class="yellow inline-wrap"><b>增加处理单元数量</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 增加处理单元（如 GPU 或 CPU）的数量可以加速数据并行计算，因为更多的处理单元可以同时处理更多的数据子集。</span></li><li id="crgHBUuMoGixNviqpAh3yf"><div class="marker"></div><span class="yellow inline-wrap"><b>批处理训练</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 使用批处理训练可以提高数据并行的效率。通过合并多个数据子集形成批次，可以减少通信和同步开销，同时更好地利用处理单元的并行计算能力。</span></li><li id="nJxoHjwTcjeedSD53eMc2n"><div class="marker"></div><span class="yellow inline-wrap"><b>异步更新</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 对于参数更新，可以采用异步更新的策略。不同的处理单元可以在计算完成后立即更新自己的参数，而不必等待其他处理单元完成计算。虽然这可能会导致一定程度的参数不一致，但可以提高整体的训练速度。</span></li><li id="iUfzPmxVzVpZwsN3rEGyKz"><div class="marker"></div><span class="yellow inline-wrap"><b>模型和数据并行结合</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 在一些情况下，可以结合使用模型并行和数据并行来提高效率。将模型分布到多个处理单元上，同时每个处理单元处理不同的数据子集，可以有效地利用多个处理单元的计算能力。</span></li><li id="vQCqrXKPM4ax93zEkmhUYG"><div class="marker"></div><span class="yellow inline-wrap"><b>减少通信开销</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 优化通信机制可以降低处理单元之间的通信开销。采用高效的通信协议或减少同步频率等方法可以提高数据并行的效率。</span></li><li id="hTPy91yJkJX22yjU2zNXnc"><div class="marker"></div><span class="yellow inline-wrap"><b>负载均衡</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 确保数据在不同处理单元间的分配是均衡的，避免某些处理单元负载过重或过轻，以充分利用所有的计算资源。</span></li></ol><h4 id="pY5uHA9kRiScY4XuUuu6NB" class="wolai-block"><span class="inline-wrap">1.6 什么是 流水线并行？</span></h4><div id="wNL1RLEf4r2aFFqWg2z1xV" class="wolai-block wolai-text"><div><span class="inline-wrap">所谓流水线并行，就是由于模型太大，无法将整个模型放置到单张<span class="jill"></span>GPU<span class="jill"></span>卡中；因此，将</span><span class="red inline-wrap"><b>模型的不同层放置到不同的计算设备</b></span><span class="inline-wrap">，降低单个计算设备的显存消耗，从而实现超大规模模型训练。
如下图所示，模型共包含四个模型层（如：Transformer<span class="jill"></span>层），被切分为三个部分，分别放置到三个不同的计算设备。即第 1 层放置到设备 0，第 2 层和第三 3 层放置到设备 1，第 4 层放置到设备 2。</span></div></div><h4 id="hLk3fNv1t5TLixGM1D6V3F" class="wolai-block"><span class="inline-wrap">1.7 什么是 张量并行 (intra-layer)？</span></h4><div id="x6nMgHVLZNrWL5iEiiasX2" class="wolai-block wolai-text"><div><span class="inline-wrap">和流水线并行类似，</span><span class="red inline-wrap">张量并行也是将模型分解放置到不同的<span class="jill"></span>GPU<span class="jill"></span>上，以解决单块<span class="jill"></span>GPU<span class="jill"></span>无法储存整个模型的问题</span><span class="inline-wrap">。和流水线并行不同的地方在于，</span><span class="yellow inline-wrap"><b>张量并行是针对模型中的张量进行拆分，将其放置到不同的<span class="jill"></span>GPU<span class="jill"></span>上</b></span><span class="inline-wrap">。</span></div></div><div id="iwRk79sX4VsrYwyDDSfuk" class="wolai-block wolai-text"><div><span class="inline-wrap">模型并行是不同设备负责单个计算图不同部分的计算。而将计算图中的层内的参数（张量）切分到不同设备（即层内并行），每个设备只拥有模型的一部分，以减少内存负荷，我们称之为张量模型并行。</span></div></div><h4 id="rJKDatPkF3kwDRmUS4AMSK" class="wolai-block"><span class="inline-wrap">1.8 数据并行 vs 张量并行 vs 流水线并行?</span></h4><div id="2zrKsFuXQ6KtZbR3raAz63" class="wolai-block wolai-text"><div><span class="inline-wrap">数据并行、张量并行和流水线并行是在并行计算中常见的三种策略，它们有不同的应用场景和优势：</span></div></div><div id="imRMFmcx9gyWrm1mvVG4LW" class="wolai-block wolai-text"><div><span class="inline-wrap">1、</span><span class="yellow inline-wrap"><b>数据并行（Data Parallelism）</b></span><span class="inline-wrap"><b>：</b></span></div></div><ul class="wolai-block"><li id="mmRcEgtcC7tkdFkzUvSN9n"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>概念：</b></span><span class="inline-wrap"> </span><span class="red inline-wrap">数据并行是指将整个模型复制到每个处理单元上，不同处理单元处理不同的数据子集</span><span class="inline-wrap">。每个处理单元独立计算，并通过同步更新模型参数来实现训练。</span></li><li id="n59YitsLEwdAt7oW3d9Fw8"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>适用场景：</b></span><span class="inline-wrap"> 数据并行适用于大型模型和数据集，特别是在深度学习中。每个处理单元负责计算不同数据子集上的梯度，然后同步更新模型参数。</span></li><li id="mV7yhJYm4k8d6wNek3RG1y"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>优势：</b></span><span class="inline-wrap"> 易于实现，适用于大规模数据和模型。</span></li></ul><div id="pXLU21F1mUeorsQuWVRyPS" class="wolai-block wolai-text"><div><span class="inline-wrap">2、</span><span class="yellow inline-wrap"><b>张量并行（Tensor Parallelism）：</b></span></div></div><ul class="wolai-block"><li id="tgmnC2J5CbUM9e2PapKncf"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>概念：</b></span><span class="inline-wrap"> </span><span class="red inline-wrap">张量并行是指将模型分解成多个部分，每个部分在不同处理单元上进行计算</span><span class="inline-wrap">。通常，这涉及到在层与层之间划分模型，并在不同的 GPU 或处理单元上执行这些部分。</span></li><li id="qPWPw4cbkhsWGzCaPPXhnN"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>适用场景：</b></span><span class="inline-wrap"> 张量并行适用于非常大的模型，其中单个 GPU 的内存容量无法容纳整个模型。它允许将模型的不同部分分配到不同的处理单元上，从而扩展模型的规模。</span></li><li id="qrou5QHt4K27BEz3GEnay5"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>优势：</b></span><span class="inline-wrap"> 适用于大型模型的规模扩展，可用于解决内存限制问题。</span></li></ul><div id="f2RCjJSNSZmvupYS1MgU7A" class="wolai-block wolai-text"><div><span class="inline-wrap">3、</span><span class="yellow inline-wrap"><b>流水线并行（Pipeline Parallelism）：</b></span></div></div><ul class="wolai-block"><li id="wWxdxYz3i8LD8b5Nwxg1Am"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>概念：</b></span><span class="red inline-wrap"> 流水线并行是指将模型的不同层分配到不同的处理单元上，并通过将不同层的输出传递给下一层来实现计算。每个处理单元负责一个模型层的计算</span><span class="inline-wrap">。</span></li><li id="soY3Y97FANvgQeRcFcDLhT"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>适用场景：</b></span><span class="inline-wrap"> 流水线并行适用于深层次的模型，其中各层之间的计算相对独立。它可以提高模型的整体计算速度，特别是在层之间存在较大的计算延迟时。</span></li><li id="biGdMsavYLnmTWeFQMNcL2"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>优势：</b></span><span class="inline-wrap"> 适用于深层次模型，减少整体计算时间。</span></li></ul><div id="dTzebLaXSpuVPic9wxA9GT" class="wolai-block wolai-text"><div><span class="inline-wrap">这三种并行策略通常可以结合使用，具体取决于应用的场景和问题的性质。在深度学习等领域，常常会使用数据并行和张量并行相结合的方式，以提高模型的训练速度和规模。</span></div></div><h4 id="9R1HwBvEDf3vozmyTekV9U" class="wolai-block"><span class="inline-wrap">1.9 什么是 3D<span class="jill"></span>并行？</span></h4><div id="a6usENfXD6M3CEG4VaZBxW" class="wolai-block wolai-text"><div><span class="inline-wrap">3D<span class="jill"></span>并行，或者混合并行 (Hybrid Parallelism)，则是将以上三种策略结合起来使用，达到同时提升存储和计算效率的目的。Megatron-Turing NLG 就是先将 Transformer block 使用流水线和张量 2D 并行，然后再加上数据并行，将训练扩展到更多的<span class="jill"></span>GPU。</span></div></div><h4 id="nGFQ9MXok8ri2mZYHQReBv" class="wolai-block"><span class="inline-wrap">1.10 想要训练<span class="jill"></span>1<span class="jill"></span>个<span class="jill"></span>LLM，如果只想用<span class="jill"></span>1<span class="jill"></span>张显卡，那么对显卡的要求是什么？</span></h4><div id="d76wVen26K4uMLUDU3mdPQ" class="wolai-block wolai-text"><div><span class="inline-wrap">显卡显存足够大，nB<span class="jill"></span>模型微调一般最好准备<span class="jill"></span>20nGB<span class="jill"></span>以上的显存。</span></div></div><ol class="wolai-block"><li id="t4i4g1HPVZZ2AHEAcfdNk"><div class="marker"></div><span class="yellow inline-wrap"><b>显存大小</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 大型语言模型需要大量的显存来存储模型参数和中间计算结果。通常，至少需要 16GB 或更多的显存来容纳这样的模型。对于较小的模型，8GB 的显存也可能足够。</span></li><li id="8VNmapzH2UhFfYg8ThUzfm"><div class="marker"></div><span class="yellow inline-wrap"><b>计算能力</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 针对大型神经网络模型，较高的计算能力通常可以加快训练速度。通常情况下，NVIDIA 的 RTX 系列或者 A<span class="jill"></span>系列的显卡具有较高的性能和计算能力，例如 RTX 2080 Ti、RTX 3080、RTX 3090 等。这些显卡提供了更多的 CUDA 核心和更高的计算能力，能够更快地处理大型模型。</span></li><li id="qyEyETx8Th1CiYprgt9vTx"><div class="marker"></div><span class="yellow inline-wrap"><b>带宽和速度</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 显卡的显存带宽和速度也是一个考虑因素。较高的内存带宽可以更快地从内存读取数据，对于大型模型的训练非常重要。</span></li><li id="9qDxHMkeq9vmAFR7Xt3uR7"><div class="marker"></div><span class="yellow inline-wrap"><b>兼容性和优化</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 良好的软硬件兼容性以及针对深度学习训练任务的优化也是考虑的因素。确保显卡与所选深度学习框架兼容，并且可以利用框架提供的优化功能。</span></li></ol><h4 id="aWeW6NdJ3vBScVZbZMQNAC" class="wolai-block"><span class="inline-wrap">1.11 如果有<span class="jill"></span>N<span class="jill"></span>张显存足够大的显卡，怎么加速训练？</span></h4><ol class="wolai-block"><li id="cy3qNf7cXWEGxHXL6i3KSH"><div class="marker"></div><span class="red inline-wrap"><b>数据并行化</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 在数据并行化中，模型的多个副本在不同的 GPU 上训练相同的数据批次。每个 GPU 计算梯度，并将结果汇总到主 GPU 或进行参数更新。这种方法适用于模型过大而无法完全容纳在单个 GPU 内存中的情况。</span></li><li id="opKhSiJGuZaMSchudEDJgk"><div class="marker"></div><span class="yellow inline-wrap"><b>模型并行化</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 在模型并行化中，模型的不同部分分配到不同的 GPU 上。每个 GPU 负责计算其分配的部分，并将结果传递给其他 GPU。这对于大型模型，特别是具有分层结构的模型（如大型神经网络）是有益的。</span></li><li id="nQvXBMPNZgq8dQa66ToxhG"><div class="marker"></div><span class="yellow inline-wrap"><b>分布式训练</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 使用分布式框架（例如 TensorFlow 的 </span><span class="inline-wrap"><code>tf.distribute</code></span><span class="inline-wrap"> 或 PyTorch 的 </span><span class="inline-wrap"><code>torch.nn.parallel.DistributedDataParallel</code></span><span class="inline-wrap">）来实现训练任务的分布式执行。这允许将训练任务分配到多个 GPU 或多台机器上进行加速。</span></li><li id="5RLFruD2uRp2MuhjBeeqjf"><div class="marker"></div><span class="yellow inline-wrap"><b>优化批处理大小</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 增大批处理大小可以提高 GPU 利用率，但需要注意的是，批处理大小的增加也可能导致内存不足或梯度下降不稳定。因此，需要根据模型和硬件配置进行合理的调整。</span></li><li id="mNXHK6sCXUaWmYhWV7idpz"><div class="marker"></div><span class="yellow inline-wrap"><b>混合精度训练</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 使用半精度浮点数（例如 TensorFlow 的 </span><span class="inline-wrap"><code>tf.keras.mixed_precision</code></span><span class="inline-wrap"> 或 PyTorch 的 AMP）来减少内存占用，加速训练过程。</span></li><li id="5SE6Ja6xbDtYpgiyN2aMx4"><div class="marker"></div><span class="yellow inline-wrap"><b>模型剪枝和优化</b></span><span class="inline-wrap"><b>：</b></span><span class="inline-wrap"> 对模型进行剪枝和优化以减少模型的大小和计算负荷，有助于提高训练速度和效率。</span></li></ol><h4 id="aTrAFvPPG7pfNPRaXWpXsh" class="wolai-block"><span class="inline-wrap">1.12 如果显卡的显存不够装下一个完整的模型呢？</span></h4><div id="axEbkvEBt8rhQgXp7yuL7Q" class="wolai-block wolai-text"><div><span class="inline-wrap">最直观想法，需要分层加载，把不同的层加载到不同的<span class="jill"></span>GPU<span class="jill"></span>上（accelerate<span class="jill"></span>的<span class="jill"></span>device_map）也就是常见的<span class="jill"></span>PP，流水线并行。</span></div></div><h4 id="vUQXgJq7TC28Ay922hntpv" class="wolai-block"><span class="inline-wrap">1.13 PP<span class="jill"></span>推理时，是一个串行的过程，1<span class="jill"></span>个<span class="jill"></span>GPU<span class="jill"></span>计算，其他空闲，有没有其他方式？</span></h4><div id="f5iYT5KuWQeoeYCgoFDvXk" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>微批次流水线并行</b></span><span class="inline-wrap">：</span></div></div><div id="Ve1RK4Diy2pPC2HDRFg7m" class="wolai-block wolai-text"><div><span class="red inline-wrap">微批次（MicroBatch）流水线并行与朴素流水线几乎相同，但它通过将传入的小批次（minibatch）分块为微批次（microbatch）</span><span class="inline-wrap">，并人为创建流水线来解决 GPU 空闲问题，从而允许不同的 GPU 同时参与计算过程，可以显著提升流水线并行设备利用率，减小设备空闲状态的时间。目前业界常见的流水线并行方法 GPipe 和 PipeDream 都采用微批次流水线并行方案。</span></div></div><div id="cCJ9hR2aozAX7wEtCKj9WL" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image.png" style="width: 100%"/></figure></div><div id="x68mH2EV265ih5pXinhqDE" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>GPipe</b></span><span class="inline-wrap">（Easy Scaling with Micro-Batch Pipeline Parallelism），由谷歌提出的一种流水线并行方案。</span></div></div><div id="9ntjPwAb5jedsTQD2GKJAi" class="wolai-block wolai-text"><div><span class="inline-wrap">Gpipe 流水线并行主要用来解决这两个问题：</span></div></div><div id="bor8EUA4fVMqXUmM4M7XiU" class="wolai-block wolai-text"><div><span class="inline-wrap">第一，</span><span class="red inline-wrap"><b>提高模型训练的并行度</b></span><span class="inline-wrap">。Gpipe 在朴素流水线并行的基础上，</span><span class="green inline-wrap"><b>利用数据并行的思想，将 mini-batch 细分为多个更小的 micro-batch，送入<span class="jill"></span>GPU<span class="jill"></span>进行训练</b></span><span class="inline-wrap">，来提高并行程度。</span></div></div><div id="gxRKoBfsj7NjwpckduHTsw" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_1.png" style="width: 100%"/></figure></div><div id="pB1cHH8LmzCYqTWEdagj6i" class="wolai-block wolai-text"><div><span class="inline-wrap">上图即为朴素流水线并行与 GPipe 微批次流水线并行对比，通过 GPipe 可以有效降低流水线并行<span class="jill"></span>bubble 空间的比例。其中，F<span class="jill"></span>的第一个下标表示 GPU 编号，F<span class="jill"></span>的第二个下标表示 micro-batch 编号。假设我们将 mini-batch 划分为 M 个，则 GPipe 流水线并行下， GPipe 流水线 Bubble 时间为： </span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>K</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>K</mi><mo>+</mo><mi>M</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(\frac{K−1}{K+M-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.225em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.05em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span><span class="inline-wrap">。其中，K<span class="jill"></span>为设备，M<span class="jill"></span>为将<span class="jill"></span>mini-batch<span class="jill"></span>切成多少个<span class="jill"></span>micro-batch。当<span class="jill"></span>M&gt;&gt;K<span class="jill"></span>的时候，这个时间可以忽略不计。</span></div></div><div id="3wCk3cW7DC9ZqJehtTPfx7" class="wolai-block wolai-text"><div><span class="inline-wrap">第二，</span><span class="red inline-wrap"><b>通过重计算（Re-materialization）降低显存消耗</b></span><span class="inline-wrap">。在模型训练过程中的前向传播时，会记录每一个算子的计算结果，用于反向传播时的梯度计算。</span></div></div><h4 id="qBR18JQUs1ytedKbCqc1nS" class="wolai-block"><span class="inline-wrap">1.14 3<span class="jill"></span>种并行方式可以叠加吗？</span></h4><div id="aeNp5zceCo3bkKkW9eA5WL" class="wolai-block wolai-text"><div><span class="inline-wrap">是可以的，DP+TP+PP，这就是<span class="jill"></span>3D<span class="jill"></span>并行。如果真有<span class="jill"></span>1<span class="jill"></span>个超大模型需要预训练，3D<span class="jill"></span>并行那是必不可少的。毕竟显卡进化的比较慢，最大显存的也就是<span class="jill"></span>A100 80g。</span></div></div><h4 id="uETdis13CNwQRSTpMgsQYX" class="wolai-block"><span class="inline-wrap">1.15 Colossal-AI 有<span class="jill"></span>1D/2D/2.5D/3D，是什么情况？</span></h4><div id="uJGncRZzYCyWCwnMcVHfjR" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>1<span class="jill"></span>维（1D）张量并行（Megatron-LM）</b></span></div></div><div id="edQ8VsLMqhqs888ZJX1LC7" class="wolai-block wolai-text"><div><span class="inline-wrap">张量并行则涉及到不同的分片 (sharding)方法，现在最常用的都是 1D 分片，即</span><span class="red inline-wrap"><b>将张量按照某一个维度进行划分（横着切或者竖着切）</b></span><span class="inline-wrap">。</span></div></div><div id="iwAbkybm9TuW5wJzTVmsSt" class="wolai-block wolai-text"><div><span class="inline-wrap">目前，在基于<span class="jill"></span>Transformer<span class="jill"></span>架构为基础的大模型中，最常见的张量并行方案由</span><span class="inline-wrap"><a href="https://link.juejin.cn?target=https://deepakn94.github.io/assets/papers/megatron-sc21.pdf"><span>Megatron-LM</span></a></span><span class="inline-wrap">提出，它是一种高效的一维（1D）张量并行实现。它</span><span class="red inline-wrap"><b>采用的则是非常直接的张量并行方式，对权重进行划分后放至不同<span class="jill"></span>GPU<span class="jill"></span>上进行计算</b></span><span class="inline-wrap">。</span></div></div><div id="qewvYBGGtVwtFy9FWgUEww" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>2D<span class="jill"></span>张量并行</b></span></div></div><div id="bZHHYkqToN3JVXK66q3f3D" class="wolai-block wolai-text"><div><span class="inline-wrap">Megatron<span class="jill"></span>中的 1D 张量并行方案并没有对激活（activations）进行划分，对于大模型而言，这也会消耗大量的内存。</span></div></div><div id="gVWCfVFuSxDUg15NwPgEn9" class="wolai-block wolai-text"><div><span class="inline-wrap">为了平均分配计算和内存负荷，在 SUMMA 算法（一种可扩展的通用矩阵乘法算法，并行实现矩阵乘法）的基础上， </span><span class="inline-wrap"><a href="https://link.juejin.cn/?target=https://arxiv.org/pdf/2104.05343.pdf"><span>2D 张量并行</span></a></span><span class="inline-wrap"> <span class="jill"></span>被引入。它</span><span class="red inline-wrap"><b>把 input 和 weight 都沿着两个维度均匀切分</b></span><span class="inline-wrap">。</span></div></div><div id="mYdcpTvgHS6RzTSP6mYuiV" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_2.png" style="width: 100%"/></figure></div><div id="oo6ecjahVETtbnBDgfwpXh" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>2.5D<span class="jill"></span>张量并行</b></span></div></div><div id="485p9kwEmUHJ2XubpDjqMx" class="wolai-block wolai-text"><div><span class="inline-wrap">与一维张量并行相比，二维并行降低了内存成本，但可能引入更多的通信。因此，</span><span class="inline-wrap"><a href="https://link.juejin.cn/?target=https://arxiv.org/pdf/2105.14500.pdf"><span>2.5D<span class="jill"></span>张量并行</span></a></span><span class="inline-wrap"> 在 2D SUMMA 的基础上被提出，它通过使用更多的设备(</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>=</mo><mi>q</mi><mo>×</mo><mi>q</mi><mo>×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">P=q×q×d </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></span><span class="inline-wrap">个处理器)来减少通信。</span></div></div><div id="6urZXhGe2kytMwgcdaqsx9" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_3.png" style="width: 100%"/></figure></div><div id="aNd8qymyaq6LvFu85rYzjb" class="wolai-block wolai-text"><div><span class="inline-wrap">Colossal-AI 的 3D 张量并行是一种将神经网络模型的计算并行化，以期望获得最佳通信成本优化的方法。与现有的 1D 和 2D 张量并行相比，具有更少的内存和网络通信开销。</span></div></div><h4 id="bvsk5PqfaXS1o1XPTan8J6" class="wolai-block"><span class="inline-wrap">1.16 除了<span class="jill"></span>3D<span class="jill"></span>并行有没有其他方式大规模训练？</span></h4><div id="qVbjPErUB1Z3C1kXcXqQmj" class="wolai-block wolai-text"><div><span class="inline-wrap">可以使用更优化的数据并行算法<span class="jill"></span>FSDP（类似<span class="jill"></span>ZeRO3）或者直接使用<span class="jill"></span>DeepSpeed ZeRO</span></div></div><h4 id="uhikBt7ZHtjcnwDk4yvYjV" class="wolai-block"><span class="inline-wrap">1.17 有了<span class="jill"></span>ZeRO<span class="jill"></span>系列，为什么还需要<span class="jill"></span>3D<span class="jill"></span>并行？</span></h4><div id="jc1WaSL5thQbDhRxL4vz3E" class="wolai-block wolai-text"><div><span class="inline-wrap">根据<span class="jill"></span>ZeRO<span class="jill"></span>论文，尽管张量并行的显存更省一点，张量并行的通信量实在太高，只能限于节点内（有<span class="jill"></span>NVLINK）。如果节点间张量并行，显卡的利用率会低到<span class="jill"></span>5%
</span></div></div><div id="4vMDC8DwDhULhuEpFJkffn" class="wolai-block wolai-text"><div><span class="inline-wrap">但是，根据<span class="jill"></span>Megatron-LM2<span class="jill"></span>的论文，</span><span class="red inline-wrap"><b>当显卡数量增加到千量级，ZeRO3<span class="jill"></span>是明显不如<span class="jill"></span>3D<span class="jill"></span>并行的</b></span><span class="inline-wrap">。</span></div></div><h4 id="q6vKxWMtjrPC8gL79WJTsL" class="wolai-block"><span class="inline-wrap">1.18 平民适不适合玩<span class="jill"></span>3D<span class="jill"></span>并行？</span></h4><div id="tN6KGh8jpWEEgkqqDv4bGF" class="wolai-block wolai-text"><div><span class="inline-wrap">不适合。</span></div></div><div id="7PZDNGFa8htCvtYLhUMVSV" class="wolai-block wolai-text"><div><span class="inline-wrap">3D<span class="jill"></span>并行的基础是，节点内显卡间<span class="jill"></span>NVLINK<span class="jill"></span>超高速连接才能上<span class="jill"></span>TP。有没有<span class="jill"></span>NVLINK<span class="jill"></span>都是个问题。</span></div></div><div id="sHobaGkP6XN7Au4gVD3qRX" class="wolai-block wolai-text"><div><span class="inline-wrap">而且，节点间特殊的网络通常有<span class="jill"></span>400Gb/s？远超普通<span class="jill"></span>IDC<span class="jill"></span>内的万兆网络<span class="jill"></span>10Gb/s。</span></div></div><h4 id="du7SbCJHxpeu9Sr32hoKZX" class="wolai-block"><span class="inline-wrap">1.19 平民适不适合直接上多机多卡的<span class="jill"></span>ZeRO3（万兆网）？</span></h4><div id="sdTgeU1zM4cJ98acR9Rjh1" class="wolai-block wolai-text"><div><span class="inline-wrap">不适合。</span></div></div><div id="kbyMKo7WJxMcKsWwxmjqXT" class="wolai-block wolai-text"><div><span class="inline-wrap">想象一下，当<span class="jill"></span>65B<span class="jill"></span>模型用<span class="jill"></span>Zero3，每一个<span class="jill"></span>step<span class="jill"></span>的每一张卡上需要的通信量是<span class="jill"></span>195GB（3<span class="jill"></span>倍参数量），也就是<span class="jill"></span>1560Gb。万兆网下每步也要<span class="jill"></span>156s<span class="jill"></span>的通信时间，这画面太美。</span></div></div><h4 id="5KN98ZqmGMsM2c4MHWUgok" class="wolai-block"><span class="inline-wrap">1.20 分布式并行及显存优化技术并行技术有哪一些，都有什么特点？</span></h4><div id="fovU2XDoy8jEpCd1iPUB8m" class="wolai-block wolai-text"><div><span class="inline-wrap">分布式并行和显存优化技术是在深度学习和大规模计算中常用的并行技术。它们有不同的特点和用途：</span></div></div><div id="xcupogCURWHq4QKKhVKvee" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>分布式并行技术：</b></span></div></div><ol class="wolai-block"><li id="6kZxYBpKx3DvA3jWLereqj"><div class="marker"></div><span class="yellow inline-wrap"><b>数据并行（Data Parallelism）</b></span><span class="inline-wrap"><b>：</b></span><ul class="wolai-block"><li id="5uMJiooEa5vQxnHJeV5uSs"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>特点：</b></span><span class="inline-wrap"> 将数据分成多个子集，分配给不同的处理单元，每个处理单元计算不同的数据子集。处理单元之间共享模型参数，然后同步参数更新。</span></li><li id="5VUcaAW5FysGbok7u1MUwh"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>优点：</b></span><span class="inline-wrap"> 可以处理大规模数据和模型，易于实现，能够加速训练过程。</span></li></ul></li><li id="jLUp35888fmr7er7tBhzHc"><div class="marker"></div><span class="yellow inline-wrap"><b>模型并行（Model Parallelism）</b></span><span class="inline-wrap"><b>：</b></span><ul class="wolai-block"><li id="ueNmzQ77zFCVSWqVetYTnR"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>特点：</b></span><span class="inline-wrap"> 将模型划分成多个部分，在不同的设备上并行计算这些部分。通常用于大型模型，每个设备负责处理整个模型的不同部分。</span></li><li id="4hkp78AFNRrhrVDeLVWQYp"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>优点：</b></span><span class="inline-wrap"> 可以应对模型过大，无法放入单个设备内存的情况。</span></li></ul></li><li id="rq5f6Qpk1RTiqJEjNHBjxF"><div class="marker"></div><span class="yellow inline-wrap"><b>流水线并行（Pipeline Parallelism）</b></span><span class="inline-wrap"><b>：</b></span><ul class="wolai-block"><li id="7kGEZVvDaCTY3ivRUFT4of"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>特点：</b></span><span class="inline-wrap"> 将计算过程划分为多个阶段，不同设备同时执行不同阶段的计算。每个设备负责处理流程中的不同阶段，类似于流水线。</span></li><li id="ht1AyJqA1XndQHbJHWPdMS"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>优点：</b></span><span class="inline-wrap"> 可以在一定程度上减少设备空闲时间，提高并行效率。</span></li></ul></li></ol><div id="6VCaKp5ZyEEBapGDM8c8NQ" class="wolai-block wolai-text"><div><span class="inline-wrap"><b>显存优化技术：</b></span></div></div><ol class="wolai-block"><li id="ipAS6KtJpAvUtuXCbp15Yj"><div class="marker"></div><span class="yellow inline-wrap"><b>模型裁剪（Model Pruning）</b></span><span class="inline-wrap"><b>：</b></span><ul class="wolai-block"><li id="h3w5UVa1FPWDuMTYP9B3pu"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>特点：</b></span><span class="inline-wrap"> 去除模型中不必要的参数或结构，减小模型大小和内存占用。</span></li><li id="smKD2HnkECpFDVVRCtHxdJ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>优点：</b></span><span class="inline-wrap"> 可以降低模型的存储需求，适用于显存不足的情况。</span></li></ul></li><li id="6sYm5nE1FdvBbs7AK7YvEm"><div class="marker"></div><span class="yellow inline-wrap"><b>模型压缩（Model Compression）</b></span><span class="inline-wrap"><b>：</b></span><ul class="wolai-block"><li id="tfRNrPohHxAowV9LGqtnSd"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>特点：</b></span><span class="inline-wrap"> 使用量化、剪枝等方法减小模型大小，减少显存占用。</span></li><li id="xnygU2zRTFUsL9C3nH9Eo9"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>优点：</b></span><span class="inline-wrap"> 降低模型存储空间，适用于显存限制的场景。</span></li></ul></li><li id="fsLuETjMzeNjbF3ExWroTr"><div class="marker"></div><span class="yellow inline-wrap"><b>混合精度计算（Mixed Precision Computing）</b></span><span class="inline-wrap"><b>：</b></span><ul class="wolai-block"><li id="mZAG6Ka5FPMQycB6EpvWf4"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>特点：</b></span><span class="inline-wrap"> 使用较低精度（如半精度浮点数）进行计算，减少显存使用。</span></li><li id="w9cNygRnq6NvLXipE4dTeU"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>优点：</b></span><span class="inline-wrap"> 可以在一定程度上减少显存需求，提高计算效率。</span></li></ul></li></ol><div id="eFk9Qcpj51k1dDnSLrA9JH" class="wolai-block wolai-text"><div><span class="inline-wrap">这些并行技术和显存优化技术都有各自的特点和适用场景，可以根据实际需求和硬件资源进行选择和组合使用，以提高训练效率和解决显存限制的问题。</span></div></div><h4 id="gmfh41rBojxsabkhGWcwHv" class="wolai-block"><span class="inline-wrap">1.21 常见的分布式训练框架哪一些，都有什么特点？</span></h4><div id="c58YkF8hRBYVDvTGkDapfZ" class="wolai-block wolai-text"><div><span class="yellow inline-wrap"><b>1、Megatron-LM</b></span></div></div><div id="ugSTnWZ4Ke2B7uN1mTrEbs" class="wolai-block wolai-text"><div><span class="inline-wrap">Megatron 是由 NVIDIA 深度学习应用研究团队开发的大型 Transformer 语言模型，该模型用于研究大规模训练大型语言模型。</span></div></div><div id="2HuZ2W3VJdAV1V1YV1BZ64" class="wolai-block wolai-text"><div><span class="inline-wrap">Megatron 支持<span class="jill"></span>transformer<span class="jill"></span>模型的模型并行（张量、序列和管道）和多节点预训练，同时还支持 BERT、GPT 和 T5 等模型。</span></div></div><div id="9ByksUTbJvFXveyJUCkEBU" class="wolai-block wolai-text"><div><span class="yellow inline-wrap"><b>2、DeepSpeed</b></span></div></div><div id="v8r6Vashw3GX2gWaNnaAQL" class="wolai-block wolai-text"><div><span class="inline-wrap">DeepSpeed<span class="jill"></span>是微软的深度学习库，已被用于训练 </span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/"><span>Megatron-Turing NLG 530B</span></a></span><span class="inline-wrap"> 和 </span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://huggingface.co/blog/bloom-megatron-deepspeed"><span>BLOOM</span></a></span><span class="inline-wrap">等大型模型。</span></div></div><div id="gX39EZvFEeDzqyjUvaWUE6" class="wolai-block wolai-text"><div><span class="inline-wrap">DeepSpeed<span class="jill"></span>的创新体现在三个方面：</span></div></div><ul class="wolai-block"><li id="wzheUbiXi1JJGzqJfgNBG2"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">训练</span></li><li id="cyZQYfoMtGA3FUpKsxGFbe"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">推理</span></li><li id="iWZRkZ3JJ3DuWdAQe3su3r"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">压缩</span></li></ul><div id="erWQyx8nu519LRr8H1vVni" class="wolai-block wolai-text"><div><span class="inline-wrap">DeepSpeed<span class="jill"></span>具备以下优势：</span></div></div><ul class="wolai-block"><li id="hAF6TJFa8m9uUQJHmSZT1H"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">训练/推理具有数十亿或数万亿个参数的密集或稀疏模型</span></li><li id="gZWekGbo7d9Ku4aFKhCQpJ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">实现出色的系统吞吐量并有效扩展到数千个 GPU</span></li><li id="j5qscKQYLkygyxDa6EqwN6"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">在资源受限的 GPU 系统上训练/推理</span></li><li id="iSFiAsNapP16ucFBzszDH1"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">为推理实现前所未有的低延迟和高吞吐量</span></li><li id="gge4hyUYfGvqVqJNPyGqS5"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">以低成本实现极致压缩，实现无与伦比的推理延迟和模型尺寸缩减</span></li></ul><div id="5EzbfsUMgDpCmbCLN4miyJ" class="wolai-block wolai-text"><div><span class="yellow inline-wrap"><b>3、FairScale</b></span></div></div><div id="demkKYaYHnrMGcXJVENPs9" class="wolai-block wolai-text"><div><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://github.com/facebookresearch/fairscale"><span>FairScale</span></a></span><span class="inline-wrap">（由 Facebook 研究）是一个用于高性能和大规模训练的 PyTorch 扩展库。 FairScale 的愿景如下：</span></div></div><ul class="wolai-block"><li id="m27qpYwSSfnTD596obTB8U"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">可用性：用户应该能够以最小的认知代价理解和使用 FairScale API。</span></li><li id="kR24XTxPcdxaGhB5crJGro"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">模块化：用户应该能够将多个 FairScale API 无缝组合为训练循环的一部分。</span></li><li id="4SfCT3hfMeK4NWgq8jbrB6"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">性能：airScale API 在扩展和效率方面提供了最佳性能。</span></li></ul><div id="g2m88LXkhkuJVU9hVtW5x1" class="wolai-block wolai-text"><div><span class="inline-wrap">FairScale 支持<span class="jill"></span>Fully Sharded Data Parallel (FSDP)，这是扩展大型神经网络训练的推荐方式。</span></div></div><div id="fR5wJFFiKhpoouNwNTandw" class="wolai-block"><figure class="wolai-center" style="width: 100%"><img src="media/image_4.png" style="width: 100%"/></figure></div><div id="u6TZkXjggDNHARGwxgLx6a" class="wolai-block wolai-text"><div><span class="yellow inline-wrap"><b>4、ParallelFormers</b></span></div></div><div id="qLPREDepm3K7KEZH3X4QqX" class="wolai-block wolai-text"><div><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://github.com/tunib-ai/parallelformers"><span>Parallelformers</span></a></span><span class="inline-wrap"> 是一个基于 Megatron-LM 的库。 它与 Huggingface 库很好地集成在一起。 Huggingface 库中的模型可以用一行代码并行化。 目前它只支持推理。</span></div></div><code-block id="fLpE4CPSHihFKv78GSvUgd" class="wolai-block"><div class="wolai-pre"><div data-lang="Python" class="marker"></div><pre style="white-space: pre-wrap; word-break: break-all"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer
<span class="token keyword">from</span> parallelformers <span class="token keyword">import</span> parallelize
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"EleutherAI/gpt-neo-2.7B"</span><span class="token punctuation">)</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"EleutherAI/gpt-neo-2.7B"</span><span class="token punctuation">)</span>

parallelize<span class="token punctuation">(</span>model<span class="token punctuation">,</span> num_gpus<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token string">'detail'</span><span class="token punctuation">)</span></pre></div></code-block><div id="pH74Xg9sNdJzBVGxtQaZ2R" class="wolai-block wolai-text"><div><span class="yellow inline-wrap"><b>5、ColossalAI</b></span></div></div><div id="oeVm4c47rw2naoAVwJ399h" class="wolai-block wolai-text"><div><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://github.com/hpcaitech/ColossalAI"><span>Colossal-AI</span></a></span><span class="inline-wrap">提供了一组并行组件，可以用来实现定制化的分布式/并行训练，包含以下并行化策略和增强功能：</span></div></div><ul class="wolai-block"><li id="icZYP6UnuhYifDoqYgQL99"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>Data Parallelism</b></span></li><li id="8URU1oQ7prz24rLCJH3b2L"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>Pipeline Parallelism</b></span></li><li id="2ndN3qP3xHTN7qZzyDLr8V"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">1D，2D，2.5D，3D Tensor Parallelism</span></li><li id="cNYrCB1DmexPydNDuwBv12"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2105.13120"><span><b>Sequence Parallelism</b></span></a></span></li><li id="85V3tT8ZaMXcNhbXibJETm"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1910.02054"><span><b>Zero Redundancy Optimizer (ZeRO)</b></span></a></span></li><li id="uqWX18EzKDcfss3Ea2Xr5A"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>Heterogeneous Memory Management (</b></span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2108.05818"><span><b>PatrickStar</b></span></a></span><span class="inline-wrap">)</span></li><li id="omXoG5jTniZJJwo6msaRXx"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap"><b>For Inference</b></span><span class="inline-wrap">**</span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://github.com/hpcaitech/EnergonAI"><span>Energon-AI</span></a></span><span class="inline-wrap">**</span></li></ul><div id="7ytng2r4eYbhLxJveswdJA" class="wolai-block wolai-text"><div><span class="yellow inline-wrap"><b>6、Alpa</b></span></div></div><div id="6gWWP8eqYgxU4m5nuyiemd" class="wolai-block wolai-text"><div><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://github.com/alpa-projects/alpa"><span>Alpa</span></a></span><span class="inline-wrap">是一个用于训练和服务大规模神经网络的系统，具备如下特点：</span></div></div><ul class="wolai-block"><li id="cBX98f7Hkns1wgcog5K7me"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">自动并行化：Alpa<span class="jill"></span>基于数据、运算符和管道并行机制自动化地实现单设备代码在分布式集群并行化。</span></li><li id="KeC47ywfBngp2p9zdJe8W"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">完美的表现：Alpa 在分布式集群上实现了数十亿参数的训练模型的线性缩放。</span></li><li id="aRfCXvtFp1LZiVhrikBbsX"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">与机器学习生态系统紧密集成：Alpa<span class="jill"></span>由开源、高性能和生产就绪的库（如 Jax、XLA 和 Ray）提供支持。</span></li></ul><div id="9RFsitEEEt5Q69hmkZKJbc" class="wolai-block wolai-text"><div><span class="yellow inline-wrap"><b>7、Hivemind</b></span></div></div><div id="qv9pRc2qWzC93CqtUy7jx3" class="wolai-block wolai-text"><div><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://github.com/learning-at-home/hivemind"><span>Hivemind</span></a></span><span class="inline-wrap">是一个在互联网上使用 Pytorch 进行去中心化深度学习的库。 它主要服务场景是在来自不同大学、公司和志愿者的数百台计算机上训练一个大型模型。</span></div></div><div id="9rLmKy9Gi8qMDKfJWpXY6x" class="wolai-block wolai-text"><div><span class="inline-wrap">其主要特点是：</span></div></div><ul class="wolai-block"><li id="v3EEyJwgh5ZnTw98JR3mbt"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">没有主节点的分布式训练：分布式哈希表允许连接分散网络中的计算机。</span></li><li id="xipUuBT6tBP1EjtgzAaf8C"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">容错反向传播：即使某些节点没有响应或响应时间过长，前向和后向传递也会成功。</span></li><li id="iuqJQW1YHXhsDr2zZR1JKd"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">分散的参数平均：迭代地聚合来自多个工作人员的更新，而无需在整个网络中同步（</span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2103.03239"><span>论文</span></a></span><span class="inline-wrap">）。</span></li><li id="eCWu5YSXAon4Q1X2NyLz4S"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">训练任意大小的神经网络：它们的部分层通过分散的专家混合（</span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2002.04013"><span>论文</span></a></span><span class="inline-wrap">）分布在参与者之间。</span></li></ul><div id="pRa68toEAhXwpiRuyrqHRt" class="wolai-block wolai-text"><div><span class="yellow inline-wrap"><b>8、OneFlow</b></span></div></div><div id="wv6dbH639VrHArTjRXSwSw" class="wolai-block wolai-text"><div><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://github.com/Oneflow-Inc/oneflow"><span>OneFlow</span></a></span><span class="inline-wrap"> 是一个深度学习框架，旨在实现用户友好、可扩展和高效。 使用 OneFlow，很容易：</span></div></div><ul class="wolai-block"><li id="2Xqs8FKuuBRimfozYZL2zJ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">使用类似 PyTorch 的 API 编写模型</span></li><li id="57cES1uWnKXt8XpvF8TBuZ"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">使用 Global View API 将模型缩放到 n 维并行/分布式执行</span></li><li id="8VmVEC2k1LoRKZTehfxt8W"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">使用静态图编译器加速/部署模型。</span></li></ul><div id="qid6XahANymSWnD9d6hHws" class="wolai-block wolai-text"><div><span class="yellow inline-wrap"><b>9、Mesh-Tensorflow</b></span></div></div><div id="rd1cQ7HD81DQpPXKebRZw2" class="wolai-block wolai-text"><div><span class="inline-wrap">根据 github 页面：</span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://github.com/tensorflow/mesh"><span>Mesh TensorFlow (mtf)</span></a></span><span class="inline-wrap"> 是一种用于分布式深度学习的语言，能够指定广泛的分布式张量计算类别。 这里的“Mesh”是指处理器或计算设备的互连网络。</span></div></div><h3 id="ZXNoWMxA9ZYMHuaS8REAq" class="wolai-block"><span class="inline-wrap">2. 实践篇</span></h3><h4 id="5ap9GpRuVZb1ZM4xTC1tJS" class="wolai-block"><span class="inline-wrap">2.1 假如有超多的<span class="jill"></span>8<span class="jill"></span>卡<span class="jill"></span>A100<span class="jill"></span>节点（DGX A100），如何应用<span class="jill"></span>3D<span class="jill"></span>并行策略？</span></h4><div id="2mtWFBYq3kvff2sx2PAqdG" class="wolai-block wolai-text"><div><span class="inline-wrap">参考<span class="jill"></span>Megatron-Turing NLG 530B</span></div></div><ul class="wolai-block"><li id="8BpjghLmWhBEAC84J6Q1oH"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">首先，张量并行。3<span class="jill"></span>种并行方式里，张量并行（TP）对于<span class="jill"></span>GPU<span class="jill"></span>之间的通信要求最高，而节点内有<span class="jill"></span>NVLINK<span class="jill"></span>通信速度可以达到<span class="jill"></span>600GB/s。</span></li><li id="d8Ab2pxCBPhsrqJRs65AGy"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">其次，流水线并行，每个节点负责一部分层，每<span class="jill"></span>35<span class="jill"></span>个节点组成一路完整的流水线，也就是一个完整的模型副本，这里一个模型副本需<span class="jill"></span>280<span class="jill"></span>卡。</span></li><li id="xgB2wx5PFQP2gzfnZS99Ng"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">最后，数据并行，官方也做了<span class="jill"></span>8<span class="jill"></span>路，10<span class="jill"></span>路，12<span class="jill"></span>路的并行实验，分别使用<span class="jill"></span>280<span class="jill"></span>个节点，350<span class="jill"></span>个节点和<span class="jill"></span>420<span class="jill"></span>个节点。</span></li></ul><div id="nzKkDcEDn4Y4q6LqM9zWYk" class="wolai-block wolai-text"><div><span class="inline-wrap">集群规模越大，单个<span class="jill"></span>GPU<span class="jill"></span>利用率越低。</span></div></div><h4 id="7ercfCwP9wTvkoDHd9u4S3" class="wolai-block"><span class="inline-wrap">2.2 如果想构这样一个大规模并行训练系统，训练框架如何选？</span></h4><div id="gWEb1vcHTfz3aVgMAqC3iV" class="wolai-block wolai-text"><div><span class="inline-wrap">可以参考<span class="jill"></span>Megatron-Turing NLG 530B，NVIDIA </span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://github.com/NVIDIA/Megatron-LM"><span>Megatron-LM</span></a></span><span class="inline-wrap"> + Microsoft </span><span class="inline-wrap"><a href="https://link.zhihu.com/?target=https://github.com/microsoft/DeepSpeed"><span>DeepSpeed</span></a></span></div></div><div id="tKZjsPdU5pk96eGSabEwcf" class="wolai-block wolai-text"><div><span class="inline-wrap">BLOOM</span><span class="inline-wrap"><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/625958641#ref_5"><span>[5]</span></a></span><span class="inline-wrap">则是<span class="jill"></span>PP+DP<span class="jill"></span>用<span class="jill"></span>DeepSpeed，TP<span class="jill"></span>用<span class="jill"></span>Megatron-LM</span></div></div><div id="ry9TkmGkVUa2G5pH5Nx9uM" class="wolai-block wolai-text"><div><span class="inline-wrap">当然还有一些其他的训练框架，在超大规模下或许也能<span class="jill"></span>work。</span></div></div><h4 id="EGpGwwNn6vVQXir3KKbhw" class="wolai-block"><span class="inline-wrap">2.3 训练框架如何选？</span></h4><h3 id="bUm3A16nXm39zr7q9k4Sqg" class="wolai-block"><span class="inline-wrap">3. 并行化策略选择篇</span></h3><h4 id="6ZkZz2m1qUDq3g9kE4SGXh" class="wolai-block"><span class="inline-wrap">3.1 单机单卡场景</span></h4><div id="bQruK4KRKNMxdgBehedfS6" class="wolai-block wolai-text"><div><span class="inline-wrap">当你的模型可以在单张 GPU 卡进行训练时，正常使用。</span></div></div><div id="p4RMSdLD7N3f3fKE6V3boQ" class="wolai-block wolai-text"><div><span class="inline-wrap">当你的模型不能在单张 GPU 卡进行训练时，</span></div></div><ul class="wolai-block"><li id="dQi5bsKpccau9id8rBPtN2"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">ZeRO + Offload CPU 和 NVMe（可选的）。</span></li><li id="41m3Kk1adRDwtCit8i4MsP"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">启用以</span><span class="red inline-wrap"><b>内存为中心的平铺</b></span><span class="inline-wrap"> 。</span></li></ul><div id="x2a6NEmPhBcTHvyic6zSWF" class="wolai-block wolai-text"><div><span class="inline-wrap">如果最大层无法放置在单张<span class="jill"></span>GPU，则使用 ZeRO - 启用以</span><span class="red inline-wrap"><b>内存为中心的平铺</b></span><span class="inline-wrap"> (MCT)。 它允许您通过自动分割层并按顺序执行来运行任意大的层。 MCT 减少了 GPU 上实时参数的数量，但不影响激活内存。</span></div></div><h4 id="2AZ3ZDY5VUexUTegt49Cwb" class="wolai-block"><span class="inline-wrap">3.2 单机多卡场景</span></h4><div id="hBumJT8Zvme9PCAx8rTooS" class="wolai-block wolai-text"><div><span class="inline-wrap">当你的模型可以在单张 GPU 卡进行训练时，可以选择 DDP 或 ZeRO：</span></div></div><ul class="wolai-block"><li id="t7EzQp4sPnMaLFjQfQXMkw"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">DDP：分布式 DP。</span></li><li id="9Z7TJ63BJ8RxABuqjGXVcd"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">ZeRO：可能会更快，也可能不会更快，具体取决于所使用的情况和配置。</span></li></ul><div id="qSeNssywUyMP6H5g3fTGsR" class="wolai-block wolai-text"><div><span class="inline-wrap">当你的模型不能在单张 GPU 卡进行训练时，可以选择 PP、ZeRO、TP：</span></div></div><ul class="wolai-block"><li id="tZNdMFeFqYb8ZpGL6BsFPd"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">PP</span></li><li id="nidP6mAnu1ZKgmcUu3wM7a"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">ZeRO</span></li><li id="63br5heFo8BLdpf1gURton"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">TP</span></li></ul><div id="n1unobMDMMmL1bCW66m33M" class="wolai-block wolai-text"><div><span class="inline-wrap">如果使用 NVLINK 或 NVSwitch 进行节点内通信，这三者应该基本处于同等水平。</span></div></div><div id="m8SrXary5hLDLZmLBB3WWW" class="wolai-block wolai-text"><div><span class="inline-wrap">如果没有这些， PP 将比 TP 或 ZeRO 更快。 TP 的大小也可能产生影响，最好在您特定设置上进行试验以找到最优的方式。</span></div></div><div id="3d7iUBE4DomRZ6kRN4TSvV" class="wolai-block wolai-text"><div><span class="inline-wrap">注意： TP 几乎总是在单个节点内进行使用。 即：TP 大小 &lt;= 每个节点的 GPU 数。</span></div></div><h4 id="k7M5GgyHT4SnCp4kJYBP62" class="wolai-block"><span class="inline-wrap">3.3 多机多卡场景</span></h4><div id="5W9bf9XQhjReMtT3fFqd5H" class="wolai-block wolai-text"><div><span class="inline-wrap">当服务器节点间网络通信速度较快时，可以选择 ZeRO、PP+TP+DP：</span></div></div><ul class="wolai-block"><li id="aS5N5ujkdgnwqxeXKX2iTp"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">ZeRO - 因为它几乎不需要对模型进行任何修改。</span></li><li id="fb8yhvy8Kco62JLLBGN43V"><div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path></svg></div><span class="inline-wrap">PP+TP+DP - 通信较少，但需要对模型进行大量更改。</span></li></ul><div id="2s1JYEXSXg1GY9W8DMDjmk" class="wolai-block wolai-text"><div><span class="inline-wrap">当您服务器节点间网络通信速度较慢，并且 GPU 内存仍然不足时，可以选择 DP+PP+TP+ZeRO-1。</span></div></div><div id="vV13hjZHVjL7meXMtiZRwn" class="wolai-block wolai-text"><div><span class="inline-wrap">这里采用 PP 与 ZeRO-1 进行混合并行，</span><span class="inline-wrap"><b>那么 PP 能与 DeepSpeed ZeRO 2/3<span class="jill"></span>一起训练吗</b></span><span class="inline-wrap">？</span></div></div><div id="rmgrPFnHjGinxVSZtrENBL" class="wolai-block wolai-text"><div><span class="inline-wrap">答：PP + ZeRO 2/3 不推荐一起训练。 PP 需要累积梯度（accumulate gradients），但 ZeRO2 需要对梯度进行分块（chunk）。 即使能够实现，也没有真正的性能提升。</span></div></div><div id="efrDpAUMrfSEwjwEywGSf5" class="wolai-block wolai-text"><div><span class="inline-wrap">将两者结合使用来提高效率并不容易，PP + ZeRO 2 实际上比 ZeRO2（无 PP）更慢且内存效率低。如果用户内存不足，用户可以使用 ZeRO3 代替 ZeRO2 + PP。而正因为如此，在 DeepSpeed 中， PP + ZeRO 2/3 之间不兼容。但可以将 PP 与 ZeRO 1 进行组合使用。</span></div></div><div id="eCKmPv1ygfEWvJRtKF8ANo" class="wolai-block wolai-text"><div><span class="inline-wrap">这里多说一点：即使该方法效率不高，但是 ColossalAI 为了支持更多的并行训练方法。ColossalAI 还是提供了 ZeRO 3 + PP + TP 一起组合的方案。</span></div></div><div id="5ocMj9syeLvPHjmAzLcdMM" class="wolai-block wolai-text"><div><span class="inline-wrap"></span><br/></div></div></article><footer></footer></body></html></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/wdn_icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Dongnian</div><div class="author-info__description">A salty fish swimming in the sea of deep learning!</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/wdndev"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wdndev" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:dongnian.wang@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #24292e;"></i></a><a class="social-icon" href="https://blog.csdn.net/wdnshadow" target="_blank" title="CSDN"><i class="fas fa-rss" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to My Personal Blog! <br /> If Not, Please Visit <a target="_blank" rel="noopener" href="https://wdndev.gitee.io/"> <font color=#00BFFF>Gitee Mirror</font></a>.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#2jxMTmbiEQtHJ3ZNP6NMqH"><span class="toc-text">1. 理论篇</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#nyWsZgxwB9nxSrYEYG1QoV"><span class="toc-text">1.1 训练 大语言模型 存在问题？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#u2XWVo5oGqBzn3ZzqJnYKk"><span class="toc-text">1.2 什么是 点对点通信？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gNoJreub76M4QUAGvDviw"><span class="toc-text">1.3 什么是 集体通信？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#o31hwvYZfq9Hro2zHCmBmn"><span class="toc-text">1.4 什么是 数据并行？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nXkZPFtizzjiUKAwjXnxVQ"><span class="toc-text">1.5 数据并行 如何 提升效率？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pY5uHA9kRiScY4XuUuu6NB"><span class="toc-text">1.6 什么是 流水线并行？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hLk3fNv1t5TLixGM1D6V3F"><span class="toc-text">1.7 什么是 张量并行 (intra-layer)？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#rJKDatPkF3kwDRmUS4AMSK"><span class="toc-text">1.8 数据并行 vs 张量并行 vs 流水线并行?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9R1HwBvEDf3vozmyTekV9U"><span class="toc-text">1.9 什么是 3D并行？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nGFQ9MXok8ri2mZYHQReBv"><span class="toc-text">1.10 想要训练1个LLM，如果只想用1张显卡，那么对显卡的要求是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#aWeW6NdJ3vBScVZbZMQNAC"><span class="toc-text">1.11 如果有N张显存足够大的显卡，怎么加速训练？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#aTrAFvPPG7pfNPRaXWpXsh"><span class="toc-text">1.12 如果显卡的显存不够装下一个完整的模型呢？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#vUQXgJq7TC28Ay922hntpv"><span class="toc-text">1.13 PP推理时，是一个串行的过程，1个GPU计算，其他空闲，有没有其他方式？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#qBR18JQUs1ytedKbCqc1nS"><span class="toc-text">1.14 3种并行方式可以叠加吗？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#uETdis13CNwQRSTpMgsQYX"><span class="toc-text">1.15 Colossal-AI 有1D&#x2F;2D&#x2F;2.5D&#x2F;3D，是什么情况？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bvsk5PqfaXS1o1XPTan8J6"><span class="toc-text">1.16 除了3D并行有没有其他方式大规模训练？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#uhikBt7ZHtjcnwDk4yvYjV"><span class="toc-text">1.17 有了ZeRO系列，为什么还需要3D并行？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#q6vKxWMtjrPC8gL79WJTsL"><span class="toc-text">1.18 平民适不适合玩3D并行？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#du7SbCJHxpeu9Sr32hoKZX"><span class="toc-text">1.19 平民适不适合直接上多机多卡的ZeRO3（万兆网）？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5KN98ZqmGMsM2c4MHWUgok"><span class="toc-text">1.20 分布式并行及显存优化技术并行技术有哪一些，都有什么特点？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gmfh41rBojxsabkhGWcwHv"><span class="toc-text">1.21 常见的分布式训练框架哪一些，都有什么特点？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ZXNoWMxA9ZYMHuaS8REAq"><span class="toc-text">2. 实践篇</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5ap9GpRuVZb1ZM4xTC1tJS"><span class="toc-text">2.1 假如有超多的8卡A100节点（DGX A100），如何应用3D并行策略？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7ercfCwP9wTvkoDHd9u4S3"><span class="toc-text">2.2 如果想构这样一个大规模并行训练系统，训练框架如何选？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#EGpGwwNn6vVQXir3KKbhw"><span class="toc-text">2.3 训练框架如何选？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bUm3A16nXm39zr7q9k4Sqg"><span class="toc-text">3. 并行化策略选择篇</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6ZkZz2m1qUDq3g9kE4SGXh"><span class="toc-text">3.1 单机单卡场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2AZ3ZDY5VUexUTegt49Cwb"><span class="toc-text">3.2 单机多卡场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#k7M5GgyHT4SnCp4kJYBP62"><span class="toc-text">3.3 多机多卡场景</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_article/9.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BALLM/" title="检索增强LLM">检索增强LLM</a><time datetime="2024-01-12T16:00:00.000Z" title="Created 2024-01-13 00:00:00">2024-01-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/6.%E6%96%87%E6%9C%AC%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/" title="LLMs公开课 - 6.文本理解和生成大模型">LLMs公开课 - 6.文本理解和生成大模型</a><time datetime="2024-01-09T16:00:00.000Z" title="Created 2024-01-10 00:00:00">2024-01-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/5.%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/" title="LLMs公开课 - 5.高效训练&amp;模型压缩">LLMs公开课 - 5.高效训练&amp;模型压缩</a><time datetime="2024-01-06T16:00:00.000Z" title="Created 2024-01-07 00:00:00">2024-01-07</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DSA/"><span class="card-category-list-name">DSA</span><span class="card-category-list-count">24</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLMs/"><span class="card-category-list-name">LLMs</span><span class="card-category-list-count">16</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/PL/"><span class="card-category-list-name">PL</span><span class="card-category-list-count">7</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/PL/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">6</span></a></li></ul></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/DSA/" style="font-size: 1.42em; color: rgb(91, 145, 17)">DSA</a><a href="/tags/RL/" style="font-size: 1.28em; color: rgb(176, 79, 19)">RL</a><a href="/tags/Transformer/" style="font-size: 1.45em; color: rgb(127, 170, 70)">Transformer</a><a href="/tags/LLMs/" style="font-size: 1.32em; color: rgb(112, 148, 55)">LLMs</a><a href="/tags/PaperReading/" style="font-size: 1.38em; color: rgb(110, 146, 60)">PaperReading</a><a href="/tags/DeepLearning/" style="font-size: 1.25em; color: rgb(90, 48, 1)">DeepLearning</a><a href="/tags/CV/" style="font-size: 1.15em; color: rgb(82, 200, 174)">CV</a><a href="/tags/GPT/" style="font-size: 1.18em; color: rgb(7, 16, 91)">GPT</a><a href="/tags/PL/" style="font-size: 1.22em; color: rgb(17, 30, 26)">PL</a><a href="/tags/leetcode/" style="font-size: 1.35em; color: rgb(106, 126, 145)">leetcode</a><a href="/tags/algo/" style="font-size: 1.15em; color: rgb(181, 144, 151)">algo</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">January 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">December 2023</span><span class="card-archive-list-count">14</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">November 2023</span><span class="card-archive-list-count">26</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">October 2023</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/09/"><span class="card-archive-list-date">September 2023</span><span class="card-archive-list-count">4</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">87</div></div><div class="webinfo-item"><div class="item-name">Run time :</div><div class="item-count" id="runtimeshow" data-publishDate="2023-05-31T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Total Count :</div><div class="item-count">411.2k</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2024-12-08T03:57:10.055Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By Dongnian</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'oe7vzWxH80qwJJjWslYTCViT-gzGzoHsz',
      appKey: 'k89nSbK0BTbmzmpQottRHvNI',
      avatar: 'monsterid',
      serverURLs: 'https://oe7vzwxh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>